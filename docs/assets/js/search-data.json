[{"doc":"This page has not yet sprouted","title":"This page has not yet sprouted","hpath":"403","content":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/403.html","relUrl":"notes/403.html"},{"doc":"Root","title":"Root","hpath":"root","content":"\n# Digital Garden\n\nThis Wiki of Personal Information is organized according to domains and their\nsub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git, would be `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner` etc.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the aha moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nThe ability I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course, this is all\nvague sounding, and really depends on the domain, along with the ends I am\ntrying to reach.\n","url":"https://tycholiz.github.io/Digital-Garden","relUrl":"/"},{"doc":"Yarn","title":"Yarn","hpath":"yarn","content":"\n### yarn.lock\n- The purpose of a lock file is to lock down the versions of the dependencies specified in a package.json file\n\t- This means that in a `yarn.lock` file, there is an identifier (ie. exact version specified) for every dependency and sub-dependency that is used for a project\n\t\t- sub-dependencies are the dependencies of a dependency\n- The equivalent of `yarn.lock` for npm is `package-lock.json`. If using both npm and yarn, we need both of them, and they need to remain in sync (use yarn's import directive to accomplish this)\n- if we didn't have a `yarn.lock`, then if a co-worker cloned our repo and ran `yarn install`, they may get different versions of a dependency, since `package.json` can specify version ranges. \n\t- Instead, since `yarn.lock` is checked into version control, when the co-worker clones the repo and runs `yarn install`, `yarn.lock` will be checked and the version specified will be installed.\n- critical to have if working on a team or if working alone with a CI server.\n- `yarn.lock` gets updated any time a dependency is added, removed or modified\n\t- If we want to ensure `yarn.lock` is not updated, use `--frozen-lockfile`\n\t\t- The difference between `--frozen-lockfile` and `--pure-lockfile` is that the former will fail if an update is needed \n- In a perfect world, yarn.lock is unnecessary, because the point of semver is that unless the major version changes, the upgraded package will still work. In other words, if the version in package.json is listed as ^16.0.1, then `yarn install` is free to go to the latest minor version, which doesn't matter since semver defines that as fully backwards compatible.\n\t- however, in the real world not everyone follows semver best practices, and sometimes it is just mistakes which ruin backward compatibility \n\n#### Upgrading packages\n- if we have a dependency version in `package.json` specified at `^3.9.1`, this means that any version between 3.9.1 and 4.0.0 will be acceptable. Of course, since we have a lockfile, upgrades will not automatically happen.\n- `yarn upgrade` allows us to upgrade all dependencies in `package.json`. If we use the `^` specifier, then the latest version within the range will be added. This will be reflected in `yarn.lock`\n- we can ignore the version range by passing the `--latest` flag.\n\t- This modifies both `yarn.lock` and `package.json`\n- We can see all packages that can be upgraded with `yarn upgrade-interactive --latest`\n\n### Link\n- `yarn link` allows us to create symlinks to local projects, from within the project (with package.json) we are currently in.\n- ex. if we have a `rn-client` project and a `components` project, and we want to use `components` within `rn-client`, we can do the following:\n\t1. go to `components` project and run `yarn link`\n\t2. go to `rn-client` project and run `yarn link components` (name field of package.json)\n\t\t- this creates a symlink at `rn-client/node_modules/components`\n\t4. from `rn-client` project, `import components from 'components'` \n- It is meant for development-only purposes\n- spec: think of `yarn link` as exporting the package, and `yarn link <package>` as importing it.","url":"https://tycholiz.github.io/Digital-Garden/notes/3577d6e8-1437-4f7b-9da2-c353ed232038.html","relUrl":"notes/3577d6e8-1437-4f7b-9da2-c353ed232038.html"},{"doc":"Workspaces","title":"Workspaces","hpath":"yarn.workspaces","content":"\n## Workspaces\n- allows us to install dependencies from multiple package.json files at once.\n- Yarn can also create symlinks between workspaces that depend on each other, ensuring directories are correct and consistent\n- When there are repeated dependencies in the `node_modules` of sub-modules, yarn workspaces will pull up (hoist) the common dependencies to live in the root-level `node_modules`.\n\t- This is why we don't need to run `lerna bootstrap --hoist` when using yarn + lerna — yarn already hoists.\n- if package1 depends on package2, then a symlink to package2 will also be created in the root `node_modules`. This allows package2 to `require` in the package, and take advantage of node's recursive resolver to find the package.\n- workspaces uses a single yarn.lock file at the root.\n- any module (either your own code or the code of a dependency) that contains native code (ie. Switft, Java etc) must not be hoisted.\n\t- ex. ","url":"https://tycholiz.github.io/Digital-Garden/notes/e9d510dd-6f9a-461a-9e9d-cd866c5213f3.html","relUrl":"notes/e9d510dd-6f9a-461a-9e9d-cd866c5213f3.html"},{"doc":"package.json","title":"package.json","hpath":"yarn.package-json","content":"\n# package.json\n## Scripts\n- every script we define has `pre` and `post` version to it\n    - ex. if we define a `install` script, then there is a `preinstall` that is implicitly defined\n\n## Dependencies\n### Peer Dependencies\n- peer dependencies are not automatically installed like dependencies and devDependencies are.\n\t- Instead, when we list a package as a peerDependency, we expect it to be provided from the host (the host would be our application)\n- peer dependencies are most likely to be used only when creating npm packages.\n- ex. `react-dom` lists `react` as a peerDependency, since `react-dom` is useless unless we have `react` installed.\n\t- if `react-dom` had listed `react` as a dependency, then effectively we would have 2 different versions of react in our project.  \n\t- by listing `react` as a peerDependency, `react-dom` is basically saying \"hey npm, I don't have react as a dependency, but I do need react to work. If someone tries to install me, but they don't have react installed, spit a warning at them, cause I ain't gonna work properly\"\n\n ","url":"https://tycholiz.github.io/Digital-Garden/notes/f56cf30b-74ef-4fd8-a793-0e3528fa4dc0.html","relUrl":"notes/f56cf30b-74ef-4fd8-a793-0e3528fa4dc0.html"},{"doc":"Webpack","title":"Webpack","hpath":"webpack","content":"\nNode.js is built with a modular-first mentality. However, the web has been slow to adopt this philosophy of having clearly defined and swappable modules. Webpack aims to solve this by allowing us to implement modules in our code\n- unlike Node, webpack can also treat css/sass files, and image URLs as modules.\n\nWebpack is eager, in that javascript modules are figured out ahead of time, and are delivered in one big file. This negates the need to make network requests to get more of the javascript. The consequence of this fact is:\n- we cannot import dynamically in a data-driven way (since the importing will already have been done by the time the data-driven code is run.)\n- we cannot import in a function, since a function occurs after webpack does its job.\n- Module objects are frozen. There is no way to hack a new feature into a module object, polyfill style.\n- import errors are detected at compile-time\n- There is no hook allowing a module to run some code before its dependencies load. This means that modules have no control over how their dependencies are loaded.\n\n# Concepts\n- each webpack config has an entrypoint from which the dependency graph starts to be made. Unless specified, this defaults to `./src/index.js`\n- each webpack config also has an output, which specifies where the bundle will be emitted. Unless specified, this defaults to `./dist/main.js` for the main file, and the rest of the files also end up in `dist/`\n\n## Loaders\n- in Webpack 4 and below, webpack only understands js and json files. For this reason, we use loaders to allow different files to form a part of the dependency graph.\n- ex. `{ test: /\\.txt$/, use: 'raw-loader' }`\n\t- basically, loaders say \"hey Webpack, when you come across `.txt` files whenever you are trying to import, use the `raw-loader` to transform it before adding it to the bundle.\n- loaders are declared in `module.rules`\n\n## Types of Modules\n- There are more, but the 2 main types of modules we are concerned with are ECMAScript Modules, and Asset Modules\n\n### Asset Modules\n- In webpack 5, Asset Modules allow us to use asset files (ie. fonts, icons), without configuring additional loaders.\n\t- before Webpack 5, we had to use `raw-loader`, `url-loader`, `file-loader` etc., since out of the box, those versions of webpack only understood js and json files.\n\n### Module Resolution\nA resolver is a library which helps in locating a module by its absolute path\n- Effectively, webpack can be in charge of locating the files, and creating an alias for that path, so that that alias can be used throughout the codebase, giving us the benefit of only having to change that path once, in the event that it needs to change.\n\n* * *\n### Resolve\n#### resolve.modules\n- in webpack.config, we can tell webpack where it should go to look for certain modules.\n\t- ex. `node_modules` will be a default location, but we can specify paths that have higher priority so they are checked first before `node_modules`\n- if relative paths are used, then webpack will scan for that file recursively, similar to how node resolves `node_modules`\n\n#### resolve.alias\n- allows us to import more easily in our project.\n- we basically tell webpack, \"hey, any time you see me `import _ from 'ui-components`, just look for that module at whatever path I specify.\"\n\nwebpack continuously builds our app as we go.\n\n* * *\n\n### Contexts (require.context)\n- allows us to create our own context by passing a pattern that will match a filename.\n- One of the main functions of webpack's compiler is to recursively parse all modules to form a dependency graph.\n- A context is simply a base directory that resolves paths to modules (ie. it is a directory and its dependencies)\n\t- ex. in a React app, we can choose to declare the `components/` directory a context. This will allow us to search for files within that directory \"tree\"\n- Realistically, the root directory (current working directory) is a context for webpack to actually resolve the path to `index.js`\n- The intention is to tell webpack at compile time to transform that expression into a dynamic list of all the possible matching module requests that it can resolve, in turn adding them as build dependencies and allowing you to require them at runtime.\n- In short, you would use require.context in the exact same situation when in Node.js at runtime you would use globs to dynamically build a list of module paths to require. The return value is a callable object that behaves like require, whose keys contain the necessary module request data that can be passed to it as an argument to require the module.\n\n# Alternatives\n- Snowpack: very fast\n- Rollup\n- Parcel: simple config that works out of the box","url":"https://tycholiz.github.io/Digital-Garden/notes/6733cf80-f2b1-41eb-8ddf-e2fa947f6b43.html","relUrl":"notes/6733cf80-f2b1-41eb-8ddf-e2fa947f6b43.html"},{"doc":"Watermelon","title":"Watermelon","hpath":"watermelon","content":"\n# Philosophy\ndeclaratively define the connection between the component, and the data you want it to display. When the data changes, every component that is connected will automatically update.\nwatermelon is fast in part because it uses a declarative API. The declarative API means that all of the expensive computation is being done natively (Java or Swift). Since Javascript is quite slow compared to these 2 languages, this allows our computations to be done more efficiently.\n\n# Tables\n### Columns\nColumns have one of three types: string, number, or boolean\nFields of those types will default to '', 0, or false respectively\n\nTo allow fields to be null, mark the column as isOptional: true\n\n### withObservables\n- This is the principal way that we connect WatermelonDB to our component\n- let's us enhance a component by turning a non-reactive component to become reactive, meaning that UI will update in accordance with localdb changes\n- we make our component reactive by feeding it an observable for the data we want to display\n\n```js\nwithObservables(['post'], ({ post }) => ({\n  post: post.observe(), // inject enhanced props into the component\n  author: post.author.observeCount()\n}))\n```\n- above:\n\t- `({ post })` are the input props for the component\n\t- The first argument: `['post']` is a list of props that trigger observation restart. So if a different post is passed, that new post will be observed\n\t- Rule of thumb: If you want to use a prop in the second arg function, pass its name in the first arg array\n\t- This is also the place that we should make relations\n\t\t- the relation is enabled by the `@children` decorator on the parent model\n\n# Actions\nMutation (Create, Update) queries can be made from anywhere in the app, but the preferred way is to execute them through actions\n- An action is a function that can modify the database\n\n# Migrations\nEach migration must migrate to a version one above the previous migration\n- of course, each migration simply builds on the previous ones, meaning that when we want to make changes, we need to add the new changes as an item in the `migrations` array (to the front) and mark it with the next integer `toVersion`.\n\nSteps to making schema changes:\n1. make the change in migrations.js\n2. wait for the error in the simulator:\n\t- `Migrations can't be newer than schema. Schema is version 1 and migrations cover range from 1 to 2`\n3. if the error is there, make the change in schema.js, updating the `schemaVersion` to the latest migration\n\n### Testing migrations work properly\n1. *Migrations test*: Install the previous version of your app, then update to the version you're about to ship, and make sure it still works\n2. *Fresh schema install test*: Remove the app, and then install the new version of the app, and make sure it works\n\n# Q Module\n- This module provides us to make SQL-like clauses to help construct our query\n\t- This is where we can use WHERE, JOIN (on), AND, OR, LIKE etc.\n\t\t- note: remember to escape `Q.like`\n- JOINs are done through `Q.on`\n\n# Observable\n- `.observe()` will return an observable\n- we can hook up observables to components\n- Because WatermelonDB is fully observable, we can create a @lazy function that will observe a database value and give us updated results in real-time (ie. without having to query the database)\n\t- ex. imagine we have a blog site, and blog posts can have a \"popular\" banner if they have at least 10 comments. We can make a function on the model layer that will observe the number of comments and will reactively give us the correct flag for the boolean:\n\t```js\n\tclass Post extends Model {\n\t\t@lazy isPopular = this.comments.observeCount().pipe(\n\t\t\tmap$(comments => comments > 10),\n\t\t    distinctUntilChanged()\n\t\t)\n\t}\n\t```\n\tand then directly connect it to the component:\n\t```js\n\tconst enhance = withObservables(['post'], ({ post }) => ({\n\t\tisPopular: post.isPopular,\n\t}))\n\t```\n\t- since this is reactive, a rise above/fall below the 10 comment threshold will cause the component to re-render.\n\t- Dissecting:\n\t\t- `this.comments.observeCount()` - take the Observable number of comments\n\t\t- `map$(comments => comments > 10)` - transform this into an Observable of boolean (popular or not)\n\t\t- `distinctUntilChanged()` - this is so that if the comment count changes, but the popularity doesn't (it's still below/above 10), components won't be unnecessarily re-rendered\n\t\t- `@lazy` - also for performance (we only define this Observable once, so we can re-use it for free)\n\n# Sync\nAny backend will work, as long as it complies with the following spec:\n![6d0a0837a90af34681ce452b015d4b19.png](:/860953c1a363424aac94bdff9d490b90)\n- `changes` is an object with a field for each model (table) that has changes. For each model, there are 3 fields: `created`, `updated`, `deleted`.\n\t- When the `changes` object is received from a Pull Request, it is the selection of changes that were made on the server since our last sync, that we need to now update locally.\n\t- When the `changes` object is sent with a Push Request, it is the selection of changes that we've made locally, that have not yet been sent to the remote database.\n\n### Pulling\nWhen Watermelon runs `synchronize()`, `pullChanges` will get run, which will pass along with it information about the last time a pull was made (`lastPulledAt`). `pullChanges` will call an endpoint to the backend, passing along that `lastPulledAt` timestamp, and the server will confer with the backend DB, and send back all of the changes made since the last pull, along with the current timestamp. When the mobile app receives the response, it will then proceed to apply those changes to the local db.\n\n### Pushing\nWe send to the server a `change` object, containing everything that needs to be updated remotely, as well as a timestamp of the last time a pull was made (`lastPulledAt`). When the server receives the request, it will use `lastPulledAt` to check conflicts with the remote db. If there is no conflict, the server will then update the db with the provided changes.\n\n### Sync limitations\nThere are currently limitations of Sync, as outlined in this blog: [How to Build WatermelonDB Sync Backend in Elixir | Fahri NH](:/f615cdc32a5c4be4a768caee30774aa9)\n\n### How does it know when to re-render?\nfor individual records, just listen to changes, and if the record changes, re-render\n\nfor queries, like \"tasks where a=b\", listen to the collection of tasks, and when a record in that collection changes, check if the record matches the query. If it does: if record was on the rendered list, and was deleted — remove from rendered list. if it wasn't on the rendered list, and now matches — add to rendered list.\n\nfor multi-table queries like \"tasks that belong to projects where a =b\", listen to all relevant collections, and if there's a change in any of them, re-query the database. There's ways to make it more efficient, but need to measure if the perf benefit is worth it\n\n# Solutions to:\n- [prop drilling](https://nozbe.github.io/WatermelonDB/Components.html#database-provider)\n\n# UE Resources\n[Logrocket Tutorial](https://blog.logrocket.com/offline-app-react-native-watermelondb/)\n[how sync works](https://fahri.id/posts/how-to-build-watermelondb-sync-backend-in-elixir/)\n[conf](https://www.youtube.com/watch?v=uFvHURTRLxQ)","url":"https://tycholiz.github.io/Digital-Garden/notes/3baf3651-9e5a-46bd-bba1-f0c0caf7385c.html","relUrl":"notes/3baf3651-9e5a-46bd-bba1-f0c0caf7385c.html"},{"doc":"Vscode","title":"Vscode","hpath":"vscode","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/734cd78d-0bc9-426b-803d-1efc84dfffe5.html","relUrl":"notes/734cd78d-0bc9-426b-803d-1efc84dfffe5.html"},{"doc":"Workspaces","title":"Workspaces","hpath":"vscode.workspaces","content":"\n# Workspace\nA collection of one or more folders that are opened in a VS Code window (instance)\n- Think of a workspace as the root of a project that has extra VS Code knowledge and capabilities.\n\nThe concept of a workspace enables VS Code to:\n- Configure settings that only apply to a specific folder or folders but not others.\n- Persist task and debugger launch configurations that are only valid in the context of that workspace.\n- Store and restore UI state associated with that workspace (for example, the files that are opened).\n- Selectively enable or disable extensions only for that workspace.\n\n## Multi-root workspaces\nallow us to pick and include other folder trees to exist within the current workspace, allowing us the benefits that having all files under a workspace gives us.","url":"https://tycholiz.github.io/Digital-Garden/notes/eab3608a-f84e-4437-b875-d64b013116ce.html","relUrl":"notes/eab3608a-f84e-4437-b875-d64b013116ce.html"},{"doc":"Keybindings","title":"Keybindings","hpath":"vscode.keybindings","content":"\nEach rule consists of:\n- a key that describes the pressed keys.\n- a command containing the identifier of the command to execute.\n- an optional when clause containing a boolean expression that will be evaluated depending on the current context.\n\n### Parts of UI\n\"Workbench\" refers to the overall Visual Studio Code UI that encompasses the following UI components:\n- Title Bar\n- Activity Bar\n- Side Bar\n- Panel\n- Editor Group\n- Status Bar","url":"https://tycholiz.github.io/Digital-Garden/notes/929e2b96-e491-4e09-a94c-4922337adf24.html","relUrl":"notes/929e2b96-e491-4e09-a94c-4922337adf24.html"},{"doc":"Debugger","title":"Debugger","hpath":"vscode.debugger","content":"\n## Setup\n1. Create a Javascript Debug Terminal (from command palette)\n2. Run command to start servers (ie. `yarn start`)\n    - command will take longer than normal to run, because vscode is attaching debuggers to all the processes that are about to run.\n3. Set some breakpoints of where we want the code to stop\n4. Manually cause the code to be executed\n    - ex. by visiting a url, clicking a button in UI etc.\n\n## Analysis\n- We can see what each variable evaluates to in the main window\n\n### Debug panel\n- In the debugger left panel, we can see the variables\n\n### Debug Console\n- We can open the Debug Console to get a Debug Console REPL (cmd+shift+y)\n\n\n* * *\nthe debugger will not work for client-side code, since this code is executed in a browser. We would need to use the browser debugging tool to accomplish this.\n- alternatively, we can have a `launch.json` file to allow us to debug\n\n# Chrome Debugger\n- The vscode debugger can connect to Chrome via its Chrome Debugger protocol. Doing this allows us to map files loaded in the browser to the files open in Visual Studio Code.\n    - This enables developers to debug in vscode, by...\n        - setting breakpoints directly in their source code.\n        - setting up variables to watch and see the full call stack when debugging","url":"https://tycholiz.github.io/Digital-Garden/notes/5b41ef0d-040a-4fbf-b2c3-a6a4009d44ef.html","relUrl":"notes/5b41ef0d-040a-4fbf-b2c3-a6a4009d44ef.html"},{"doc":"Actions","title":"Actions","hpath":"vscode.debugger.actions","content":"\n* Debugger Actions\n### Continue\nMove along to the next debugger point\n\n### Step Over\nCall the function without causing our debugging context to be inside the function call\n- Therefore, if we are at a function and \"step over\", we will not enter the function. However, the function itself will of course still be executed.\n\n### Step Into\nGo into the current function and execute each line within 1 by 1\n\n### Step Out\nExit the current function debugger context.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/caa1fd66-c1b1-4776-b80f-7f0765397eda.html","relUrl":"notes/caa1fd66-c1b1-4776-b80f-7f0765397eda.html"},{"doc":"Cmds","title":"Cmds","hpath":"vscode.cmds","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/b26eb34c-d0e3-4499-8342-6b6b911d7aea.html","relUrl":"notes/b26eb34c-d0e3-4499-8342-6b6b911d7aea.html"},{"doc":"Nav","title":"Nav","hpath":"vscode.cmds.nav","content":"\nChange vscode workspace:\n`<C-r>`\n\nToggle Intellisense popup\n`Cmd-k Cmd-i`","url":"https://tycholiz.github.io/Digital-Garden/notes/3d28e6ef-3531-4c2e-b8cf-199a2a5be7db.html","relUrl":"notes/3d28e6ef-3531-4c2e-b8cf-199a2a5be7db.html"},{"doc":"Vm","title":"Vm","hpath":"vm","content":"\n# Virtual Maachines (VM)\nA VM is made up of a Host VM and a Guest VM\n\n### Host VM\n- the server component of a VM, which provides resources to a Guest VM, such as processing power, memory, disk, network I/O etc.\n\n### Guest VM\n- can exist on a single phsyical machine, but is usually distributed across multiple hosts for load balancing.\n- The guest VM is not even aware that it is a guest VM, and therefore is not aware of any other physical resources that are allocated to other guest VMs.\n\n### Hypervisor\n- a piece of software in a computer that will create and run virtual machines\n- the hypervisor intermediates between the host and guest VM, which isolates individual guest VMs. This allows a host VM to support multiple guests running on different operating systems.\n\n* * *\n\n### Swap Space\n- Swap space in Linux is used when the amount of physical memory (RAM) is full\n\n### Memory Page (a.k.a. Virtual Page)\n- a fixed length contiguous block of virtual memory.\n- each page is described by an record in a page table","url":"https://tycholiz.github.io/Digital-Garden/notes/63a922b8-2d60-4ea7-9f47-bcfaa0caef7e.html","relUrl":"notes/63a922b8-2d60-4ea7-9f47-bcfaa0caef7e.html"},{"doc":"Vim","title":"Vim","hpath":"vim","content":"\n# Operators and Motions\n- Operator --> Number --> Motion\n\n## operator (Verb)\n- what we are doing\n- ***ex.*** - `d`, `c`, `y`, `f`, `t`\n    - `f#`/`F#` - (*mn.* find)\n        - inclusively go to first occurrence of `#` in the line\n        - `;` to cycle through, `,` to go backwards\n    - `t#`/`T#` - (*mn.* til)\n        - exclusively go to first occurrence of `#` in the line\n\n## Modifier\n- more specific information about the modifier\n- ***ex.*** - `i` (inside), `a` (around)\n\nCombine with numbers to further specify text objects\n- ***ex.*** - `d2f:` - delete until the second `:` character\n- ***ex.*** - `d2/:<cr>` - delete until the second `:` character\n\n## motion (noun) - what we are operating on\n- ***ex.*** - `$`, `0`, `w`, `e`, `b`\n\n* * *\n\n# strat submodules\nStructure the vim module from the perspective of a user who is trying to accomplish a certain task in vim.\n- Are they trying to substitute all occurrences in a file?\n- are they trying to modify text in some way (surround, uppercase, )\n\n- nav (moving the cursor around)\n    - line\n        - search\n        - find\n    - buffer\n        - search\n    - windows\n    - tree\n        - netrw\n        - MRU\n        - <C-6>\n- modify\n    - surround\n    - substitute\n        - greplace\n    - uppercase\n- feat\n    - marks\n    - macro\n    - shell (execute shell commands)\n- plug\n    - fzf\n    - fugitive\n    - guter (git-gutter)\n- repeating\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5730411b-fefa-49e0-aced-451f5b148299.html","relUrl":"notes/5730411b-fefa-49e0-aced-451f5b148299.html"},{"doc":"Substitution","title":"Substitution","hpath":"vim.substitution","content":"\n# Substitution\nrepeat last substitute\n- `:&`\n    - `:&&` command repeats the last substitution with the same flags.\n        - You can supply the additional range(s) to it (and concatenate as many as you like):\n\nreplace multiple search terms at once\n- `:%s/Kang\\|Kodos/alien/gc`\n\n`:6,10s/<search_string>/<replace_string>/g | 14,18&&`\n\n## within Visually Selected Area\n1. visually select the area you want substitutions to take place in, then `<ESC>`\n2. `:%s/\\%Vfoo/bar/g` to replace all `foo` with `bar`\n\nAlternatively, the vis.vim plugin can be used\n\n### Upper/Lower Case\n- `guw`/`gUw` - make word lowercase/uppercase\n    - `guu`/`gUU` - make line uppercase/lowercase\n- `g~` - swap case\n\n## Flags\n### c\ny/n/a/q substitute (within single file)\n- `:%s/search/replace/gc`\n\n#### Values\n`l` - last (substitute this match, then quit)\n`a` - all (this, and all remaining)\n    - note: `l` and `a` are opposites, since `l` will treat the current match as the final one in our substitution process, whereas `a`\n`<C-e>`/`<C-d>` - scroll","url":"https://tycholiz.github.io/Digital-Garden/notes/19b2537b-3924-4e23-a63c-77b3df0d338c.html","relUrl":"notes/19b2537b-3924-4e23-a63c-77b3df0d338c.html"},{"doc":"Search","title":"Search","hpath":"vim.search","content":"\n#### Partial Word Search\n- When you're typing and you enter a partial word, you can cause Vim to search for a completion by using the `<C-p>` (search for previous marching word) and `<C-n>` (search for next match).\n\n### Where you land\nSearches in Vim put the cursor on the first character of the matched string by default\n- if you search for Debian, it would put the cursor on the D.\n\nWe can offset the cursor placement in a few ways:\n- To land on the last character in the matched string, rather than the beginning, add an /e to your search:\n    - ex. `/Debian/e`\n- To land 2 lines above/below, add `-2`/`+2` to the end\n    - ex. `/Debian/-2`\n- To offset 2 chars from the beginning of the string, add `b+3`\n- To offset 2 chars from the end of the string, add `e-3`\n\n\n## Tips\n- use search with an operator to find the location easier\n    - ex. Yank til `})` \n        - `y/})<Enter>`","url":"https://tycholiz.github.io/Digital-Garden/notes/ef420c39-c187-420e-8fad-43f48bc3537b.html","relUrl":"notes/ef420c39-c187-420e-8fad-43f48bc3537b.html"},{"doc":"Special Chars","title":"Special Chars","hpath":"vim.search.special-chars","content":"\n### `.`\nmatches every single character.\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/cc559c3a-f36e-46dc-a199-a245a45c980f.html","relUrl":"notes/cc559c3a-f36e-46dc-a199-a245a45c980f.html"},{"doc":"Options","title":"Options","hpath":"vim.search.options","content":"\n# Search options\nTo remove these options, prepend the command with `no`(`:set nohls`)\n- `:set ic` - ignore case\n- `:set hls` - highlight search\n- `:set is` - incsearch\n","url":"https://tycholiz.github.io/Digital-Garden/notes/70449235-f293-4e0e-8d5a-695d2fc3a7e4.html","relUrl":"notes/70449235-f293-4e0e-8d5a-695d2fc3a7e4.html"},{"doc":"Regex","title":"Regex","hpath":"vim.regex","content":"\nRegex in Vim largely follows Basic Regex (BRE) (POSIX)\n- see `:help perl-patterns` for differences between Perl Regex and Vim's\n\nTo get Regex to behave more like traditional Regex, use Vim \"very magic\" mode by prepending with `\\v`\n- See `:help /\\v`\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/54dad762-b422-43ad-85ed-4b400cd8a19f.html","relUrl":"notes/54dad762-b422-43ad-85ed-4b400cd8a19f.html"},{"doc":"Cook","title":"Cook","hpath":"vim.regex.cook","content":"\nfind empty lines (only whitespace)\n- `^\\s.*$`\n\nfind lowercase/non-lowercase\n- `\\l`/`\\L`\n\nfind uppercase/non-uppercase\n- `\\l`/`\\L`\n\nfind word boundary\n- `\\<term\\>`\n\nfind both \"dog\" and \"Dog\"\n- `/[Dd]og`\n\n### Greedy vs Non-greedy Quantifiers\nVim allows you to specify how many of something you wish to match by adding quantifiers to the term.\n- \"greedy quantifiers\" are those that match as many of the specified characters as possible.\n    - `*` matches 0 or more\n        - ex. `/abc*` will match abc, abby, and absolutely\n    - `\\+` matches 1 or more\n        - ex. `/abc\\+` will match abc, but not abby or absolutely\n    - `\\{5,}` matches 5 or more characters\n- \"non-greedy quantifiers\" are those that match only the specified number, or as few as possible.\n    - `\\=` matches 0 or 1\n        - ex. `/abc\\=` will match abc, abby, and absolutely.\n    - `\\{0,10}` matches between 0 and 10 of a character\n    - `\\{5}` matches 5 characters\n    - `\\{,5}` matches 5 characters or less\n    - `\\{-}` matches as little as possible","url":"https://tycholiz.github.io/Digital-Garden/notes/200be528-dab8-42ab-b3e7-6e89e32cf5a0.html","relUrl":"notes/200be528-dab8-42ab-b3e7-6e89e32cf5a0.html"},{"doc":"Plug","title":"Plug","hpath":"vim.plug","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/859081c5-87e5-4585-abcf-520f2d874947.html","relUrl":"notes/859081c5-87e5-4585-abcf-520f2d874947.html"},{"doc":"Gutter","title":"Gutter","hpath":"vim.plug.gutter","content":"\n# Vim-gitgutter\n- `[c`/`]c` - jump between hunks\n- `<leader>hs`/`<leader>hu` - stage/unstage individual hunk (when cursor is\n    within it)\n  - can get more granular with visual mode selection\n- `<leader>hp` - preview hunk","url":"https://tycholiz.github.io/Digital-Garden/notes/d6070bd1-ac8f-4673-b37e-8c5a538aec70.html","relUrl":"notes/d6070bd1-ac8f-4673-b37e-8c5a538aec70.html"},{"doc":"Greplace","title":"Greplace","hpath":"vim.plug.greplace","content":"\n### Usage\n1. run `:Gsearch <term>`\n2. specify directories to search\n3. make the substitution in the resulting buffer\n4. run `:Greplace`, and respond: yes/no/all/quit","url":"https://tycholiz.github.io/Digital-Garden/notes/d8879597-308f-4354-8928-be5bbd300571.html","relUrl":"notes/d8879597-308f-4354-8928-be5bbd300571.html"},{"doc":"Nav","title":"Nav","hpath":"vim.nav","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/b0f9ffe8-9af0-423c-9021-07f026cbbce7.html","relUrl":"notes/b0f9ffe8-9af0-423c-9021-07f026cbbce7.html"},{"doc":"Line","title":"Line","hpath":"vim.nav.line","content":"\n- go to column 15 of current line - `15|`\n- `3f:` - find the 3rd occurrence of `:` in the line","url":"https://tycholiz.github.io/Digital-Garden/notes/0414d3e1-8c97-4d3a-8d60-a7c7bd70a635.html","relUrl":"notes/0414d3e1-8c97-4d3a-8d60-a7c7bd70a635.html"},{"doc":"Dir","title":"Dir","hpath":"vim.nav.dir","content":"\n# Directory Navigation\nopen file under cursor\n- `gf` \n  - If doesn't work, try visually selecting first.\nopen file under cursor in new h-split (`<C-w>L` for v-split)\n- `<C-w>f`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/39af8f1a-606c-4ff2-acd7-3953815b35bd.html","relUrl":"notes/39af8f1a-606c-4ff2-acd7-3953815b35bd.html"},{"doc":"Buffer","title":"Buffer","hpath":"vim.nav.buffer","content":"\n# Buffer Navigation\n## \nmove cursor up/down full screen\n- `ctrl + b`/`ctrl + f` \n\nscroll page up/down\n- `ctrl + e`/`ctrl + y`\n\njump between empty lines \n- `{`/`}`\n\ngo back 3 sentences \n- `3(`\n  - tip: trigger in *i n mode*\n\nmove to top/bottom\n- `zt`/`zb` \n\nmove current line to top of screen \n- `z<CR>`\n\nreplace a word with a yanked one \n- `vep`\n\ngo to top/mid/end line of current screen (home) \n- `H`/`M`/`L`\n    - go to 50% of page - `50%`\n\n## Returning to previous locations\ngo to last place you inserted text \n- `gi`\n\ngo to last place that you were \n- ***``***\n\ngo back and forth through list of positions you were in \n- `g;`/`g`,\n\ngo to start/end of previously changed or yanked text \n- `'[`/`']`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/cbe6ff75-5eb3-44f2-9dba-d19a3d9d361c.html","relUrl":"notes/cbe6ff75-5eb3-44f2-9dba-d19a3d9d361c.html"},{"doc":"Search","title":"Search","hpath":"vim.nav.buffer.search","content":"\n# Search within Buffer\n### Search\n- `/`/`?` to find forward/backward\n    - `n` to show next, `N` to show previous\n    - `gn` will visually select the match\n- `/<CR>` - search for last searched pattern \n- You can search with word boundary by doing `/\\<word\\>`.\n\n### Symbol find\n- `*` to find forward for the word that the cursor is under\n    - prepend with `g` so find without word delimiters\n- `#` to find backward for the word that the cursor is under\n\n- `<C-o>`/`<C-i>` to go back/forward","url":"https://tycholiz.github.io/Digital-Garden/notes/03ad9681-96cf-4a91-8ace-92c8de2ca7d4.html","relUrl":"notes/03ad9681-96cf-4a91-8ace-92c8de2ca7d4.html"},{"doc":"Marks","title":"Marks","hpath":"vim.nav.buffer.marks","content":"\n# Marks\n- `m[a-zA-Z]` marks the current cursor location with the designated letter\n    - lowercase marks are local to the buffer, while uppercase are globally accessible\n        - therefore, if we have 5 buffers open, each buffer can have mark \"a\", but only one can have mark \"A\"\n- **`a** will jump to the exact spot of *mark \"a\"*\n    - *mn.* - more precise, just like js template literals (which use back ticks)\n- **'a** will jump to the **line** of *mark \"a\"*\n    - more useful in the context of an *ex command*\n- **`** and **'** will both jump to marks. **'** will take you to the line, and **`** will shoot you to the extact spot.\n\n## Automatic marks\n- vim automatically sets up some marks for us:\n\n| Keystroke | Buffer Contents                               |\n| --------- | --------------------------------------------- |\n| \"         | Position before last jump within current file |\n| 0         | Position of cursor when the file was last closed|\n| [         | Start of last change/yank                     |\n| ]         | End of last change/yank                       |\n| <         | Start of last visual selection                |\n| >         | End of last visual selection                  |\n| 1, 2..    | latest position of cursor in last file opened |\n","url":"https://tycholiz.github.io/Digital-Garden/notes/cae94949-010b-4bbb-beff-697d34c25cac.html","relUrl":"notes/cae94949-010b-4bbb-beff-697d34c25cac.html"},{"doc":"Modes","title":"Modes","hpath":"vim.modes","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/b79cca0c-d7cd-4513-a338-361d64925267.html","relUrl":"notes/b79cca0c-d7cd-4513-a338-361d64925267.html"},{"doc":"Visual","title":"Visual","hpath":"vim.modes.visual","content":"\n# Visual Mode\nreselect last v-mode selection \n- `gv`\n\ngo to the *other* end of the v-mode selection \n- `o`\n\nwhen using *dot command* to repeat a v-mode command, it acts on the same amount of text as was marked by the recent visual selection\n\n## Tips\n- use visual block mode to create multiple cursors to edit multiple places at once\n    - ***ex.*** - to append `;` to end of three lines (hence the `jj`), `<C-v>jj$a;`\n- when visually selecting by motion (ex. `vi(`), you can increase the level of enclosing parens that you navigate\n    - for example, if we have nested parens, we can start off with the above command, then say `i(` to go an additional layer","url":"https://tycholiz.github.io/Digital-Garden/notes/9499b879-b556-4bc0-bab8-ead2011c415e.html","relUrl":"notes/9499b879-b556-4bc0-bab8-ead2011c415e.html"},{"doc":"Insert","title":"Insert","hpath":"vim.modes.insert","content":"\n# Insert Mode\n- vim records keystrokes until we leave *i mode*\n- moving around in *i mode* resets the change, meaning once we exit *i mode* and undo with `u`, it will undo all changes made in that *i mode*.\n\n## Insert-Normal Mode\n- use *insert normal mode* from within insert mode for a one shot command within normal mode (then return to i mode)","url":"https://tycholiz.github.io/Digital-Garden/notes/3fcb6ae8-f766-4cb6-abd1-38971e471e96.html","relUrl":"notes/3fcb6ae8-f766-4cb6-abd1-38971e471e96.html"},{"doc":"Cmds","title":"Cmds","hpath":"vim.modes.insert.cmds","content":"\n## Modifying text from Insert-mode\n- CTRL-h (delete one character left)\n- CTRL-w (delete word left)\n- CTRL-u (clear line)","url":"https://tycholiz.github.io/Digital-Garden/notes/21a4d5fb-4eda-47ba-b526-ebf7333366e4.html","relUrl":"notes/21a4d5fb-4eda-47ba-b526-ebf7333366e4.html"},{"doc":"Cmdline","title":"Cmdline","hpath":"vim.modes.cmdline","content":"\n### Command line Completion\nWhen executing a command line command, use `<C-d>` to see suggestions based on the text you've already typed\n- doing this will give us a list of all the commands containing the string we've already typed\n- ex. `:help CTRL-W`, then type `<C-d>` to see all commands with `CTRL-W`\nAlternatively we can `<TAB>` to get the completion window\n\n* * *\n\n## Executing Shell commands\n### Running commands without using output\nPrecede any shell command with `!` in the command prompt to have it interpreted as a shell command.\n\n### Backtick expansion\nIf we wrap text in backticks, vim will run the enclosed command using our configured shell, and use its stdout as the argument for the given vim command\n    `` :args `cat files.txt` ``\n- if files.txt is just a list of files, then vim will add all those files to the argument list\n\n## Getting pwd/filename\n- `%` refers to filepath of current buffer\n    - if our current buffer was 3 levels deeper than our pwd, we could open a sibling (same level) file with `:e path/to/file`, or we could just say `:e %:h<Tab>`, which would auto complete the folder of the current buffer\n        - `:h` here removes the filename from the path\n        - `%:h` has been aliased to `%%`\n\n## Functions\nwe can call functions like so:\n- `:call MyFunction()`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/510ec843-6f18-4fa4-b74f-d899a8a1b742.html","relUrl":"notes/510ec843-6f18-4fa4-b74f-d899a8a1b742.html"},{"doc":"Innner","title":"Innner","hpath":"vim.innner","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/eaa7919d-abb6-4266-bd80-a43917e4ca91.html","relUrl":"notes/eaa7919d-abb6-4266-bd80-a43917e4ca91.html"},{"doc":"Text Object","title":"Text Object","hpath":"vim.innner.text-object","content":"\nOne of the keys to using Vim effectively is trying to edit by text object\n\n# Text object \n- a defined region of text that is defined by the motions, and modifier of that motion (eg. `vaw`, or `v2aw`)\n    - The definition of the text's scope depends on what vim commands we are using\n        - ex. When we say `vi\"` we are defining a text object, whose definition is \"everything inside `\"\"`\". Text objects define regions of text by some sort of structure.\n- text objects are good because we don't have to worry about where in the word the cursor is located. as a result, they work well with `.` \n\n## Types\n- Word (`w`)\n- Sentence (`s`)\n\n## Extended Text Objects (Plugins)\n### CamelCaseMotion\nDefines a text object that is a single word in a larger camel-case word\n- Delete inside the camel-case word\n    - `di,w`\n\n### VimTextObject\nDefines a text object that is a function argument/parameter\n- Delete inside the argument\n    - `dia`\n\n# UE Resources\n[info](https://blog.carbonfive.com/2011/10/17/vim-text-objects-the-definitive-guide/)","url":"https://tycholiz.github.io/Digital-Garden/notes/975a8a6b-013e-4eda-bf14-60aa618d04a1.html","relUrl":"notes/975a8a6b-013e-4eda-bf14-60aa618d04a1.html"},{"doc":"Inner","title":"Inner","hpath":"vim.inner","content":"\n# viminfo file\nThe viminfo is like a cache, to store cut buffers persistently\n- If you exit Vim and later start it again, you would normally lose a lot of\ninformation.  The viminfo file can be used to remember that information, which\nenables you to continue where you left off.\n- Vim writes this file for us.\n\nThe viminfo file is used to store:\n- The command line history.\n- The search string history.\n- The input-line history.\n- Contents of non-empty registers.\n- Marks for several files.\n- File marks, pointing to locations in files.\n- Last search/substitute pattern (for 'n' and '&').\n- The buffer list.\n- Global variables.","url":"https://tycholiz.github.io/Digital-Garden/notes/a400153d-01d1-4d41-8e98-b8afacb8289f.html","relUrl":"notes/a400153d-01d1-4d41-8e98-b8afacb8289f.html"},{"doc":"Register","title":"Register","hpath":"vim.inner.register","content":"\n# Register \n([info](https://www.brianstorti.com/vim-registers/))\n- a space in memory with an identifier that vim uses to store text\n    - analogous to a clipboard\n- access register `a` - `\"a`\n    - access register in *i mode* - `<C-r>` (instead of `\"`)\n- vim has a default register, which is where any deleted or yanked text will be\n    - denoted `\"\"` - so `p` is shorthand for `\"\"p`\n    - this is more like copy/paste\n- when writing to a register, lowercase letters will overwrite the register, while uppercase will append to the register\n- Sequential register\n    - `\"1` is most recent (or `\"\"`), `\"2` is second most recent, and so on\n    - this is only for `dd` and `yy` operation\n- Yank register\n    - `\"0`\n- Blackhole register\n     - `\"_` - prepending a command with this will prevent anything from entering a register.\n- Clipboard register\n    - `\"+`/`\"*` - gives access to the system clipboard\n- Expression register\n    - `\"=`\n    - when fetching contents from the register, we enter command-line moe\n    - an exception to the idea that \"registers are containers that hold a block of text\"\n- use `:reg` to see all registers\n    - `:reg a b c` will show registers `a b c`\n- copy text into register `r`, then paste it - `\"ry`, `\"rp`\n- *Yank Register* - when yanking, the copied text is placed into the yank register (`\"0`), as well as the default register.\n- *Delete/yank Register* - `\"-`\n    - only small delete (ie. no `dd`)\n- when using visual mode to make a selection of text that we will replace with `p`, the highlighted text will go into the register as the *pasted* text exits\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c9912db2-494f-4401-9997-b8e86699dbd7.html","relUrl":"notes/c9912db2-494f-4401-9997-b8e86699dbd7.html"},{"doc":"Buffer","title":"Buffer","hpath":"vim.inner.buffer","content":"\n# Buffer\n- a part of vim's memory that can hold text\n- This is normally in the form of:\n    - an actual file\n    - text stored temporarily (yank, paste etc)\n- when opening an existing file, the text of that file is copied and put into a new buffer. When we save that text, the original file is replaced by writing the buffer to disk\n- buffer list vs argument list\n    - argument list is subset of buffer list\n    - buffer list can be seen as more random access, while argument list could have more organization to it\n        - if you are working with only a few files in your current micro session, you'd have just those ones in the arg list, while having many more in the buffer list\n    - idea is that we might have lots of buffers open, and want to execute a macro across many files, but not all buffers.\n        - one solution is to prune the buffer list, but the most practical solution is to populate the argument list with just the files we are interested in. \n","url":"https://tycholiz.github.io/Digital-Garden/notes/469709d3-8cf5-4890-805a-ddcaaa2db54c.html","relUrl":"notes/469709d3-8cf5-4890-805a-ddcaaa2db54c.html"},{"doc":"Help","title":"Help","hpath":"vim.help","content":"\n# Help\nThe Vim documentation consists of two parts:\n1. The User manual\n   Task oriented explanations, from simple to complex.  Reads from start to\n   end like a book.\n2. The Reference manual\n   Precise description of how everything in Vim works.\n\nIf you only have a generic keyword to search for, use `:helpgrep` and open the quickfix window:\n```\n:helpgrep quickfix\n:copen\n```\n\n### Navigate\nJump to a subject\n- put cursor on a subject tag (blue text), and hit `<C-]>`\n    - In fact, this works on any word\n- return with `<C-o>`, or `<C-t>`\n\n### Specific Help\n- we can prepend a binding with the mode to see what it does, and get help on it\n    - ex. `:help v_u` to get help on what `u` command does in visual mode.\n    - ex. `:help CTRL-W_CTRL-I`\n    - ex. `:help i_CTRL-G_<Down>`\n    - ex. `:help i^W`\n- prefixes: `v_`, `i_`, `:`\n    - normal mode has no prefix\n\n\n# UE Resources\n[Guide to using vim help](https://vi.stackexchange.com/questions/2136/how-do-i-navigate-to-topics-in-vims-documentation)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/fe9c7071-18c8-4602-a1b7-567994079ee9.html","relUrl":"notes/fe9c7071-18c8-4602-a1b7-567994079ee9.html"},{"doc":"Fugitive","title":"Fugitive","hpath":"vim.fugitive","content":"\n# Vim-Fugitive\n## Gstatus\nor simply `:G`\n`<C-n>`/`<C-p>` - navigate files\n`-` stage/unstage the file that you are hovering over\n- can use visual mode\n\n`p` - run `git add --patch` for the file under cursor\n- splits file into hunks of changes and allows you to pick which parts to stage for commit\n- not as useful, since we can just use `diffget` as described below.\n\n`cc` - commit\n`D` - open file under cursor as a diff\n`U` - unstage a file (\"I've staged a file and want to remove it from the index\")\n`X` - untrack a file (\"I've changed a file in the working tree and want to discard all changes\")\n  - `git checkout -- <filename>`\n\n`]c`/`[c` - navigate between hunks when in vimdiff window\n\n## Gblame\n`A` resizes to end of author column\n`C` resizes to end of commit column\n`D` resizes to end of date/time column\n`<CR>` opens the patch that was applied by that commit\n\n## Reconciling differences between working copy and index version\n- `:Gw[rite]` - write (save) the current file and stage it\n- `:Gr[ead]` - read the index version of the file into the current buffer\n\t- good for seeing what changes were made (that haven't been staged yet)\n\n- `Gwrite` and `Gread` are opposites. Running `Gread` in the working file will do the exact same thing as running `Gwrite` in the index file +vv (both cause the working copy to revert to the index version)\n  - Both of these commands reconcile differences between the working copy and\n      the index version of a file by overwriting the contents of one buffer.\n- We can be more granular with our reconciliation by using vim built-in `diffget`/`diffput` (alias `do`/`dp`, respectively. These aliases also run :diffupdate afterwards, to fix coloring issues)\n  - While running `:Gdiff`, if we are in the index window and position the\n     cursor over a hunk of text and run `diffget`, it will pull that hunk into\n     the working copy of the file.\n  - If there are changes in the working copy (right side) that you'd like to move to the index, you can visually select those lines and run `diffput`\n    - this opens a new buffer with just the hunks you've added. we need to save this buffer before we `\n  - `diffget` makes changes in the currently active buffer, while `diffput` makes changes in the inactive buffer (by drawing in differences from the active buffer)\n![](/assets/images/2021-03-08-21-36-16.png)\n  - run `:diffupdate` if colors get messed up as a result of doing this\n- `:Gedit :0` - open index version of current file (remember: staging a file\n    updates the index version of the file (?))\n- `:Gremove` - wipe out the buffer and run `git remove` on the current file\n- `:Gmove` - rename the current file\n  - does a whole bunch more than `git move`, so it's not a direct replacement\n  - ex. `:Gmove new-file-name` - (relative to the path of the current file)\n- jump between conflicting regions with `[c`/`]c`\n\n## Merge conflicts\n- open a file with merge conflicts and run `:Gdiff`\n\t- might need to run `Gdiffsplit!` to get 3 windows\n- The windows shown are:\n\t1. Target Branch - The branch we are merging into (normally master)\n\t2. Working Tree - The file as it currently is (with changes visible from both target branch and merge branch)\n\t3. Merge Branch - The branch we are introducing\n- if using `diffget` and want to accept changes from the target branch (likely master), run `:diffget //2`. If we wan to accept changes from merge branch (likely feature-branch), run `:diffget //3`\n\n# Diff\nLeft window is the commit we are comparing against, and right window is the current working copy\n\n### Colors\nred - lines were deleted\ngreen - lines were added\nteal - lines were modified\n\n- knowing colors, we can easily notice if something has actually been removed, or if it was just moved/has a new structure\n  - In this case, you'd find the green code in the left hand side, and try and match it up with a green chunk on the righthand side. If we find a match, then we know the code format just changed, and that no actual code was deleted.\n\n## Seeing diff between 2 versions of a file\n`:Gdiff <sha>`\n\n`:Gdiff :0`\n- diff between current file and staged version\n\n`:Gdiff !~3`\n- diff between current file and 3 commits ago\n\n\n### UE Resources\n- http://vimcasts.org/episodes/fugitive-vim-resolving-merge-conflicts-with-vimdiff/\n- [cheatsheet](https://gist.github.com/mikaelz/38600d22b716b39b031165cd6d201a67)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/2738937e-273a-4dd1-81a9-267eab7a907e.html","relUrl":"notes/2738937e-273a-4dd1-81a9-267eab7a907e.html"},{"doc":"Feat","title":"Feat","hpath":"vim.feat","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a7f905c5-18dc-4e64-b1a2-c216f17ccd30.html","relUrl":"notes/a7f905c5-18dc-4e64-b1a2-c216f17ccd30.html"},{"doc":"Macros","title":"Macros","hpath":"vim.feat.macros","content":"\n# Macros\n- allow us to record and number of keystrokes into a register\n- ideal for repeating changes over a set of similar lines, paragraphs and files.\n- macros allow us to easily append commands to the end of an existing one\n    - for complex ones, we can paste the macro into a document, edit it, then yank it back into the register\n- golden rule - ensure every command in a macro is repeatable\n- before starting to record a macro, ask: where am I, where did I come from, where am I going?\n    -  before doing anything make sure the cursor is positioned to the next command does as we'd expect, and where we'd expect\n- any motions that fail cause the macro to fail.\n    - ex. - pressing `k` on line 1 does nothing, so it would fail if we tried.\n- `@@` - repeat last macro\n","url":"https://tycholiz.github.io/Digital-Garden/notes/47de1b48-74b2-4dd8-9574-a16657941f1c.html","relUrl":"notes/47de1b48-74b2-4dd8-9574-a16657941f1c.html"},{"doc":"Cmds","title":"Cmds","hpath":"vim.feat.macros.cmds","content":"\n# Macros\n- `qa` - start recording into register `a`\n- `q` - stop recording\n- `@a` - execute macro stored in register `a`\n- `@@` - execute the most recently executed macro\n- execute macro on each line in selection - `:'<,'>normal @q`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/44fdfb1d-1c95-4eed-a45f-9e42e231119c.html","relUrl":"notes/44fdfb1d-1c95-4eed-a45f-9e42e231119c.html"},{"doc":"Jump List","title":"Jump List","hpath":"vim.feat.jump-list","content":"\nVim remembers the locations you have recently visited (where you jumped from). Each position (file name, column number, line number) is recorded in a jump list, and each window has a separate jump list that records the last 100 positions where a jump occurred.\n- The commands that are regarded as \"jumps\" include searching, substitute and marks. Scrolling through a file is not regarded as jumping.\n\nLike a web browser, you can go back, then forward:\n- Press Ctrl-O to jump back to the previous (older) location.\n- Press Ctrl-I (same as Tab) to jump forward to the next (newer) location.\n\nuse `:jump` to see the jumplist","url":"https://tycholiz.github.io/Digital-Garden/notes/5be48492-116f-42b9-aefb-6d48054c884f.html","relUrl":"notes/5be48492-116f-42b9-aefb-6d48054c884f.html"},{"doc":"Change List","title":"Change List","hpath":"vim.feat.change-list","content":"\nVim remembers the locations where changes occurred. \n- Each position (column number, line number) is recorded in a change list, and each buffer has a separate change list that records the last 100 positions where an undo-able change occurred.\n- if you make a change at a certain position, then make another change nearby, only the location of the later change is recorded \n    - \"nearby\" means on the same line, within a certain number of bytes.\n\n## Usage\n- Type g; to jump back to the position of the previous (older) change.\n- Type g, to jump to the position of the next (newer) change.\n\nType `:changes` to view the list","url":"https://tycholiz.github.io/Digital-Garden/notes/12d8351d-e984-4b18-9891-27a443cda180.html","relUrl":"notes/12d8351d-e984-4b18-9891-27a443cda180.html"},{"doc":"Config","title":"Config","hpath":"vim.config","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/ee17da05-fd4b-4e3e-a440-915c8edb0820.html","relUrl":"notes/ee17da05-fd4b-4e3e-a440-915c8edb0820.html"},{"doc":"Viminfo","title":"Viminfo","hpath":"vim.config.viminfo","content":"\n```\nset viminfo=%,<800,'10,/50,:100,h,f0,n~/.vim/cache/.viminfo\n\"           | |    |   |   |    | |  + viminfo file path\n\"           | |    |   |   |    | + file marks 0-9,A-Z 0=NOT stored\n\"           | |    |   |   |    + disable 'hlsearch' loading viminfo\n\"           | |    |   |   + command-line history saved\n\"           | |    |   + search history saved\n\"           | |    + files marks saved\n\"           | + lines saved each register (old name for <, vi6.2)\n\"           + save/restore buffer list\n```\n`set viminfo=xxx` should come after `set nocompatible`\n\nif we set `'` parameter, then marks and other things set during the buffer editing will persist even after we close vim.","url":"https://tycholiz.github.io/Digital-Garden/notes/24e5d275-ed51-4fb6-ad9c-f850d57a7568.html","relUrl":"notes/24e5d275-ed51-4fb6-ad9c-f850d57a7568.html"},{"doc":"Runtime Path","title":"Runtime Path","hpath":"vim.config.runtime-path","content":"\n# Runtime path\n- vim looks for scripts in various directories. The directories that vim will look in is known as the `runtime path`\n- to see it, type `:set runtimepath?`\n\n# Runtime directory\nAssuming that you're using some flavor of Unix, your personal runtime directory is ~/.vim. This is where you should put any plugin used only by you.\nYou should not install any plugins into the $VIMRUNTIME directory. That directory is intended for plugins distributed with Vim.","url":"https://tycholiz.github.io/Digital-Garden/notes/e16ecf6b-ec7f-4550-931c-7dd06be386af.html","relUrl":"notes/e16ecf6b-ec7f-4550-931c-7dd06be386af.html"},{"doc":"Mapping","title":"Mapping","hpath":"vim.config.mapping","content":"\n# Mapping\n- `:map` - recursive version of mapping command\n- `:noremap` - non-recursive version of mapping command\n    - \"Recursive\" means that the mapping is expanded to a result, then the result is expanded to another result, and so on.\n- below, `j` will be mapped to `gg`. `Q` will also be mapped to `gg`, because `j` will be expanded for the recursive mapping. `W` will be  mapped to `j` (and not to `gg`) because `j` will not be expanded for the non-recursive mapping.\n    - The expansion stops when one of these is true:\n        1. the result is no longer mapped to anything else.\n        2. a non-recursive mapping has been applied (i.e. the \"noremap\" [or one of its ilk] is the final expansion).\n```\n:map j gg\n:map Q j\n:noremap W j\n```\n- since VIM is a **modal editor**, there are corresponding mapping commands for each mode\n    - ***ex.*** - `:imap`, `:nmap`, `:map!` (*i mode* and command line)\n\n- Recursive map - the rhs says to the lhs \"if you get triggered, when I get triggered. It stops with me, and nothing past me gets triggered\"\n- Non-recursive map (nore)- the rhs says \"When you get triggered, I may server as the lhs of another mapping that will trigger something else.\"\n- map is the \"root\" of all recursive mapping commands. \n    - The root form applies to \"normal\", \"visual+select\", and \"operator-pending\" modes. \n- \"Recursive\" means that the mapping is expanded to a result, then the result is expanded to another result, and so on.\n- The expansion stops when one of these is true:\n    1. the result is no longer mapped to anything else.\n    2. a non-recursive mapping has been applied (i.e. the \"noremap\" [or one of its ilk] is the final expansion).\n- At that point, Vim's default \"meaning\" of the final result is applied/executed.\n- \"Non-recursive\" means the mapping is only expanded once, and that result is applied/executed.\n    - Example:\n```\nnmap K H\nnnoremap H G\nnnoremap G gg\n```\n- The above causes K to expand to H, then H to expand to G and stop. It stops because of the nnoremap, which expands and stops immediately. The meaning of G will be executed (i.e. \"jump to last line\"). At most one non-recursive mapping will ever be applied in an expansion chain (it would be the last expansion to happen).\n- The mapping of G to gg only applies if you press G, but not if you press K. This mapping doesn't affect pressing K regardless of whether G was mapped recursively or not, since it's line 2 that causes the expansion of K to stop, so line 3 wouldn't be used.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d36d7329-b56d-4491-8bdc-6a4969be4a94.html","relUrl":"notes/d36d7329-b56d-4491-8bdc-6a4969be4a94.html"},{"doc":"Autocmd","title":"Autocmd","hpath":"vim.config.autocmd","content":"\n## Autocommands\n- Autocommands are a way to tell Vim to run certain commands whenever certain events happen.\n\n[info](https://learnvimscriptthehardway.stevelosh.com/chapters/12.html)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/27e76de0-686b-423c-8f12-4f0f2776e6b6.html","relUrl":"notes/27e76de0-686b-423c-8f12-4f0f2776e6b6.html"},{"doc":"Cmds","title":"Cmds","hpath":"vim.cmds","content":"\n- when an operator command is invoked twice, it means it is operating on the current line\n    - `dd`, `yy`, `>>`","url":"https://tycholiz.github.io/Digital-Garden/notes/e4b8bd71-4f1d-4012-a17b-7b3022dc48ab.html","relUrl":"notes/e4b8bd71-4f1d-4012-a17b-7b3022dc48ab.html"},{"doc":"Surround","title":"Surround","hpath":"vim.cmds.surround","content":"\n# Surround\nsurround in visual mode with `()` \n- `Sb`\nchange surrounding `''` with `\"\"` \n- `cs'\"`\nchange `<p>` with `<b>` \n- `cst<b>`\ndelete `\"\"` surroundings \n- `ds\"`\ndelete tags \n- `dst`\nsurround parentheses content with `{}` \n- `ysib{`\n    - *mn.* - \"you surround!\"\nsurround line with `\"\"` \n- `yss`\nsurround 3 words with `\"\"` \n- `v3eS\"`\ndelete the second outer pair of brackets \n- `2ds}` ","url":"https://tycholiz.github.io/Digital-Garden/notes/028f017f-76e8-4506-8a0a-6e50a8e06433.html","relUrl":"notes/028f017f-76e8-4506-8a0a-6e50a8e06433.html"},{"doc":"Unix","title":"Unix","hpath":"unix","content":"\n# Unix \n\n## Philosophy\nIn Unix philosophy, \"everything is a file\"\n- ex. sockets (both have functions to read, write, open, close etc)\n\n## Unix Domain Socket\n- unix domain sockets allow us to exchange data between processes on the same host machine.\n- a.k.a IPC socket","url":"https://tycholiz.github.io/Digital-Garden/notes/240d9848-b03d-445a-8787-090d79024b6a.html","relUrl":"notes/240d9848-b03d-445a-8787-090d79024b6a.html"},{"doc":"Threads","title":"Threads","hpath":"unix.threads","content":"\n# Threads\n**Scheduling** is the action of assigning work to different resources in a system. \n- this work is carried out on threads.\nTherefore a thread is the smallest amount of instructions that can be handled by a scheduler.\n- think of it as the atomic unit from the scheuler's perspective.\n\nA **process** is an instance of a program that is being executed on 1 or more threads.\n- multiple threads can exist on process simultaneously.\n\t- the number of threads depends on the programming language and source code that the process is running on.\n- threads within a process share resources like memory, but threads in different processes to not share resources.\n- a process can be forked (split) into many child processes\n\n### Multi-process vs Multi-threaded\nIf a single process in a multi-process application crashes, that process alone dies.  The buffer is flushed, and all the other child processes continue happily along.  In a multi-threaded environment, when one thread dies, they all die.\n\n- On some OSs, processes tend to be more expensive than threads (though on Linux, the difference is not that great)","url":"https://tycholiz.github.io/Digital-Garden/notes/c5921e7a-452e-4151-bb86-385fa4b584bb.html","relUrl":"notes/c5921e7a-452e-4151-bb86-385fa4b584bb.html"},{"doc":"Streams","title":"Streams","hpath":"unix.streams","content":"\n# What is it?\n- Streams are used to read or write input into output sequentially\n- What makes streams unique, is that instead of a program reading a file into memory all at once like in the traditional way, streams read chunks of data piece by piece, processing its content without keeping it all in memory.\n\t- This makes streams really powerful when working with large amounts of data, for example, a file size can be larger than your free memory space, making it impossible to read the whole file into the memory in order to process it. That’s where streams come to the rescue!\n- Streams are a way to handle reading/writing files, network communications, or any kind of end-to-end information exchange in an efficient way\n- streams are not only about working with media (ie. streaming movies/music) or big data. They also give us the power of ‘composability’ in our code\n\t- it’s possible to compose powerful pieces of code by piping data to and from other smaller pieces of code, using streams.\n\nit is best to think of a Stream as an established connection to some input, some output, or both. A lot of things can muddy this definition of stream, but that is the fundamental idea... A connection to some input or some output or some I/O endpoint that can handle both input and output.\n- ex. one might have a readable stream to a file that one wants to get the contents of. In this case the stream is an input connection to the file. The input stream doesn't hold the contents of the file, but rather holds the connection to the file. It is up to the program to then read and store the contents if that is the desired operation.\n\t- One place a stream might store the content is into a [[Buffer|memory.buffer]] object (if we were using Node).\n\n## Why use streams over other data handling methods?\n1. Memory efficiency: you don’t need to load large amounts of data in memory before you are able to process it\n2. Time efficiency: it takes significantly less time to start processing data as soon as you have it, rather than having to wait with processing until the entire payload has been transmitted\n\n## Standard Streams\n- communication channels between a program and its environment (ie. the host that executes it).\n- stdin and stdout are inherited from the parent process, though it can be redirected.\n- anal: The Services menu, as implemented on Mac OSX, is analogous to standard streams. On this operating systems, graphical applications can provide functionality through a systemwide menu that operates on the current selection in the GUI, no matter in what application.\n- The correct way to think about redirection is that one redirects output to a file (`>`), and one redirects a file's contents to stdout (`<`).\n\n### File Descriptor (FD)\n- A FD is a number that uniquely identifies an open file in a computer's operating system\n\t- A file can be thought of simply as a resource. In other words, something that provides data. A file as we know it fulfills this definiion, but also consider that streams do as well. Therefore, a file in this sense is more general, and simply stands for \"a resource that provides data\".\n- A FD gives us the means to access a file, or perhaps more interestingly, a standard stream\n\t- it is a reference to the standard stream\n- ex. The file descriptor for standard input is 0\n- except for redirections with `<&`, any time we do a redirection,\n\n### STDIN\nStandard input is a stream from which a program reads its input data.\n- the program requests this data transfer by using the *read* operation\n- ex. the keyboard is the stdin to a text editor, and also to an interactive shell.\n\nWhen we redirect with `<`, implicitly we are running `1<`\n\n### STDOUT\nStandard output is a stream to which a program writes its output data.\n- the program requests this data transfer by using the *write* operation\n\n### STDERR\n- having 2 different output streams is analogous to functions that return a pair of values\n- The terminal that executes the program is the default stderr destination.\n\t- sometimes stdout is redirected, so the fact that stdout and stderr are different streams allows stderr to continue outputting its streams elsewhere than stdout.\n\t\t- ex. when we pipe the output of one program into another. Imagine we were piping data between different hosts. If there was a single stream, it would get passed along the chain. Now with stderr, we are able to see stderr in the executing terminal and trace down the location of the logs to find the offending program.\n#### 2>&1\n- we can append `2>&1` on any unix command to redirect stderr to the same destination as stdout\n\t- often, we see `2>/dev/null`, which means \"redirect stderr to `/dev/null`, a blackhole\"\n\t- ex. the **find** utility displays both stdout and stderr to the terminal. If we append the command with blackhole redirection, then only the stdout will be shown.\n- placement of `2>&1` is critical to determining its meaning. Consider the following variants of the same command:\n\n- `|&` is shorthand for `2>&1 |`\n- `echo hello > stuff.txt` === `echo hello 1> stuff.txt`.\n\t- ie. the `1` (stdout) is implicit. we are saying \"redirect the stdout of the echo command to `stuff.txt`\"\n- `0` means stdin, `1` means stdout, `2` means stderr\n- in the context of redirections, `&` means \"the following is a file descriptor (ie. the type of standard stream) and not a filename\". Without `&`, we would be writing to a file named `1`\n```\n// stderr goes to terminal, and nothing gets written to outfile\ncommand_does_not_exist 2>&1 > outfile\n\n// stderr goes to stdout, and gets written into outfile\ncommand_does_not_exist > outfile 2>&1\n```\nIn the first example, when we encountered `2>&1`, we were saying \"pipe all stderr into the same stream as stdout\". At this point in the command, stdout was simply the terminal (spec: since that is the default stdout of a command). Since stdout was the terminal at this point, we also declared that stderr should point to the terminal. In this way, `2>&1` doesn't cause stderr to equal stdout. It merely points it the same way stdout is facing. Nothing is stopping stdout from changing direction later on, which is exactly what happens. When we write `> outfile`, we are declaring that stdout should be `outfile`.\n\nIn the second example, we see that by the time we evaluate `2>&1`, stdout is `outfile`. This causes stderr to also point at `outfile`, hence is why in this example, the file is populated with `command not found: command_does_not_exist`\n- the `2>&1` operator points file descriptor 2 to the same target file descriptor 1 is pointing at.\n\n## Special File\n- spec: In Unix, the most comfortable means for programs to communicate is via files. It is easy to set up standard streams with files, so it is an appropriate level of abstraction to communicate with the Unix system. For this reason, Unix has this concept of a *special file*, which is a file that represents something that is not a file at all.\n- ex. consider a partition of a hard drive that \"exists\" in the FS at `/dev/sda3`. In reality, this is just how the filesystem \"knows of\" the partition. The `sda3` file enables the partition to communicate with the Unix system via standard streams.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/40f0d606-3aac-4b21-b700-1ebb0c31f523.html","relUrl":"notes/40f0d606-3aac-4b21-b700-1ebb0c31f523.html"},{"doc":"Process","title":"Process","hpath":"unix.process","content":"\nsince a process is a type of environment, environment variables can be associated with a process.\n- ex. when we say `process.env` we are talking about getting environment variables that exist during a specific process\n\na **Signal** is the way that a process can communicate with the OS\n- Signal and interrupt are basically same, but with a small distinction: \n    - interrupts are generated by the processor and handled by the kernel \n    - signals are generated by the kernel and handled by the process.\n\n\n# Debugging\n- strace/truss, ltrace and gdb are generally good ideas for looking at why a process is stuck. (truss -u on Solaris is particularly helpful; I find ltrace too often presents arguments to library calls in an unusable format.) Solaris also has useful /proc-based tools, some of which have been ported to Linux. (pstack is often helpful).","url":"https://tycholiz.github.io/Digital-Garden/notes/f16d8566-b26f-4d91-a11e-bf891e504da3.html","relUrl":"notes/f16d8566-b26f-4d91-a11e-bf891e504da3.html"},{"doc":"Netrc","title":"Netrc","hpath":"unix.netrc","content":"\nThe `.netrc` file contains configuration and autologin information for the ftp client \n- It resides in the user's home directory\n\n`.netrc` file takes the form:\n> `remote-machine` `name`\n\nThe auto-login process searches the `.netrc` file for a machine token that matches the remote machine specified on the ftp command line\n- Once a match is made, the subsequent `.netrc` tokens are processed, stopping when the end of file is reached or another machine or a default token is encountered\n\nThe following three lines must be included in the file. The lines must be separated by either white space (spaces, tabs, or newlines) or commas:\n\n```\nmachine <remote-instance-of-labkey-server>\nlogin <user-email>\npassword <user-password>\n```\nAn example:\n```\nmachine mymachine.labkey.org\nlogin user@labkey.org\npassword mypassword\n```\nor:\n```\nmachine mymachine.labkey.org login user@labkey.org password mypassword\n```\n\n### Using API Keys\nWhen API Keys are enabled on your server, you can generate a specific token representing your login credentials on that server and use it in the netrc file. The \"login\" name used is \"apikey\" (instead of your email address) and the unique API key generated is used as the password\n\n* * *\n\nNote: the `.netrc` file only deals with connections at the machine level and should not include a port or protocol designation, meaning both \"mymachine.labkey.org:8888\" and \"https://mymachine.labkey.org\" are incorrect.","url":"https://tycholiz.github.io/Digital-Garden/notes/b9da7e57-7675-41fa-8c87-ae4f06a5ad7b.html","relUrl":"notes/b9da7e57-7675-41fa-8c87-ae4f06a5ad7b.html"},{"doc":"Man Pages","title":"Man Pages","hpath":"unix.man-pages","content":"\n# Man Pages\n- anything in `[]` brackets indicates optional\n- `--` - signify end of command options. Only positional params are accepted after this point.\n    - ex. say we want to grep for the string `-v`. If we just executed `grep -v file.txt`, the `-v` would be interpreted as an argument on grep. If we execute `grep -- -v file.txt`, `--` tells us \"ok, that's it. No more arguments accepted\". Since the section after the args section is the pattern section, `-v` gets interpreted as a pattern.\n\n### Valid command form\n- angle brackets for required parameters: ping <hostname>\n- square brackets for optional parameters: mkdir [-p] <dirname>\n- ellipses for repeated items: cp <source1> [source2…] <dest>\n- vertical bars for choice of items: netstat {-t|-u}","url":"https://tycholiz.github.io/Digital-Garden/notes/3f925157-f4d8-40fd-a89e-13a62dd51fa6.html","relUrl":"notes/3f925157-f4d8-40fd-a89e-13a62dd51fa6.html"},{"doc":"Fs","title":"Fs","hpath":"unix.fs","content":"\n# Unix filesystem\nThe UNIX filesystem unifies all physical hard drives and partitions into a single directory structure. It all starts at the top–the root (/) directory. All other directories and their subdirectories are located under the single Linux root directory. This means that there is only one single directory tree in which to search for files and programs.\n\n## Types of files\n- ordinary file\n- directory\n- special (device) file - represents a physical device (ex. printer).\n\t- special files are used for sending outputs to the device, and receiving inputs\n- pipe - a temporary file that gets created when we pipe commands into each other. Pipes are the mechanism through with programs chain together.\n- socket - a stream of data very similar to network stream, but all the transactions are local to the filesystem.\n- symbolic link\n\n## Filesystem metadata\n- The general file system model, to which any implemented file system needs to be reduced, consists of several well-defined metadata entities: `superblock`, `inode`, `file`, and `dentry`\n\t- they are metadata about the filesystem\n- each entity here interacts using the VFS, and each entity is treated as an object.\n- each entity has its own data structure, and a pointer to a table of methods \n\n## Mount Point\n- a mount point is a special directory within the unix filesystem. It is special because when an external filesystem is mounted (ex. HDD, SSD, USB, SD), its contents are dumped into the mount point, which is accessible from the root folder (`/`)\n- ex. `/media`\n- so when we run `mount /dev/hda2 /media/photos`, we are saying \"hey, I want you take all of the contents of `/dev/hda2` (partition 2 of HDD), and make it all accessible through `/media/photos`\".\n\t- spec:think of it like a map, where it will take the external file system on the USB (in FAT or NTFS format) and it will make all of the contents visible within the UNIX filesystem\n\t- to carry out this \"mapping\", unix has filesystem drivers \n\n## Directory structure\n`/bin` - binaries generally needed by all users of the system.\n`/lib` - contains system libraries\n`/usr` - contains executables, libraries, and shared resources that are not system critical (ex. X Window)\n`/usr/bin` - stores binaries distributed with the OS, that aren't in `/bin` \n`/usr/lib` - stores libraries for programs in `/usr`\n`/dev` - contains file representations of peripheral devices. Therefore, to view files, the external device needs to be mounted (see Mount Point below) \n`/media` - default mount point for removable devices\n`/mnt` - default mount point for temporary filesystems\n`/etc` - contains system-wide config files/system databases.\n`/proc` - contains all running processes displayed as their own files.\n`/tmp` - files that get cleaned frequently. Often, this directory gets cleared on reboot. \n`/var` - contains files that get changed often. usually where files go that are not managed by the system-wide package manager\n`/var/mail` - contains all incoming mail\n`/var/www/html` - default root folder of the web server (e.g. Apache)\n\n### Subdirectories\n`sys/` - operating system kernel, handling memory management, process scheduling, system calls etc.","url":"https://tycholiz.github.io/Digital-Garden/notes/c67c11e8-b7e8-4e6c-a1b0-2ea294667cc3.html","relUrl":"notes/c67c11e8-b7e8-4e6c-a1b0-2ea294667cc3.html"},{"doc":"Permissions","title":"Permissions","hpath":"unix.fs.permissions","content":"\n# File Permissions\n- There are 3 permission groups: Owner, Group, Other\n- each permission group has 3 permissions, called a permission set\n\t- ex. `rw-` is a single set\n- each file/directory has 3 permission sets— one for each permission group.\n- rwx mean different things if we are referring to a directory or a file\n\t- `r` \n\t\t- on file means we can read the contents \n\t\t- on directory means we can run `ls`\n\t- `w` \n\t\t- on file means we can modify file contents\n\t\t- on directory means we can add/delte files \n\t- `x`\n\t\t- on a file means we can run it\n\t\t- on a directory means we can `cd` into it \n\n\n### File ownership\n- every file is owned by a specific user (`UID`) and a specific group (`GID`)\n\t- `chown` is used to change both\n\t\t- ex. `chown <user>:<group> test.txt`\n- each member can belong to many groups (`/etc/group`), though a user can only have one primary group (`/etc/passwd`).\n\t- run `$ id` to see the groups the current user belongs to\n\t- When a user creates a file, the file will be owned by the primary group\n- similar to how we need to source the `.zshrc` before changes are live, we need to log out and log back in before group membership is \"activated\"\n\n### Settings Permissions\nThe numerical method is quite easy. For example, we can just replace each `rwx` set by it's binary positional value (from RTL: 1, 2, 4, 8, 16, 32...) and add the the numbers in each set.\n```\n-(rw-)(rw-)(r--)\n-(42-)(42-)(4--)\n664\n```\n- By this, we can define a `7` as `rwx`, a `5` as `r-x`, and so on\n\n* * *\n\nSet new files/directories in a subdirectory to follow the group ownership of the specified directory\n- `chmod g+s /var/www/my-project`\n\n[Unix permissions guide](https://support.plex.tv/articles/200288596-linux-permissions-guide/)","url":"https://tycholiz.github.io/Digital-Garden/notes/237917e1-a8c9-48cc-bbe9-672a94fad203.html","relUrl":"notes/237917e1-a8c9-48cc-bbe9-672a94fad203.html"},{"doc":"Inode","title":"Inode","hpath":"unix.fs.inode","content":"\n# inode\n- a data structure that describes a FS object (like a file/directory)\n- an inode is a virtual filesystem entity (in other words, it exists in the VFS world)\n- the inode stores metadata about the object, such as its disk block location, file size, file type \n- it also stores owner/permission data about the object\n- In reality, a directory is just a list of names that are each assigned to an inode\n\t- A directory contains an entry for itself, its parent, and each of its children.\n- on a UNIX system, files are user facing (ie. we work with them directly). the structure of a file exists only as a virtual FS entity in memory (there is no phsyical correspondent of it)\n- From the point of view of the underlying filesystem, the inode abstracts away the files that the user would directly interact with. From the point of view of the user, the file abstracts away the inode.\n- stands for *index node*\n\n## Dentry\nThe dentry (directory entry) associates an inode with a file name","url":"https://tycholiz.github.io/Digital-Garden/notes/a96f9b82-c38a-4136-a00b-b4c0185af9c3.html","relUrl":"notes/a96f9b82-c38a-4136-a00b-b4c0185af9c3.html"},{"doc":"Daemons","title":"Daemons","hpath":"unix.daemons","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/6ae803c4-b22a-4ba9-85b4-11b046451c35.html","relUrl":"notes/6ae803c4-b22a-4ba9-85b4-11b046451c35.html"},{"doc":"Inetd","title":"Inetd","hpath":"unix.daemons.inetd","content":"\n### Internet Daemon (inetd)\n- To preserve system resources, UNIX handles many of its services through the internet daemon (inetd), as opposed to a constantly running daemon.\n- inetd is a super server that listens to the various ports and handles connection requests as it receives them by initiating a new copy of the appropriate daemon (program). The new copy of the program then takes it from there and works with the client, and instead goes back to listening to the server ports waiting for new client requests to handle. Once the request is processed and the communication is over, the daemon exits.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/22456f9d-1098-411a-afbe-d7be8991be64.html","relUrl":"notes/22456f9d-1098-411a-afbe-d7be8991be64.html"},{"doc":"CLI","title":"CLI","hpath":"unix.cli","content":"\n# Misc\nIn Unix `<C-d>` on command line specifies an EOF character to end the input.","url":"https://tycholiz.github.io/Digital-Garden/notes/cabfc7c5-837a-4edf-954c-6feb1daa90e9.html","relUrl":"notes/cabfc7c5-837a-4edf-954c-6feb1daa90e9.html"},{"doc":"Zip","title":"Zip","hpath":"unix.cli.zip","content":"\n### File Compression\n- The core idea of file compression is: the content within a file has redundancies. Why not list data once, then simply back to it when it occurs again?\n\t- ex. the JFK quote: *\"Ask not what your country can do for you -- ask what you can do for your country.\"* has 17 words, made up of 61 letters, 16 spaces, 1 `-` and 1 `.`. If each character was 1 unit of memory, that would be 79 units. If instead of storing each instance of a word in memory, we just refered the repeated words back to the first occurrence, we would cut the phrase in half.\n- In reality, the compression program doesn't look for words, but looks for patterns. Therefore, the larger the file we are compression, the more compression there will be.\n\t- This is why binary files like mp3 don't compress well — there is hardly any repetition of patterns.","url":"https://tycholiz.github.io/Digital-Garden/notes/4c7b61dc-76b2-44ae-8038-1c7b854f4c99.html","relUrl":"notes/4c7b61dc-76b2-44ae-8038-1c7b854f4c99.html"},{"doc":"Tar","title":"Tar","hpath":"unix.cli.tar","content":"\n# Tar\n\n### Compress a folder into a tarball\n`tar -czvf output.tar.gz input optional_input2`\n\n### Uncompress a tarball using a gzip compressor\n`tar xvzf file.tar.gz`\n\nSend to different directory than current\n`tar xvzf file.tar.gz -C ~/Downloads`\n\nspec: it seems that we have to cd into the directory that we will create the tar from. If we don't, then the resulting tar will take the whole absolute directory along with it, and when we uncompress it, we will get something like `/Users/kyletycholiz/tarred-file`\n- The same can definitely be said for `zip` utility","url":"https://tycholiz.github.io/Digital-Garden/notes/b2513f0d-f380-44ac-8152-caabe5fa35ce.html","relUrl":"notes/b2513f0d-f380-44ac-8152-caabe5fa35ce.html"},{"doc":"Ssh","title":"Ssh","hpath":"unix.cli.ssh","content":"\n## Config \n\nClient config (~/.ssh/config)\n[good resource with lots of info](https://gravitational.com/blog/ssh-config/)\n\n# SSH Key\n## Components\n- Each SSH key pair has 2 components: public key and private key\n- They are both created with `ssh-keygen` \n### Public key\n- Public key is copied to the SSH server so it knows it can trust the user with the corresponding private key when it tries to connect via SSH\n- Anyone that has the public key is able to encrypt the data in such a way that only the bearer of the private key can decrypt it. \n- Once an SSH server receives the public key from a user, it marks the key as trustworthy and is moved to the `authorized_keys` file.\n- `ssh-copy-id` allows us to copy an SSH key from our local machine to a server to be used as an authorized key, allowing s the SSH client to login without password\n\t- What it does is it assembles a list of one or more fingerprints, and tries to log in with each key. It then makes a list of all the keys that failed login, and enables logins with those keys on the remote server (by adding them to the remote's `authorized_keys` file).\n- Public keys are stored on SSH clients \n\n### Private key\n- Considered proof of the user's identity, and only if the private key corresponds to the public key will the user be authenticated\n- Because they are used for authentication, they are called identity keys\n- The private key is stored on the SSH server\n- When we open the private key, what we see is its encrypted form. To decrypt it, we need to enter the password that we created with `ssh-keygen`.\n\t- Therefore, without the password, the private key is useless.\n\n* * *\n\n## SSH Agent\n- ssh-agent stores unencrypted private keys in memory, allowing us to login without entering our password each time.\n\t- Therefore, ssh-agent is a form of Single-Signon (SSO)\n- Without ssh-agent, upon entering the password, the decrypted key is stored in program memory (which is associated with a process (PID)). However, the SSH client process cannot be used to store the decrypted key, since the process is terminated once the remote login session has ended. This is why we use `ssh-agent`\n- `ssh-agent` works by creating a socket then checks for connections from ssh.\n\t- anyone who is able to connect to this socket can also connect to the ssh-agent.\n\t- upon starting, the agent creates a new directory in `/tmp` that determines permissions.\n- On most Linux systems, ssh-agent is automatically configured and run at login. \n- On the server side, `PubkeyAuthentication` must be enabled in the `ssh_config` file to allow these key-based logins. \n- we can use `ssh-add` tool to add identities to the agent.\n\t- running `ssh-add` without args will add the default private keys to the agent (`~/.ssh/id_rsa`, `~/.ssh/id_dsa` etc.)\n\t- `ssh-add -l` will list out the private keys accessible to the agent\n\n### Agent Forwarding\n- Agent forwarding is a mechanism that gives an SSH server access to the SSH client's `ssh-agent`, allowing it to use it as if it were local. \n- To use agent forwarding, the `ForwardAgent` option must be set to yes on the client \n\n## Debugging\n- check client logs with `ssh -vvv <user>@<host>`\n\n- check server logs at `/var/log/auth.log`\n\n- on server, try editing `/etc/ssh/sshd_config`, changing: \n\t1. `PasswordAuthentication yes`\n\t2. `PermitRootLogin yes`\n\t- restart sshd `service sshd restart`\n\t- on client, run `ssh-copy-id -i <remote-user>@<remote-ip>`\n\t- on host, revert the `sshd_config` options to their prior state\n\n* * *\n\n## Tunneling\n- In order to tunnel, we need to set up a protocol. SSH is an example of such a protocol, where the receiver of the SSH command must have an SSH server running so that it may intercept the SSH message, and interpret it and enable the SSH connection. ","url":"https://tycholiz.github.io/Digital-Garden/notes/f9ae4e58-9235-4fc2-bf20-7c51348aa3a2.html","relUrl":"notes/f9ae4e58-9235-4fc2-bf20-7c51348aa3a2.html"},{"doc":"Sed","title":"Sed","hpath":"unix.cli.sed","content":"\nSed is a steam editor\n- here a steam can be thought of as a body of text\n- sed works by reading text line-by-line into a buffer. For each line, it performs pre-defined instructions, if applicable.\n- Sed helps us automate the same sort of tasks that we'd accomplish manually by opening a textfile and making manual, predictable edits.\n- like Vim, if we substitute without the `g` flag, then only the first occurrence on the line will substituted.\n- though `/` is the most common delimiter, we can use (almost) anything, like `|` or `:`. This is helpful if the `/` is part of the pattern we want to substitute.\n\nWord-Boundary Expression\n- use `\\b` to disallow partial-word matches\n\t- ex. we want to replace `foo` with `kyle`, but want to leave `foobar` alone: `sed -i '' 's|\\bfoo\\b|kyle|g file.txt'`\n\n![[dendron://code/unix.cli.sed.formulas]]\n\n### UE Resources\nhttps://www.brianstorti.com/enough-sed-to-be-useful/\nhttps://linuxize.com/post/how-to-use-sed-to-find-and-replace-string-in-files/","url":"https://tycholiz.github.io/Digital-Garden/notes/763aeb69-a11e-45e9-9cd5-721156101a98.html","relUrl":"notes/763aeb69-a11e-45e9-9cd5-721156101a98.html"},{"doc":"Formulas","title":"Formulas","hpath":"unix.cli.sed.formulas","content":"\n### Formulas\nFind and Replace a pattern in all files of a tree\n- find and replace all occurrences of `foo` with `bar` within a directory tree\n\t- `grep -rl 'foo' . | xargs sed -i .bk 's|foo|bar|g'`\n\t- the first part gets a list of all files that have the pattern `foo` in it, then it pipes that list into the second part, which runs the sed substitution on each file\n\t- if we don't want to create a backup, then replace .bk with ''\n\nFind and Replace a pattern in certain files\n- find and replace all occurrences of `foo` with `bar` in all .js files\n\t- `find . -name \"*.js\" -exec sed -i '' s/foo/bar/g {} +`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/327af5ac-0002-4cce-927e-7b87a068e1a5.html","relUrl":"notes/327af5ac-0002-4cce-927e-7b87a068e1a5.html"},{"doc":"Regex","title":"Regex","hpath":"unix.cli.regex","content":"\n### Types\nThe \"normal\" regex is Perl-Compatible Regex (PCRE)\n- may be contrasted with Basic Regular Expressions (BRE)\n- Vim Regex largely follows BRE\n\n#### BRE\nThe main difference between the two is that BRE tends to treat more characters as literals - an \"a\" is an \"a\", but also a \"(\" is a \"(\", not a special character - and so involves more backslashes to give them \"special\" meaning.\n\n\n- **magic** means we don't have to escape a character for it to take on its special meaning. **nomagic** means we have to escape, because otherwise the character will be taken literally\n    - ***ex.*** with magic, `.` will mean \"stand in for any character\". with nomagic, it will literally mean \"match the `.` character\"\n    - this makes sense, because with magic, a lot of cool stuff is happening that we have no idea how it's being done. Without magic, it's just looking for a character\n- `\\V` Verbatim Switch - only `\\` has special meaning (ie. very nomagic)\n    - prevent regex symbols from taking over - `/\\Va.k.a` (equivalent to `/a\\.k\\.a\\.`)\n    - in regex, `.` means \"match any character\". making a verbatim search removes that functionality\n    - when we use `\\V`, it means for the following search, only `\\` will have a special meaning\n- `\\v` Literal Switch - all characters (except word characters (**[a-zA-Z_]**)) have special meaning (ie. very magic)\n- delimit words - since the word \"the\" appears in \"these\", if we wanted to just search for the word \"the\", we can use delimiters\n    - `/\\v<the>`\n- ignore case - `/search\\c`\n- enforce case - `/search\\C`\n- Anything that matches inside of parentheses is automatically assigned to a temporary silo.\n    - we can reference these matches in any later commands, such as a `:%S` search and replace command\n    - We can reference the captured text within the first set of parens as `\\1` (and `\\2` for the second set of parens, and so on) \n    - `\\0` refers to the entire match, whether or not there were parens\n    - tip: use `%` to not capture the following parens\n        - imagine we want to find and replace all occurrences of a first and last name, then replace it by the format `LAST, FIRST`. Notice the where we use and omit the `%` in order to control which matches are going to the `\\1` and `\\2` registers. Here, we don't care whether \"Drew\" or \"Andrew\" was matched, so we don't bother registering it.\n            - `/\\v(%(And|D)rew) (Neil)`\n            - `:%s//\\2, \\1/g`\n\n### Character Classes (make the letter capital to negate the character (ex. `\\S` for non-whitespace char)\n- whitespace char - `\\s`\n- digit - `\\d`\n- hex-digit - `\\x`\n- word character (alphanumeric + underscore) - `\\w`\n- alphabetic char - `\\a`\n- lowercase char - `\\l`\n- uppercase char - `\\u`\n\n# Word boundaries\n- `pattern` - the regex text that we type into the search field\n- `match` - any text in the document that is highlighted as a result of the search\n- `<`/`>` - match the beginning/end of a word\n\n## Lookaheads/Lookbehinds\n- while the boundaries of a match normally correspond with the start and end of a pattern, we can use `\\zs` and `\\ze` to crop the beginning and end of a match, making the new match a subset of the pattern\n    - ex. search for matches of \"eagle\", but only when it follows the word \"bald\"\n        - `/bald \\zseagle`()\n        - In this example, \"bald eagle\" still forms part of the pattern, but only the word \"eagle\" is matched\n    - ex. search for everything inside quotes, without the quotes themselves\n        - `/\\v\"\\zs[^\"]+\\ze\"`\n\n### Backreference\n- allow you to match a particular pattern, then use a variable that refers back\n    to it\n    - Say we wanted to match an html tag and its content. Since there is\n        repetition in an opening and closing tag (`<div>`/`</div>`), we can use\n        a back reference to capture `div` in a variable, in order to use it\n        later in the regex pattern\n- a related, but distinct term.\n\n# Fine slicing\n#### Character Set\n- match any character specified - `[a-zA-Z0-9]`\n- match either an `f` or an `o` - `(f|o)`\n- search for any 6 of the preceding characters - `{6}`\n    - ex. `/\\v#([a-fA-F0-9]{6})` will match any hex code\n- search for between 4 and 6 matches of the preceding character - `{6,8}`\n- search for at least 6 matches of the preceding character - `{6,}`\n    - In other words, leave upper range unbounded\n- negate the following search - `^`\n- multiple occurrences of the same character\n    - 0 or 1 times (ie. it is optional) -s `?`\n    - 0 or more times - `*`\n    - 1 or more - `+`\n- match any character - `.`\n- match any character 0 or more times - `.*`\n    - doesn't have to be the same character many times to work\n        - ex. `.*dog` matches `yyydog` and `yfwdog`\n\n# Useful Regex Patterns\n- `ag '\\{\\s.*ListView.*from\\s.react-native.'` - search for a non-default\n    exported module from a specific module\n","url":"https://tycholiz.github.io/Digital-Garden/notes/887233a2-8653-46ef-adce-2e482007942b.html","relUrl":"notes/887233a2-8653-46ef-adce-2e482007942b.html"},{"doc":"Less","title":"Less","hpath":"unix.cli.less","content":"\n- `-S<CR>` - enable horizontal scrolling","url":"https://tycholiz.github.io/Digital-Garden/notes/4631641b-6d6a-4c08-b33f-3f7f74e365c8.html","relUrl":"notes/4631641b-6d6a-4c08-b33f-3f7f74e365c8.html"},{"doc":"Kill","title":"Kill","hpath":"unix.cli.kill","content":"\n- Generally, you should use kill (short for kill -s TERM, or on most systems kill -15) before kill -9 (kill -s KILL) to give the target process a chance to clean up fter itself. (Processes can't catch or ignore SIGKILL, but they can and often do catch SIGTERM.) If you don't give the process a chance to finish what it's doing and clean up, it may leave corrupted files (or other state) around that it won't be able to understand once restarted.\n\n- Generally, you should use kill (short for kill -s TERM, or on most systems kill -15) before kill -9 (kill -s KILL) to give the target process a chance to clean up fter itself. (Processes can't catch or ignore SIGKILL, but they can and often do catch SIGTERM.) If you don't give the process a chance to finish what it's doing and clean up, it may leave corrupted files (or other state) around that it won't be able to understand once restarted.\n\t- strace/truss, ltrace and gdb are generally good ideas for looking at why a stuck process is stuck. (truss -u on Solaris is particularly helpful; I find ltrace too often presents arguments to library calls in an unusable format.) Solaris also has useful /proc-based tools, some of which have been ported to Linux. (pstack is often helpful).\n\n# SIGTERM vs SIGKILL\n- SIGKILL is a higher process killing a lower one, such as a kernel shutting down one of the applications that is currently running\n- SIGTERM is a request to the runner of the application to shut itself down.\n- analogy: we humans are the running of the application, and there is boss above us, giving us commands. SIGTERM would be the boss telling us to shut it down, which we then carry out. This is better, because I have information about what I'm currently working on that will allow me to shut down my process gracefully (ie. without losing data). Sometimes the runner of the application is not in a state where they can shut themselves down. SIGKILL on the other hand is like the boss coming by and forcefully shutting down my computer. The SIGTERM never knew about it, so it's possible something got screwed up while the runner (us) was in the middle of working on something.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d05920a6-e26b-418f-b641-06175efc161a.html","relUrl":"notes/d05920a6-e26b-418f-b641-06175efc161a.html"},{"doc":"Grep","title":"Grep","hpath":"unix.cli.grep","content":"\n`-l` - only print out the filenames that have the pattern\n`-r` - recurse, allowing us to run grep on directories\n\n- grep for lines with *pattern1* while filtering out lines with *pattern2* from *file.txt*\n    - `grep pattern1 file.txt | grep -v pattern2`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/0200d7dc-b68f-4aba-b6da-295025388405.html","relUrl":"notes/0200d7dc-b68f-4aba-b6da-295025388405.html"},{"doc":"Fzf","title":"Fzf","hpath":"unix.cli.fzf","content":"\n# FZF\nFZF afts like an interactive filter. The responsibility of the program is not to get us the list of occurrences of our search pattern; that is the job of the search tool we use (eg. grep, ack, ag, find). FZF's only job is to take the list of occurrences as input, and perform the fuzzy searching logic on that list.\n- FZF's default search program is `find`, but it can be changed with the `FZF_DEFAULT_COMMAND` env variable.\n- Silver Searcher is good, because it respects the `.gitignore` and `.ignore` files.","url":"https://tycholiz.github.io/Digital-Garden/notes/727339f7-0479-4075-b65f-a03b552084cd.html","relUrl":"notes/727339f7-0479-4075-b65f-a03b552084cd.html"},{"doc":"Curl","title":"Curl","hpath":"unix.cli.curl","content":"\n## HTTP Requests\n\n#### Post\n```\ncurl -X POST [options] [URL]\n```\n\nexample\n```sh\ncurl -X POST -H \"Content-Type: application/json\" \\\n    -d '{\"email\": \"linuxize@example.com\", \"password\": \"123\"}' \\\n    http://localhost:5678/login\n```\n\n#### Setting return value of curl to variable\n```\nhttp_code=$(curl -s -o /dev/null -w \"%{http_code}\" https://www.google.com)\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/98005032-25c1-4f9a-ac17-a89fee0fe8d9.html","relUrl":"notes/98005032-25c1-4f9a-ac17-a89fee0fe8d9.html"},{"doc":"Cron","title":"Cron","hpath":"unix.cli.cron","content":"\nSee all active Crontabs:\n`crontab -e`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1ebd16f8-e4d6-47c2-a543-fc4e554338d2.html","relUrl":"notes/1ebd16f8-e4d6-47c2-a543-fc4e554338d2.html"},{"doc":"Ag","title":"Ag","hpath":"unix.cli.ag","content":"\n# Patterns\n- `ack <search-string> ../src`\n\n# Flags\n- `-i` - ignore case\n- `-w` - only match whole words\n- `-1` - stop searching after 1 match found\n- `-Q` - treat all characters as literal\n    - so if we use `/w` in the pattern, it is taken literally and will not mean \"match word only\"\n\n# Types\n- `ack --react <PATTERN>` - search all files with type react (js, jsx)\n- `--help-types` - see all types defined\n- `ack 'my pattern' ./src` - search for *my pattern* in *src* directory\n\n# Regex (Ag)\n- Ag is magic by default, meaning if we want to match literal characters, we need to escape them. all letters will be literal, and will need to be escaped to get their special meaning (ex. `\\d`, `/s`)\n    - ex. ag '.*\\ddog' - will match `sdfhj5dog` (anything any number of time,\n\tfollowed by a number, followed by 'dog')\n- see `man pcre2pattern` for regex flavors\n\n\nAll devDependencies should be at the root\neverything we need to build the proejct is at the root","url":"https://tycholiz.github.io/Digital-Garden/notes/5988dd52-7a0f-480f-aea8-9061115db015.html","relUrl":"notes/5988dd52-7a0f-480f-aea8-9061115db015.html"},{"doc":"TypeScript","title":"TypeScript","hpath":"typescript","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f3813a1e-67df-4fdd-891f-bc4e16e3de1d.html","relUrl":"notes/f3813a1e-67df-4fdd-891f-bc4e16e3de1d.html"},{"doc":"Types","title":"Types","hpath":"typescript.types","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f9e68623-fa06-463e-b72e-0cc44c4ddb21.html","relUrl":"notes/f9e68623-fa06-463e-b72e-0cc44c4ddb21.html"},{"doc":"Util","title":"Util","hpath":"typescript.types.util","content":"\nUtility types allow us to construct a new type, taking an initial type, and modifying it some way (this \"way\" is determined by which utility type we construct with","url":"https://tycholiz.github.io/Digital-Garden/notes/102f3803-35c1-4bb7-aaca-9094aa200542.html","relUrl":"notes/102f3803-35c1-4bb7-aaca-9094aa200542.html"},{"doc":"Pick","title":"Pick","hpath":"typescript.types.util.pick","content":"\nIn the below example, we are creating a new type `TypePreview` based on a subset of fields available on the `Todo` type.\n```ts\ninterface Todo {\n  title: string;\n  description: string;\n  completed: boolean;\n}\n\ntype TodoPreview = Pick<Todo, \"title\" | \"completed\">;\n\nconst todo: TodoPreview = {\n  title: \"Clean room\",\n  completed: false,\n};\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/5bea9398-daa5-4187-9a52-e6a7f40f5334.html","relUrl":"notes/5bea9398-daa5-4187-9a52-e6a7f40f5334.html"},{"doc":"Partial","title":"Partial","hpath":"typescript.types.util.partial","content":"\nA Partial type is derived from an existing type. The only difference between the Partial type and the original type is that the Partial type has all properties set to optional.\n\nExample:\n```ts\ninterface Todo {\n\timportance: number;\n\ttext: string;\n}\n\nfunction updateTodo(todo: Todo, fieldsToUpdate: Partial<Todo>) {\n  return {\n\t\t...todo,\n\t\t...fieldsToUpdate,\n\t};\n}\n```\nAbove, the resulting Partial type would be like this:\n```ts\ninterface PartialTodo {\n\timportance?: number,\n\ttext?: string,\n}\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a1bb6fd0-ffb0-4725-819d-dd372c67b96e.html","relUrl":"notes/a1bb6fd0-ffb0-4725-819d-dd372c67b96e.html"},{"doc":"Assertion","title":"Assertion","hpath":"typescript.types.assertion","content":"\nType assertions helps you to force types when you are not in control of them. For e.g. -\n1. you are processing user data (with unreliable types)\n2. working with data that has changed shape over years (employee code was numeric, now it is alphanumeric :))\n3. receiving data from an external program\n\nType assertions let the Typescript compiler know that a given variable should be treated as belonging to a certain type. There are no “exceptions” or data restructuring associated with assertions, except minimal validations (we refer this behaviour as “validations that are applied statically”).\n\nThere are two ways to do type assertions.\n1. Bracket syntax. e.g. `let length: number = (<string>lengthField);`\n2. Use as. e.g. `let length: number = lengthField as string;`\nThere is no difference between the two ways and the end result is always the same. Note that if you are using JSX you have to use `as` syntax.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/84347358-44ef-47bb-80f9-228a345cfd56.html","relUrl":"notes/84347358-44ef-47bb-80f9-228a345cfd56.html"},{"doc":"Overloading","title":"Overloading","hpath":"typescript.overloading","content":"\nTypescript supports [[overloading|paradigm.oop.overloading]]\n\nExample:\nStripe has 2 different versions of `stripe.paymentIntents.list`. The first version looks like this:\n```\n(params?: PaymentIntentListParams, options?: RequestOptions): ApiListPromise<PaymentIntent>\n```\n\nThe second looks like this:\n```\n(options?: RequestOptions): ApiListPromise<PaymentIntent>\n```\n\nThe benefit to doing this is that we can call the same function, even if we don't want to pass in a `params` object. When we call `stripe.paymentIntents.list`, the TS compiler will figure out which version of the function it should call, based on the arguments that are passed to the function call. This process is what is known as Method overloading.\n- If we call `stripe.paymentIntents.list` and our arguments don't line up with the first function signature (ex. because one of the args we pass doesn't line up with `PaymentIntentListParams` interface), then TS will attempt to use the second version of the function. This will result in Intellisense spitting both errors at us, even though only the first one is what we should pay attention to","url":"https://tycholiz.github.io/Digital-Garden/notes/b99b86bf-e918-408b-8ed6-2feff96044b0.html","relUrl":"notes/b99b86bf-e918-408b-8ed6-2feff96044b0.html"},{"doc":"Declaration Merging","title":"Declaration Merging","hpath":"typescript.declaration-merging","content":"\n## Overview\nA fundamental concept in TS is the language's ability to describe the shape of javascript objects at the type level.\n- One example of the implementation of this concept is the idea of **Declaration Merging**\n\nIn TS, a declaration creates entities in at least one of three groups: namespace, type, or value.\n- Namespace-creating declarations create a namespace, which contains names that are accessed using a dotted notation.\n- Type-creating declarations do just that: they create a type that is visible with the declared shape and bound to the given name.\n- Value-creating declarations create values that are visible in the output JavaScript.\n![](/assets/images/2021-04-04-13-05-32.png)\n\n## Interface merging\nthe merge mechanically joins the members of both declarations into a single interface with the same name.\n```ts\ninterface Box {\n  height: number;\n  width: number;\n}\ninterface Box {\n  scale: number;\n}\nlet box: Box = { height: 5, width: 6, scale: 10 };\n```\n\nIf 2 objects have a function member with the same name but different signatures, then they will be [[overloaded|paradigm.oop.overloading]], and both functions will appear on the merged object:\n```ts\ninterface Cloner {\n  clone(animal: Animal): Animal;\n}\ninterface Cloner {\n  clone(animal: Sheep): Sheep;\n}\n\n// The two interfaces merged will create a single declaration:\ninterface Cloner {\n  clone(animal: Animal): Animal;\n  clone(animal: Sheep): Sheep;\n}","url":"https://tycholiz.github.io/Digital-Garden/notes/e4e9f001-3c11-4deb-84e4-b568908cd22c.html","relUrl":"notes/e4e9f001-3c11-4deb-84e4-b568908cd22c.html"},{"doc":"Declaration File","title":"Declaration File","hpath":"typescript.declaration-file","content":"\nTypeScript is a superset of JavaScript, meaning you can write and use JavaScript libraries from within TypeScript. In such situations how should TypeScript handle the lack of Type information in JavaScript? We can either:\n- Accept the lack of types from the JS file\n- use Declaration files, which give us an ad-hoc way to specify the shape of things in Javascript.\n\n`.d.ts` files are analogous to C++ header files.\n\n### Incorporating type declarations from third parties\nWhen we want to use Javascript 3rd party libraries in our TS project, we can often find declaration modules as packages\n- for example, `@types/lodash` is a module that gives us types out of the box with Lodash.\n- these declaration files get saved in `node_modules/@types/`\n\nWhen a type declaration is included in the `@types` directory, the types are automatically available for us to use in our project, as the TS compiler finds these types and makes them available during compilation time.\n\n[DefinitelyTyped](http://definitelytyped.org/) is a project that hosts declaration files for popular JS libraries.\n\n### Config\n\"typeRoots\" and \"types\" are the two properties of the tsconfig.json file that can be used to configure the behavior of the type declaration resolution.\n\n### Example\nImagine we had a Javascript file in a Typescript project:\n```js\n// main.ts\nlet ajala\n\najala = {\n name: \"Ajala the traveller\",\n age: 12,\n getName: function() {\n     return this.name;\n   }\n};\n\najala.lol()\n```\nWhen we compile this file with `tsc main.ts`, the compilation would be successful, and `main.js` is outputted.\n\nIf we then run `node main.js`, we will get a runtime error that `ajala.lol is not a function`. Since no type information was available for it, Typescript could not have warned us about this at compile time.\n\nTo fix this, we can create a file `main.d.ts`:\n```ts\ndeclare module \"MyTypes\" {\n\texport interface Person {\n\t\tname: string;\n\t\tage: number;\n\t\tgetName(): string;\n\t}\n}\n```\n\nAnd then add a triple-slash directive and type annotation in `main.ts`\n- note: this triple-slash directive is possible, but not recommended. Instead, just install npm type declaration modules (below)\n```ts\n/// <reference path=\"Main.d.ts\" />\n\nimport * as MyTypes from \"MyTypes\"\n\nlet ajala: MyTypes.Person\n\najala = {\n\tname: \"Ajala the traveller\",\n\tage: 12,\n\tgetName: function() {\n\t\treturn this.name;\n\t}\n};\n\najala.lol();\n```\nWith the declaration file provided and included via the triple-slash directive, the TS compiler now has information about the shape of `Person`, and will throw us an error at compile time.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/faefbe49-d422-4ada-aace-da94ff48b6d3.html","relUrl":"notes/faefbe49-d422-4ada-aace-da94ff48b6d3.html"},{"doc":"Tmux","title":"Tmux","hpath":"tmux","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1a6173cd-cf13-4b34-a522-8350bf9a364f.html","relUrl":"notes/1a6173cd-cf13-4b34-a522-8350bf9a364f.html"},{"doc":"Cmds","title":"Cmds","hpath":"tmux.cmds","content":"\n# Command line commands\nnote: all prefixed with prefix `<C-a>`, except where noted\n- `?` see list of available commands\n\n# Sessions\n- `d` - detach\n- `$` - rename session\n- `(`/`)` - swap between sessions\n- `a` - attach\n    - `-t 0` - attach to \"session 0\"\n- `:new` - new session\n    - `-s` - add a name to the session\n- `tmux new -s <session-name>`\n- `$` - rename session\n- `s` - list sessions\n- `:kill-ses -t <session-name>`\n- `:attach -c /path/to/default/directory` - change default directory of new windows\n\n# Windows\n- `c` - create new window\n- `1` - go to window 1\n- `I` - go to last window\n- `<C-h>`/`<C-l>` - swap between windows\n- `&` - close window\n\n# Panes\n- `shift + |` - split vertically\n- `shift + _` - split horizontally\n- `x` - close pane\n- `;` - last active pane\n- `{`/`}` - move current pane left/right\n- `z` - pane zoom (cmd+shift+return in iTerm2)\n- `<`/`>` - swap/rotate pane locations\n- `<space>` - toggle between horitontal and vertical split\n- `!` - break current pane out of window\n- `H`/`J`/`K`/`L` - resize pane\n\n# Plugins\n- `I` - fetch new plugins (like PlugInstall)\n- `U` - update plugins\n- `opt+u` - uninstall plugins not on list\n\n# Integration shortcuts\n- `p` - paste from clipboard\n    - `P` - choose which buffer to paste from\n\n* * *\n\n## Modes\n### Copy Mode (no prefix)\n- enter copy mode - prefix + `[`\n- move up/down a paragraph - `{`/`}`\n- go to middle - `M`\n- copy until end of line - `D`\n\n### Normal\n- paste from buffer - `]`\n- send the contents of the current buffer to a temp file - `P`\n\t- custom bind\n- open current pane in vim - `v`\n- If you want to switch to a window based on something displayed in it (this also includes window names and titles but not history), (starting with more than one window open) press Ctrl-b f then type the string to search for and press Enter. You will be switched to a window containing that text if it's found. If more than one window matches, you'll see a list to select from.\n\n* * *\n\n### Other\n- edit tmux.conf - `e`\n\n# Resources\nhttp://tmuxp.git-pull.com/en/latest/examples.html\nhttps://www.barbarianmeetscoding.com/blog/2019/12/25/jaimes-guide-to-tmux-the-most-awesome-tool-you-didnt-know-you-needed","url":"https://tycholiz.github.io/Digital-Garden/notes/a8133cd2-34aa-4a4e-b56f-021463739748.html","relUrl":"notes/a8133cd2-34aa-4a4e-b56f-021463739748.html"},{"doc":"Testing","title":"Testing","hpath":"testing","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/84143a1b-aa79-40ff-bcae-6d9858fec127.html","relUrl":"notes/84143a1b-aa79-40ff-bcae-6d9858fec127.html"},{"doc":"E2e","title":"E2e","hpath":"testing.e2e","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/7984eb8a-7958-468c-9f43-67baa1366f82.html","relUrl":"notes/7984eb8a-7958-468c-9f43-67baa1366f82.html"},{"doc":"Tools","title":"Tools","hpath":"testing.e2e.tools","content":"\n## Frameworks\n[Strong recommendation from Juam](https://playwright.dev/)","url":"https://tycholiz.github.io/Digital-Garden/notes/657d101f-0923-432a-adc2-091f7d40acd0.html","relUrl":"notes/657d101f-0923-432a-adc2-091f7d40acd0.html"},{"doc":"Tcp","title":"Tcp","hpath":"tcp","content":"\n- TCP transports application-layer messages such as HTTP requests.\n- Works by establishing a connection between 2 hosts.\n- Data is guaranteed to be delivered, and in the order it was sent.\n- TCP breaks long messages in to shorter segments.","url":"https://tycholiz.github.io/Digital-Garden/notes/17f7fe87-cbc5-4276-8bc6-a7083c936de3.html","relUrl":"notes/17f7fe87-cbc5-4276-8bc6-a7083c936de3.html"},{"doc":"Stripe","title":"Stripe","hpath":"stripe","content":"\n## Misc\nwith (spec) stripe elements, we can use the `stripe_code` on the client as a way to get the credit card information (presaved) into the UI.\n\nBy default, anyone can POST data to our webhook. This is why we have the webhook secret to protect us, and only react to the POST request once we have verified that it is coming from Stripe\n- each webhook secret is associated with a single webhook endpoint\n\n### Credit Cards Internationally\nRegions like Europe and India require our application to handle requests from banks to authenticate a purchase (3DS or OTP)\n- If we are using webhooks, then the authentication is already handled and we don't have to worry about this.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/b28d278d-4fa0-4eac-9d26-dd6b10eb169d.html","relUrl":"notes/b28d278d-4fa0-4eac-9d26-dd6b10eb169d.html"},{"doc":"Integration","title":"Integration","hpath":"stripe.integration","content":"\n## Integrating Stripe\nThere are different ways to integrate Stripe into your application.\n\n### Only Client-side handling\nStripe allows you to integrate a Checkout mechanism, where you only need to implement client code to perform payments.\n\nHere is a reference for one time payments with only client-side code: https://stripe.com/docs/payments/checkout/client\n\n### Client-side and Server-side\nBut there are sometimes usecase where client and server need to communicate with each other before/while a payment process (https://stripe.com/docs/payments/integration-builder). For example if you don't manage your inventory with Stripe you will need some own logic on the server. One reason would be that you just want to offer Stripe just as a payment gateway for credit cards besides other payment methods like PayPal.\n\n### Webhooks\nWell webhooks can be used in both cases. Webhooks allow you to let Stripe communicate with your backend server to inform you about succeeded payment, failed payments, customer updates, orders, billings and so on. By defining a webhook URL you can specify which events you want to receive from Stripe. You can then use events to update specified data in your database.","url":"https://tycholiz.github.io/Digital-Garden/notes/a00c7155-72ff-492a-adcf-7f2e76c85983.html","relUrl":"notes/a00c7155-72ff-492a-adcf-7f2e76c85983.html"},{"doc":"CLI","title":"CLI","hpath":"stripe.cli","content":"\n# CLI\n## listen\n- from the CLI, we can listen for events that happen on the stripe API, and have those events forwarded to us.\n- by default, `listen` will listen to the events happening on our live configuration (endpoint), found in Stripe dashboard. If we want to test, then we can listen for events happening at a different endpoint, with `stripe listen --forward-to localhost:5000/webhook`\n\n## trigger\n- from the CLI, we can fake an event happening (ie. fake a call to Stripe's API)\n- if we are listening in another terminal, then we should see the fakes command being listened to.\n\n## payment intents\n### create\n```\nstripe payment_intents create \\\n  --amount=2000 \\\n  --currency=cad \\\n  -d \"payment_method_types[]\"=card\n```\n[ref](https://stripe.com/docs/api/payment_intents/create)","url":"https://tycholiz.github.io/Digital-Garden/notes/eeaada29-0cad-41dd-bf59-602a3a0b5183.html","relUrl":"notes/eeaada29-0cad-41dd-bf59-602a3a0b5183.html"},{"doc":"API","title":"API","hpath":"stripe.api","content":"\n# API Keys\n## Keys\nSecret key is used on server side code and publishable key is to be used on the client side to associate with the Stripe account.","url":"https://tycholiz.github.io/Digital-Garden/notes/b97c8a88-2a3e-40cb-b5b6-bcb534d5d48b.html","relUrl":"notes/b97c8a88-2a3e-40cb-b5b6-bcb534d5d48b.html"},{"doc":"Setup Intent","title":"Setup Intent","hpath":"stripe.api.setup-intent","content":"\n## Setup Intent\n- A SetupIntent guides you through the process of setting up and saving a customer's payment credentials for future payments.\n- If the SetupIntent is used with a Customer, upon success, it will automatically attach the resulting payment method to that Customer.\n- if we wanted to save a credit card at the time of purchase, then we can add the `setup_future_intent` at the time of creating a PaymentIntent. This negates our need to use the SetupIntent. spec: Therefore, the SetupIntent is needed only when saving a payment for later, but not making a purchase at the time (like when someone click \"add payment method\".\n- Creating a SetupIntent will generate a PaymentMethod to be attached to a customer, which can then be used to create a PaymentIntent when you are ready to charge them.","url":"https://tycholiz.github.io/Digital-Garden/notes/f97a4312-072b-4fb5-afef-86a57f1afdfb.html","relUrl":"notes/f97a4312-072b-4fb5-afef-86a57f1afdfb.html"},{"doc":"Payment Method","title":"Payment Method","hpath":"stripe.api.payment-method","content":"\n## Payment Method\n- represent the customer's payment instruments\n- they can be either:\n\t1. included as part of the payment intent when we are creating (so we know which credit card to use)\n\t2. saved to a stripe customer object for future payments","url":"https://tycholiz.github.io/Digital-Garden/notes/ece01b51-c37f-403e-9760-ea490ca7f2c0.html","relUrl":"notes/ece01b51-c37f-403e-9760-ea490ca7f2c0.html"},{"doc":"Payment Intent","title":"Payment Intent","hpath":"stripe.api.payment-intent","content":"\n## Payment Intent\nThere should be exactly one payment_intent for each order or customer session so we can reference the PaymentIntent later to see the history of payment attempts for a particular session.\n- The PaymentIntent API tracks a payment, from initial creation through the entire checkout process, and triggers additional authentication steps when required. Therefore, a payment intent has different states during its lifetime.\n\n- The best practice is to create a PaymentIntent as soon as the purchase amount is known\n\t- ex. this might be when the customer begins the checkout process. This would be fine, since we can always update the amount if the user backs out, adds another item to their cart, then begins checkout again\n- We should store the `PaymentIntentID` in our database, to be associated with the customer's shopping cart (order). If this isn't feasible, then we can store it on their session.\n\t- This would allow us to track and failed payment attempts for a given cart (or session)\n- We should provide an `idempotency key` to the PaymentIntent\n\t- Doing this will help us avoid creating duplicate PaymentIntents for the same purchase\n\t- this key would be an ID associated with the cart/session, which we store in the database\n- The PaymentIntent contains a client secret key which is unique to the individual PaymentIntent. In other words, a client secret uniquely identifies a payment intent.\n\t- This client secret is used as a parameter when calling `stripe.confirmCardPayment` on the client.\n\t- We retrieve the client secret from the PaymentIntent on our server, which we then can pass to the client. There are different approaches to getting the client secret to the client side.\n- The client secret of a paymentintent is dependent on the paymentintentid.\n\t- An example of the secret is `pi_1IR0JiKQXfStDK4vk3nikCNH_secret_GAWuc4MSiWoezicYMqMe5qgWj`, showing that the client secret would change if the payment intent was different.\n- If the customer leaves the CheckoutSession incomplete, it will expire and cancel the PaymentIntent automatically after 24 hours.","url":"https://tycholiz.github.io/Digital-Garden/notes/a844369b-1433-4eff-a596-fafc1536c9ec.html","relUrl":"notes/a844369b-1433-4eff-a596-fafc1536c9ec.html"},{"doc":"Storage","title":"Storage","hpath":"storage","content":"\n# Storage\nalmost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away\n- Generally the fast volatile technologies (which lose data when off power) are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".\n\n## Levels\n### Registers (L0)\n- registers hold words from cache memory\n### On-chip cache (L1)\n- L1 cache holds cache lines retrieved from the L2 cache\n- ex. SRAM\n### Off-chip cache (L2)\n- L2 cache holds cache lines retrieved from memory\n- ex. SRAM\n### Main memory (L3)\n- Main memory holds disk blocks retrieved from local disks\n- ex. DRAM\n### Local secondary storage (L4)\n- local disks hold files retrieved from disks on remote network servers\n- ex. local disk\n### Remote secondary storage (L5)\n- ex. distributed filesystems, web servers\n\n* * *\n\n### Storage on a CD\n- the data on a CD is stored in an outward spiral pattern. The CD port on the computer has motor which rotates the CD. It also has a laser arm which follows the spiral, as the disk spins.\n- the spiral track has little bumps along it. The bumps are used to encode the binary value. The laser arm in the CD port has a sensor attached to it, which \"listens for\" the reflection of the laser. If the sensor receives the light, an electrical signal is generated and sent to the microprocessor, which interprets it as a `1`\n![](/assets/images/2021-03-09-21-22-35.png)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/fee6c25b-8ea8-45bf-89d5-8c6d036b03b6.html","relUrl":"notes/fee6c25b-8ea8-45bf-89d5-8c6d036b03b6.html"},{"doc":"Methods","title":"Methods","hpath":"storage.methods","content":"\n# Methods\n\n## RAID (Redundant Array of Independent Disks) \n- A RAID is simply multiple physical hard drives acting as a single virtual hard drive\n- The two main reasons are to safeguard your data with some sort of backup/failsafe, or to speed up reading/writing.\n\t- When data is read, it is read from both hard drives and checked to make sure they agree.\n- There are 3 main types (with example of storing a song):\n\t1. *RAID-0* - the song is stored across multiple hard drives. Therefore, if one fails then the file is corrupted \n\t\t- Least safe\n\t2. *RAID-1* - all hard drives have the same data. Therefore, if one fails, the redundancy provided by the second will allow us to retain all of the data.\n\t\t- RAID-1 is where disk mirroring is most commonly used\n\t4. *RAID-5* - the song is stored across multiple hard drives (like RAID-0), but each hard drive will have its own backup. If one hard drive fails, the backup is restored so we don't get a corrupted file. \n\t\t- This is the safest, but also more expensive and a bit slower.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/027316ff-3de0-4edb-9549-e2a0541a3f7e.html","relUrl":"notes/027316ff-3de0-4edb-9549-e2a0541a3f7e.html"},{"doc":"Formats","title":"Formats","hpath":"storage.formats","content":"\n# Storage Formats \nThere are 3 main ways (formats) that data can be stored digitally: \n1. file storage \n2. block storage \n3. object storage\n\n## File storage\n- organizes and represents data as a hierarchy of files in folders\n- ex. Unix, NAS\n- Since a system must be the thing that implements a filesystem, data stored in this way is not easily portable \n\t- ex. using Windows FS and trying to open it on Mac will prove difficult\n\n## Block storage\n- chunks data into arbitrarily organized, evenly sized blocks. Each block is given an ID, making it easily retrievable from storage.\n\t- it takes several blocks to make up one file.\n- Data can be retrieved fast because it doesn't rely on a single path to find the data.\n- the more data you need to store, the better off you’ll be with block storage.\n- ex. Storage Area Network (SAN)\n\n## Object storage\n- manages data and links it to associated metadata.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/2511325e-f162-4d34-8281-b3e44c1bf79d.html","relUrl":"notes/2511325e-f162-4d34-8281-b3e44c1bf79d.html"},{"doc":"Backups","title":"Backups","hpath":"storage.backups","content":"\n# Backups\n- 3-2-1 rule of backups: 3 copies, 2+ formats, at least one copy offsite.\n- If something is built into a software workflow, then it is not a backup\n\t- ex. Joplin sync does not count as backup, nor does Mackup. The reason is that we interact with these tools in a dynmaic way. We don't treat them as immutable snapshots. A backup is only a backup if it's immutable (or treated as such).\n- If you haven't yet tried to restore all of your data via your backup file, then you don't have a backup system in place.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/871f52d1-f6bc-4dcf-9681-7b9142ca723c.html","relUrl":"notes/871f52d1-f6bc-4dcf-9681-7b9142ca723c.html"},{"doc":"Sqlite","title":"Sqlite","hpath":"sqlite","content":"\n- show tables - `.tables`\n- show databases - `.databases`\n- open database - `.open ~/.config/joplin/database.sqlite`\n- show signature of table - `.shema <tablename>`\n\t- more readable form - `pragma table_info('<tablename>')`\n\n### Migrations\nThe sqlite package includes a database migrations feature","url":"https://tycholiz.github.io/Digital-Garden/notes/e9bf14ec-5801-4be8-ba1e-3034dc742fa6.html","relUrl":"notes/e9bf14ec-5801-4be8-ba1e-3034dc742fa6.html"},{"doc":"Sql","title":"Sql","hpath":"sql","content":"\n- SQL is a declarative language — we declare the result we want to obtain in terms of a data processing pipeline that is executed against a known database model and a dataset.\n- related data can be modeled as a graph and can thus power a GraphQL API. - This is precisely what the GraphQL Engine does.\n\n## Parts of SQL\nAll SQL can be broken down into either DDL or DML\n- both DDL and DML have their own CRUD operations\n\n### DDL (data definition language)\n- used to define data structures (working with the structure of the data)\n    - associated with creating tables, defining relationships (primary, foreign and composite keys, for example) and data types, constraints, etc, and also for modifying them.\n- ex. `CREATE`, `ALTER`, `DROP`\n\n### DML (data manipulation language)\n- used to manipulate data (working with the data itself)\n- ex. `INSERT`, `UPDATE`, `DELETE`\n\n# E Resource\n[quality tutorial (current progress)](https://pgexercises.com/questions/joins/self2.html)\n[quality tips](https://blog.jooq.org/2016/03/17/10-easy-steps-to-a-complete-understanding-of-sql/)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f9503aa6-bceb-4b11-89cd-d7e6f50d5cb0.html","relUrl":"notes/f9503aa6-bceb-4b11-89cd-d7e6f50d5cb0.html"},{"doc":"Views","title":"Views","hpath":"sql.views","content":"\n# View\n- A view is like a derived table, but instead of it only living within the context of a single query, it exists more in perpetuity \n\t- in other words, it is the result set of a stored query\n\t- users of the database can query the view just as they would query any other table\n- while a view is a virtualized table that represents the result of a particular db query, when a query is made against a view, that query is converted into a query that can be made against the base table (in other words, the underlying base table(s) that the view is based off of)\n\t- *materialized view* - in contrast to the way a view is queried, a materialized view actually takes up storage space, making it a proper table (and not just a conceptual table). When you query a materialized view, the result set is cached and stored (or, materialized) as a sort of pseudo table \n\t\t- can be thought of as a snapshot of a query set\n\t\t- makes queries more efficient at the expense of space  \n","url":"https://tycholiz.github.io/Digital-Garden/notes/c68d9bcb-203e-4703-89c1-be6f2228a261.html","relUrl":"notes/c68d9bcb-203e-4703-89c1-be6f2228a261.html"},{"doc":"Terms","title":"Terms","hpath":"sql.terms","content":"\n# SQL Term Map\n- relation -> table\n- attribute -> column\n- tuple -> row\n![](/assets/images/2021-03-09-17-13-13.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/9cdd3e44-2753-43b4-abcd-1bde111deee1.html","relUrl":"notes/9cdd3e44-2753-43b4-abcd-1bde111deee1.html"},{"doc":"Relation","title":"Relation","hpath":"sql.terms.relation","content":"\n# What is relational?\nA relation is a set of tuples (fixed length array)\n- in other words, a single row in a single database is a relation between all the values that represent that row\n- in SQL, a relation is represented by a table, with each row being a tuple\n- a tuple is described as a function, since it maps names to values.\n\t- ex. for row 1, name -> \"Kyle\"\n- each element (cell) of the tuple is termed an `attribute value`\n\t- an attribute is the combination of the column name and its type\n\t\t- ex. age int\n\t- an attribute value is the cross-section of an attribute and a tuple (ie. it is a single piece of data)\n- each SQL clause will transform one or several relations in order to produce new relations.\n- Data is not relational. Rather, data has relationships.\n- *relational* is the concept that we have a set of elements that all share the same properties (aka attribute domains)\n\t- ex. if we have a table `foo` with 2 columns: id: int and name: text, then every single record in this database will look exactly like that.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/028094cb-047d-409e-9fee-bb49d08d5609.html","relUrl":"notes/028094cb-047d-409e-9fee-bb49d08d5609.html"},{"doc":"Heading","title":"Heading","hpath":"sql.terms.relation.heading","content":"\n# Heading\n- A heading is a set of attributes in which column names are unique amongst each other\n- In SQL, the Heading is simply the set of all column names in a table\n- This logically follows from the earlier definition, that each tuple has a corresponding (and unique) Heading","url":"https://tycholiz.github.io/Digital-Garden/notes/e2603383-9c84-4ad8-a767-0519702f0fb7.html","relUrl":"notes/e2603383-9c84-4ad8-a767-0519702f0fb7.html"},{"doc":"Body","title":"Body","hpath":"sql.terms.relation.body","content":"\nBody - the set of tuples that correspond to a single Heading is called a *Body* \n- In SQL, the Body is simple all rows in the table\n- A relation is thus a heading paired with a body\n\t- in other words, a table","url":"https://tycholiz.github.io/Digital-Garden/notes/ad1c4020-6a69-4fd5-bfc4-01eff7cedee4.html","relUrl":"notes/ad1c4020-6a69-4fd5-bfc4-01eff7cedee4.html"},{"doc":"Tables","title":"Tables","hpath":"sql.tables","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/2020f6b6-7608-4c6f-93ba-6bfe01ef97a6.html","relUrl":"notes/2020f6b6-7608-4c6f-93ba-6bfe01ef97a6.html"},{"doc":"Partition","title":"Partition","hpath":"sql.tables.partition","content":"\n# Partitioning\nrefers to splitting what is logically one large table into smaller physical pieces\nbenefits:\n- faster querying","url":"https://tycholiz.github.io/Digital-Garden/notes/6f87dae7-7eef-4697-8cce-7f284c76c29f.html","relUrl":"notes/6f87dae7-7eef-4697-8cce-7f284c76c29f.html"},{"doc":"Junction","title":"Junction","hpath":"sql.tables.junction","content":"\n# Junction tables\nMany:Many relationships are achieved by associative tables\n- An associative (or junction) table maps two or more tables together by referencing the primary keys of each data table. In effect, it contains a number of foreign keys, each in a many-to-one relationship from the junction table to the individual data tables. The PK of the associative table is typically composed of the FK columns themselves.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/6f30aa07-8040-4a99-a4a3-2d91ef35b2ae.html","relUrl":"notes/6f30aa07-8040-4a99-a4a3-2d91ef35b2ae.html"},{"doc":"Inheritance","title":"Inheritance","hpath":"sql.tables.inheritance","content":"\n## Table Inheritance\n- Imagine we had 2 data sets: cities and capitals. A naive approach might be to create 2 tables called `non_capitals` and `capitals`, then join them together in a UNION\n\t- This approach is bad because problems arise once we need to update multiple rows at once.\n- Instead, a better solution is to use inheritance:\n```\nCREATE TABLE cities (\n  name       text,\n  population real,\n  elevation  int     -- (in ft)\n);\n\nCREATE TABLE capitals (\n  state      char(2) UNIQUE NOT NULL\n) INHERITS (cities);\n```\n- here, a row of `capitals` will inherit all columns from its parent, and will gain an additional column `state`\n- a table can inherit from more than one other table\n- using the ONLY clause, we can prevent the query from running over tables below the specified table\n\t- ex. in the capital cities example, if we query `SELECT * FROM ONLY cities...`, then we prevent capitals from showing up in the result set.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3dcc3b83-9dbb-4c74-ac84-fb736ab6dc1c.html","relUrl":"notes/3dcc3b83-9dbb-4c74-ac84-fb736ab6dc1c.html"},{"doc":"Design","title":"Design","hpath":"sql.tables.design","content":"\n## Should this be a table or not?\n- can you imagine the current data being *expanded* in the future?\n\t- ex. imagine we had a Book table, which included 3 properties: genre1, genre2, genre3. Now if we decided that each book should have 4 potential genres, we would have to change the structure. The solution would be to have a Books table and a Genres table\n- Is the data being held in a table \"genericable\"? If you are tempted to combine different concepts into something that has a common thread, it might be a sign that they should be different tables. Framed another way, if you find yourself in a position where it would be difficult to extend a table, it should give us pause.\n\t- ex. Imagine we have to keep track of `InvoiceStatus`, `BackOrderStatus`, `ShipViaCarrier`, `CreditStatus`, and `CustomerStatusType`. Each of these concepts has a common thread, which is that aside from their respective PKs, they only have a single text field. We might therefore think it appropriate to combine all 5 concepts into a single generic table. However, this becomes problematic when we start to query information. What results is a necessity to JOIN ON many different fields, and to use aliases to distinguish multiple \"versions\" of the same table:\n- if we were to look at 5 rows of a single table and noticed that 3/5 of the rows have an identical value for one of the columns, that would be a sign that the column should be its own table\n\t- ex. we have a table called `shoes` and there is a field to describe what type of shoe it is (business, casual etc). If we were to query some records, we would find that 'business' would show up a lot in the 'type' column. This is a sign that we should create a \"shoe type\" table.\n\n```sql\nSELECT *\n\tFROM Customer\n\tJOIN GenericDomain as CustomerType\n\tON Customer.CustomerTypeId = CustomerType.GenericDomainId\n\t  and CustomerType.RelatedToTable = \"Customer\"\n\t  and  CustomerType.RelatedToColumn = \"CustomerTypeId\"\n\tJOIN GenericDomain as CreditStatus\n\tON Customer.CreditStatusId = CreditStatus.GenericDomainId\n\t  and CreditStatus.RelatedToTable = \"Customer\"\n\t  and CreditStatus.RelatedToColumn = \"CreditStatusId\"\n```\n\t- on the other hand, if we separate them all out as seperate tables, then it greatly simplifies our query:\n```sql\nSELECT * FROM Customer  \n\tJOIN CustomerType \n\t\tON Customer.CustomerTypeId = CustomerType.CustomerTypeId  \n\tJOIN CreditStatus    \n\t\tON Customer.CreditStatusId = CreditStatus.CreditStatusId\n```\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8789d502-1661-4acf-8dea-9993794b2fed.html","relUrl":"notes/8789d502-1661-4acf-8dea-9993794b2fed.html"},{"doc":"Derived","title":"Derived","hpath":"sql.tables.derived","content":"\n# Derived Tables\n- Imagine we were in a position where we needed to make complex queries that involved some aggregation functions. What we could do to fulfill this, is create a temporary table, fill it with data (using INSERT), and use a *cursor* to loop through the temporary data, making updates as we go. Finally, we could execute a query against the temporary table, allowing us to obtain the final results. \n\t- An alternative approach for some data problems is to use *derived tables* (also known as inline views)\n- A derived table is a virtual table that is created within the scope of a query\n\t- The table is created with a SELECT statement of its own and is given an alias using the AS clause\n\t- the contents of this table can then be used within the query.\n\t- the result is similar to the above naive solution, and doesn't carry the overhead of having to actually CREATE/DELETE tables and run INSERTs on them\n\t\t- Therefore, derived tables eliminate the requirement to use cursors and temporary tables\n- you can think of a derived table as a temporary variable that points to a table that we just defined \n\n## Using Derived Tables\n- Because derived tables are used in place of an actual table, the query to build the derived table goes in the same spot as we normally have our FROM clause. All we need to do is replace the table name with parentheses, and define an alias with AS afterward\n- In the below example, all our derived table is really doing is making a new table that we can query against, where instead of having 3 columns: `first_name`, `last_name` and `value`, we have 2 columns: `salesperson` and `value`. This is a contrived example to show that a derived table allows us to transform how the original table appears, which we can then take advantage of to support our more complex queries of the data. \n```sql\nSELECT * FROM\n(\n\tSELECT first_name || ' ' || last_name AS salesperson, value_of_sales FROM sales_table\n) as derived_sales_table\nWHERE value_of_sales > 100 \n```\n- The data from this inner query can be used in the column list, in joins, and any other area where table data would be acceptable.\n\nIn Postgres, we can use the WITH clause to clean up the logic of derived tables:\n```sql\nSELECT first_name, last_name, age\nFROM (\n  SELECT first_name, last_name, current_date - date_of_birth age\n  FROM author\n)\n```\nbecomes\n```sql\nWITH a AS (\n  SELECT first_name, last_name, current_date - date_of_birth age\n  FROM author\n)\nSELECT *\nFROM a\nWHERE age > 10000\n```\n- with the above example, if we were anticipating the need to query the data set resulting from that subquery, we could just make it into a view so we wouldn't have to repeat that logic with each query we make\n\n[tutorial](http://www.blackwasp.co.uk/SQLDerivedTables.aspx)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d0c7c496-463e-4187-823d-63050275c714.html","relUrl":"notes/d0c7c496-463e-4187-823d-63050275c714.html"},{"doc":"Query","title":"Query","hpath":"sql.query","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3d382180-e957-4c3f-9ea2-80b9d78267e6.html","relUrl":"notes/3d382180-e957-4c3f-9ea2-80b9d78267e6.html"},{"doc":"Subquery","title":"Subquery","hpath":"sql.query.subquery","content":"\n# Subquery\n- think of subqueries as fluent syntax that's chained by nesting rather than step by step after each other.\n\t- similar to functional programming and how you read from the inside-out, using the output of the inner function as input for the next outer function \n- subqueries return a subset of data from a larger set of data (which arose from a different query)\n\t- ie. perform a query to get some data, then immediately perform another query on that data.\n- if a subquery is slow, try and convert it to a derived table, as those are much faster\n- Subqueries can usually be executed independently, allowing us to see what each one is doing so we can mentally make substitutions as we are trying to understand an SQL query\n- use subqueries any time you want to first query a set of data, which you will then query again on\n\t- note: in reality it is not as inefficient as it sounds. This is just a simplification\nex. below, if `dr_recommended` is a table with a column called `activity`, then this query will return all activities in both tables, since they will be \"doctor recommended\" \nselect the `type` column go in the `dr_recommended` table\n- you can use the WITH clause in place of sub-queries\n\n```sql\nselect * \nfrom exercise_logs \nwhere activity in (\n\tselect type from dr_recommended\n);\n```\nex. below, we first query for the highest joindate, then out of the subset of data, we query for the joindate, firstname and surname\n```sql\nselect joindate, firstname, surname\nfrom cd.members\nwhere joindate = (select\nmax(joindate)\nfrom cd.members)\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/6da5cc04-cb4f-4d2c-9ec3-06bf273a6a49.html","relUrl":"notes/6da5cc04-cb4f-4d2c-9ec3-06bf273a6a49.html"},{"doc":"Cte","title":"Cte","hpath":"sql.query.cte","content":"\n### Common Table Expression (CTE)\n- def - a temporary result that we can reference with another SELECT.\n- A CTE always returns a result set\n- They are used to simplify queries\n\t- ex., you could use one to eliminate a derived table from the main query body.\n- CTEs start with WITH clause\n\n### CTE and UPDATE..FROM\nWe can use a CTE in combination with `UPDATE..FROM` to update every row in the result set of a different query (the CTE)\n- Here, we are updating the `book_orders.purchase_status` column on book_orders that have been `PENDING` for more than 24 hours:\n```sql\nWITH cte AS (\n\tselect * from app_public.book_orders as book_orders\n\twhere book_orders.purchase_status = 'PENDING'\n\tand created_at < NOW() - interval '24 hours'\n)\nupdate app_public.book_orders set purchase_status = 'EXPIRED'\nfrom cte\n```\n\n[more](https://www.essentialsql.com/introduction-common-table-expressions-ctes/)","url":"https://tycholiz.github.io/Digital-Garden/notes/4a73cc4a-fd22-4330-9158-899de263c011.html","relUrl":"notes/4a73cc4a-fd22-4330-9158-899de263c011.html"},{"doc":"Normalization","title":"Normalization","hpath":"sql.normalization","content":"\n# Normalization\n- Denormalization is the process whereby you put data often used together in a single table to increase performance, at the sake of some database purity. Many find it to be an acceptable trade, even going so far as to design the schema intentionally denormalized to skip the intermediary step of having to join tables.\n- degree of normalization is defined as ~6 levels called Normal Forms (NF)\n\t- getting to level 6 (6NF) is not necessarily better than just getting to level 3 (3NF). \"more normalization\" does come at a cost.\n\t- ex. 6NF requires tables to have \"1 PK, and at most 1 attribute\"\n- purpose is to reduce data redundancy and improve data integrity\n- Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design).\n\t- more normalization means more tables\n\t- more \"normal\" schemas have less attributes when designing a schema, the goal is to find the right balance of normalization: too much: you have a proliferation of tables; too little: you have coupled, repeated, and difficult to query data\n- A hypothetical example of a failure to properly normalize SQL tables would be a hospital database having a table of patients which included a column for the telephone number of their doctor. The phone number is dependent on the doctor, rather than the patient, thus would be better stored in a table of doctors. The negative outcome of such a design is that a doctor's number will be duplicated in the database if they have multiple patients, thus increasing both the chance of input error and the cost and risk of updating that number should it change (compared to a third normal form-compliant data model that only stores a doctor's number once on a doctor table)\n\n## How much normalization?\n- A good rule of thumb for whether or not to normalize is to think about if the attribute might be updated in the row's lifetime. If the attribute probably won't be, then it's fine to denormalize.\n\t- ex. imagine Medium.com, which has the concept of organizations (eg. FreeCodeCamp) that can post blogs. Since we can be reasonably assured that a post will not change the organization it is attached to, it's probably ok to denormalize `organization_id` into `posts` (ie. organization_id becomes a column of the posts table)\n- Normalization can be embraced more when a table has a lower row count.\n- Denormalization can be embraced when a table has many rows and/or has expensively calculated properties that are accessed often.\n\t- we should make use of triggers to automatically update attributes in other tables where data has been duplicated as a result of denormalization. In this case, we can prefix the columns with `gen_` (generated), to be explicit about the fact that this column should not be updated by hand, but is a column that is only updated as a result of the trigger.\n- If we have a write heavy table that is not read often, the overhead of adding denormalized data at write time might decrease performance more than it improves read performance.","url":"https://tycholiz.github.io/Digital-Garden/notes/b34f8788-c2c5-4e60-b65b-849f39859137.html","relUrl":"notes/b34f8788-c2c5-4e60-b65b-849f39859137.html"},{"doc":"Keys","title":"Keys","hpath":"sql.keys","content":"\n## Keys\n### Primary Key\n- a foreign key of a table can also be made into its PK\n\t- ex. Imagine we had a `public.user` table that held more public info about a user like `username`, `interests`, and a `private.user_account` which held sensitive info like `password` and `email`. Since it is logical to think of the `user_account` table as just an extension of the `user` table, we can simply use the already existing `id` from the `user` table, and make `user_id` the primary key of `user_account`, which references the `user` table.\n- PK is a type of candidate key.\n\n#### Surrogate Key\n- A surrogate key is a (likely automatically generated) primary key that is chosen because there is nothing else that uniquely identifies a row. Put another way, the only reason it exists is for the purposes of our database.\n\t- By contrast, a key that actually means something outside our database (ex. an item SKU)\n- The danger in just relying on surrogate keys is if we have a situation where there are other columns that should uniquely identify the row, then it leaves us open to developer error\n\t- ex. what if we have an `items` table, and 2 rows are `id` and `SKU`. There's nothing stopping us from inputting the same SKU twice, which result in a duplicated row with different ids.\n\n### Foreign Key\n- for foreign keys, we definitely want one column to uniquely identify a row (likely a primary key), otherwise we would have to reference multiple columns in the other table.\n- foreign keys can reference tables or columns\n- the foreign key is on the many side of a 1:many relationship\n- foreign keys are not unique\n- foreign keys can be null\n\n### Candidate key\n- a column or combination of columns that uniquely identifies a row.\n\t- ex. if the rules of our application dictated that emails in the db must be unique, then email could qualify as a candidate key \n- Any column or column combination that can guarantee uniqueness is referred to as a *candidate key*\n\t- if this uniqueness is achieved by 2 or more columns, then it is referred to as a composite key. Therefore, composite key is a type of candidate key.\n\n#### Differences with PK\n- a table can have multiple candidate keys.\n- candidate key column(s) can have null values\n\n#### Composite key\n- If a table does not have a single column that qualifies for a Candidate key, then you have to select 2 or more columns to make a row unique\n\t- ex. if no EmployeeID or SocialSecurityNumber that could qualify as a candidate key, then you can make FullName + DoB as composite primary key (though there is small risk of duplication still)\n- Using composite keys could save us from having to perform lookups before making insertions.\n\n### Super key\n- if you add any column to a PK, then it becomes a super key\n\t- ex. EmployeeID + FullName\n\n## Constraining tables\n- foreign key constraints ensure that there is another table in existence that has the same column-value\n\t- ex. if we have an attribute `product_no` in an Order table, then we should expect that attribute to reference an `id` from a Product table\n\t- maintains the referential integrity between two related tables\n- when we create a foreign key, we essentially are saying \"a row cannot be entered into this table, unless we have an established reference to the other table\"\n\t- in other words, if the id in the other table (the *referenced table*) doesn't exist, then neither can the row in the *referencing table* (the table with the FK) \n\n## Constraining columns\n- below, we reference columns `c1` and `c2` from `table2`, calling the `b` and `c` in our table (`table1`)\n```\nCREATE TABLE table` (\n  a integer PRIMARY KEY,\n  b integer,\n  c integer,\n  FOREIGN KEY (b, c) REFERENCES table2 (c1, c2)\n);\n```\n\n1:1 and 1:many relationships are achieved by the use of key contraints (foreign keys)\n- ex. User table with an `id` (PK) property can be used as the value for the `owner_id` (FK) property on the Nugget table\n\n\n### Shoestore example\nLets start with a  sensible table: `product` (ex. Air, Jordans). Since we are going to keeping inventory in the store, we will need actual instances of those shoes, which we can call the `item` table. For instance, this table will hold the information about the shoe's size, its particular color, its price etc. (All of this information is specific to a shoe instance, and doesn't make sense to be included in the product table). Next, we might be tempted to add a column to the `product` table indicating what type of shoe it is (business, running etc). What we would find when querying data however, is that 'business' would be repeated many times, which is a sign that it should be its own table. So we create a table called `product_type` and relate it to the `product` table with ids. Next, we will have to consider that we will have to store data on the actual sale of the item (called `sales_item`). There is a distinct but important difference between difference between the `item` table and the `sales_item`. `item` is a table that includes the actual instances of the shoe in inventory. `sales_item` is the item that is being sold. Therefore, it is going to be part of another table `sales_order`. In `sales_item`, we will include information like the specific tax rate that was used (since different people are subject to different tax rates), the discount that was applied, the quantity bought etc. Next, in our `sales_order` table, we will include the form of payment (credit, cash), what time the order was placed etc. \n![8da5a5b7c2d3633de044d693884219ee.png](:/7d7f11b898814196afe51643fe2a6f8d)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3696b2d5-4281-4ffd-b5b0-79bd616fef91.html","relUrl":"notes/3696b2d5-4281-4ffd-b5b0-79bd616fef91.html"},{"doc":"Joins","title":"Joins","hpath":"sql.joins","content":"\n# JOIN\n- a JOIN is a query that accesses multiple rows of the same or different tables at one time\n```sql\nSELECT * FROM student_grades \n\tJOIN students ON student_grades.student_id = students.id\n```\n- every JOIN must have an ON\n\t- if the column names are identical in both tables, you can use USING\n- we may alias the tablename for one of 2 reasons: firstly, it's convenient, and secondly we might join to the same table several times, requiring us to distinguish between columns from each different time the table was joined in.\n```sql\nselect bks.starttime as start, facs.name as name\n\tfrom \n\t\tcd.facilities facs\n\t\tinner join cd.bookings bks\n\t\t\ton facs.facid = bks.facid\n\twhere \n\t\tfacs.facid in (0,1) and\n\t\tbks.starttime >= '2012-09-21' and\n\t\tbks.starttime < '2012-09-22'\norder by bks.starttime;  \n```\n- any time you write `FROM table1, table2` (ie. including 2 tables instead of 1), you are writing a JOIN, even though it doesn't explicitly use `JOIN`\n- all filtering of rows and columns should be done before the JOIN\n\n## 4 Reasons When to use a JOIN\n1. You need a column for the selec\n2. you need a field for the where clause\n3. you need the join as a bridge to a third table\n4. you need to join to the table to filter records (such as retrieving details on customer who have orders but not needing the order details, this can often be done better with an IF EXISTS where clause)\n- Joins are fairly expensive, which might be a reason to denormalize.\n![83fe4afa6ddc9f8d5b3665cfde631660.png](:/cad05cee711947d682fb8e53e85ab34c)\n## Inner Join\n- create new tables with common data between the two reference tables. \n- Will not include rows where one of the relevant piece of data are missing\n\t- ex. if joining `students` tables and `cohorts` table, and one student doesn't have a `cohort_id`, then that student won't be included\n- The common columns are typically the primary key columns of the first table and foreign key columns of the second table\n- with inner joins, order doesn't matter\n- the inner join will examine each row in the first table and compare each row's value (specified after `ON`). If these values are equal, the inner join creates a new row that contains columns from both tables\n\t- ex. `... FROM table1 INNER JOIN table2 ON table1_name_col = table2_name_col`\n- ~99% of the time, we want to use an INNER JOIN\n- if we made a venn diagram of table1 and table2, an inner join would give us the intersect.\n\t- ie., performing an inner join will give us the columns(?) in common between the 2 tables\n![820988c3645cc0df3da155abc938228f.png](:/066d1e23ea0a45738633ae20ae3f38ee)\n- you can join a single table with itself. You may do this if you want to derive information from the table (that is not explicitly stored), while using it to determine  \n\t- ex. below we are getting \"the members who have bee recommended by another member\". since we are using data within the same table to do the determination, we need to join with itself.  \n```sql\nselect \nrecs.firstname as firstname,\nrecs.surname as surname\nfrom cd.members mems\njoin cd.members recs\non recs.memid = mems.recommendedby\norder by surname, firstname\n```\n\n### Using Inner or Outer Join\n- in the employee table example, we can picture the `reports_to` property as a chain that goes up all the way to the CEO. If we wanted to also include employees with `reports_to=NULL` (ex. CEO), we would have to do a LEFT JOIN\n\t- In this way, the table that gets joined is the one that gets the extra columns (at the result of the other columns getting NULLs)\n\t\n\n## Left Outer Join\n- called *left outer* because the table on the left of the join operator will have each of its rows in the output at least once, whereas the table on the right will only have those rows output that match some row of the left table.\n\t- if there is not right-table match, a null value will be substituted in for right-table columns.\n\t- a common way to distinguish the left and right side of the join is to use aliases\n- there are times when we do want to include data where there isn't a match (ex. there is no matching FK for a given PK between 2 tables of a corresponding row). In these cases, we have to use an OUTER JOIN.\n\t- ex. we are doing a left join on \"bucket\", and we have buckets in our db that have no nuggets. this would result in a `null` value for the \"nugget\" column associated with that row.\n\t![2890a1430af9f888455d50224e795554.png](:/2af5c67cf5fb4b898d5323ce1cb0849b)\n- It's the LEFT or RIGHT keyword that makes the JOIN an \"OUTER\" JOIN\n\t- ie, all JOINs designated either LEFT or RIGHT are by definition OUTER JOINs\n\t- We use LEFT, RIGHT or FULL to specify which table should have all of its rows returned, regardless of the condition being met.\n- the first table is the \"left\" table\n- best practice is to avoid Right Outer Joins\n- when joining tables, all rows inclded in the left table will show up on the new table, even if one of the cells is missing, as in prev. example.\n- the left outer join starts selecting data from the left table. it compares the 2 values (specified after `ON`). \n\t- If they are equal, a new row is created that contains columns of both tables and adds the new row to the result set\n\t- if they are not equal, a new row is still created, but the right table's columns will be filled with `null` \n\t- If we want, we can add a WHERE clause to this query to only return rows from the left table that do not having matching rows in the right table\n\t\t- this effectively cuts out the intersection from the venn diagram\n\t\t- ex. `...WHERE tableb_col IS NULL;`\n- if we made a venn diagram of table1 and table2, an outer join would give us the outer parts of the venn diagram Union.\n\t- we will get all of the rows of the left table, but only the \"rows in common\" that the right table has\n\t\t- ex. if the left table has `1`, `2`, `3`, those will always show up no matter what. If the right table has `2`, `3`, `4`, then only `2` and `3` will show up (the rest will be null)\n- Venn diagram illustrates the left join that returns rows from the left table that do not have matching rows from the right table:\n![6457179c137a03db0dd9765c9cd6c988.png](:/69d6642385194490a8d62f93f8cf1bdb)\n\n## Full Outer Join\n- returns a result set that contains all rows from both left and right tables, with the matching rows from both sides if available. \n\t- In case there is no match, the columns of the table will be filled with NULL.\n- similar to the above techniques using `WHERE` clause to cut out the intersection, we can do the same here\n\t- This would give us the rows from one table that do not have the corresponding rows in another table\n![9e04ed55193420bac12aa21a38c81141.png](:/56dbccd24a6d449fa4d6ba8a0427d96a)\n\n# Self Join\n- join a table to itself using an inner or outer join\n- this allows you to create a result set that joins the rows with the other rows within the same table.\n- Imagine we need to compare a column of each row with all other rows in that table, and get a result set of only the rows that pass a certain condition \n\t- ex. we have a table `temperature_readings` that include both the high and low for the day. What we conceptually need to do is take row1 and compare it to every other row. Then we take row2 and compare it to every other row, and so on. Conceptually, this is like a nested for loop, where w1 is the outer loop and w2 is the inner loop:\n```\nselect\n     w1.city,\n     w1.temp_lo as low,\n     w1.temp_hi as high,\n     w2.city,\n     w2.temp_lo as low,\n     w2.temp_hi as high\n from weather w1, weather w2\n where w1.temp_lo < w2.temp_lo\n and w1.temp_hi > w2.temp_hi;\n```\n- need to use an alias because you cannot refer to the same table more than once in a query, so we assign it a name when it is joining against itself\n\t- since we are using 2 versions of the same table, we should give both an alias. For example in the below example, give alias `m` for the manager's version and `e` for the employee's version \n\t- [why use aliases?](https://www.reddit.com/r/SQL/comments/9e5yi9/why_use_table_alias_noob_question/)\n- two reasons for using a self-join:\n\t1. query parents/child relationship stored in a table \n\t\t- ex. in `employee` table, demonstrate relationship between `employee_id` and `reports_to` columns. Here, the employee's `reports_to` would match up with the manager's `employee_id` \n\t\t\t- `INNER JOIN employees m ON m.employeeid = e.reportsto`\n\t2. obtain running totals\n\n* * *\n\n# Combining result set of 2+ SELECT statements\nThe queries that involve UNION, INTERSECT, or EXCEPT need to follow these rules:\n1. The number of columns and their orders must be the same in the two queries.\n2. The data types of the respective columns must be compatible.\n\n## UNION\n- take 2+ tables with equal number of columns and comine them into a single result set.\n\t- ie. combines results of 2+ SELECTs into a single result set.\n\t- \"append\" could have also been an appropriate name\n- UNION is conceptually opposite of JOIN. \n\t- JOIN is for taking 2 tables and comining their columns, while UNION is for taking 2 tables and combining their rows(ie. records)\n```sql\nSELECT select_list_1\nFROM table1\nUNION\nSELECT select_list_2\nFROM table2\n```\n- The number and the order of the columns in the select list of both queries must be the same.\n- The UNION operator by default removes all duplicate rows from the combined data set (`UNION ALL` will bring them back)\n- In practice, you often use the UNION operator to combine data from similar tables, which are not perfectly normalized\n- ex. with 2 tables: `popular_movies`, `top_rated_movies`, return the movies found in both tables, removing any duplicates along the way\n![8d2bb4d874e9ee042506b53f0a5eb8cd.png](:/202885018e6045e4acd744771788c297)\n\n## INTERSECT\n- returns only the rows that are available in both result sets.\n- ex. with 2 tables: `popular_movies`, `top_rated_movies`, return only the movies that appear in both tables\n\n![b4e5d3a9f4dd1214eb4273e9d24fa8c7.png](:/da1f2c53c1e349bb944f4f33172bb9b6)\n\n## EXCEPT\n- returns distinct rows from the first (left) query that are not in the output of the second (right) query.\n- ex. with 2 tables: `popular_movies`, `top_rated_movies`, return the movies that are popular, but not top rated.\n\t- assuming we have `SELECT * FROM popular_movies` first\n![daddae8a308e1bf9405e038b101affbd.png](:/d8dc07ccdd01418c816ddaf6951dfdd7)","url":"https://tycholiz.github.io/Digital-Garden/notes/94163eaa-91bf-4551-acd0-6286805571a6.html","relUrl":"notes/94163eaa-91bf-4551-acd0-6286805571a6.html"},{"doc":"Outer","title":"Outer","hpath":"sql.joins.outer","content":"\n- Making a LEFT JOIN and then putting a column from the outer table in the WHERE clause would give us an error, (spec): since that column would be unselectable.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/9e0dd223-c36b-49ad-a99d-7ee481430e68.html","relUrl":"notes/9e0dd223-c36b-49ad-a99d-7ee481430e68.html"},{"doc":"Clause","title":"Clause","hpath":"sql.clause","content":"\n## SQL Clause Execution Order\n`FROM ➡ WHERE ➡ GROUP BY ➡ HAVING ➡ SELECT ➡ DISTINCT ➡ ORDER BY ➡ LIMIT`\n\nin other words...\n- The system first executes the FROM clause i.e. it creates the data set on which the WHERE and SELECT clauses are to be run. This step includes any joins specified in the FROM clause.\n- Then, it eliminates rows from this data set by using the filters specified in the WHERE clause.\n- It then groups the data set by the columns specified in the GROUP BY clause, and finally runs the SELECT clause. This is why you can never use the aliases specified in the SELECT clause to filter in the WHERE clause - because the aliases haven’t been declared yet.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3423e9f5-c4fc-48b7-b4c2-b48cfe12a05f.html","relUrl":"notes/3423e9f5-c4fc-48b7-b4c2-b48cfe12a05f.html"},{"doc":"Where","title":"Where","hpath":"sql.clause.where","content":"\n# WHERE\n- WHERE determines which rows will be included in the aggregate calculation\n\t- this show that the WHERE clause must be evaluated before aggregate functions are computed.\n```sql\n-- WRONG\nSELECT city FROM weather WHERE temp_lo = max(temp_lo);\n\n-- CORRECT\nSELECT city FROM weather\n    WHERE temp_lo = (SELECT max(temp_lo) FROM weather);\n```\n![](/assets/images/2021-03-09-16-30-49.png)\n- In contract with HAVING, WHERE selects input rows before groups and aggregates are computed \n\t- thus, it controls which rows go into the aggregate computation","url":"https://tycholiz.github.io/Digital-Garden/notes/fdf5ce11-aeb5-4413-b920-ca97f8e16705.html","relUrl":"notes/fdf5ce11-aeb5-4413-b920-ca97f8e16705.html"},{"doc":"Update","title":"Update","hpath":"sql.clause.update","content":"\n# UPDATE\n- we can update a table while leveraging the commands that help us create a result set.\n- ex. imagine we discover that all temperature readings after Nov 14 are off by 2 degrees:\n```sql\nUPDATE weather\nSET temp = temp - 2\nWHERE date > '2020-11-14'\n```\n- UPDATE always requires a SET token.\n\n![[dendron://code/sql.query.cte###CTE and UPDATE..FROM]]","url":"https://tycholiz.github.io/Digital-Garden/notes/cbd6e1da-1a73-4d32-910a-2ccb60fb297e.html","relUrl":"notes/cbd6e1da-1a73-4d32-910a-2ccb60fb297e.html"},{"doc":"Select","title":"Select","hpath":"sql.clause.select","content":"\n# SELECT\n- SELECT is known as a \"projection\" in relational algebra.\n\t- Once you’ve generated your table reference, filtered it, transformed it, you can step to projecting it to another form\n- Within the SELECT clause, you can finally operate on columns, creating complex column expressions as parts of the row.\n- SELECT accepts expressions, allowing us to say `select (temp_lo + temp_hi)/2 as temp_avg...`\n- The SELECT clause may be one of the most complex clauses in SQL, even if it appears so simple. All other clauses just “pipe” table references from one to another\n- In order to understand SQL, it is important to understand everything else first, before trying to tackle SELECT\n\n### SELECT INTO\n- SELECT INTO creates a new table and fills it with data computed by a query.\n- The data is not returned to the client, as it is with a normal SELECT.\n- The new table's columns have the names and data types associated with the output columns of the SELECT.\n\n#### with `record` type\n```\ndo\n$$\ndeclare\n\trec record;\nbegin\n\t-- select the film\n\tselect film_id, title, length\n\tinto rec\n\tfrom film\n\twhere film_id = 200;\n\n\traise notice '% % %', rec.film_id, rec.title, rec.length;\n\nend;\n$$\nlanguage plpgsql;\n```\n\n### SELECT DISTINCT\nremove duplicate rows *after* the SELECT has been applied (in other words, after the projection)\n\n### SELECT (inside a WITH)\n- purpose is to break down complicated queries into simpler parts\n","url":"https://tycholiz.github.io/Digital-Garden/notes/be2580a4-0c2a-4ce9-b674-e2b97b8314d3.html","relUrl":"notes/be2580a4-0c2a-4ce9-b674-e2b97b8314d3.html"},{"doc":"Returning","title":"Returning","hpath":"sql.clause.returning","content":"\ntakes in same arguments as the SELECT statement.\n- however, cannot execute aggregate functions like SELECT statements can.","url":"https://tycholiz.github.io/Digital-Garden/notes/3a33d476-f869-4de7-bd9b-f4d0783becb0.html","relUrl":"notes/3a33d476-f869-4de7-bd9b-f4d0783becb0.html"},{"doc":"Order by","title":"Order by","hpath":"sql.clause.order-by","content":"\n# ORDER BY\n- order by the column name.\n- you can use `order by 1` to sort the view or table by 1st column of query's result.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d262a718-24c4-4aea-838d-80c0359d6e4a.html","relUrl":"notes/d262a718-24c4-4aea-838d-80c0359d6e4a.html"},{"doc":"Like","title":"Like","hpath":"sql.clause.like","content":"\n# LIKE\n- use if you want to match a string against your query\n\t- ex. return all emails that have *@gmail* in them\n\t`...WHERE email LIKE '%gmail.com'`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/317d7760-6f22-47b5-b13c-85cad7b01352.html","relUrl":"notes/317d7760-6f22-47b5-b13c-85cad7b01352.html"},{"doc":"Insert","title":"Insert","hpath":"sql.clause.insert","content":"\nThere are generally three methods in PostgreSQL with which you can fill a table with data:\n1. Use the `INSERT INTO` command with a grouped set of data to insert new values.\n2. Use the `INSERT INTO` command in conjunction with a `SELECT` statement to insert existing values from another table.\n3. Use the `COPY` (or `\\copy`) command to insert values from a system file.\n\n## Inserting a composite type (a whole row at once)\n```sql\ninsert into stats.daily_registration_historical (registration_date, count_registrations)\nselect * from app_private.daily_registration;\n```\n\n\n\n### RETURNING\n- allows us to get back the columns that were successfully inserted into the table\n\t- ex. INSERT INTO\n- Using this method, we can get back default values (like id or timestamps)\n- This method is also useful to insert rows into a variable, which we can then simply return from the whole function\n```sql\ndeclare\n  v_person forum_example.person;\nbegin\n\tinsert into forum_example.person (first_name, last_name) values\n\t\t(first_name, last_name)\n\t\treturning * into v_person;\n```\n\n## Misc\n### Insert all default values\n`insert into users default values`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3ccdc85d-cfa1-4557-a99a-1e4acd01da19.html","relUrl":"notes/3ccdc85d-cfa1-4557-a99a-1e4acd01da19.html"},{"doc":"Having","title":"Having","hpath":"sql.clause.having","content":"\n# HAVING\n- HAVING aggregate function and condition / expression\n- The WHERE clause allows you to filter rows based on a specified condition. However, the HAVING clause allows you to filter groups of rows according to a specified condition.\n\t- In other words, the WHERE clause is applied to rows while the HAVING clause is applied to groups of rows\n\t- The WHERE clause applies the condition to individual rows before the rows are summarized into groups by the GROUP BY clause. However, the HAVING clause applies the condition to the groups after the rows are grouped into groups.\n- ex. from a People table, we can SELECT income, and \"filter\" in the rows that have income > 1000000: `... HAVING income > 1000000;`\n- In contrast with WHERE, HAVING selects group rows after groups and aggregates are computed. Thus, the WHERE clause must not contain aggregate functions; it makes no sense to try to use an aggregate to determine which rows will be inputs to the aggregates.\n\t- The HAVING clause almost always contains aggregate functions.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/15cfe183-cb0e-4131-a5b8-05f199dc70c7.html","relUrl":"notes/15cfe183-cb0e-4131-a5b8-05f199dc70c7.html"},{"doc":"Group by","title":"Group by","hpath":"sql.clause.group-by","content":"\n# GROUP BY\n- divide rows into distinct groups.\n\t- for each group, we can apply an aggregate function, ex `SUM()`\n- `GROUP BY X` means put all those with the same value for X in the one group\n- `GROUP BY X, Y` means put all those with the same values for both X and Y in the one group.\n- GROUP BY produces a new table reference with only as many columns as are listed after the GROUP BY clause\n\t- Note that other columns may still be available as arguments of aggregate functions\n\t- ex. `GROUP BY \"nugget\".id, \"nugget\".title` will reduce the number of available columns in all subsequent logical clauses (inc. SELECT) to 2. \n\t\t- This is the syntactical reason why you can only reference columns from the GROUP BY clause in the SELECT clause.\n- if we simply GROUP BY an id and don't apply any aggregate function, then the functionality will be the same as DISTINCT (remove duplicate rows)\n\t- this is why GROUP BY only really becomes valuable once we add an aggregate function\n- ex. imagine we have an `employee` table and we are interested in average salaries. Naturally, we have different employee positions, so if we ran the aggregate function AVG, we would get back an average of every salary:\n```\nSELECT avg(salary)\nFROM \"employee\"\n```\n- since the average of all salaries is not that interesting, we want to get the average of salaries GROUPed BY position\n```\nSELECT position, avg(salary)\nFROM \"employee\"\nGROUP BY position\n```\n- note: if we didn't have the GROUP BY clause here, we would get an error\n\t- here, GROUP BY basically says \"let's see the aggregate data *per* position\"\n- Sometimes we may see syntax such as `GROUP BY 2`. Here, 2 refers to the second column, and would be equivalent to writing out the actual column name.\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/9f17a963-93b1-4e00-aac0-156a34213686.html","relUrl":"notes/9f17a963-93b1-4e00-aac0-156a34213686.html"},{"doc":"Filter","title":"Filter","hpath":"sql.clause.filter","content":"\n# FILTER\n- def - adds a filter to an aggregate function\n- ex. if we are making a json array with `json_agg`, we can filter on anything that would appear in that json\n\t- ex. `filter (where \"bucket\".id is not null)`","url":"https://tycholiz.github.io/Digital-Garden/notes/2072d733-f2f7-4685-a381-22cba80273fa.html","relUrl":"notes/2072d733-f2f7-4685-a381-22cba80273fa.html"},{"doc":"Cascade","title":"Cascade","hpath":"sql.clause.cascade","content":"\n# CASCADE\n- whenever rows in the parent (referenced) table are deleted/updated, the respective rows in the child (referencing) table with matching FK will be deleted/updated as well\n\t- put another way, not only do we delete an object (ex. a schema), but we delete everything contained within that object (ex. tables)\n- ex. if we mark the `nugget_id` of `nugget_bucket` table as CASCADE, when a row in the `nugget` table is deleted, all rows in `nugget_bucket` table that reference that nugget will also be deleted\n","url":"https://tycholiz.github.io/Digital-Garden/notes/42f52ad8-17cb-43c7-a721-1391ed1884df.html","relUrl":"notes/42f52ad8-17cb-43c7-a721-1391ed1884df.html"},{"doc":"As","title":"As","hpath":"sql.clause.as","content":"\n# AS\n- alias a column name as you are selecting it\n\t- ex. `SELECT title AS bucket_title...`\n- alias a tablename\n    - ex. `SELECT ___ from users as u`\n\n- the alias only exists during execution of the query\n- if you use column aliases in the SELECT clause, you cannot use them in the WHERE clause.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8af6eb78-2253-41e8-9a1d-339850635c90.html","relUrl":"notes/8af6eb78-2253-41e8-9a1d-339850635c90.html"},{"doc":"Sketch","title":"Sketch","hpath":"sketch","content":"\n# Styles\n- apply styles to things like text to be able to reuse them easily\n    - when selecting text, it is under the `Appearance` headline\n\n# Layout\n## Smart Layout\n- Create a symbol that when modified (ex. text), will retain its ratio\n    - ex. make a button, group it, then make into a symbol. On the naming\n\twindow, select \"horizontal align\". Now, when you replace the text on\n\tthat button, the horizontal size will automatically adjust\n- Use smart layout any time you create a list of horizontal text (ex. a navbar,\n    or tabs).\n    - Each item should be a symbol that has a smart layout of \"horizontal\n\talign\".\n    - The whole navbar itself should also be a symbol with \"horizontal align\",\n\tmaking the spacing between the items to be consistent even with changes\n\tto the text.\n    - On the sidebar, you can choose to override styles in order to remove an\n\titem from the list. Each item will shift horizontally to take its place.\n\n# Reusing\n- Mainly done through `Layer Styles`, `Text styles` and `Symbols`\n## Symbols\n- similar to the concept of components in programming\n- ex. create a button from scratch, similar to grouping them together, we can\n    make them into a symbol to be reused throughout the design\n- to make edits to the master symbol, just click on any symbol to be transported\n    to the master symbol's canvas.\n- [guide to reusing colors](https://www.reddit.com/r/sketchapp/comments/gwft1f/if_you_could_just_edit_these_and_for_the_change/)\n\n### Nested Symbols\n- ability to override the symbols\n- Imagine we have a navbar and want to create an active/inactive state for each\n    button.\n- Create the whole navbar as one symbol, then when editing the master navbar symbol, click\n    into each button and convert it to a symbol, naming it `nav/icon{1,2,3}/off`\n- Duplicate each icon and make the ON state by changing color, then renaming to\n    `nav/icon{1,2,3}/on`\n- This allows us to easily insert each state of a symbol easily.\n- We can quickly swap in on/off states now by clicking on the symbol (from the\n    artboard) and overriding the styles on the sidebar\n- If we want to swap the icon for another, we can just go to the navbar master\n    symbol and modify each nested symbol (the icon), and it will be updated\n    across all instances of the navbar symbol \n    \n#### Thought experiment\nLet’s say that a project requires 3 buttons in 3 different colours. To use only one symbol for all of them at once, we need to be able to change the colour of the button without detaching it from a symbol.\nWhen you put a symbol inside another, you can swap the one inside with any other available symbol of the same size. In Sketch, this function is named nested overrides. This way, if you have multiple colour swatches saved as symbols, you can easily replace one swatch with another.\n\n## Overriding symbols\n- Say we have a vertical scrollable list of airbnb properties, each with common styles. What we can\n    do is create 1, make it a symbol, then insert that symbol into the document.\n    On the sidebar, you can see all of the overrides that are available\n    (adjusting text, adjusting colors, images etc)\n\n* * *\n## Mask\n- use to chop a hole out of the object (ex. picture) in the shape of the\n    overlying object.\n    - take a square and hover over a picture, then click 'mask', and you have an\n\timagine in the shape of the square.\n### Alpha mask\n- created when using opacity with gradients\n- allows us to place a rectangle with a gradient at 0%-100% over text, and hit\n    'mask' to impose that gradient on the text.\n- the object on top will apply its gradient onto the object below\n\n# Shortcuts\n- `cmd+1`/`cmd+2` - zoom in/out on object\n- `shift+cmd+l` - lock an object from movement\n- `shift+cmd+h` - hide an object \n- `cmd+[`/`cmd+]` - send layer back/fwd\n    - `cmd+opt+[]` - send to bottom/top\n","url":"https://tycholiz.github.io/Digital-Garden/notes/9cc2f536-ad6d-49d2-bf6a-30284dfa1158.html","relUrl":"notes/9cc2f536-ad6d-49d2-bf6a-30284dfa1158.html"},{"doc":"Sdk","title":"Sdk","hpath":"sdk","content":"\n## SDK\nAn SDK is an interface that is defined by some provider","url":"https://tycholiz.github.io/Digital-Garden/notes/e02953c0-95d2-4101-a117-f8e0221800f0.html","relUrl":"notes/e02953c0-95d2-4101-a117-f8e0221800f0.html"},{"doc":"Retool","title":"Retool","hpath":"retool","content":"\nRetool allows us to create internal tools that can hook into our APIs and database. With Retool, we can read/write data in a UI that we don't have to build ourselves.\n- [home](https://retool.com/)","url":"https://tycholiz.github.io/Digital-Garden/notes/b6746509-f9f8-4823-b3b9-0d6d5f1fdb69.html","relUrl":"notes/b6746509-f9f8-4823-b3b9-0d6d5f1fdb69.html"},{"doc":"Redux","title":"Redux","hpath":"redux","content":"\nRedux uses a different instance of the store for every request.\nRedux is actually based in part on [[CQRS|general.patterns.CQRS]] and [[event sourcing|general.patterns.event-sourcing]]","url":"https://tycholiz.github.io/Digital-Garden/notes/a02e32ca-2839-46bc-a452-684f01a4e671.html","relUrl":"notes/a02e32ca-2839-46bc-a452-684f01a4e671.html"},{"doc":"Selectors","title":"Selectors","hpath":"redux.selectors","content":"\n# Selectors \nIf actions are the entry points, selectors are the exit.\n- After dispatching an action, the application can use selectors to get the resulting data from the store after reducers and sagas have done something with it.\n### useSelector\n- equivalent to the purpose of `mapStateToProps`, which is provide easy access to the particular parts of the store that you want. \n\t- when a component has multiple instances, the selector needs to be defined inside the component, so that each instance gets its own selector instance.\n\n## Reselect\n- Will memoize the value that is returned by the selected value\n    - Imagine there is a list of questions in the redux store, and we want to retrieve all questions that have an even-numbered `id`. We could write a reselect selector that gets this array, performs a `filter` on it, memoizes (caches) that function, then return it to us. Then, next time we call that selector, the value is already available to us, rather than having to perform that `filter` operation on the array all over again.\n- If we are just getting data without modification (ex. getting `store.title`), then reselect is not necessary. Only when there is expensive computation is reselct beneficial.\n- `createSelector` will take in 1+ selectors (functions), call them, and passes their return values (the selected part of the store) as arguments to the final arg (an anonymous transform function) of `createSelector`. That function will accept 1 parameter for every selector that came before it. \n\t- If one of the input selectors has changed since the last call, the transform function will be called. Otherwise, it will just return the memoized value.\n    - this function is a thunk (returns a function)\n    - ***ex.***\n    ```\n    export const getFeedbackForYou = createSelector(\n        getByRecipient,\n        (byRecipientState) => byRecipientState.myFeedback,\n    );\n    ```\n    - here, `createSelector` is being called and returns a function that takes state as the argument (since that's what thunks do).\n    - conceptually, we can therefore replace `createSelector(...)` with `(state) => \n\n* * *\n\n- `mapStateToProps` is really just a type of selector when you think about it.\n\t- specifically, it is the selector that (potentially) uses other selectors to get data from the store. It is meant to clump them together for easy access to components that get rendered by the container \n- `state` refers to the data that currently exists in the store. `store` refers to the instance","url":"https://tycholiz.github.io/Digital-Garden/notes/361c21f2-8e6e-4938-89ea-ece3d99efd56.html","relUrl":"notes/361c21f2-8e6e-4938-89ea-ece3d99efd56.html"},{"doc":"Reducers","title":"Reducers","hpath":"redux.reducers","content":"\n# Reducers\nreducers are supposed to return the initial state when they are called with undefined as the first argument, no matter the action.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/abc137c6-b197-415b-a78f-f6919dc4fcab.html","relUrl":"notes/abc137c6-b197-415b-a78f-f6919dc4fcab.html"},{"doc":"Mw","title":"Mw","hpath":"redux.mw","content":"\nMiddlewares only wrap store.dispatch()","url":"https://tycholiz.github.io/Digital-Garden/notes/3a780b23-cc61-4f9c-a93a-1b4b90bdca74.html","relUrl":"notes/3a780b23-cc61-4f9c-a93a-1b4b90bdca74.html"},{"doc":"Thunk","title":"Thunk","hpath":"redux.mw.thunk","content":"\nA thunk is a pure function that returns an impure function \n## The pattern of providing dispatch to a helper function\n- imagine we wanted to dispatch 2 actions that normally occur together, such as an action to show a notification, and an action to hide it 5 seconds later. The simplest approach is to dispatch an action to show it, then dispatch an action 5 seconds later to hide it. But what if we want to use this combination of 2 actions elsewhere in the app? What we can do is create an external function that accepts `dispatch` as the first arg, and the payload that goes to both dispatches as the rest of the args. \n\t- Not only does this give us code reuse, but it also allows us to have better control over *that* action, and only that action. In our first naive method, we have no idea if the notification related to the action being dispatched 5 seconds later is associated with the notification related to the action dispatched instantly. For instance, if a notification is trigged and then milliseconds later another is triggered, we might end up in a race condition where our app doesn't know how to match up the `show` and `hide` actions with the correct notification. \n\n```js\n//Naive method\nthis.props.dispatch({ type: 'SHOW_NOTIFICATION', text: 'You logged in.' })\nsetTimeout(() => {\n  this.props.dispatch({ type: 'HIDE_NOTIFICATION' })\n}, 5000)\n\n// external function to increase code reuse\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(dispatch, text) {\n  // Assigning IDs to notifications lets reducer ignore HIDE_NOTIFICATION\n  // for the notification that is not currently visible.\n  const id = nextNotificationId++\n  dispatch(showNotification(id, text))\n\n  setTimeout(() => {\n    dispatch(hideNotification(id))\n  }, 5000)\n}\n```\n\n## Motivation for Thunks\n- the above solution works but it presents 2 issues:\n\t- we have to pass `dispatch` around everywhere. This means that where we need to call that helper function, we need to have access to `dispatch`, meaning we need to create a container for that component. \n\t- We have to mentally keep track which redux `actions` are just regular action creators, and which are actually more like action helpers (above example. since they do not return an action, they are not action creators)\n\n## What are thunks\n- A thunk is a middleware that gives `dispatch` the ability to accept a function. Normally dispatch can only accept an action (usually dispatch accepts the invocation of an action creator, which returns an action). With Thunk middleware (mw being something that gives enhanced functionality), `dispatch` gains the ability to accept functions.  \n\t- When a function is dispatched, Thunk MW will recognize that and pass `dispatch` as the first argument, allowing us to dispatch any action we want within the function (ie within the thunk)\n- The result is a function that is very similar to the external helper function we made before. There are 2 key differences: \n\t- our new function doesn't accept `dispatch` as the first prop\n\t- our new function *returns* a function of its own, and *that* function accepts `dispatch` as it's first argument\n```js\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  return function (dispatch) {\n    const id = nextNotificationId++\n    dispatch(showNotification(id, text))\n\n    setTimeout(() => {\n      dispatch(hideNotification(id))\n    }, 5000)\n  }\n}\n```\n\n## Using thunks\nwithout the Thunk MW, we could execute this function like so:\n```\nshowNotificationWithTimeout('You just logged in.')(this.props.dispatch)\n```\nbut since we are using the MW, any time a function is dispatched instead of an action object, thunk takes over and calls that function with `dispatch` as its first argument, giving the function access to `dispatch`\n- therefore, we just do this \n```\nthis.props.dispatch(showNotificationWithTimeout('You just logged in.'))\n```\n\nWith thunks, dispatching an asynchronous action (really, a series of actions) looks no different than dispatching a single action synchronously to the component. Which is good because components shouldn’t care whether something happens synchronously or asynchronously. We just abstracted that away. The component only knows about the thunk, and the thunk deals with the async logic\n\nThunks also receive as the second arg the application store. This is useful for if we want to bail out of an API call \n- ex. user has notifications turned off:\n```js\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  return function (dispatch, getState) {\n    // Unlike in a regular action creator, we can exit early in a thunk\n    // Redux doesn’t care about its return value (or lack of it)\n    if (!getState().areNotificationsEnabled) {\n      return\n    }\n\n    const id = nextNotificationId++\n    dispatch(showNotification(id, text))\n\n    setTimeout(() => {\n      dispatch(hideNotification(id))\n    }, 5000)\n  }\n}\n```\n- note:  If you use `getState()` only to conditionally dispatch different actions, consider putting the business logic into the reducers instead.\n\nThunks may also return promises. In fact, Redux doesn’t care what you return from a thunk, but it gives you its return value from `dispatch()`\n- This is why you can return a Promise from a thunk and wait for it to complete by calling `dispatch(someThunkReturningPromise()).then(...)`\n- You may also split complex thunk action creators into several smaller thunk action creators. The dispatch method provided by thunks can accept thunks itself, so you can apply the pattern recursively. Again, this works best with Promises because you can implement asynchronous control flow on top of that.\n\n## When thunks aren't enough, and to reach for Sagas\nFor some apps, you may find yourself in a situation where your asynchronous control flow requirements are too complex to be expressed with thunks.\n- ex. retrying failed requests, reauthorization flow with tokens, or a step-by-step onboarding\n* * *\n[source](https://stackoverflow.com/questions/35411423/how-to-dispatch-a-redux-action-with-a-timeout/35415559#35415559)","url":"https://tycholiz.github.io/Digital-Garden/notes/e0c988a0-dae4-4b86-bf6f-731babce5253.html","relUrl":"notes/e0c988a0-dae4-4b86-bf6f-731babce5253.html"},{"doc":"Saga","title":"Saga","hpath":"redux.mw.saga","content":"\n# What is it?\n- a saga is a daemon that lets us define long-running processes that take actions as they come, and transform or perform requests before outputting actions. This moves the logic from action creators into sagas\n- a saga is a separate thread in the app that's just for side-effects\n\t- while thunks utilize callbacks, a saga thread can be started, paused (`yield`) and cancelled by dispatching actions within generator functions\n- in synchronous redux, a dispatched action is assigned to a reducer. In Async redux, a dispatched action is assigned to a saga. The saga does its side effect (`resourceListReadRequest`), and takes the returned data, and dispatches another action (`resourceListReadSuccess`) which is then picked up by the reducer.\n\n## Reconciling Generator fns and Saga\n- Imagine a saga as a thread that constantly calls `next()` and tries to execute the `yield` lines as soon as it can\n- spec: like a promise, it will wait on that value to \"return\" before it calls `next()` and goes to the next `yield`\n- In Sagas, we are \"yielding\" to the redux-saga middleware\n\t- The MW suspends the saga until the yielded side effect resolves. At this point, the MW calls `next()`\n- When we say `yield call(___)`, we are only describing what we want to happen. We aren't describing the actual outcome. In this sense, it is declarative.\n\n# Types of Saga\n### Worker Saga\n- def - Sagas that perform side effects and dispatch other actions asynchronously.\n\n### Watcher Saga\n- def - Sagas that listen for dispatched actions and call worker sagas in response\n\n### Root Saga\n- def - Sagas that run all watcher sagas in parallel\n\n# Example process\n1. An action `resourceListReadRequest` is dispatched somewhere in the code\n2. A *Watcher Saga* that is designed to listen for `resourceListReadRequest` picks up on the fact that it was dispatched, and notifys the *Worker Saga*\n\n# API Reference\n### Effects\n*def* - an object containing instructions to be fulfilled by middleware. When the MW retrieves an effect yielded by a saga (ie. when a saga executes put, call, take etc), the saga pauses until the effect is done.\n- `call` - call the fn (1st arg) with args (rest args)\n- `put` - dispatch action\n- `take` - block execution of the saga until the provided action is dispatched\n\t- therefore, this is used in a watcher saga\n- `fork` - useful when a saga needs to start a non-blocking task\n\t- Non-blocking means: the caller starts the task and continues executing without waiting for it to complete\n\t- Situations:\n\t\t1. grouping sagas by logical domain\n\t\t2. keeping a reference to a task in order to be able to cancel/join it\n- `takeEvery` - each time a particular action is dispatched, spawn a saga\n\t- `takeEvery` uses `take` and `fork` under the hood\n- `takeLatest` - spawn a saga only for the latest dispatched action of a given type\n\t- ex.  imagine a user is mashing the login button. with Thunk, an API call would be made with each button press. with redux-saga, we get to just take the latest one and ignore the rest\n\n# UE Resources\n[redux-saga primer](https://flaviocopes.com/redux-saga/)\n- [also](https://medium.com/appsflyer/dont-call-me-i-ll-call-you-side-effects-management-with-redux-saga-part-2-cd16f6bcdbcd)","url":"https://tycholiz.github.io/Digital-Garden/notes/e974100d-7f60-4f7c-8c7c-bdfa88b01279.html","relUrl":"notes/e974100d-7f60-4f7c-8c7c-bdfa88b01279.html"},{"doc":"Rxjs","title":"Rxjs","hpath":"redux.mw.rxjs","content":"\nobservables are conceptually similar to a fancy event emitter.\n# Observable\n- ***Observable*** - A data structure representing the idea of an invokable collection of future values or events\n\t- in other words, they are a representation of any set of values over any amount of time\n- Observables are lazy Push collections of multiple values\n- variables that reference a stream are denoted with `$` (ex. `action$`)\n    - ex. *observable*\n- An observable is a function that returns a stream\n- An observable is a function that takes an observer and returns a cancellation function\n    - an observer is an object with `next`, `error` and `complete` methods\n- Observables are inert (they just sit there until they are *subscribed* to), while observers stay active and listen for events from the producers\n- The problem is this: we can consider an array where we already know all the values as eager, and an array that receives values (ie. increases length) at a set interval (1s) as lazy. Normally, we perform data processing in an eager way, since the data is processed immediately as it's received, which is instant. What if we are in a position where the array grows over time? It would be beneficial if we could call a method on an array value as it enters the array. In this sense, we need to subscribe to the array to execute a method when the array grows.\n- There are 2 sides to an observable: producer and consumer\n    - **Producer** - adds to the array\n        - ex. button clicks add that click event to the array\n    - **Consumer** - calls the function on the new array item\n        - ex. calls `console.log` in response to the new click event\n\n- An `Observer` subscribes (ie. is consumer) to an `Observable`\n    - an observer is a collection of callbacks\n- Observables are lazy Push collections of multiple values. They fill the missing spot in the following table:\n![](/assets/images/2021-03-07-22-35-55.png)\n    - They *push* with `.next()`\n\n\n## Characteristics\n- They are time-independent (ie. lazy)\n- They are mostly used in asynchronous data streams, like web sockets or multiple concurrent api calls\n\n- Just as promises abstract time away from our concern for a single asynchronous operation, observables abstract time away from a set of data (eg. array)\n    - ex. Observable can be set up to listen for mouseclicks, by pushing onto an array each time the mouse is clicked. The fact that the *subscribed* function exists means that it will fire whenever the array increases in size.\n\n\n- If you combine the functionality of an Observer and an Observable, you get a Subject\n\n- each operator on an Observable returns a new Observable, meaning they are chainable (this is known as a *stream*)\n\n- A map(..) on an array runs its mapping function once for each value currently in the array, putting all the mapped values in the outcome array. A map(..) on an Observable runs its mapping function once for each value, whenever it comes in, and pushes all the mapped values to the output Observable.\n\n**epic** - a collection of observables\n\n```\nexport default function fetchTeams(action$) {\n    // epic\n    return action$.pipe(\n        //operator\n        ofType('FLASHCARDS_TEAMS_GET'), \n        //operator\n        switchMap(() => \n            queryGraphQL('flashcards__teams'),\n        ),\n        //operator\n        map(resp => { operator\n            const flattenedData = flattenGraphqlNode(resp).data.teams;\n            return getTeamsSuccess(flattenedData);\n        }),\n        catchError(err => {\n            getError(err);\n        }),\n    );\n}\n```\n\n## Using observables\n- An `observable` is a function that takes in an `observer` (an object with `next`, `error`, and `complete`) and returns cancellation logic (i.e. unsubscribe).\n- `.next()` is called when the observable produces values\n- When an observer subscribes to an observable, the observer will keep receiving values until one of 2 things happens:\n    - there are no more values to be sent (in which case `.complete()` is called)\n    - the observer calls `.unsubscribe()` on the observer\n- fromEvent will turn an event into an observable\n    - `fromEvent(<event to be listened to>, <eventName>)`\n```\nconst input$ = Rx.Observable.fromEvent(node, 'input')\n  .map(event => event.target.value)\n  .filter(value => value.length >= 2)\n  .subscribe(value => {\n    // use the `value`\n  });\n```\nHere are the steps of this sequence:\n1. Let’s assume the user types the letter “a” into our input (node is a variable that has query selected the html input element)\n2. The Observable then reacts to this event, passing the value to the next observer\n3. The value “a” is passed to `.map()`, which is subscribing to our initial observable\n4. `.map()` returns a new Observable of event.target.value and calls `.next()` on it’s observer\n5. The `.next()` call will invoke `.filter()`, which is subscribing to `.map()`, with the resulting value of the .map() call\n6. .filter() will then return another Observable with the filtered results, calling `.next()` with the value if the length is 2 or above\n7. We get the final value through our `.subscribe()` block\n\n- Each time a new Observable is returned, a new observer is hooked up to the previous Observable (allowing us to pass values along a stream of observers, which do something we've asked, then call `.next()`, then pass the result to the next observer.\n    - Basically, an operator returns a new Observable each time, allowing the stream to continue\n    \n## Subscription\n- ***subscription*** - an object that represents the execution of an *observable*\n- `.create` accepts a subscribe function\n    - `subscribe` accepts an *observer argument*\n```\n// This first part is the observable, which emits things\nconst myObservable = Observable.create(function subscribe(observer) {\n    observer.next('hey!') //this is emitting a value\n})\n\n// To grab the value, we define an observer \n// (x is the observer)\nconst observer = myObservable.subscribe((x) {\n    console.log(x) // hey!\n})\n```\n\n## \"Mapping + Flattening\" Operators\n- ***flatten*** - subscribing inside a subscribe\n- All work mostly in same manner\n    - They map some value to an observable (you are the one in charge of returning an observable value from them, they just map it)\n    - They flatten the observable you return ( they just subscribe to it)\n    - They decide about what to do before / after they flatten (“Flattening Strategy”)\n### mergeMap - the slacker operator\n- simply keep subscribing to every new observable that we return from the map\n- Other than mapping + flattening the observable, it does nothing else.\n- ex. Imagine Netflix shows up-to-date ratings for each movie, retrieved from IMDB's API. We can `mergeMap` the movie into an http request to IMDB to get this data and enhance our UI.\n### switchMap - the \"latest and greatest\" operator\n- unsubscribe from the last mapped observable\n- ex. Imagine we are typing in Google and the autocomplete box shows up. Of course, these suggestions change with each key press. If we use `switchMap`, each previous request will be cancelled if a new one happens. If we'd used `mergeMap`, a request for each keystroke would be made\n### concatMap - the \"wait in line\" operator\n- queue up the observables one after the other, and play their events in that order (i.e. subscribe to the next Observable in the queue only when the previous one is completed).\n- Similar to `mergeMap`, except order matters\n- ex. top 10 list\n### exhaustMap - the \"do not disturb\" operator\n- ex. login button - since we don't want multiple clicks to be registered, we want want to disable mapping while the first http request is on the go, ensuring that we never call the server while the current request is running.\n\n\n# Redux-Observable (library)\n- ***def*** - a redux middlware allowing us to `map` and `filter` actions with RxJS operators.\n    - While javascript `map` and `reduce` allows us to transform arrays, these versions allow us to transform *streams of actions*\n## Epic \n- ***def*** - a function that takes in a stream of actions, and returns a modified stream of actions.\n    - receive `variable$` as input\n- epics can be thought of as a description of what additional actions redux-observable should dispatch\n- epics are analogous to sagas from redux-saga\n\nexample:\n```\nconst pingEpic = action$ => action$.pipe(\n  ofType('PING'), //equivalent to filter(action => action.type === 'PING')\n  mapTo({ type: 'PONG' })\n);\n\n// later...\ndispatch({ type: 'PING' });\n```\n- pingEpic will listen for actions of type PING and map them to a new action, PONG. This example is functionally equivalent to doing this:\n```\ndispatch({ type: 'PING' });\ndispatch({ type: 'PONG' });\n```\n- Epics run alongside the normal Redux dispatch channel, after the reducers have already received them. When you map an action to another one, you are not preventing the original action from reaching the reducers; that action has already been through them\n\n## Operators\n- `ofType` - filter by a specific type of action\n\n# Stream\n- ***def*** - a sequence of data elements made available over time\n- Can be thought of as items on a conveyor belt being processed one at a time, rather than in large batches\n- Can also be seen as a sequence of ongoing events ordered in time\n    - ex. number of button clicks in 1 second\n        - All the clicks will be grouped together as a stream\n- The stream is the subject which is being observed\n\n* * *\n### Promise vs Observable\n- Promises handle a single event (ie. the failure or success of an asyn operation).\n- An observable is a function that returns a stream, and we can pass in zero or more events to it. The callback provided to the observable will be called for each event.\n\t- Observables are preferred over promises because they don't care how many events you have, while promises require 1, and only 1.\n- Observable libraries provide methods to help interact with the emitted (returned?) value(s). For instance, we can use `map` to transform each value's output \n- a Promise is eager, whereas an Observable is lazy\n- If you wanted to subscribe to the reactive way of programming, then you could just \"observable all the things\"\n\nAdditionally...\n- Observables are cancellable.\n\n# Questions\n- what is meant by inner/outer observable\n    - [source](https://academind.com/learn/javascript/callbacks-vs-promises-vs-rxjs-vs-async-awaits/) \n    \n# Resources\n- [Operators](http://reactivex.io/documentation/operators.html)\n- [thinking in nested streams](https://rangle.io/blog/thinking-in-nested-streams-with-rxjs/)","url":"https://tycholiz.github.io/Digital-Garden/notes/1426c9b1-f3c1-4c01-8efd-a732c782c48c.html","relUrl":"notes/1426c9b1-f3c1-4c01-8efd-a732c782c48c.html"},{"doc":"Actions","title":"Actions","hpath":"redux.actions","content":"\n# Actions\nAn action creator is found in the /actions directory. When they are called, an action is created. However, it doesn't do anything, it just floats there. The action becomes mobile only once it is dispatched. When store.dispatch is called with the action as its argument, the action is able to reduce the payload into the state\n","url":"https://tycholiz.github.io/Digital-Garden/notes/93e44161-3474-4264-963b-85fa8c5d5030.html","relUrl":"notes/93e44161-3474-4264-963b-85fa8c5d5030.html"},{"doc":"Redis","title":"Redis","hpath":"redis","content":"\n- stands for *Remote Dictionary Server*\n- used as a database, cache, message broker, and queue\n- All Redis data resides in-memory RAM\n- Redis delivers response times of less than 1ms, enabling millions of requests per second\n- Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps.\n\nRedis data types include:\n- Strings – text or binary data up to 512MB in size\n- Lists – a collection of Strings in the order they were added\n- Sets – an unordered collection of strings with the ability to intersect, union, and diff other Set types\n- Sorted Sets – Sets ordered by a value\n- Hashes – a data structure for storing a list of fields and values\n- Bitmaps – a data type that offers bit level operations\n- HyperLogLogs – a probabilistic data structure to estimate the unique items in a data set","url":"https://tycholiz.github.io/Digital-Garden/notes/f691cb23-bc18-4b33-b454-155b1e785d62.html","relUrl":"notes/f691cb23-bc18-4b33-b454-155b1e785d62.html"},{"doc":"React","title":"React","hpath":"react","content":"\n- purity and immutability between components is critical. mutability within a component is fine.\n\t- ex. pushing onto an array within a component is fine.\n- When React is interpreting JSX, it will treat custom components (`<Form>`) as recursive functions, and regular elements (`<form>`) as the finality of the chain\n\t- ex. We say \"React, render a `<Sidebar>` for me\". React responds \"ok what's in a `<Sidebar>`?\", and it will go on like this until React gets to the regular elements and it has therefore built up the entire component tree.\n- React splits all work into the “render phase” and the “commit phase”.\n\t- Render phase is when React calls your components and performs reconciliation. \n\t- Commit phase is when React touches the host tree. It is always synchronous.\n- In React, anything other than updating the page is considered a side-effect. If you’re not using React to update state or render HTML, that’s a side effect. It’s any non-React thing.","url":"https://tycholiz.github.io/Digital-Garden/notes/b6226bd4-551d-4670-a956-158bdce3f5a6.html","relUrl":"notes/b6226bd4-551d-4670-a956-158bdce3f5a6.html"},{"doc":"Tools","title":"Tools","hpath":"react.tools","content":"\n[Rehooks: premade react hooks](https://github.com/rehooks)","url":"https://tycholiz.github.io/Digital-Garden/notes/b1484099-1066-4b51-b56a-1fe336d610cf.html","relUrl":"notes/b1484099-1066-4b51-b56a-1fe336d610cf.html"},{"doc":"Lang","title":"Lang","hpath":"react.lang","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/734beb81-47b0-41b5-a964-fe05af534458.html","relUrl":"notes/734beb81-47b0-41b5-a964-fe05af534458.html"},{"doc":"Suspense","title":"Suspense","hpath":"react.lang.suspense","content":"\n# Suspense\nSuspense refers to React’s ability to “suspend” rendering while components are waiting for something, and display a loading indicator\n- solves the problem of having some of the data, but not being able to render\n    until all data is in. With suspense, we can load some of the UI without all\n    of the data having to be available.\n    - Suspense lets you pause rendering a component while it’s loading async data\n- React Suspense will specify a fallback component in case the main component\n    suspends. \n\n![28a01e0b811e84e2c3c20a4faca82641.png](:/3b101f50bf264119a5a3d57e387812f4)\n\n## Process for data fetching and rendering in browser\n1. Browser downloads the code\n2. React begins to render the `<Home />` component\n3. React will attempt to use the data needed for the component, then will realize that the data doesn't exist yet. React will then look up the component tree, find its nearest Suspense boundary. Then it will then render the fallback component, and fetch the data simultaneously.\n4. Once data has been fetched (signified by a function like `useQuery` having been completed), React will render `<Home />` (which contains an `<img />` tag)\n5. The browser sees the `<img />` tag and makes a fetch to get that image.\n* * *\n- If we have multiple components within a `<Suspense />`, then we have to wait\n    for all components inside to be ready before anything renders. Conversely if\n    each component is nested within its own `<Suspense />`, then each can render\n    when it is done\n```jsx\n<Suspense fallback={<DotLoader />}>\n    <NewsFeed />\n</Suspense>\n```\n\n- If we want to control the order in which something will render, we can use a\n    `<SuspenseList>` component. This component takes a prop `revealOrder`.\n    - if we set `revealOrder=\"forwards\"`, then we ensure that the first\n        component will always render first. \n    - We might want to do this if component1 and component2 are stacked on top\n        of each other, and component1 has dynamic dimensions (content determines\n        the height). If component2 were to load first, then once component1\n        loads, it would push everything down.\n        - ex. Facebook's `<Composer>` and `<NewsFeed>`\n- \"In the long term, we intend Suspense to become the primary way to read asynchronous data from components — no matter where that data is coming from.\"\n    - from React team\n  ","url":"https://tycholiz.github.io/Digital-Garden/notes/532c66f8-d533-4a31-9a07-1712b6c516ef.html","relUrl":"notes/532c66f8-d533-4a31-9a07-1712b6c516ef.html"},{"doc":"Ref","title":"Ref","hpath":"react.lang.ref","content":"\n## Ref\n- Ref - a mutable value that is shared by all component renders.\n\t- Think of a ref as a piece of state that doesn't abide by the same re-rendering rules that apply to normal state.\n\t- a ref can be a function or an object\n- ex. Imagine we had 2 counters: one made with state, and the other with refs. As expected, when we increment the state count, the component understands that it needs to re-render, so it does so and reads in the new value of count. When we increment the ref count however, the component understands that it should not update, so we end up still seeing the old value, even though underneath the hood, the count has still risen. Since nothing has caused the component to re-render, this value will remain static. Now imagine that we increment the state count. React will recognize that an updated state means that it needs to re-render. Upon doing so, the current value of the ref will be read.\n- traditionally, refs were meant to let you directly access an html element.\n\t- ex. focus an input field\n- The `useRef()` hook helps create mutable variables inside a functional component that won’t update on every render.\n- a ref can be used when the \"React-way\" of interacting with children (ie. Passing props) doesn't cut it, or we want to manually access DOM nodes or React elements created in the render method\n- The variables created with useRef() persists in between component renders (like state).\n- The nature of refs is that we can change the value of the ref, and it will not cause the component to re-render.\n- Refs have a value called `.current()`, which gives us the value of the ref in the current rendering of the component.\n- use refs To manage focus, text selection, or media playback\n- Most elements inside of your document have a ref attribute, which facilitates the use of useRef to reference elements inside of your HTML.\n- refs created using createRef are not persisted between re-renders. A new ref is always created whenever the component is re-rendered.\n- Updating a ref value is considered a side effect\n\nThis is the reason why you want to update your ref value in event handlers and effects and not during rendering\n\n\n- Ex. Imagine we have a list of users. When we click one, it logs out the username after 1 second. Now imagine that we click on a user, then click on another before the `console.log` occurs.\nIn this scenario, class components would log out user2, while functional components would log out user1.\n\t- This shows that functional components attach the call made to the user selected at the time. If we want to make a functional component that behaves like a class, we can use a ref, which is a value that is shared between all renders (in this sense it's sort of like an instance variable) <-- (in retrospect I think this should read \"static variable\", since it exists across all renders, similar to how static variables exist across all instances of that class; however this information came from React docs)\n\nref QUESTION\nI'm hoping someone can help me understand Refs and their use in React. To be clear, I have a decent high-level understanding of them. For instance, I understand that they are an escape hatch from the regular re-rendering lifecycle of react components. We can use them to access DOM nodes directly, and we can use `useRef` to get a value that persists underneath the re-rendering nature of components.\n\nHowever, I struggle to see how this relates to the use of refs in the modal library I'm using (`react-native-modalize`). This library exports a component called `<Modalize />` which wraps a component, putting it in a modal. In addition to others, `<Modalize />` takes in 2 props: `ref` and `contentRef`. `ref` lets us control the modal via 2 methods `myRef.current.open()` and `myRef.current.close()`. This makes sense, but why do we need refs here? Why couldn't the library be written in a way that we just control the modal with state `isOpen`/`isClosed` (perhaps it is an implementation detail rather than the nature of how modals work?). If the value of refs here is supposedly to persist values through successive renders, how does this relate?\n\n### Forward Ref\n- What if we want to take a **ref** that is currently existing in the component and pass it down the tree?\n\t- ie. we can reference a component rendered by `<Child />` from `<Parent />`\n\t- ex. we have a `<Form />` and `<App />`. We have a ref defined in `App` and want to attach it to an input box inside `Form`. We wrap `Form` with `forwardRef`, enabling the component to receive the `ref`. in `Form`, we pass the ref to the input. Now, we can control that component from `App`.\n- the component that has `forwardRef` in it is the component that is receiving the ref (ie. the child)\n- `forwardRef()` will create a component, accepting 2 arguments: `props` and `ref` (a regular component just accepts props)\n\n#### Example\nImagine we have a CheckoutPage component which renders a PaymentForm. The CheckoutPage handles all logic related to network calls in the app, and PaymentForm handles all form logic, including managing its own state. This is a reasonable separation of concerns. However, what happens if we want to access a piece of state in the parent from the child? Imagine the network call our parent made needed to get the address from the form. The \"React Way\" of accomplishing this is to bring the form state up one level to live in the parent component. However, this breaks our separation of concerns pattern. We can use an escape hatch forward ref in this circumstance.\n\nIllustration:\n```js\nconst Parent = () => {\n    const myRef = useRef();\n    return <Child ref={myRef} />;\n}\n\nconst Child = React.forwardRef((props, ref) => {\n    const [myState, setMyState] = useState('This is my state!');\n    useImperativeHandle(ref, () => ({getMyState: () => {return myState}}), [myState]);\n})\n```\nThen you should be able to get myState in the Parent component by calling: `myRef.current.getMyState()`\n\n#### Combined ref\n- for low-level UI development, it's common to want to use both a local ref and an external one (`forwardRef`).\n\n### UER\n[Dan Abramov on refs](https://overreacted.io/making-setinterval-declarative-with-react-hooks/)","url":"https://tycholiz.github.io/Digital-Garden/notes/ee25fc8c-9602-405c-960a-98601fb2ca8a.html","relUrl":"notes/ee25fc8c-9602-405c-960a-98601fb2ca8a.html"},{"doc":"Memo","title":"Memo","hpath":"react.lang.memo","content":"\n# Memo\n- def - a HOC that memoizes the rendered output of a component\n- `Memo` accepts a second arg that lets us determine whether or not the component should re-render (defaults to `true`, meaning it won't re-render)\n- When deciding to update DOM, React first renders your component, then compares the result with the previous render result. If the render results are different, React updates the DOM.\n\t- To be clear, React will always render, but won't always update DOM\n- When we wrap our component with `Memo`, the first render will be memoized. Every subsequent render will involve the usual check of oldProps/oldState vs nextProps/nextState, but instead of rendering the component *then* comparing the output to the DOM, it will just take the memoized output. The logic is: since props or state didn't change, we shouldn't need to re-render it (which may well be the case for some components; in which circumstance we should certainly wrap the component with `Memo`\n- Components using hooks can be freely wrapped in React.memo() to achieve memoization\n- use mostly when component trees get too wide or too deep\n\n## When to use\n1. the component is purely functional (if class, use PureComponent and modify SCU), and given same props, will always return the same component\n2. the component renders often\n\t- often what causes this is a Parent with state/props that change often, and a Child who would receive the same props for a longer period of time. In this case, memoize the Child\n\t\t- ex. we have a `<MoviePage />` component and inside it a `<MovieInfo title={props.title} />`. MoviePage holds the state to determine how many thumbs up a movie has, and it pings the server every second. Therefore, when the number changes, the component re-renders, and thereby forces MovieInfo to render. The issue here is that `props.title` is not going to change each time, so React is needlessly rendering the component each time. \n3. the component often receives the same props in between renders\n4. the component contains a decent amount of components that are subject to conditional rendering (and depend on a prop)","url":"https://tycholiz.github.io/Digital-Garden/notes/14742488-3d85-40aa-a171-8d0d5a384e6c.html","relUrl":"notes/14742488-3d85-40aa-a171-8d0d5a384e6c.html"},{"doc":"Hooks","title":"Hooks","hpath":"react.lang.hooks","content":"\nThe value of using Hooks is being able to decouple React-y logic things, like state management and side effects, from the actual component that that it is actually existing in\n","url":"https://tycholiz.github.io/Digital-Garden/notes/59dedd99-adb7-430f-a849-b303425e3209.html","relUrl":"notes/59dedd99-adb7-430f-a849-b303425e3209.html"},{"doc":"useReducer","title":"useReducer","hpath":"react.lang.hooks.useReducer","content":"\nPrefer `useReducer` over `useState` when either:\n1. The state is complex, where it has multiple sub-values (ie. an object)\n2. the next state depends on the previous state\n\n`useReducer` also lets you optimize performance for components that trigger deep updates because you can pass dispatch down instead of callbacks.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d2cb1cf4-1f2d-4e47-8e7c-ca422ea4d700.html","relUrl":"notes/d2cb1cf4-1f2d-4e47-8e7c-ca422ea4d700.html"},{"doc":"useEffect","title":"useEffect","hpath":"react.lang.hooks.useEffect","content":"\n# useEffect\n- *The question is not “when does this effect run”, the question is “with which state does this effect synchronize with:”*\n\t- `useEffect(fn)` - all state\n\t- `useEffect(fn, [])` - no state\n\t- `useEffect(fn, [these, states])`\n- if one of the variables in the dependency array changes, `useEffect` runs again. If the array is empty the hook doesn't run when updating the component at all, because it doesn't have to watch any variables.\n- if you have no dependency array and are getting infinite loops, see if there are functions being defined each time in the component. It might be a simple fix to memoize them with `useCallback`\n\t- simply adding an empty dependency array is a bandaid solution and is not really addressing the root of the problem.\n- By passing an empty array, we’re saying “only ever do this once”.\n\t- on rare occasions that’s ok, but most of the time you want something in there. The reason is that usually you want to synchronize with something in your code, not just perform the effect once\n\n## Shortcoming of useEffect\n- Compared to class component lifecycle methods, the shortcoming of `useEffect` is that we while we can set new state, we are unable to access current state (because of stale closure)\n```js\nuseEffect(() => {\n    const intervalId = setInterval(() => {\n        setCount(count + 1)\n    }, 1000)\n    return () => clearInterval(intervalId)\n}, [])\n```\n- in this example, `count` is always pointing to the previous reference.\n- we can work around this shortcoming with refs. Essentially, because refs exist between renders, that value is never lost between mounts. We simply take the value from the ref, and update the state with that value:\n```\nuseEffect(() => {\n    const intervalId = setInterval(() => {\n        countRef.current = countRef.current + 1\n        setCount(countRef.current)\n    }, 1000)\n    return () => clearInterval(intervalId)\n}, [])\n```\n\n`this` is mutable state, and the problem with mutable state is that it is always up to date. The good thing about hooks in react is that there is no `this` get retrieve values from. State always stays the same within a given render of a component.","url":"https://tycholiz.github.io/Digital-Garden/notes/f33d5218-165a-4996-8603-7f039bcc227d.html","relUrl":"notes/f33d5218-165a-4996-8603-7f039bcc227d.html"},{"doc":"useCallback","title":"useCallback","hpath":"react.lang.hooks.useCallback","content":"\n# useCallback & useMemo\nThe purpose of useCallback and useMemo is to prevent unnecessary re-renders and make your code more efficient.\n- In the days of class components, we had a `render()` function. Anything placed inside would get re-calculated on every render, but we could also give the class methods (putting functions outside of `render()`). These methods would not get recalculated on each render, and would lead to performance gains. Since functional components naturally render everything (`render()` method is implicit), they inherently have a performance issue. `useCallback` and `useMemo` exist to alleviate this problem.\n- *useCallback* - Allows you to cache an instance of a function between renders.\n- *useMemo* - Allows you to cache a value between renders.\n\t- the cache is local to the component position w/in the tree\n- You should consider using useCallback and/or useMemo hooks on the following situations:\n1) Processing large amounts of data\n2) Working with interactive graphs and charts\n3) Implementing animations\n4) Incorporating component lazy loading (useMemo specifically)\n\n## useCallback\n- useCallback returns a memoized callback.\n- `useCallback` accepts a function, and returns the same instance of the function being passed instead of creating a new one when a component re-renders, which is the default behavior.\n-  with this hook, a new function is only created when the variables passed to the dependency array change (kind of like `useEffect`). Otherwise, we use the same instance of that function between re-renders of the component (ie. different instances of the same component).\n- Recall that in React, components will only re-render (and will ALWAYS re-render) when there are changes to either its state or changes to the props it receives. This means that not only will the JSX re-render, but the functions defined in the component will be called each time. But what if we want to prevent those functions from being called each time, and only call them when some values change?\n```js\nconst additionResult = useCallback(add(firstVal, secondVal), [firstVal, secondVal])\n```\n- In this example, the `additionResult` doesn't necessarily get called each time the component re-renders. In fact, it only gets called if either `firstVal` or `secondVal` have changed between those 2 renders.\n\n- [when to use](https://kentcdodds.com/blog/usememo-and-usecallback)\n\n## useMemo\n- similar to `useCallback`, but instead of returning a function, it calls the function and gives us the return value of the function (again, only when one of the dependencies change)\n\t- In other words, useMemo calls the passed function only when necessary and it returns a cached value on all the other renders.\n- `useMemo` \"caches\" the value, while `useCallback` \"caches\" the function\n\n### Example\nImagine we had a function that we passed down to a child component.\n```js\nconst getItems = () => {\n\treturn [number, number + 1, number + 2]\n}\n```\n[Fayez example](https://gist.github.com/fayezosaadi/8d54fe30d8bf855c23e6d7c13a31e346)","url":"https://tycholiz.github.io/Digital-Garden/notes/f4849eb5-f56b-409b-bbe4-344f4521ce4a.html","relUrl":"notes/f4849eb5-f56b-409b-bbe4-344f4521ce4a.html"},{"doc":"Controlled Components","title":"Controlled Components","hpath":"react.lang.controlled-components","content":"\n***[Controlled Component](https://dev.to/stanleyjovel/simplify-controlled-components-with-react-hooks-23nn)*** - components within a form whose state is controlled with `setState`, rather than the built-in functionality provided by HTML forms\n- ie. an HTML `input` that receives its `value` property from a prop in the component\n```js\nconst ControlledInput = ({ value, eventHandler }) => (\n\t<input value={value} onChange={eventHandler} />\n)\n```\n- It means that when the user types the letter “J” on the input, what is visible is not the same “J”, it may be an identical “J” that comes from the state, or whatever the event handler has put in there.\n- Controlled components are the highly recommended approach","url":"https://tycholiz.github.io/Digital-Garden/notes/6fbef260-750f-4a85-b1ed-a80636b72366.html","relUrl":"notes/6fbef260-750f-4a85-b1ed-a80636b72366.html"},{"doc":"SSR","title":"SSR","hpath":"react.SSR","content":"\n# Server Side Rendering (SSR)\nThe most common use case for server-side rendering is to handle the initial render when a user (or search engine crawler) first requests our app. When the server receives the request, it renders the required component(s) into an HTML string, and then sends it as a response to the client. From that point on, the client takes over rendering duties.\n- When using Redux with server rendering, we must also send the state of our app along in our response, so the client can use it as the initial state. This is important because, if we preload any data before generating the HTML, we want the client to also have access to this data. Otherwise, the markup generated on the client won't match the server markup, and the client would have to load the data again.\n- To send the data down to the client, we need to:\n\n1. create a fresh, new Redux store instance on every request;\n2. optionally dispatch some actions;\n3. pull the state out of store;\n4. and then pass the state along to the client.\nRedux's only job on the server side is to provide the initial state of our app.\n\nOn the server, you will want each request to have its own store, so that different users get different preloaded data.\n\n## UE Resource\n- [Good breakdown of SSR](https://www.freecodecamp.org/news/react-server-components/)","url":"https://tycholiz.github.io/Digital-Garden/notes/b60fd6e8-74f9-4223-8395-0af4663a5410.html","relUrl":"notes/b60fd6e8-74f9-4223-8395-0af4663a5410.html"},{"doc":"React Native","title":"React Native","hpath":"react-native","content":"\n# How React Native Works\n- When we `debug remotely`, we are running our JS code in the browser, as opposed to on the device.\n\t- when we do this, a web socket connection is made between the device and the browser\n\t- Since this is a different JS engine than would be used in production, it's possible that errors occur in one but not the other.\n- You can consider React Native as a browser that instead of rendering divs, spans, h1, inputs it renders instead native components. The nice thing is that the native components run in their own native thread (that means you get native performance) and your javascript runs in it's own thread and orchestrates all the native components.\n- In production, when the application starts, it starts running code from your javascript bundle and that will drive which components are to be created on the screen.\n- In development, React Native uses watchman to detect when you've made code changes and then automatically build and push the update your device without you needing to manually refresh it.\n\n## Building the app (run-ios/run-android)\n- When we run `react-native run-ios`, a list of iOS simulators is searched, and the default one is started.\n- Since by default it is run in Debug mode (ie. development mode), a series of `xcrun` commands are run, culminating in the `xcodebuild` commands being run, which results in the app being built on the device.\n- Once the app is successfully built, a request is sent to the metro bundler URL (API at port 8081 mentioned above). Metro will bundle the javascript in response to the app having been built.\n\t- In Release mode (ie. production), the bundle is pre-packaged, which is why we need to change where the ios device looks for the bundle (`App.Delegate.m` file)","url":"https://tycholiz.github.io/Digital-Garden/notes/fb9dbfa7-978e-4f56-aeb7-3c97fb6c9e93.html","relUrl":"notes/fb9dbfa7-978e-4f56-aeb7-3c97fb6c9e93.html"},{"doc":"Threads","title":"Threads","hpath":"react-native.threads","content":"\n## Threads\n- There are 2 main threads in a RN app: the main thread  and the javascript thread\n\t- The main thread runs on native platforms and handles displaying the elements of the UI and processes user gestures.\n\t- The JS thread executes JS code in a separate JS engine \n\t\t- Therefore, it deals with the business logic of the app\n\t\t- Also, it defines the structure and functionality of the UI\n- These 2 threads never communicate directly and never block each other\n- Between these 2 threads is the bridge, which has 3 characteristics:\n\t1. Asynchronous communication between the threads\n\t2. Batched communication between threads\n\t3. Serializable, meaning the threads never operate on the same data— instead exchanging serialized messages\n","url":"https://tycholiz.github.io/Digital-Garden/notes/7b3c85d2-bf47-48c2-a81e-46dd1d111b18.html","relUrl":"notes/7b3c85d2-bf47-48c2-a81e-46dd1d111b18.html"},{"doc":"Performance","title":"Performance","hpath":"react-native.performance","content":"\n# Performance\n- adding `shouldComponentUpdate` will drastically help with optimization.\n    Consider this as you are building the app rather than considering it afterwards.\n    - Alternatively, if you're concerned about a component re-rendering too many times, try using React.PureComponent\n- [library to check how many times a component re-rendered needlessly](https://github.com/maicki/why-did-you-update)\n    - the library is archived, but apparently works perfectly. alternatively: [why-did-you-render](https://github.com/welldone-software/why-did-you-render)\n- [Spying the queue (focusing on the data passing over the bridge)](https://callstack.com/blog/react-native-how-to-check-what-passes-through-your-bridge/?ref=hackernoon.com)","url":"https://tycholiz.github.io/Digital-Garden/notes/56fb3cf1-0e0e-4082-8f34-0122acafe458.html","relUrl":"notes/56fb3cf1-0e0e-4082-8f34-0122acafe458.html"},{"doc":"Metro","title":"Metro","hpath":"react-native.metro","content":"\n# Metro Bundler\n- when we run `yarn start` both a node.js server and metro bundler are started.\n- Metro can be thought of as similar to \"webpack for react native\"\n- Metro is a JavaScript bundler. It takes in an entry file and various options, and gives you back a single JavaScript file that includes all your code and its dependencies.\n\t1. Metro builds a dependency graph of all modules required from the entry point (ie, our whole react-native app)\n\t2. Metro transforms modules into a format that React-Native can understand\n\t3. Metro serializes the transformed modules to form a bundle\n\t\t- A bundle is just a bunch of modules combined into a single js file.\n\t4. The bundlefile gets installed on the device, where its code is then executed.\n\t\t- Remember that when you are writing code for a React Native application, your code is not \"translated\" to Java/Swift/whatever. The Native Modules will send events to a Javascript thread, and the JS thread will execute your bundled React Native code.\n- I/O of Metro\n\t- Input - entryfile along with any options\n\t- Output - the bundle\n- Metro bundler is not used for production\n\t- ex. running `react-native run-ios --configuration Release`\n\nThe Bundle\n- The bundlefile can be found at `http://localhost:8081/index.bundle?platform=ios&dev=true&minify=false`. This is the API through which the device will access the bundle\n\t- this is served from memory, therefore does get written into our project directory\n- The bundle may be stored in different formats, such as binary or .bundle file.\n- Metro bundler also translates JSX to standard javascript\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f29d12b9-5f09-4f15-9cc1-62491736698d.html","relUrl":"notes/f29d12b9-5f09-4f15-9cc1-62491736698d.html"},{"doc":"Ramda","title":"Ramda","hpath":"ramda","content":"\nThere are two main guiding principles of Ramda:\n- Data comes last\n- Everything gets curried\n\nThese two principles lead to a style that functional programmers call *point-free* (a.k.a *tacit programming*)\n- Think of point-free code as “Data? What data? There’s no data here.”\n\nWe can make *point-free* transformations by converting functions that take in data to new functions that accept that data and after having already decided what to do with it:\n```\nconst forever21 = age => ifElse(gte(__, 21), always(21), inc)(age)\n// becomes:\nconst forever21 = ifElse(gte(__, 21), always(21), inc)\n```\nnow, the function `forever21` can be used without having to worry about data until the last minute.\n- Note: there is no behavioral difference in these two versions. We’re still returning a function that takes an age, but now we’re not explicitly specifying the age parameter.\n\nRamda `map` and `filter` (maybe more?) automatically convert different datatypes into functors\n- This allows us to take a function that was made for an array, and apply it to an object, or even a string\n```\n// An array\ntransform(['Optimus Prime','Bumblebee','Ironhide','Sunstreaker','Ratchet'])\n//=> [ 'EMIRP SUMITPO', 'EDIHNORI', 'REKAERTSNUS', 'TEHCTAR' ]\n \n// A string\ntransform('Optimus Prime')\n// => [ 'R' ]\n \n// Even an object\ntransform({ leader: 'Optimus Prime', bodyguard: 'Ironhide', medic: 'Ratchet' })\n// => { leader: 'EMIRP SUMITPO', bodyguard: 'EDIHNORI', medic: 'TEHCTAR' \n```\n\n## Math functions\nthese functions take their arguments in what seems like normal order (is the first argument greater than the second?) That makes sense when used in isolation, but can be confusing when combining functions. These functions seem to violate Ramda’s “data-last” principle, so we’ll have to be careful when we use them in pipelines and similar situations. That’s when `R.flip` and the placeholder `R.__` will come in handy.\n\n## Symbol replacements\n### Function methods\n- The following methods are best suited for functions\n    - ex. `either(wasBornInCountry, wasNaturalized)`\n    \n`&&` -> `R.both`\n\n`||` -> `R.either`\n\n`!` -> `R.complement`\n\n### Value methods\n- The following methods are best used with values\n\n`&&` -> `R.and`\n\n`||` -> `R.or`\n\n`!` -> `R.not`\n\n### Inequality\n`gt`, `lt`, `gte`, `lte`\nif we are passing data in, then we most likely want to provide a placeholder for the first arg\nex. using conditionals (`R.ifElse`), `R.pipe`/`R.compose`\n```\nconst forever21 = age => ifElse(gte(__, 21), always(21), inc)(age)\n```\n\n## Debugging Ramda\n[more info](https://blog.carbonfive.com/2017/12/20/easy-pipeline-debugging-with-curried-console-log/)\n\nRamda provides us with a function `R.tap` which we can use to create side effects without interrupting the flow of an existing composition\n- `tap` accepts a function, and returns a function (ex. `console.log`) that may take in the passed in arg\n```\nR.compose(\n    R.tap(x => console.log('REVERSE:',x)),\n    R.map(R.reverse),\n)\n```\nthis allows us figure out what happened after `reverse` was called on the data\n\nControl flow (`if`/`else`) is less necessary in functional programming, but still occasionally useful.\n\n### Data flow\nRemember that Ramda is flexible with how you combine functions, as long as the expected types are received\n```\nconst alwaysDrivingAge = age => ifElse(lt(__, 16), always(16), a => a)(age)\n```\nHere, an age variable goes through the ifElse, and if fails the condition, will proceed to the third arg (else), where age is the first argument of the function\n\n#### Arg order on `pipe`/`compose` when function arg passed in through the data flow\n```\nconst publishedInYear = year => \n    book => \n        book.year === year\nconst titlesForYear = year => \n\tR.pipe(\n\t\t//pIY returns a function that takes a book as its arg\n\t\tR.filter(publishedInYear(year)), \n\t\tR.map(book => book.title)\n\t)\n\nconsole.log(titlesForYear(1934)(books))\n```\n\n#### `R.when`/`R.unless`\nsimilar to `ifElse`, but with only one conclusion. Implicitly, `R.identity` is the second conclusion (ie., it is *else*)\n```\nconst alwaysDrivingAge = age => when(lt(__, 16), always(16))(age)\n```\n\n\n\n## Working with functions that you didn't write\n### `R.apply`\nspread out a single array into individual arguments. Think of it as `spreadArgs()`\nThis is useful for creating a fixed-arity function from a variadic (variable arity) function\n```\nfunction spreadArgs(fn) {\n    return function spreadFn(argsArr){\n        return fn( ...argsArr );\n    };\n}\n```\nThis is related to usingSpread syntax with functions\n```\nconst arr = [1, 2, 3]\nconst fn = (a, b, c)\nfn(...arr) === fn(arr[0], arr[1], arr[2])\n```\n### `R.unapply`\ngather individual args into a single array\n\n### `R.unary`\nCan be used to cut off all arguments past the first. The benefit of this is that if we call a function and pass in too many arguments, the extra ones will just drop off. \nex. map takes a function, and the first argument of map (the iterated value) gets passed to parseInt. Then the second arg (index) gets passed to parseInt, which corresponds to its radix. Since we don't want index to be interpreted this way, we make it a unary function, and the index argument safely falls off\n```\n[\"1\", \"2\", \"3\"].map(R.unary(parseInt)\n```\n\n### `R.flip`\nswap the first and second argument of a function\n\n### `R.__`\nuse a placeholder argument to go in place of a parameter\nWhen we pass data through some sort of chain (`pipe`, `compose`, `ifElse`), that data will be passed through the methods as the argument in the first available spot.\n- ex. if a fn in `pipe` has no args provided, then the data will be the first arg. If it has one arg, then the data will be applied in the second place (and so on). If we use a placeholder `R.__` as the first arg, then the data will be applied to that space, since it is the first available argument.\n- `const forever21 = age => ifElse(gte(__, 21), always(21), inc)(age)`\n\n## Object manipulation\n### `R.assoc`\nupdate (or create) a property of an object\n```\nconst myObj = {\n\tname: 'kyle'\n}\n\nconst newObj = R.assoc('number', '7788713377')\n\nconsole.log(newObj(myObj))\n// { name: 'kyle', number: '77887133' }\n```\n\n### `R.prop`\nread a single property from an object and return the value\n\n### `R.pick`\npick multiple properties of an object and return a new object with just those properties\n- complement of `R.omit`\n\n### `R.has`\nreturn boolean indicating whether or not a property exists on the object\n\n### `R.path`\ndive into nested objects returning the value at the given path\n\n### `R.propOr`/`R.pathOr`\nsearch for the property on an object, allowing you to provide a default value in case the property/path cannot be found on the obj.\n\n### `R.evolve`\ndeclaratively provides transformations to happen on each property of an object:\n- note: cannot add new properties with this\n```\nvar tomato  = {firstName: '  Tomato ', data: {elapsed: 100, remaining: 1400}, id:123};\nvar transformations = {\n  firstName: R.trim,\n  lastName: R.trim, // Will not get invoked.\n  data: {elapsed: R.add(1), remaining: R.add(-1)}\n};\nR.evolve(transformations, tomato); //=> {firstName: 'Tomato', data: {elapsed: 101, remaining: 1399}, id:123}\n```\nalso\n```\n//before\nconst nextAge = compose(inc, prop('age'))\nconst celebrateBirthday = person => assoc('age', nextAge(person), person)\n\n// after\nconst celebrateBirthday = evolve({ age: inc })\n```\n\n## Array manipulation\n`R.nth`\naccess an array at the given index\nequivalent of object's `R.prop`\n\n`R.slice`\nequivalent of object's `R.pick`\n\n`R.contains`\ncheck if array contains given value\nequivalent of object's `R.has`\n\n`R.head`\naccess first element of array\n\n`R.last`\naccess last element of array\n\n`R.append`/`R.prepend`\nFP versions of `.push` and `.unshift`\n\n`R.drop`/`R.dropLast`\nFP versions of `.shift` and `.pop`\n\n`R.insert`\ninsert an element at given index of an array\n\n`R.update`\nreplace an element at the given index of an array\n\n`R.adjust`\nequivalent of object's `R.evolve`, except only works for one element. We would use this function over `R.update` when using a function to update the array element\n```\nconst numbers = [10, 20, 30, 40, 50, 60]\n \nadjust(multiply(10), 2, numbers) // [10, 20, 300, 40, 50, 60]\n```\n\n`R.remove`\nremove element by index\n\n`R.without`\nremove element by value\n\n### `R.reject()`\ncomplement of `R.filter()`. If `filter()` is *filter in*, then `reject()` is *filter out*\n\n### `.reduce()`\n\nwith `initialValue`\n![](/assets/images/2021-03-07-22-18-20.png)\n\nwithout `initialValue`. The first value of the list will act in place of the initialValue and the combining will start with the second value in the list\n![](/assets/images/2021-03-07-22-18-41.png)\n\nIf the array passed to `.reduce()` is empty, then there must be an `initialValue` specified, otherwise there will be an error\n\n### `.map()`\n#### use cases\n- transform a list of functions into their return values:\n```\nvar one = () => 1;\nvar two = () => 2;\nvar three = () => 3;\n\n[one,two,three].map( fn => fn() );\n// [1,2,3]\n```\n- transform a list of functions by composing each of them with another function, and then execute them:\n```\nvar increment = v => ++v;\nvar decrement = v => --v;\nvar square = v => v * v;\n\nvar double = v => v * 2;\n\n[increment,decrement,square]\n.map( fn => compose( fn, double ) )\n.map( fn => fn( 3 ) );\n// [7,5,36]\n```\n\n### `R.chain` (a.k.a. flatMap)\n\niterate over values of a list, performing the provided function on each element, then concatenating all of the results together\n- what results is if we were to perform a function of each element, then flatten that resulting array\n```\nvar firstNames = [\n\t{ name: \"Jonathan\", variations: [\"John\", \"Jon\", \"Jonny\"] },\n\t{ name: \"Stephanie\", variations: [\"Steph\", \"Stephy\"] },\n\t{ name: \"Frederick\", variations: [\"Fred\", \"Freddy\"] }\n];\n\nR.chain(entry => [entry.name, ...entry.variations], firstNames)\n// [\"Jonathan\",\"John\",\"Jon\",\"Jonny\",\"Stephanie\",\"Steph\",\"Stephy\",\n//  \"Frederick\",\"Fred\",\"Freddy\"]\n```\n\n### `R.zip`\nAlternate through 2 arrays and take each value that appears at the same index and put them into their own array. The shorter of the 2 lists is considered the list length\n```\nconst arr1 = [1, 2, 3]\nconst arr2 = ['a', 'b', 'c']\n\nR.zip(arr1, arr2)\n// [[1, a], [2, b], [3, c] ]\n```\n### `R.invoker()` a.k.a. `unboundMethod()`\n\n## Number manipulation\n`R.clamp`\nrestrict a number to be within a certain range\n```\nR.clamp(1, 10, -5) // => 1\nR.clamp(1, 10, 15) // => 10\nR.clamp(1, 10, 4)  // => 4\n```\n\n### `R.complement()`\n implements the same idea for functions as the ! (not) operator does for values.\n nothig good ever comes out of this\n my daring susan\n why cant we jst love each other aagain???\n\n## Fusion (with `.map()`)\nImagine we have multiple functions that each take 1 argument, and each function's return can be passed directly into the next as input:\n```\nfunction truncate(word) {\n\tif (word.length > 10) {\n\t\treturn `${word.substring(0, 6)}...`\n\t}\n\treturn word\n}\n\nfunction upper(word) {\n\treturn word.toUpperCase()\n}\n\n\nfunction append1(word) {\n\treturn word + '1'\n}\n```\nNaively, we could do this:\n```\nconst generateList = arr\n    .map(append1)\n    .map(upper)\n    .map(truncate)\n```\nwhen we pass a function to map, like `.map(fn)`, the element of the list gets passed as the first argument to that function. Here, when we pass `R.pipe`, the element gets passed as first argument to `pipe` (since pipe is ltr)\n```\nconst generateList = arr.map(R.pipe(append1, upper, truncate))\n```\n\n# Lenses\nWith the following more specific ways of creating lenses, `R.lens()` is not often needed\n\n#### `R.lensProp`\nlets us create a lens that focuses on a *non-nested* property of an object\n\n#### `R.lensPath`\nlets us create a lens that focuses on a *nested* property of an object\n\n#### `R.lensIndex`\nlets us create a lens that focuses on an element of an array\n- these are for the times where we know in advance the index that we are interested in, but don't yet have the data (*ex. capitalize first letter*)\n```\nconst toTitle = R.compose(\n\tR.join(''),\n\tR.over(R.lensIndex(0), R.toUpper)\n)\n```\n\n### Three functions for working with lenses\n#### `R.view()`\nread value of the lens\n\n#### `R.set()`\nset the value of the lens\n- note: this does not mutate the original object supplied. In other words, `R.view()` will give us the exact same result before and after #### `R.set()`\n\n#### `R.over()`\napply a transformation function to the lens\n```\nover(nameLens, toUpper, person)\n// => {\n//   name: 'RANDY',\n//   socialMedia: {\n//     github: 'randycoulman',\n//     twitter: '@randycoulman'\n//   }\n// }\n```\n\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8e83deb1-8f7c-439e-b991-70e544645bfb.html","relUrl":"notes/8e83deb1-8f7c-439e-b991-70e544645bfb.html"},{"doc":"CLI","title":"CLI","hpath":"ramda.cli","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/25e0eab0-64a4-48a1-ba21-f1c98fd45df9.html","relUrl":"notes/25e0eab0-64a4-48a1-ba21-f1c98fd45df9.html"},{"doc":"Pipe","title":"Pipe","hpath":"ramda.cli.pipe","content":"\n# `R.pipe()`\n`pipe(..)` is identical to `compose(..)` except it processes through the list of functions in left-to-right order:\n`var pipe = reverseargs( compose )`\n\nthe function returned by `pipe` takes the same number of arguments as the first function `.pipe()` is given\n\nwhen we call `operate(3, 4)`, pipe passes the 3 and 4 to the `multiply` function, resulting in 12. it passes that 12 to addone, which returns 13. it then passes that 13 to square, which returns 169, and that becomes the final result of operate.\n```\nconst operate = pipe(\n  multiply,\n  addOne,\n  square\n)\n```\n\nex. improvement with `R.pipe`\n```\n// before\nconst titlesForYear = (books, year) => {\n  const selected = filter(publishedInYear(year), books)\n \n  return map(book => book.title, selected)\n}\n\n// after\nconst titlesForYear = year =>\n  pipe(\n    filter(publishedInYear(year)),\n    map(book => book.title)\n  )\n```\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e968a5c6-9ca5-4b6e-bcb5-6ec3990e8ad9.html","relUrl":"notes/e968a5c6-9ca5-4b6e-bcb5-6ec3990e8ad9.html"},{"doc":"Compose","title":"Compose","hpath":"ramda.cli.compose","content":"\n# `R.compose`\ncomposing two functions and then mapping the resulting function over a functor should be the same as first mapping one function over the functor and then mapping the other one.\n- ie. if you plan to map over the same array multiple times, just compose the functions and then map over the array once with that composed function\n```\n[\"1\", \"2\", \"3\"].map(r.unary(parseint)\n```\nthe data flow is: \n```\narrayvalue <-- map <-- unary <-- parseint\n```\n(p.s., any time you see data flow presented this way, think `r.compose()`\n\n`parseint` is the input to `unary(..)`. the output of `unary(..)` is the input to `map(..)`. the output of `map(..)` is `arrayvalue`. this is the composition of map(..) and unary(..).\n\nhaving a series of functions whose input is the output provided by the previous (inner) function\n\n![](/assets/images/2021-03-09-09-34-17.png)\n\nencapsulating a series of functions within one function\n![](/assets/images/2021-03-09-09-34-28.png)\n\nfunctions compose from right to left (incl. `r.compose()`)\n\n- expl. this is consistent with how we evaluate functions, from inner to outer (ie. right to left)\nex:\n```\n[\"1\", \"2\", \"3\"].map(r.unary(parseint)\n```\n\ncomposition is the wrapper around the big machine, whose contents are the parameters of `r.compose()`. in other words, the components of the machine (under the wrapper) are the individual functions that the origvalue goes through (as an argument). the composed machine returns a new function, so it is in essence a function factory.\nrightmost functions are at the top of the machine.\n![](/assets/images/2021-03-09-09-34-44.png)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d77facf2-792e-420d-80c3-49bbd4656c90.html","relUrl":"notes/d77facf2-792e-420d-80c3-49bbd4656c90.html"},{"doc":"Python","title":"Python","hpath":"python","content":"\n**kwargs** - similar to `...args` in javascript\n\n### Self\n- a reference to the current instance of the class\n- `self` is always the first argument of a class method (including the `init` method)\n    - In the `init` method, `self` refers to the newly created object\n    - In other class methods, it refers to the instance whose method was called.\n\n### .init\n- it is a constructor\n- Called when an object is instantiated.\n- It will initialize the attributes of the class\n\n### super\n- give the current object access to the methods from its superclass\n- `super` returns a temporary object of the superclass, which allows us to call that superclass's methods\n- Common use case\n    - building classes that extend functionality of previously built classes\n  \n# Celery\n- Async task queueing system that can allow you to queue up jobs at schedules, or have them queue up on a trigger (ex. on some get request).\n","url":"https://tycholiz.github.io/Digital-Garden/notes/7a81c659-7511-48c0-a78f-552d0e6cfff7.html","relUrl":"notes/7a81c659-7511-48c0-a78f-552d0e6cfff7.html"},{"doc":"Pulumi","title":"Pulumi","hpath":"pulumi","content":"\nA Framework for Infrastructure as Code\nDescribes how infra will look in the cloud\nex. here is what our postgres database looks like\n\nall config is done through pumuli.yaml\n- get access to the encrypted secrets (which we can check in through version control)","url":"https://tycholiz.github.io/Digital-Garden/notes/db9cdb23-f02b-426c-b27e-baafb72916bd.html","relUrl":"notes/db9cdb23-f02b-426c-b27e-baafb72916bd.html"},{"doc":"Postgres","title":"Postgres","hpath":"postgres","content":"\nPostgres is a DBMS.\n- \"SQL tells the database what information you want, and the DBMS determines the best way to provide it.\"\n\n[Exercises for learning SQL with PG](https://pgexercises.com/)\n\nIt's possible to scale Postgres to storing a billion 1KB rows entirely in memory - This means you could quickly run queries against the full name of everyone on the planet on commodity hardware and with little fine-tuning.\n- Postgres can easily handle 10,000 insertions per second.\n\nIt's rarely a mistake to start with Postgres and then switch out the most performance critical parts of your system when the time comes.","url":"https://tycholiz.github.io/Digital-Garden/notes/dff06309-f792-4ee2-a0ef-529972167027.html","relUrl":"notes/dff06309-f792-4ee2-a0ef-529972167027.html"},{"doc":"Tools","title":"Tools","hpath":"postgres.tools","content":"\n- [SchemaSpy: tool for documenting database](http://schemaspy.org/)\n- [Citus: Convert into distributed database](https://www.citusdata.com/)\n\t- To your application it still looks like a single database, but then under the covers it’s spread across multiple physical machines and Postgres instances.","url":"https://tycholiz.github.io/Digital-Garden/notes/e245849e-03c3-4903-91e2-b197baf280d6.html","relUrl":"notes/e245849e-03c3-4903-91e2-b197baf280d6.html"},{"doc":"Terms","title":"Terms","hpath":"postgres.terms","content":"\n### Operation\n`select`, `insert`, `update`, `delete`","url":"https://tycholiz.github.io/Digital-Garden/notes/1b131639-a72b-4bca-893d-57648714c80a.html","relUrl":"notes/1b131639-a72b-4bca-893d-57648714c80a.html"},{"doc":"Tablespaces","title":"Tablespaces","hpath":"postgres.tablespaces","content":"\n# Tablespace\n- Tablespaces are where PostgreSQL stores the database objects. Therefore, it is an abstraction between the physical and logical layers (ie. what the data looks like on disk, and what the data looks like in sql-format.)\n- Tablespaces allow you to move your data to different physical locations across drivers easily by using simple commands.\n\t- ex. 2 different objects in one schema might have 2 different underlying tablespaces.\n- By default, PostgreSQL provides you with two tablespaces:\n\t1. The `pg_default` is for storing user data.\n\t2. The `pg_global` is for storing system data.\n- A common use of tablespaces is for performance\n\t- ex. an index that is used often can be tablespaced on a faster SSD. \n\t- ex. a table with cold data that is rarely accessed can be tablespaced on a slower magnetic HDD.\n- At the file layer (of data storage), a single file can only correspond to a single tablespace.\n- Using tablespaces, we can provision different locations for our data, based on the idea of classifying data as hot, warm or cold (determined by frequency that data will be needed)\n\t- This would involve us having a different tablespace in each storage group (ie. a different path to the physical localtion). This means that moving data from hot to warm involves changing the tablespace that data is associated with.\n\nhttps://techchannel.com/SMB/9/2012/storage-groups-hot-warm-cold#:~:text=The%20classifications%20are%20often%20referred,stored%20on%20even%20slower%20storage.","url":"https://tycholiz.github.io/Digital-Garden/notes/4f29630c-df08-4a56-a4f2-cddda8fb6be4.html","relUrl":"notes/4f29630c-df08-4a56-a4f2-cddda8fb6be4.html"},{"doc":"Scaling","title":"Scaling","hpath":"postgres.scaling","content":"\n### Streaming Replication\n- streaming replication allows us to continuously apply WAL XLOG records to standby servers, so that they are kept current\n\t- WALs are write-ahead logs and are synonymous with, which are XLOG are transaction logs (X stands for transaction)","url":"https://tycholiz.github.io/Digital-Garden/notes/b4071915-0b79-4f6a-bd50-4deeff7751bb.html","relUrl":"notes/b4071915-0b79-4f6a-bd50-4deeff7751bb.html"},{"doc":"Pub Sub","title":"Pub Sub","hpath":"postgres.pubsub","content":"\nVanilla Postgres allows us to define workers which watch a new events channel, and attempt to claim a new job whenever one is pushed to the channel.\n- Postgres also lets other services watch the status of the events with no added complexity.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f0d27239-57fc-40f5-b395-23de06b39ed7.html","relUrl":"notes/f0d27239-57fc-40f5-b395-23de06b39ed7.html"},{"doc":"Psql","title":"Psql","hpath":"postgres.psql","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e381e37f-eb16-4371-a599-ac105c1e4dbb.html","relUrl":"notes/e381e37f-eb16-4371-a599-ac105c1e4dbb.html"},{"doc":"Cmds","title":"Cmds","hpath":"postgres.psql.cmds","content":"\n# CLI (psql/pgcli) \ncreate database - `CREATE DATABASE mydb;`\nshow all dbs - `\\l`\ncreate user - `CREATE USER bob WITH password 'hello'`\ndrop db - `drop database mydb;`\ndrop schema - `DROP SCHEMA myschema`\nconnect to db - `\\c mydb`\ncreate schema - `CREATE SCHEMA myschema`\nshow schemas - `\\dn`\nchange user password `ALTER USER davide WITH PASSWORD 'hu8jmn3';`\ndisplay tables in \"public\" schema - `\\dt public.*`\ndisplay table signature `\\d <TABLENAME>`\ndisplay types `\\dT <SCHEMA>`\ndisplay roles - `\\du`\ndisplay schema - `\\dn`\ndisplay functions - `\\df`\ndisplay extensions - `\\dx`\nread in commands from a sql file- `\\i`\n- ex. `\\i migration.sql`\n\ndisplay info about current connection - `\\conninfo`\n- current user, database, port etc.\n\ncycle command history - `<C-r>`\n\n## copy\n\\copy\n- This is the client-side version of the copy command. It gives us a distinct benefit, which is the source file is required on the client side. In other words, we can execute the copy command on the same machine as where the source file is stored; we don't need to upload the file to the postgres server first.\n- copy is much faster than running `insert into...`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/0eea0afc-c328-49ec-8969-a0d085ed67f3.html","relUrl":"notes/0eea0afc-c328-49ec-8969-a0d085ed67f3.html"},{"doc":"Plpgsql","title":"Plpgsql","hpath":"postgres.plpgsql","content":"\nThink of sql commands as having the potential to return rows. In this sense, they are like functions.\n- in javascript we can do something like this:\n```js\nconst returnedValueFromFn = fn()\n```\n\n- in plpgsql we can do something like this:\n```sql\nselect * from users returning * into v_users;\n```\n\nSince these sql commands are like functions, we can imagine that `v_user_id` is an argument to the following query:\n```sql\nselect * from users where id = v_user_id;\n```\n- When executing a SQL command in this way, PL/pgSQL may cache and re-use the execution plan for the command","url":"https://tycholiz.github.io/Digital-Garden/notes/91b2c4f6-ee54-4169-bc63-5994895fc1aa.html","relUrl":"notes/91b2c4f6-ee54-4169-bc63-5994895fc1aa.html"},{"doc":"Types","title":"Types","hpath":"postgres.plpgsql.types","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/4f4cd05c-52e2-4157-86a4-b2870d6d84a7.html","relUrl":"notes/4f4cd05c-52e2-4157-86a4-b2870d6d84a7.html"},{"doc":"Record","title":"Record","hpath":"postgres.plpgsql.types.record","content":"\nA type that holds a single row from a result-set (therefore, not just tables)\n\n`record` type is similar to `row-type`, but `record` doesn't have a predefined structure. The structure only gets established when the `select` or `for` statements assin an actual row to it.\n\nTo access a field in the record, you use the dot notation (.) syntax like this:\n```sql\nrecord_variable.field_name;\n```\n\na record is not a true data type. It is just a placeholder.\n- Also, a record variable can change its structure when you reassign it","url":"https://tycholiz.github.io/Digital-Garden/notes/b4625475-a36a-4ff7-ae2e-c29b559655b7.html","relUrl":"notes/b4625475-a36a-4ff7-ae2e-c29b559655b7.html"},{"doc":"Statements","title":"Statements","hpath":"postgres.plpgsql.statements","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/02c26b5e-8e65-46d7-8e30-69d15959df80.html","relUrl":"notes/02c26b5e-8e65-46d7-8e30-69d15959df80.html"},{"doc":"Raise","title":"Raise","hpath":"postgres.plpgsql.statements.raise","content":"\n`RAISE` can be an easy and powerful way to self-document the code.\n\n## Levels\n- DEBUG\n- LOG\n- INFO\n- NOTICE\n- WARNING\n- EXCEPTION (default level)\n - raises an error, aborting current transaction\n\n## Where and when messages get sent\nraising with different levels will generate messages of different priority levels.\n- Where or not a message of a given priority will be sent to the client or not is determined by the `log_min_messages` and `client_min_messages` configuration variables.\n\t- This file also determines if we write to server log\n\n## Body\nAfter specifying the level, we can provide a simple string literal, which can be followed by optional argument expressions to be inserted into the string\n```\nRAISE NOTICE '% % %', v_job_id, v_job_name;\n```\n\n### Using\nWe can attach additional info to the error by combining `using` with an option, one of:\n- MESSAGE\n- DETAIL\n- HINT\n- ERRCODE\n\t- Specifies the error code (SQLSTATE) to report, either by condition name or directly as a five-character SQLSTATE code\n- COLUMN\n- CONSTRAINT\n- DATATYPE\n- TABLE\n- SCHEMA\n```\nRAISE EXCEPTION 'Nonexistent ID --> %', user_id\n      USING HINT = 'Please check your user ID';\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/58432667-9d51-4163-9150-39bb83b93803.html","relUrl":"notes/58432667-9d51-4163-9150-39bb83b93803.html"},{"doc":"Perform","title":"Perform","hpath":"postgres.plpgsql.statements.perform","content":"\nallows us to do the work of a SELECT statement, but then immediately discard the result\n- useful for when we are doing side-effects with no useful result.\n\n```sql\nPERFORM create_mv('cs_session_page_requests_mv', my_query);\n```\n\n```sql\nPERFORM graphile_worker.add_job(\n\t'user__audit',\n\tjson_build_object(\n\t\t'type', 'reset_password',\n\t\t'user_id', v_user.id,\n\t\t'current_user_id', app_public.current_user_id()\n\t));\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/053c5aa6-d489-4bab-8c61-af1f8b8e2ae7.html","relUrl":"notes/053c5aa6-d489-4bab-8c61-af1f8b8e2ae7.html"},{"doc":"Assert","title":"Assert","hpath":"postgres.plpgsql.statements.assert","content":"\n# Assert\n`ASSERT` is a convenient shorthand for inserting debugging checks, but is not useful for reporting ordinary messages and errors.\n\n- we can give `ASSERT` a value that is always expects to evaluate to true.\n\t- if it does, `ASSERT` does nothing further.\n\t- if it doesn't, an `ASSERT_FAILURE` exception is raised.\n- if we don't pass a message, a default \"assertion failed\" will be used.\n```sql\nASSERT v_user is not null, 'user not found, check the users table';\n```\n","url":"https://tycholiz.github.io/Digital-Garden/notes/6d4c1924-8e2d-4b66-8fb5-5a6aca2d0d16.html","relUrl":"notes/6d4c1924-8e2d-4b66-8fb5-5a6aca2d0d16.html"},{"doc":"For Loops","title":"For Loops","hpath":"postgres.plpgsql.for-loops","content":"\n# For Loops\n```\ndo\n$$\ndeclare\n\trec record;\nbegin\n\tfor rec in select title, length\n\t\t\tfrom films\n\t\t\twhere length > 50\n\t\t\torder by length\n\tloop\n\t\traise notice '% (%)', rec.title, rec.length;\n\tend loop;\nend;\n$$\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/b9b50f66-8241-4660-a45b-db533a91e1fc.html","relUrl":"notes/b9b50f66-8241-4660-a45b-db533a91e1fc.html"},{"doc":"Diagnostics","title":"Diagnostics","hpath":"postgres.plpgsql.diagnostics","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/461d1a92-ffa8-4ba9-953c-7ef3a6ccda8c.html","relUrl":"notes/461d1a92-ffa8-4ba9-953c-7ef3a6ccda8c.html"},{"doc":"Performance","title":"Performance","hpath":"postgres.performance","content":"\n## EXPLAIN ANALYZE\n\"The most powerful tool at our disposal for understanding and optimizing SQL queries\"\n- this is a command that accepts a statement such as SELECT ..., UPDATE ..., or DELETE ..., executes the statement, and instead of returning the data provides a query plan detailing what approach the planner took to executing the statement provided.\n\nThe combination of Bitmap Index Scan and Bitmap Heap Scan is much more expensive than reading the rows sequentially from the table (a Seq Scan)\n\nSeq Scan nodes often indicate an opportunity for an index to be added, which is much faster to read\n\n[source](https://thoughtbot.com/blog/reading-an-explain-analyze-query-plan)","url":"https://tycholiz.github.io/Digital-Garden/notes/ebbcd3ac-04fb-491c-a1fa-0e3ed0c9ea2c.html","relUrl":"notes/ebbcd3ac-04fb-491c-a1fa-0e3ed0c9ea2c.html"},{"doc":"Op","title":"Op","hpath":"postgres.op","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/69bb3860-7c7b-484e-9ec0-cd495bcc6aa8.html","relUrl":"notes/69bb3860-7c7b-484e-9ec0-cd495bcc6aa8.html"},{"doc":"Exists","title":"Exists","hpath":"postgres.op.exists","content":"\n# Exists\nallows us to test for existence of rows in a subquery","url":"https://tycholiz.github.io/Digital-Garden/notes/8802e481-9990-43b9-9795-3998f2a887e5.html","relUrl":"notes/8802e481-9990-43b9-9795-3998f2a887e5.html"},{"doc":"Any","title":"Any","hpath":"postgres.op.any","content":"\n`any` compares a scalar value (ie. a base type, like `text` or `number`) to a set of values returned by a subquery.\n\nThe form is:\n```\nexpression operator ANY(subquery)\n```\nIn this syntax:\n- The subquery must return exactly one column.\n- The `ANY` operator must be preceded by one of the following comparison operator =, <=, >, <, > and <>\n- The `ANY` operator returns true if any value of the subquery meets the condition, otherwise, it returns false.\n\nNote: `SOME` is a synonym for `ANY`, meaning that you can substitute `SOME` for `ANY` in any SQL statement.\nNote: The `= ANY` is equivalent to IN operator.","url":"https://tycholiz.github.io/Digital-Garden/notes/fdbfc8c1-54b5-4024-b218-ba3721e1976e.html","relUrl":"notes/fdbfc8c1-54b5-4024-b218-ba3721e1976e.html"},{"doc":"Lang","title":"Lang","hpath":"postgres.lang","content":"\n- *quotes* - Double is for identifiers (tables, columns, schemas, functions); single is for values (mostly strings)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d274a1e8-98a8-40f9-8667-be48d7c071a4.html","relUrl":"notes/d274a1e8-98a8-40f9-8667-be48d7c071a4.html"},{"doc":"Views","title":"Views","hpath":"postgres.lang.views","content":"\n# View\n- Views are named queries stored in the database. For queries that we perform often, we can effectively give them an alias. The result set of that query is conceptually placed into a table, which is referencable by the name we give it.\n- ex. Imagine we have a join that we perform often. We can put it in a view and get that data like so:\n```\nCREATE VIEW myview AS\n    SELECT city, temp_lo, temp_hi, prcp, date, location\n        FROM weather, cities\n        WHERE city = name;\n\nSELECT * FROM myview;\n```\n- Being liberal with use of Views is a good practice, as they allow us to encapsulate details of the structure of our tables behind consistent inferfaces\n\t- This value shows itself as the structure of our tables change, yet we continue to have a consistent inferface.\n- Besides the read-only views, PostgreSQL supports updatable views.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/6f02b2f1-5534-43f1-a913-dba471f7026f.html","relUrl":"notes/6f02b2f1-5534-43f1-a913-dba471f7026f.html"},{"doc":"Types","title":"Types","hpath":"postgres.lang.types","content":"\n## Data Types\n### Composite Type\n- a type defined by the \"signature\" of the row. In other words, the very combination of columns included in a row make up a **composite type**\n\t- therefore, every column in a given table is an example of a composite type. In fact, every time we create a table, a composite type representing that row's composition is made.\n- PostgreSQL allows composite types to be used in many of the same ways that simple types can be used. \n\t- ex. a column of a table can be declared to be of a composite type.\n\t\t- spec: for instance `first_name` and `last_name`?\n- In Postgres, columns can be of composite types, meaning that we can have a type defined as (email: string, username: string), and have a column in a table called `identity` and have a column with that type.\n- When used in a function, composite types can be thought of as objects, as we would access the value of each subtype with `$1.key`\n\n### JSON\n#### JSON vs JSONB\nIn general, most applications should prefer to store JSON data as jsonb, unless there are quite specialized needs, such as legacy assumptions about ordering of object keys.\n##### JSON\n- JSON - stores an exact copy of the input text, which processing functions must reparse on each execution\n\t- Because the json type stores an exact copy of the input text, it will preserve semantically-insignificant white space between tokens, as well as the order of keys within JSON objects\n\n##### JSONB\n- JSONB - stored in a decomposed binary format that makes it slightly slower to input due to added conversion overhead, but significantly faster to process, since no reparsing is needed\n\t- also supports indexing on GIN and GIST index types.\n\t- does not preserve the order of object keys\n- gives us capability to query into our JSON document for quick lookups\n\n### Text\n- don't use VARCHAR. just use TEXT, and add length limits\n\n#### Citext\n- Case insensitive text\n- behaves like text, except that it lower-cases everything before comparing it\n\t- Therefore, we may want to store things like email or postal code in `citext` rather than `text`.\n\n### Serial\n- SERIAL is the postgres equivalent of autoincrement in other flavors of SQL\n\t- Consider using uuid's though.\n\n### Range\n- We can specify a range in a single column (ex. timestamps, price etc).\n- The real benefit is that you can then have constraints \n\t- ex. certain time stamps can’t overlap \n- To work with ranges, use the [range operators](https://www.postgresql.org/docs/9.3/functions-range.html)\n[Demo](https://wiki.postgresql.org/images/7/73/Range-types-pgopen-2012.pdf)","url":"https://tycholiz.github.io/Digital-Garden/notes/ebb66488-0408-48a4-9da6-888f0677922a.html","relUrl":"notes/ebb66488-0408-48a4-9da6-888f0677922a.html"},{"doc":"Range","title":"Range","hpath":"postgres.lang.types.range","content":"\n## Inclusive/Exclusive Bounds\n- `[]` is used to represent inclusive values\n- `()` is used to represent exclusive values\n- ex. `SELECT '[3,7)'::int4range` is a range that includes 3, but discludes 7\n\n## Omitting Upper/Lower Bounds\nWe can omit upper bounds with `[7,]`\nWe can omit lower bounder with `[,7]`","url":"https://tycholiz.github.io/Digital-Garden/notes/0b049a74-6ff9-4237-8939-7b382e6ba2b0.html","relUrl":"notes/0b049a74-6ff9-4237-8939-7b382e6ba2b0.html"},{"doc":"Numeric","title":"Numeric","hpath":"postgres.lang.types.numeric","content":"\n- `decimal` is an alias for `numeric`\n- there are 2 values inherent to a `numeric` value\n\t1. Precision - total $ of significant digits in a number (ie. size of integer part plus size of decimal part)\n\t2. Scale - size of decimal part. (therefore integers have scale of 0)\n\t- these 2 values should be declared explicitly","url":"https://tycholiz.github.io/Digital-Garden/notes/99a77ea8-4d1b-4c67-b0a1-db212c9e3064.html","relUrl":"notes/99a77ea8-4d1b-4c67-b0a1-db212c9e3064.html"},{"doc":"Money","title":"Money","hpath":"postgres.lang.types.money","content":"\nDon't use money.\n- Instead use `decimal` (aka. `numeric`) with 2 units of forced precision\n\nif you only care about prices to 2 decimal places, then you shouldn't really care about using anything more precise than numeric type with 2 precision decimal places. However, this would coerce incoming values like `44.423` to be rounded to `44.42`, and the amounts could be very far off. Therefore, if we are controlling the prices coming in, and we always know that it will be to 2 decimal places then it shouldn't be a problem.\n- If we are really concerned about this, then we can use microcurrencies, which store the value as a bigint 1 millionth of the numeric value. This allows lots of precisio","url":"https://tycholiz.github.io/Digital-Garden/notes/c4e58bb8-0a6d-45b7-b31d-2bfacbad510d.html","relUrl":"notes/c4e58bb8-0a6d-45b7-b31d-2bfacbad510d.html"},{"doc":"Json","title":"Json","hpath":"postgres.lang.types.json","content":"\n## Working with JSON\n### Functions\nimagine we pass a serialized json object as an argument to a postgres function like so:\n```\n{\n\tusername,\n\tavatar_url,\n\temail,\n\tfirst_name\n}\n```\nWe can then unpack that arg into a *declared* postgres variable:\n```sql\nv_email = profile_object ->> 'email';\n-- imagine this as `email = profile_object.email`\n-- `v_email :=` syntax is identical\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/4f288dda-1a69-4f68-999e-873777d08636.html","relUrl":"notes/4f288dda-1a69-4f68-999e-873777d08636.html"},{"doc":"Interval","title":"Interval","hpath":"postgres.lang.types.interval","content":"\nAllows us to store and manipulate a period of time.\n```\ninterval '2 months ago';\ninterval '3 hours 20 minutes';\n```\n\nInternally, PostgreSQL stores interval values as months, days, and seconds. The months and days values are integers while the seconds can field can have fractions.\n\nThe interval values are very useful when doing date or time arithmetic. For example, if you want to know the time of 3 hours 2 minutes ago at the current time of last year, you can use the following statement:\n```\nSELECT\n\tnow(),\n\tnow() - INTERVAL '1 year 3 hours 20 minutes'\n             AS \"3 hours 20 minutes ago of last year\";\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/39fcba78-dc07-495d-a01c-4a4b108cf3b7.html","relUrl":"notes/39fcba78-dc07-495d-a01c-4a4b108cf3b7.html"},{"doc":"Date","title":"Date","hpath":"postgres.lang.types.date","content":"\n#### current_date\nspec: when doing \"math\" on dates that don't have a time attached to them, then midnight is used:\n```sql\nselect count(*)\nfrom app_public.users\nwhere created_at > current_date\nand created_at < current_date + interval '1 day'\n```\nIf current_date gets rounded back to midnight, then, the first `where` is saying \"give me all the rows created since midnight\", and the second `where` is saying \"until midnight 24 hours later\"","url":"https://tycholiz.github.io/Digital-Garden/notes/e524b806-3c30-40d7-8d83-02e8907b82f7.html","relUrl":"notes/e524b806-3c30-40d7-8d83-02e8907b82f7.html"},{"doc":"Triggers","title":"Triggers","hpath":"postgres.lang.triggers","content":"\n# Trigger\n- a callback that is executed whenever a table (or view) is modified. Triggers can also be set to listen for specific user actions much like a callback.\n\t- Can also be set up to be executed by using the INSTEAD OF condition.\n- 2 main types of trigger:\n\t1. row-level trigger\n\t2. statement-level trigger\n\t- the difference between these two is in how many times each would be called in response to an event. \n\t\t-  ex. if you issue an UPDATE statement that affects 20 rows, the row-level trigger will be invoked 20 times, while the statement level trigger will be invoked 1 time.\n- examples\n\t- restrict DML (actions that modify the db) operations to business hours \n\t- Automatically generate derived column values\n- You can think of a trigger like a middleware that sits between the user's request (that interacts with the db) and the sql server. Every time our db is interacted with, listeners are able \"intercept\" the query and act on it\n- Most triggers are only activated by either INSERT or UPDATE statements.\n- We can specify columns on a trigger, which will cause the trigger to only fire if those columns are operated on (ex. we update those columns)\n- If multiple triggers of the same kind are defined for the same event, they will be fired in alphanumerical order.\n\t- which is why it is useful to prepend them with numbers\n\n## Syntax\n- A trigger that is marked `FOR EACH ROW` is called once for every row that the operation modifies.\n- a trigger that is marked `FOR EACH STATEMENT` only executes once for any given operation\n- `WHEN` allows us to determine whether or not the trigger should be fired\n\t- In row-level triggers the `WHEN` condition can examine the old and/or new values of columns of the row\n\t- ex. only execute the function if `OLD.balance` does not equal `NEW.balance`\n\t\t- `WHEN (OLD.balance IS DISTINCT FROM NEW.balance)`\n\t- ex. only execute function if anything has changed\n\t\t- `WHEN (OLD.* IS DISTINCT FROM NEW.*)`\n- `CONSTRAINT` allows us to adjust the timing of when the trigger actually fires.\n\t- The trigger can be fired either:\n\t\t- At the end of the statement which caused the triggering event\n\t\t- At the end of the containing transaction, which is the `COMMIT` (called deferred triggers)\n\t- Each constraint has its own IMMEDIATE or DEFERRED mode.\n\t- only available `AFTER ROW`\n\n# Parts of a Trigger\n- 2 parts to a trigger: the trigger itself, and the function that is executed by the trigger\n\n## Trigger\n\n### Before/After Triggers\nThe trigger can be specified to fire before the operation is attempted on a row, or after the operation has completed (after constraints are checked and the INSERT, UPDATE, or DELETE has completed)\n- Trigger can also be set to fire *instead of* having the operation be performed. This only works on views\n\n#### Before\n- the constraints will not-yet have been checked, allowing us to perform some action before the actual operation has taken place.\n\t- ex. Imagine we want to keep only one of the user's credit cards specified as `is_primary`. Because of the uniqueness constraints applied on the table (allowing only one `is_primary` card per user), we must set `is_primary` to `false` for all of our cards, any time the user attempts to insert a new card where `is_primary` is true. \n\t\t- If we had instead ran the trigger *after* the `insert`, then we might have gotten an error, since we are potentially trying to insert a new card with `is_primary = true`, while one may already exist.\n- If the trigger fires before the event, the trigger can skip the operation for the current row, or change the row being inserted (for INSERT and UPDATE operations only)\n- in `BEFORE`, the `WHEN` condition is evaluated just before the function is executed\n\t- Therefore, using `WHEN` (in the trigger itself) has the same effect as testing the same condition at the beginning of the triggered function with `TG_WHEN`\n\t- The implication of this is that the `NEW` value seen by the function is the current value, and not the value that would exist *following* the operation\n\t\t- Also consider that while this \"current value\" normally means \"prior to the operation being performed\", there is the possibility that previous triggers in the chain have already executed, potentially changing what \"current value\" means to us.\n\t- Another implication is that in `BEFORE`, a trigger's `WHEN` condition cannot examine the columns of the `NEW` row that is to be inserted/updated (because they wouldn't have been set yet)\n\n#### After\n- rows will be impacted with consideration to the constraints\n- If the trigger fires after the event, all changes, including the effects of other triggers, are \"visible\" to the trigger.\n- in `AFTER`, the `WHEN` condition is evaluated after the row update occurs.\n\t- The `WHEN` condition here also determines whether an event is queued to fire a trigger, following the operation.\n\t\t- Therefore, if `WHEN` does not return true, we do not have to queue an event; nor to re-fetch the row at the end of the statement\n\t\t\t- This can result in significant speedups in statements that modify many rows, if the trigger only needs to be fired for a few of the rows.\n\n## Trigger Function\nDepending on if we are using row-level triggers or table-level triggers, the function will be called for each row that is affected, or will be called once per table.\n\nA trigger function must return either `NULL` or a row value having exactly the structure of the table the trigger was fired for.\n- ie. the the row being returned must have the same type of the table.\n\n- a trigger function returns a `trigger`\n- if a trigger function updates a row in the same table that the trigger is for and the operation is the same, then it will trigger a recursive infinite loop, since the trigger function will cause the trigger to fire in response to the \n\t- ex. we have a trigger that is set to fire on update of `users` table, and the trigger function itself updates a row in that table. This update, will cause the trigger to fire again, and so on.\n\n### Return value of trigger functions\n`BEFORE`\n- if we return `null` from the function, we are signalling to the trigger manager signal that we want to skip the rest of the operation for this row, meaning 2 things happen:\n\t1. subsequent triggers will be short-circuited \n\t2. the operation will not occur for this row.\n- if we return a non-`null` value, then the operation proceeds with that value. \n\t- returning a row value that is different from the original value of `NEW` will alter the row to be inserted/updated\n\t\t- Therefore, if we want the trigger to succeed normally without altering the `NEW` row value, then we must return `NEW`\n\t\t\t - This means that we could alter single columns in the row, with `NEW.col = X`, and proceeding to return `NEW`\n- a statement-level trigger fired `BEFORE` always ignores the return value of the trigger function, so it might as well be `null`.\n\n`AFTER`\n- return value of a row-level trigger (or statement-level trigger) fired `AFTER` will always be ignored, so it might as well be `null`\n\n### Trigger Local Variables\nA function called by a trigger receives data about its calling environment (called `TriggerData`)\n- prefixed by `TG_`\n\n`NEW` \n- variable holding the new database row for INSERT/UPDATE operations in row-level triggers\n- type: `record`\n- in statement-level triggers and `DELETE` operations, this value is `null`\n\t- this shows why we can return `null` from the trigger function on `DELETE` operations. There is nothing to return to the table!\n\t- must still return a non-`null` value from a DELETE trigger function, or we will short-circuit the trigger action. Normally, `OLD` is returned, since `NEW` is null\n\n`OLD`\n- variable holding the old database row for UPDATE/DELETE operations in row-level triggers\n- type: `record`\n- in statement-level triggers and `INSERT` operations, this value is `null`\n\t\n`TG_WHEN`\n- evaluates to BEFORE, AFTER, or INSTEAD OF, depending on the trigger's definition.\n- type: `text`\n\n`TG_OP`\n- evaluates to INSERT, UPDATE, DELETE, or TRUNCATE telling for which operation the trigger was fired.\n- type: `text`\n\n`TG_ARGV`\n- the arguments passed in to the trigger's function call\n- type: `text[]`\n\nAlso\n`TG_TABLE_NAME`, `TG_TABLE_SCHEMA`, `TG_NARGS` (# of args)\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/30b6c700-6608-49e1-a0db-e105f3001220.html","relUrl":"notes/30b6c700-6608-49e1-a0db-e105f3001220.html"},{"doc":"Transactions","title":"Transactions","hpath":"postgres.lang.transactions","content":"\nTransactions have a status to them. They might be in a \"running\" state, or they might be in a \"failed\" state.\n\nIf a function call has failed within a transaction block and we try to commit, we will instead rollback. Observe:\n```sql\nBEGIN;\nSELECT 1/0; -- ERROR: division by zero\nCOMMIT; -- error detected, ROLLBACK executed\n```\n\nPostgres transactions are isolated to individual clients, meaning that the same client must be the one to execute all code in a transaction block.\n- In `node-postgres`, we cannot use `pool.query` because of this, (spec:) because a pool may be seen as multiple different clients to the postgres server.\n\nIn Postgres\n- To execute a transaction in Postgres, use BEGIN / COMMIT / ROLLBACK \n- in Postgres, DDL commands are transactional, except when the commands are \"high-caliber\", such as creating and deleting DATABASE, TABLESPACE, CLUSTER\n- PostgreSQL supports multi-level transactions on save points level\n\t- If an error occurs inside a transaction, PostgreSQL rolls back the whole transaction but demands a command to complete the current transaction (COMMIT, ROLLBACK, ABORT) \n- Postgres treats every SQL statement as being executed within a transaction implicitly. If we do not issue a BEGIN command, then each statement will be surrounded with BEGIN..COMMIT\n","url":"https://tycholiz.github.io/Digital-Garden/notes/98e9641e-e171-42cd-afa6-97a7352fabf2.html","relUrl":"notes/98e9641e-e171-42cd-afa6-97a7352fabf2.html"},{"doc":"Variables","title":"Variables","hpath":"postgres.lang.transactions.variables","content":"\n# Transaction Variables\n- Transaction variables are variables that are cleared when the transaction exits\n```\ncurrent_setting('my_app.user_id', TRUE)\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/016ecadc-2d82-4aee-b235-75a646100cab.html","relUrl":"notes/016ecadc-2d82-4aee-b235-75a646100cab.html"},{"doc":"Schemas","title":"Schemas","hpath":"postgres.lang.schemas","content":"\n# Schema\nA schema is a logical namespace that can contain database objects such as tables, views, indexes, data types, functions, stored procedures and operators\n- by default, we use the `public` schema, if none is specified\n- a schema holds all objects, except for roles and tablespaces.\nA schema is essentially a namespace: it contains named objects (tables, data types, functions, and operators) whose names can duplicate those of other objects existing in other schemas. Named objects are accessed either by \"qualifying\" their names with the schema name as a prefix, or by setting a search path that includes the desired schema(s). A CREATE command specifying an unqualified object name creates the object in the current schema (the one at the front of the search path, which can be determined with the function current_schema).\n- \"a schema is a namespace that contains named database objects such as tables, views, indexes, data types, functions, stored procedures and operators.\"\n- A database can contain one or multiple schemas and each schema belongs to only one database.\n- ex. you may have sales schema that has staff table and the public schema which also has the staff table. When you refer to the staff table you must qualify it as follows:\n\t- public.staff\n\tOr\n\t- sales.staff\n- you can use one schema for the tables that will be exposed to GraphQL, another for the tables that should be completely private (e.g. where you store the bcrypted user passwords or other secrets never to be exposed!)\n\n## Good practice for Schema building:\n**app_public** - tables and functions to be exposed to GraphQL (or any other system) - it's your public interface. This is the main part of your database.\n**app_hidden** - same privileges as `app_public`, but it's not intended to be exposed publicly. It's like \"implementation details\" of your app_public schema. You may not need it often.\n**app_private** - SUPER SECRET STUFF 🕵️ No-one should be able to read this without a SECURITY DEFINER function letting them selectively do things. This is where you store passwords (bcrypted), access tokens (hopefully encrypted), etc. It should be impossible (thanks to RBAC (GRANT/REVOKE)) for web users to access this.\n- security definer effectively changes the role for the function being executed\n\n#### \"User\" tables\n- we should have 2 different tables about users. One should be on the private schema (`user_account`), where password and email will be kept. The other should be on the public schema, where everything else about the user is kept. The two tables have a 1:1 relationship, where the `private.user_account` table has a reference to the `public.user` table.\n\t- [good example](https://github.com/dijam/graphile-jwt-example/blob/master/db.sql)","url":"https://tycholiz.github.io/Digital-Garden/notes/b8bd0517-3b27-4bd8-8d13-9a04d5886d20.html","relUrl":"notes/b8bd0517-3b27-4bd8-8d13-9a04d5886d20.html"},{"doc":"Roles","title":"Roles","hpath":"postgres.lang.roles","content":"\n# Role\n- A role is an entity that can own database objects and have database privileges\n\t- can be considered a \"user\", a \"group\", or both depending on how it is used\n\t\t- in other words, a user role can be part of a group role\n\t- must have CREATEROLE privilege or be a database superuser to create roles.\n- When a user logs in, we want them to make their queries using a specific role\n- any time we are running `CREATE USER` or `CREATE GROUP`, we are running `CREATE ROLE` under the hood.\n\t- minor difference: `CREATE USER` also logs the user in, so a `role` having the LOGIN attribute can be thought of as a user \n\t\t- the ability to log in just means the role can be input along with a password as part of the connection string: `postgres://admin@localhost/mydb`\n\t- In the past, there were users and groups. Now, there are just roles. Roles have the ability to log in, have the ability to inherit from other roles, \n\t\t- basically, we moved from the Unix paradigm of users and groups, to the OOP paradigm of having inheritance.\n\t\nAttributes\n- attributes define privileges and info for a role \n\t- ex. login, perform superuser, database creation, role creation, password, etc\n\t\t- by default, roles don't get the ability to log in\n- the attributes are listed after the WITH clause, though that word is optional \n```\nCREATE ROLE kyletycholiz WITH\nLOGIN\n...\n```\n\nDefault roles\n- the `postgres` user is automatically created when we install Postgres. It is a superuser that we log into postgres with\n\t- all server processes work on behalf of this user\n\t- all database files belong to this user\n- roles that start with with pg_ are system roles.\n\n- roles are defined at the database cluster level, and so are valid in all databases in the cluster.\ncluster → database → schema → table\n\n## Grant\n- When an object is created, it is assigned an owner. \n\t- The owner is normally the role that executed the creation statement.\n\t- The right to modify or destroy an object is always the privilege of the owner only\n- When we use grant to grant access to a role, the now-empowered role can do 2 things: they can do everything that the source role could do, or they can actually control and become the role that it gained its powers from (this would mean that if an `admin` role became `user_login` role, it would no longer be able to perform any action that have admin-only privileges) \n\nGRANT has 2 variants:\n1. grant privileges on a database object\n\t- inc. table, column, view, sequence, database, foreign-data wrapper, foreign server, function, procedural language, schema, or tablespace\n2. grant membership in a role\n- below, we define a role `user_admin`, then give the role `postgres` the ability to do anything that `user_admin` can do.\n\t- from another viewpoint, `postgres` is gaining the power to do everything that `user_admin` can do.\n```\nCREATE ROLE user_admin;\nGRANT user_admin to postgres;\n```\n- GRANTing on a database doesn't grant rights to the schema within. Similarly, GRANTing on a schema doesnt grant rights to tables within.\n\t- ex. if we run `grant usage on schema app_public to user_guest`, we grant the role `user_guest` the right to know that the schema exists, but it doesn't give it the right to interact with the tables within.\n\t\t- if we want to grant read rights to a table, then we need to run `grant select on table <TABLENAME> to user_guest`\n\t- If you have rights to SELECT from a table, but not the right to see it in the schema that contains it, then you can't access the table\n\t\t- Therefore, the user must first be granted rights to the schema, otherwise the rights on the table are useless. \n\t\t- Likewise, if the user had been granted rights to just the schema, but not the tables within, they would still be locked out. \n\t\t\t- It's like a directory tree. If you create a directory somedir with file somefile within it then set it so that only your own user can access the directory or the file (mode rwx------ on the dir, mode rw------- on the file) then nobody else can list the directory to see that the file exists. If you were to grant world-read rights on the file (mode rw-r--r--) but not change the directory permissions it'd make no difference. Nobody could see the file in order to read it, because they don't have the rights to list the directory. If you instead set rwx-r-xr-x on the directory, setting it so people can list and traverse the directory but not changing the file permissions, people could list the file but could not read it because they'd have no access to the file. You need to set both permissions for people to actually be able to view the file. \n- the public schema has a default GRANT of all rights to the role public, which every user/group is a member of. So everyone already has usage on that schema.\n- opposite of GRANT is REVOKE\n*GRANT USAGE* - grants access to objects contained in the specified schema \n\n## Roles & Auth\n- all auth happens through postgres roles and permissions. Postgres is in charge of authenticating requests. \n- Postgres permissions work as a whitelist and not a blacklist (except for functions). \n\t- There are very few defaults, and if we want to blow them away entirely, use `alter default privileges`\n- when an authenticated user makes a request, the role will be changed for that user, having the effect of restricting queries (can be seen in `current_user` variable)\n\t- `current_user` is the username of the current execution context \n- Because we have the concept of default roles, we set a default (such as `user_guest` that every user will get by default. When the user demonstrates a verified JWT token, that role gets changed to one with more authorization rights (such as `user_login`).\n- JWTs can be generated in postgres with `pgjwt` extension","url":"https://tycholiz.github.io/Digital-Garden/notes/5e1e6144-e2ef-4418-9846-1a7636e9334c.html","relUrl":"notes/5e1e6144-e2ef-4418-9846-1a7636e9334c.html"},{"doc":"Listen/Notify","title":"Listen/Notify","hpath":"postgres.lang.listen-notify","content":"\n# Overview\nPostgres has a system of asynchronous messages and notifications, implemented by `listen` and `notify` keywords.\n- This means that as soon as a connection is established with PostgreSQL, the server can send messages to the client even when the client is idle.\n\t- This method of communication is also carried out with the `COPY` command.\n\nCommonly, the channel name is the same as the name of some table in the database, and the notify event essentially means, \"I changed this table, take a look at it to see what's new\"\n\n# Notify\n`notify` provides a simple interprocess communication (IPC) mechanism for a collection of processes accessing the same PostgreSQL database.\n\nWhen `notify` is used to signal the occurrence of changes to a particular table, a useful programming technique is to put the `notify` in a rule that is triggered by table updates.\n- In this way, notification happens automatically when the table is changed, and the application programmer cannot accidentally forget to do it.\n\nIn a transaction, `notify` events are not delivered until the surrounding transaction has been committed.\n\n# Listen\nWhen we execute `listen`, we are registering the current postgres session as a listener on the specified notification channel.\n- when the specified channel is `notified` (either by the current session, or another one connected to the same db), all sessions subscribed to that channel will receive the message, and will in turn pass it on to the client whom it is connecting.\n\nThe payload passed to the client includes 3 things:\n1. Notification channel name\n2. Session server's PID\n3. Payload string (`''` if unspecified)\n\nA session's listen registrations are automatically cleared when the session ends.\n\nThe method a client application must use to detect notification events depends on which PostgreSQL API it uses (ie. libpq, libpgtcl)\n\n# `pg_notify(channel_name text, payload text)`\nWe can also send a notification using the `pg_notify` function\n- The function is much easier to use than the `notify` command if you need to work with non-constant channel names and payloads.\n\n# Example\n```\npsql# listen my_notification_channel;\nLISTEN\n\npsql# notify my_notification_channel, 'foo';\nNOTIFY\nAsynchronous notification \"my_notification_channel\" with payload \"foo\"  ⏎\nreceived from server process with PID 40430.\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/6e45644f-c45e-4dd7-999b-88ca60c098e4.html","relUrl":"notes/6e45644f-c45e-4dd7-999b-88ca60c098e4.html"},{"doc":"Indexes","title":"Indexes","hpath":"postgres.lang.indexes","content":"\n# Index\n- Postgres has multiple types of index: B-tree, Hash, GiST, SP-GiST and GIN\n\t- B-tree is default, and is appropriate for most use-cases\n- Whether to create a single-column index or a multicolumn index, take into consideration the column(s) that you may use very frequently in a query's WHERE clause as filter conditions.\n\t- ie. add an index on the columns that we typically use WHERE on\n- Imagine studying for a test and you stumbled upon a topic you don't understand or forgot. You might consider looking at the INDEX at the back of the textbook for the topic. The index will tell you which page to go find the information. You can then jump straight to the correct page. Now imagine not having an index at the back of the textbook. You'd have to skim through the ENTIRE book until you found what you were looking for.\n- Avoid indexes on...\n\t- small tables.\n\t- Tables that have frequent, large batch update or insert operations.\n\t- columns that contain a high number of NULL values.\n\t- Columns that are frequently manipulated should not be indexed.\n\n[[Index Docs|db.strategies.indexing]]\n\n# UE Resources\n- [Indexes under the hood, using B-trees](https://rcoh.me/posts/postgres-indexes-under-the-hood/)\n- [provides a quick-n-dirty test at the bottom of the article, to see impact of indexes on millions of rows](https://www.cybertec-postgresql.com/en/postgresql-now-vs-nowtimestamp-vs-clock_timestamp/)","url":"https://tycholiz.github.io/Digital-Garden/notes/45194725-15eb-4b57-b2fe-92b74fecdb81.html","relUrl":"notes/45194725-15eb-4b57-b2fe-92b74fecdb81.html"},{"doc":"Partial","title":"Partial","hpath":"postgres.lang.indexes.partial","content":"\n# Partial Index \nAn index built over a subset of a table, and therefore only contains entries for that subset.\n- The subset is determined by a conditional expression given by us (called the predicate).\n\n## Purpose\n- To avoid indexing common values\n    - Since a query searching for a common value (ie. one that occurs for more than a few percent of all table rows) will not use the index anyway, there is no point in keeping those rows in the index at all. Doing this reduces the size of the index, increasing the speed of queries that do ue the index.","url":"https://tycholiz.github.io/Digital-Garden/notes/5f95e0ff-7dcd-4dad-9f79-0cb11ee1a199.html","relUrl":"notes/5f95e0ff-7dcd-4dad-9f79-0cb11ee1a199.html"},{"doc":"Grant","title":"Grant","hpath":"postgres.lang.grant","content":"\nThe GRANT command has two basic variants: one that grants privileges on a database objects, and one that grants membership in a role.\n\nWhen you create a new database, any role is allowed to create objects in the public schema. To remove this possibility, you may issue immediately after the database creation:\n\n```sql\nREVOKE ALL ON schema public FROM public;\n```\nafter the above command, only a superuser may create new objects inside the public schema, which is not practical. Assuming a non-superuser foo_user should be granted this privilege, this should be done with:\n\n```sql\nGRANT ALL ON schema public TO foo_user;\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/def115fc-ffed-45e0-91f9-7114fea27623.html","relUrl":"notes/def115fc-ffed-45e0-91f9-7114fea27623.html"},{"doc":"Functions","title":"Functions","hpath":"postgres.lang.functions","content":"","url":"https://tycholiz.github.io/Digital-Garden/notes/89860ec0-4e8b-4ed5-9577-6ccf65cf72b0.html","relUrl":"notes/89860ec0-4e8b-4ed5-9577-6ccf65cf72b0.html"},{"doc":"Window","title":"Window","hpath":"postgres.lang.functions.window","content":"\n# Window Function\nA window function performs a calculation across a set of table rows. Each row is somehow related to the current row.\n- Window Functions differ from aggregate functions in that they do not cause rows to become grouped into a single output row. Instead, the rows retain their separate identities.\n\t- Behind the scenes, the window function is able to access more than just the current row of the query result.\n- ex. Imagine we want a result set of all employees, and have their individual salaries compared against the average of their individual departments:\n```\nSELECT \n\tdepname, \n\tempno, \n\tsalary, \n\tavg(salary) \nOVER (PARTITION BY depname) \nFROM empsalary;\n```\nwould result in:\n```\n  depname  | empno | salary |          avg          \n-----------+-------+--------+-----------------------\n develop   |    11 |   5200 | 5020.0000000000000000\n develop   |     7 |   4200 | 5020.0000000000000000\n develop   |     9 |   4500 | 5020.0000000000000000\n develop   |     8 |   6000 | 5020.0000000000000000\n develop   |    10 |   5200 | 5020.0000000000000000\n personnel |     5 |   3500 | 3700.0000000000000000\n personnel |     2 |   3900 | 3700.0000000000000000\n sales     |     3 |   4800 | 4866.6666666666666667\n sales     |     1 |   5000 | 4866.6666666666666667\n sales     |     4 |   4800 | 4866.6666666666666667\n(10 rows)\n```\n- the OVER clause is what causes it to be treated as a window function, and therefore computed across the window frame.","url":"https://tycholiz.github.io/Digital-Garden/notes/f49e47b0-e0eb-4a4f-8740-5012d1923562.html","relUrl":"notes/f49e47b0-e0eb-4a4f-8740-5012d1923562.html"},{"doc":"Custom","title":"Custom","hpath":"postgres.lang.functions.custom","content":"\n# Custom Functions\nA function is a reusable block of SQL code that returns a scalar value of a set of rows.\n- Functions are transactional by nature. If there is an error somewhere in the function, then the function will be rolled back.\n\t- If a function is called within a transaction block, and the executing code does not reach the concluding `COMMIT`, then all code executed within the function will roll back as well.\n\t- Any `BEGIN...EXCEPT` blocks within the function operate like savepoints like the `SAVEPOINT` and `ROLLBACK TO <SAVEPOINT>` SQL statements.\n- SQL functions execute an arbitrary list of SQL statements, returning the result of the last query in the list.\n\t- In the simple (non-set) case, the first row of the last query's result will be returned\n\t- Alternatively, an SQL function can be declared to return a set, which allows *all* rows of the last query's result to be returned\n- allows us to write functions that can interact on the database. For instance, we can create a function that combines `first_name` and `last_name` to give us `fullName`\n- Since PostgreSQL permits function overloading, the function name alone does not uniquely identify the function to be called;\n\t- the parser must select the rigt function based on the data types of the supplied arguments.\n- Functions and operators in PostgreSQL support polymorphism and almost every part of the system can be extended.\n- By default, functions can be executable by public\n- anonymous IIFEs(?) can be invoked anywhere in the SQL by escaping the function identifiers:\n```sql\nDO \\$\\$\nBEGIN\n  EXECUTE 'GRANT appname TO ' || user;\n  EXECUTE 'GRANT appname_authenticator TO ' || user;\nEND;\n\\$\\$ LANGUAGE PLPGSQL;\n```\n\n\n## Using functions in queries\n- say we have a function that returns a composite type:\n```sql\nCREATE FUNCTION new_emp() RETURNS emp AS $$\n    SELECT ROW('None', 1000, 25, 'yoyo')::emp;\n$$ LANGUAGE SQL;\n```\n- here we are returning a single column that is a composite type of the signature: string, int, int, string. We also coerce it to the composite type related to the whole `emp` table. This means that the `emp` table has all 4 of those rows, and the act of us coercing means the output of the function can be used anywhere that an explicit `emp` type is required.\n- we could make a SELECT statement to get back a one-column table by using `SELECT new_emp()`\n- since the composite type is also a sort of virtual table, we can use the function as a \"table function\": `SELECT * FROM new_emp();`, which will return 4 columns.\n\n## Syntax\n- use `$$` to open and close the function:\n- stable means that this function does not mutate the database\n\nTerms\n- `setof` - Sets emulate rows of tables.\n\t- `returns setof` and `returns table(column)` are equivalent\n\t- When an SQL function is declared as returning SETOF sometype, the function's final query is executed to completion, and each row it outputs is returned as an element of the result set.\n\t\t- This feature is normally used when calling the function in the FROM clause, since everything after FROM would get interpreted as if it were a base table. In other words, we can add some columns to a base table, and query it as if those added rows were permanent\n\t\t\t- ex. imagine having a function that determined if your salary was above $100,000. we can use a function to get the boolean result and attach it to the base table, so that we can use `FROM employee, getAboveHundred(emp1)\n```sql\nCREATE FUNCTION add(a int, b int) returns int as $$\n select a + b\n$$ language sql stable;\n```\n\n# Return value\n- functions can return base types (string, int) and composite types (collection of columns), or sets of base types and composite types\n- we can return any type from a function. Since the whole row of a table is by definition a composite type, we can specify `returns nugget`. This will specify that the function must return all columns in the `nugget` table.\n\t- If we don't want to return all columns, we can always use the `ROW` construct to specify which columns we want to include in the row signature\n\t- The select list order in the query must be exactly the same as that in which the columns appear in the table associated with the composite type.\n\t- You must typecast the expressions to match the definition of the composite type\n- If the function is defined to return a base table, the table function produces a one-column table (with the column named after the function. If the function is defined to return a composite type, the table function produces a column for each attribute of the composite type.\n\t- using `setof` will return multiple columns\n- the type that follows `returns` has to match up with whatever is SELECTed during the query.\n\t- ex. if we declare `returns bucket`, then we'd better be using `SELECT * FROM bucket`\n\t\t- since in this example we are returning a base table, a one-column table is the result.\n\n### Returning a Set\n- we can also use `RETURNS TABLE(columns)` syntax to return a set\n\t- equivalent to using one or more `OUT` parameters plus marking the function as returning SETOF record (or SETOF a single output parameter's type, as appropriate)\n\t- It is not allowed to use explicit OUT or INOUT parameters with the RETURNS TABLE notation — you must put all the output columns in the TABLE list.\n\n### Output Params\n- an alternate way to define a function's signature (inputs and outputs)\n- The value of output parameters is that they provide a convenient way of defining functions that return several columns, which is why functions can have multiple outputs\n```\nCREATE FUNCTION sum_and_product (IN x int, IN y int, OUT sum int, OUT product int)\n```\n- What has essentially happened here is that we have created an anonymous composite type for the result of the function\n\t- if we wanted to be more explicit, we could have declared a composite type `sum_prod`, made up of the `sum` column and the `product` column, and declared that the function `returns sum_prod`.\n\n## Set returning function\n- *set returning functions* are functions that possibly return more than one row\n\t- currently, `series generating functions` are the only type of `set returning functions`\n- `generate_series(<start>, <stop>, <step>)`\n\t- because this function returns a result set, we can use the function after FROM\n- ex. imagine running:\n```\ngenerate_series(date :'start',\n\t\t\t    date :'start' + interval '1 month'\n\t\t\t\t\t\t\t  - interval '1 day',\n\t\t\t\tinterval '1 day'\n) as calendar(entry)\n```\nwhich would return a set of dates, starting from `start` (a variable we defined earlier), and increasing by intervals of 1 day.\n\t- this would be useful if we have a set of data by year, and have some years where there is no data. Instead of skipping those rows, we might want to display zeros instead. When we join this result set with our data, joining on the `date` column will result in `null` for the missing years. Using `coalesce`, we can default all nulls to zero.\n\n# PL/PGSQL\n### Declare\n- Declare a variable of specified type\n\n### Function Declarations\n#### Strict\n- this means that if the function receives null input, then the function won't be called and the output will automatically be null as well.\n\t- ex. imagine we have a `register_user` function, that takes name, email and password as inputs. If the function does not receive `name`, we want it to fail.\n\n#### Security Definer/Invoker\n- Security Definer - \"the privileges of the function use the privileges (security clearance) of the definer\"\n- Security invoker - \"use privileges (security clearance) of the user who invoked the function\n- this means that the function is created with the privileges of the pg role that created it.\n- This is a way to heighten the rights of the function, and can be thought of similar to sudo, instead of rights being elevated to superuser, rights are only elevated to the creator of the function itself\n\t- ie. if we run our migrations with user `postgres`, then using `security definer` will give our function the rights of `postgres`. This is helpful if we want to insert into a table that is part of a very strict schema, such as `app_private`\n- we can use `security definer` to by pass RLS\n\n### UE Resource\n[insert returning into variable](https://www.xspdf.com/resolution/50004699.html)","url":"https://tycholiz.github.io/Digital-Garden/notes/c360903e-4bd5-4d57-b717-0ffb5b057e5a.html","relUrl":"notes/c360903e-4bd5-4d57-b717-0ffb5b057e5a.html"},{"doc":"Builtin","title":"Builtin","hpath":"postgres.lang.functions.builtin","content":"\n# Built-in Functions","url":"https://tycholiz.github.io/Digital-Garden/notes/b454a272-df47-4ce9-9168-8c9359a1a24b.html","relUrl":"notes/b454a272-df47-4ce9-9168-8c9359a1a24b.html"},{"doc":"Now","title":"Now","hpath":"postgres.lang.functions.builtin.now","content":"\n## now()\n- returns the timestamp of the current transaction.\n\t- Therefore, the result is equal if we call `now()` in different parts of the transaction.\n\n## clock_timestamp()\n- returns us the actual timestamp of when the code is being executed.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/54b96a47-2cb2-474a-99f6-983e6bab4fdb.html","relUrl":"notes/54b96a47-2cb2-474a-99f6-983e6bab4fdb.html"},{"doc":"Coalesce","title":"Coalesce","hpath":"postgres.lang.functions.builtin.coalesce","content":"\n# Coalesce\n- `coalesce` accepts unlimited params, and always returns the first non-null value\n\t- In this way, you can think of the final param as the default value.\n- Embracing `null` allows us to easily inline defaults\n\t- You can chain a bunch of inputs and it returns the first non-null value\n\t- ex. imagine having a result set that shows us how much revenue was earned each year since 2010. We should define a column `coalesce(income, 0) as income` in our select statement, which will return us a 0 if there is no income for the relevant year. This is a nicer alternative than to just omitting rows without data.\n\t- ex. Lets say you have an input parameter with a default, you can the just wrap the usages of that parameters like this: coalesce (_parameter, default_value) = whatever it tests against\n\t- ex. Imagine we have a postgres function that accepts 2 args: a `start_date` and an `end_date`, and returns the amount of time that an employee worked at a company. If they currently work there still, the client passes `null` for the value of the `end_date`. In our function, we would have a smart failover that defaults the null value to `now()`.\n\t- ex. imagine we have a column `discount` that defaults to `null`. We want to use this `discount` column when determining subtotal, so we need to use coalesce to provide us with a default value of 0:\n\t```sql\n\tSELECT\n\t\tproduct,\n\t\t(price - COALESCE(discount,0)) AS net_price\n\t```","url":"https://tycholiz.github.io/Digital-Garden/notes/44a32f9e-fcd1-4de5-8abe-e97d7808e078.html","relUrl":"notes/44a32f9e-fcd1-4de5-8abe-e97d7808e078.html"},{"doc":"Agg","title":"Agg","hpath":"postgres.lang.functions.agg","content":"\n# Aggregate Functions\n- called aggregate functions because they are functions that act on multiple rows and give us a single output\n\t- ex. sum, avg, max, min\n- used to extract information about whole groups of rows, and allow us to easily ask questions like:\n1. What's the most expensive facility to maintain on a monthly basis?\n```sql\nselect max(monthlymaintenance)\nfrom cd.facilities;\n```\n2. Who has recommended the most new members?\n3. How much time has each member spent at our facilities?\n4. what is the most popular type of shoe?\n- Aggregate functions only work in SELECT or HAVING clauses\nTherefore, an aggregate function will take a group of data, perform some function on it, and have a single output that it will print in our result set. An aggregate function can perform transformation on the data, or it can also perform a calculation on it. In either case, the aggregate function is making a consideration of all rows of data, and deciding what to do with it that would result in a single piece of data (int, array etc) that will hold the results of that calculation.\n- This is why when we perform aggregate functions, we need to use ORDER BY, because we need to tell the query what we want to apply the aggregate function to.\n\t- Since the aggregate function results in an aggregate set of data *based on something*, we need to know what that \"based on\" actually is. Is the data getting combined *based on* age? Is it getting combined *based on* nugget (ie. combining buckets from multiple rows in an effort to make a json array)?\n- When you hear \"aggregate function\", you should think of taking data from multiple rows and combining (likely integers) or transforming (likely objects/arrays) it in some way\n\n[some useful examples](hashrocket.com/blog/posts/faster-json-generation-with-postgresql)\n\n## JSON Functions\n### row_to_json\nturns the table into json\n- each row becomes a json object, and the cumulative of these rows makes up the json array\n- If the query `select * from nugget` gives us the data, then the whole below query gives us that same data as json\n```\nselect row_to_json(nugget)\nfrom (select * from \"nugget\") as nugget\n```\n### array_agg\nIt aggregates its argument into a PostgreSQL array\n- accepts a set of values and returns an array. Each arg becomes an element in the array\n\t- we can determine the order of the array with ORDER BY\n- ex. Imagine we had 2 many:many tables (`actors` and `movies`) connected through a junction table `movies_actors`.\n\t- We are interested in viewing which actors were in which movie.\n\t- Assuming that the `movies` table is the table we are joining from, if we were to query the movies, we'd get duplicate rows with the same movie and actor names (since each row is a combination of the two, effectively having the same amount of rows as the junction table).\n![d26ee15403ed820f7c410d859e7c5e56.png](:/20ede76c8a314c528f1b4716c372db0d)\n\t- What we want instead is to show a different movie in each row, and consolidate the actors appearing in that movie into an array\n![939e260f1cc0aab3f7f7e6477164e5ac.png](:/9d7a1c247b064dc285cd0597a477d28a)\n\n### json_agg\n- similar to `array_agg`, except that it puts the elements into a json array\n- we could use `json_agg(json_build_object())` if we wanted to build an array of objects\n\nBefore\n```sql\nSelect\n    title,\n    first_name || ' ' || last_name as name\nfrom film\njoin film_actor using (film_id)\njoin actor using (actor_id)\ngroup by title, first_name, last_name\norder by title;\n```\n\nAfter\n```sql\nSELECT\n     title,\n     ARRAY_AGG (\n\t     first_name || ' ' || last_name\n\t     ORDER BY first_name\n     ) actors\n FROM\n     film\n INNER JOIN film_actor USING (film_id)\n INNER JOIN actor USING (actor_id)\n GROUP BY\n     title\n ORDER BY\n     title;\n```\n\n### json_build_object\n- Allows us to provide key-value pairs, thereby returning us an object\n- if we want to provide an array as the value of a key (ex. `buckets` property of a `nuggets` object), we can provide a SELECT statement in place of the value\n```sql\nSELECT\n    json_build_object(\n        'id',id,\n        'name',name,\n        'comments',(\n            SELECT json_agg(json_build_object(\n                'id', comments.id,\n                'mood', comments.mood,\n                'subject', comments.subject,\n                'content', comments.content,\n                'created_at', comments.created_at\n            ))\n            FROM user_comment_map JOIN comments ON comment_id=comments.id\n            WHERE user_id=users.id\n        )\n    )\n```\nthis works because\n\n### json_strip_nulls\n- lets us exclude fields where the value is null\n```sql\nSELECT json_strip_nulls(json_build_object('name', p.name, 'birthday', p.birthday))\nFROM person p;\n```\n\n### json_populate_record\n- allows us to give postgres a string of json, which it will then convert into SQL row-format.\n- the first arg we pass to\n\n# Pitfalls\n- because of the order of execution, aggregate functions cannot be used in the WHERE clause (see queries/WHERE)\n\t- This restriction exists because the WHERE clause determines which rows will be included in the aggregate calculation; so obviously it has to be evaluated before aggregate functions are computed","url":"https://tycholiz.github.io/Digital-Garden/notes/4780a23f-7e92-4f4c-ae85-ac5f6fe6c02c.html","relUrl":"notes/4780a23f-7e92-4f4c-ae85-ac5f6fe6c02c.html"},{"doc":"Count","title":"Count","hpath":"postgres.lang.functions.agg.count","content":"\n#### count(*)\nCounts the number of rows returned by a `select *` statement\n\n#### count(colname)\nCounts the number of rows, but does not consider `null` values of the specified column\n\n#### count(DISTINCT colname)\nCounts the number of rows, but does not consider `null` values of the specified column, and does not consier duplicate values\n- ex. if we are selecting from a `users` table and there is a column `role`, we can `select count(distinct role)...` and we will get a small list of role possibilities\n\nWe often use the COUNT() function with the GROUP BY clause to return the number of items for each group. For example, we can use the COUNT() with the GROUP BY clause to return the number of films in each film category.","url":"https://tycholiz.github.io/Digital-Garden/notes/4216a985-aa09-4e89-90a6-f69e67c871cb.html","relUrl":"notes/4216a985-aa09-4e89-90a6-f69e67c871cb.html"},{"doc":"Fdw","title":"Fdw","hpath":"postgres.lang.fdw","content":"\n### Foreign Data Wrappers (FDW)\n[FDW](https://thoughtbot.com/blog/postgres-foreign-data-wrapper)","url":"https://tycholiz.github.io/Digital-Garden/notes/0d3647c4-2c3f-404e-82f4-4dc771f9a45a.html","relUrl":"notes/0d3647c4-2c3f-404e-82f4-4dc771f9a45a.html"},{"doc":"RLS","title":"RLS","hpath":"postgres.lang.RLS","content":"\nThe main hypothesis is that we should be able to prevent access to specific rows of data based on a policy. That means our application logic only has to worry about `SELECT * FROM my_table` and RLS will handle the `WHERE user_id = my_user_id` part automagically.\n- To put it another way: our queries should only contain the clauses requested by our interfaces and not the filters and conditions demanded by access control in a multi-tenant data store.\n- The current PG role that is accessing the table must have been `grant`ed permission to use it. Otherwise, RLS errors will arise when we try to alter something in the table as that role, because we won't be able to access the table from the outset.\n\na function marked as `SECURITY DEFINER` will bypass RLS\n\n# Row Level Security (RLS)\n- While GRANT is the privilege system of Postgres, Tables can also have Row Security policies that restrict (on a per-user basis) which rows can be interacted with (INSERT, UPDATE, DELETE, SELECT)\n\t- Policies are created using the CREATE POLICY command\n\t- spec: `grant` means \"I am giving *this* user the privilege to use *this* table\". `create policy` means \"I am specifying the requirements of any particular row that must be satisfied before it is accessed by user\"\n- The basis of row level security is to create policies that define how a user interacts with rows within a given table.\n- By default, tables do not have any policies, and RLS must be opted-in for each table.\n\t- When RLS is enabled, all rows are by default not visible to any roles (superusers still have access)\n- If the value in parentheses after USING evaluates to true, then the user gets permission\n- ex. imagine we have a chat app, and we want to ensure a user can only see messages sent by him, and messages intended for him. Also, we want to ensure that users cannot modify the `message_from` column to make it seem that the message is coming from someone else:\n```sql\nCREATE TABLE chat (\n  message_uuid    UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n  message_time    TIMESTAMP NOT NULL DEFAULT now(),\n  message_from    NAME      NOT NULL DEFAULT current_user,\n  message_to      NAME      NOT NULL,\n  message_subject VARCHAR(64) NOT NULL,\n  message_body    TEXT\n);\n\nCREATE POLICY chat_policy ON chat\n  USING ((message_to = current_user)\n  OR (message_from = current_user))\n  WITH CHECK (message_from = current_user)\n```\n- RLS can be implemented using jwt claims to verify that the user is who they say they are, if we do not want to use `current_user`:\n\t- the second arg true means \"return null if the setting is missing\"\n```sql\nCREATE POLICY chat_policy ON chat\n  USING ((message_to = current_setting('request.jwt.claim.email', true))\n  OR (message_from = current_setting('request.jwt.claim.email', true)))\n  WITH CHECK (message_from = current_setting('request.jwt.claim.email', true))\n```\n- When request contains a valid JWT with a role claim (`jwt.claims.role`), we should switch to the role with that name for the duration of the HTTP request\n\n### RLS Policy using external tables\n- What if we want to enable RLS where `user_id = current_user_id()`, but the current table does not keep a `user_id` column?\n- If we are adding an RLS policy to T1, but the policy depends on a `JOIN`able table T2, then T2 must have `grant`ed privileges to the PG role accessing the table.\n\n```sql\nCREATE POLICY t2_policy_update ON t2 FOR UPDATE\n    USING (EXISTS (SELECT * FROM t1 INNER JOIN t0 ON (t1.t0id = t0.id) WHERE t0.u = session_user AND t1id = t1.id))\n```\nSubqueries in RLS policies respect the RLS policies of the tables they reference\n\n[source](https://stackoverflow.com/questions/41354818/postgresql-row-level-security-involving-a-view-or-a-select-with-join)\n\n## Per-command Policies\n### UPDATE\n- Since `UPDATE` involves pulling an existing record and replacing it with a new modified record, `UPDATE` policies accept both a `USING` expression and a `WITH CHECK` expression\n  - `USING` determines which records the `UPDATE` command will see to operate against\n  - `WITH CHECK` defines which modified rows are allowed to be stored back into the table.\n    - If the updated value fails the `WITH CHECK` expression, there will be an error.\n    - If only a `USING` clause is specified, then it will be used for both `USING` and `WITH CHECK` cases (ie. `WITH CHECK` is implemented for us implicitly)\n- Typically an `UPDATE` command needs to read data from columns in the relation being updated (e.g. in a `WHERE` clause, or `RETURNING` clause, or right side of a `SET` clause).\n  - In cases such as these, `SELECT` rights are required on the relation being updated, in addition to the `UPDATE` right.\n\n* * *\n\n## Anatomy\n### WITH CHECK vs USING\n- USING is applied before any operation occurs to the table’s rows\n\t- ex. in the case of updating a nugget, one could not update a row that does not have the appropriate user_id in the first place\n\t- must use USING with DELETE commands because a delete changes no rows, and only removes current ones.\n\t- USING implicitly runs a WITH CHECK with the same clause that USING received, meaning that the verification operation runs both before and after the data is inserted.\n- WITH CHECK is run after an operation is applied, so if it fails, the operation will be rejected\n\t- ex. in the case of an insert, Postgres sets all of the columns as specified and then compares against WITH CHECK on the new row\n\t- must use WITH CHECK with INSERT commands because there are no rows to compare against before insertion\n\n### Permissive or Restrictive\nRLS policies can be either permissive or restrictive\n- permissive (default) - in consideration of all RLS policies, only 1 must pass\n- restrictive - in consideration of all RLS policies, all must\n```\ncreate policy select_all on table_name as permissive using (true)\n```\n\n* * *\n\n### Check if RLS enabled\n```sql\nselect relname, relrowsecurity, relforcerowsecurity\nfrom pg_class\nwhere oid = 'your_table_name_with_schema'::regclass;\n```\nor\n```sql\nselect * from pg_tables where tablename = 'your_table_name_without_schema'\n```\n\n# UE Resources\n[Good information about RLS](https://info.crunchydata.com/blog/a-postgresql-row-level-security-primer-creating-large-policies)\n[RLS using columns from other tables](https://medium.com/@ethanresnick/there-are-a-few-faster-ways-that-i-know-of-to-handle-the-third-case-with-rls-9d22eaa890e5)\n[](https://blog.crunchydata.com/blog/a-postgresql-row-level-security-primer-creating-large-policies)","url":"https://tycholiz.github.io/Digital-Garden/notes/f067baf6-9db7-4672-9d30-5168f5ceee2b.html","relUrl":"notes/f067baf6-9db7-4672-9d30-5168f5ceee2b.html"},{"doc":"Internals","title":"Internals","hpath":"postgres.internals","content":"\n# UE Resources\n- [Internals of Postgres](http://www.interdb.jp/pg/)","url":"https://tycholiz.github.io/Digital-Garden/notes/b646e722-9635-424d-8713-776e344b2eaa.html","relUrl":"notes/b646e722-9635-424d-8713-776e344b2eaa.html"},{"doc":"Objects","title":"Objects","hpath":"postgres.internals.objects","content":"\n# Object-nature of Postgres\n- PostgreSQL is an object-relational database management system (ORDBMS)\n- a postgres database itself is an object. It contains other objects, such as tables, views, functions, and indexes. since a database is just an object, multiple postgres databases can be stored inside a single postgres server.\n- By making everything an object, Postgres is highly extensible, allowing us to build:\n\t- data types\n\t- functions\n\t- aggregate functions\n\t- operators\n\t- index methods\n- A special feature of PostgreSQL is table inheritance, meaning that a table (child table) can inherit from another table (parent table) so when you query data from the child table, the data from the parent table also shows up.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/baf2fc55-f92a-47a3-869a-3126964b56d0.html","relUrl":"notes/baf2fc55-f92a-47a3-869a-3126964b56d0.html"},{"doc":"Extensions","title":"Extensions","hpath":"postgres.extensions","content":"\n## PGXS\nWe can compile extensions using a tool called `pgxs`\n- PGXS is a build infrastructure for extensions\n- it allows us to build extensions against an already installed server.\n\n## Misc\n- When trying to install an extension, if we get the error message `cannot find \"postgres.h\"`, this means that to build the extension from source, the postgres header files are needed. There is a package called `postgres-server-dev-<VERSION>`. This will install all header files for us necessary for server development.\n\n## Commands\n- show installed extensions - `\\dx` OR `SELECT * FROM pg_extension`\n- show available extensions - `SELECT * FROM pg_available_extensions`","url":"https://tycholiz.github.io/Digital-Garden/notes/5fdce4e5-1744-4773-b95e-2c4299d773ca.html","relUrl":"notes/5fdce4e5-1744-4773-b95e-2c4299d773ca.html"},{"doc":"Connection","title":"Connection","hpath":"postgres.connection","content":"\n# Connecting to Postgres\n- When a new user connects to the database, postgres will fork a new process.\n\t- After this point the client and the new process communicate directly, without intervention from the original process.\n\t- This shows the dependency on processes rather than threads, giving us more stability at the cost of higher connection start-up costs.\n\t- each connection into Postgres is going to consume about 10 MB of memory from your database\n\t- These start-up costs can be overcome with pooling\n- In postgres, each client connecting to the database has its own process \n\t- connection can be verified by running `select true as \"Connection test\";`\n- *Connection String* - the database url used to connect to the database (`postgres:///...`)\n\t- if we specify the connection string as `postgres:///mydb`, then we are implicitly passing our computer's username and no password, which would be equivalent to `postgres:///kyletycholiz@localhost:5432/mydb` \n\t\t- spec: are these default variables determined by env variables like PGHOST?\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8409f37c-4ea3-4bba-acd0-d12ac64a141c.html","relUrl":"notes/8409f37c-4ea3-4bba-acd0-d12ac64a141c.html"},{"doc":"Pools","title":"Pools","hpath":"postgres.connection.pools","content":"\n### Why?\n- Connecting a new client to the PostgreSQL server requires a handshake which can take 20-30 milliseconds. During this time passwords are negotiated, SSL may be established, and configuration information is shared with the client & server. Incurring this cost every time we want to execute a query would substantially slow down our application.\n\n- The PostgreSQL server can only handle a limited number of clients at a time. Depending on the available memory of your PostgreSQL server you may even crash the server if you connect an unbounded number of clients. \n\n- PostgreSQL can only process one query at a time on a single connected client in a first-in first-out manner. If your multi-tenant web application is using only a single connected client all queries among all simultaneous requests will be pipelined and executed serially, one after the other. No good!\n\nThe client pool allows you to have a reusable pool of clients you can check out, use, and return. You generally want a limited number of these in your application and usually just 1. Creating an unbounded number of pools defeats the purpose of pooling at all.\n\n### Implementing\nYou must always return the client to the pool if you successfully check it out, regardless of whether or not there was an error with the queries you ran on the client. If you don't check in the client your application will leak them and eventually your pool will be empty forever and all future requests to check out a client from the pool will wait forever.\n\n### Connection Pooling\n- a pool sits between the postgres frontend (ex. postgraphile, postgres-node) and the postgres server. The pooler speaks the postgres language, so understands all incoming queries.\n\t- Therefore, the server sees the requests as coming from the pg pool, and the postgres frontend sees the pool as handling the requests. \n\t\t- Therefore,\n\t\t\t- without a pool, if we had 20 db connections and 8 were idle, all 20 would be eating up postgres resources (memory)\n\t\t\t- with a pool, those 8 idle connections remain in the pool and don't connect to the database, freeing up resources for us. From the database perspective, there are only 12 connections.\n\t- the frontend/backend paradigm here is no different from in web developent. In postgres, we have users who interact with the database. The client generates a query, and that query gets sent off to the postgres backend, where it interprets the request, performs the actions, then returns the data to the frontend. \n- when a client is connected to the pool, we say it is a \"virtual connection\", and if the pooled client is connected to the database, it is a \"physical connection\" \n- 2 main options: `pgpool-II` and `pgBouncer`\n\t- pgBouncer does one job and does it well. pgpool-II is more of a swiss army knife, with load balancing included\n\t\t- `pgpool-II` seems to be more out-dated\n\t- [comparison](https://scalegrid.io/blog/postgresql-connection-pooling-part-4-pgbouncer-vs-pgpool/)\n\n#### pgBouncer\n- When PgBouncer receives a client connection, it first performs authentication on behalf of the PostgreSQL server.\n\t- when a password is provided, one of 2 things happens:\n\t\t1. PgBouncer first checks the userslist.txt file – this file specifies a set of (username, md5 encrypted passwords) tuples. If the username exists in this file, the password is matched against the given value. No connection to PostgreSQL server is made. \n\t\t2. If passthrough authentication is setup, and the user is not found in the userslist.txt file, PgBouncer searches for an auth_query. It connects to PostgreSQL as a predefined user (whose password must be present in the userslist.txt file) and executes the auth-query to find the user’s password and matches it to the provided value.\n- When the client executes an SQL statement:\n\t1. PgBouncer checks for a cached connection\n\t2. if found, it returns the connection to the client. Otherwise, it creates a new connection\n![a891779b4493983dd662f11960423a72.png](:/bde888d39a1c41fa99e619df6db6f56b)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/740ec8d9-29c6-4ce6-b992-5b68ff4a3e1d.html","relUrl":"notes/740ec8d9-29c6-4ce6-b992-5b68ff4a3e1d.html"},{"doc":"Clause","title":"Clause","hpath":"postgres.clause","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a47e1e0f-09ef-47a2-a0e0-2d65b9768d07.html","relUrl":"notes/a47e1e0f-09ef-47a2-a0e0-2d65b9768d07.html"},{"doc":"On Conflict","title":"On Conflict","hpath":"postgres.clause.on-conflict","content":"\nThe newly added ON CONFLICT clause allows to specify an alternative to\nraising a unique or exclusion constraint violation error when inserting.\nON CONFLICT refers to constraints that can either be specified using a\ninference clause (by specifying the columns of a unique constraint) or\nby naming a unique or exclusion constraint.  DO NOTHING avoids the\nconstraint violation, without touching the pre-existing row.","url":"https://tycholiz.github.io/Digital-Garden/notes/fd2531b9-6e1d-4f62-a2f8-2259bbe78b62.html","relUrl":"notes/fd2531b9-6e1d-4f62-a2f8-2259bbe78b62.html"},{"doc":"Alter","title":"Alter","hpath":"postgres.clause.alter","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/4ac7ea3e-6b6b-41f9-b9a9-e95888146552.html","relUrl":"notes/4ac7ea3e-6b6b-41f9-b9a9-e95888146552.html"},{"doc":"Alter","title":"Alter","hpath":"postgres.clause.alter.cook","content":"\n### Alter column to have unique constraint\n`ALTER TABLE foo ADD UNIQUE (column_name);`","url":"https://tycholiz.github.io/Digital-Garden/notes/cd679ba3-3c52-4c80-a14c-00bb096018ba.html","relUrl":"notes/cd679ba3-3c52-4c80-a14c-00bb096018ba.html"},{"doc":"Admin","title":"Admin","hpath":"postgres.admin","content":"\n# Overview\n- When you install an instance of Postgres (ex. in Node), there will be a corresponding Postgres server, since postgres uses a client/server model.\n\t- if we use different ports, we can have multiple Postgres servers on a single physical server\n\n## Processes\n### Postgres Server\n- the server program is called `postgres`\n- purposes:\n\t- manages database files\n\t- accepts connections from clients\n\t- performs operations on behalf of clients\n\n### Postgres Client\n- the client is the application that wants to perform database operations\n- clients are diverse in nature, and could be a CLI, GUI, or a library within a web server (like `pg` in node.js)\n\n- Postgres' value is about concurrency and isolation\n\t- The idea: what happens when 2+ people are trying to do the same thing concurrently? how is that handled?\n\t\t- This value is provided by the concept of ACID\n\t\t- The fact that we have transactions in postgres means that we are ACID-compliant.","url":"https://tycholiz.github.io/Digital-Garden/notes/537e7c14-a08e-4b59-b977-0840f9f3b12c.html","relUrl":"notes/537e7c14-a08e-4b59-b977-0840f9f3b12c.html"},{"doc":"Users","title":"Users","hpath":"postgres.admin.users","content":"\n# Database Users\n- each database cluster has a set of database users, which are distinct from the users that the OS of the server manages.\n- database users are global across a cluster installation.\n- users own database objects, such as tables. These owners can assign priveleges on those objects to other users.\n- freshly initialized postgres systems will always contain a predefined user with ID 1, which has the same name as the OS user that initialized the db cluster. However, it is often a best practice to name this user `postgres` instead.\n\t- this is a superuser\n\t- To create more users, we need to connect as this user first.","url":"https://tycholiz.github.io/Digital-Garden/notes/5c6c2a9f-4177-4432-a713-e7be39121b3c.html","relUrl":"notes/5c6c2a9f-4177-4432-a713-e7be39121b3c.html"},{"doc":"Settings","title":"Settings","hpath":"postgres.admin.settings","content":"\n# System Administration\n- there are 2 functions (getter/setter) that allow us to query and alter run-time configuration parameters:\n\t1. `current_setting` - returns the current `value` of the setting associated with the provided `key`\n\t2. `set_config` - pass the setting name and the new value.\n\t- these functions are executed with a SELECT statement. The result is displayed as a table.","url":"https://tycholiz.github.io/Digital-Garden/notes/7021e5e2-813c-42ec-ac07-5e05f2a83cae.html","relUrl":"notes/7021e5e2-813c-42ec-ac07-5e05f2a83cae.html"},{"doc":"Search Path","title":"Search Path","hpath":"postgres.admin.search-path","content":"\n# Search path\n- Implicitly, every object in postgres gets operated on as a path. running `...into nuggets`, postgres runs under the hood `...into neverforget.public.nuggets`.\n- We can run `SHOW search_path;` to get the path\n\t- the first element of the output `$user` indicates that a schema with the same name as current user (ex. a schema called `kyletycholiz`) should be searched first\n\t- the first schema listed (default `public`) is the default location for creating new objects \n\t\t- when objects are referenced without qualifying a schema (ex. `...into nuggets`), the search_path is traversed, starting with public, and onto the next one (by default only 1, so we need to specify more) \n- to modify the `search_path`, we can run `SET search_path TO myschema,public;`, which will inform postgres that `myschema` should be searched first when there is no specified schema.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/88b9e181-72f0-4147-8778-2420e5088abc.html","relUrl":"notes/88b9e181-72f0-4147-8778-2420e5088abc.html"},{"doc":"Dir","title":"Dir","hpath":"postgres.admin.dir","content":"\n# Database File\n- known by the environment variable PGDATA, this is where all the data for a database cluster is stored.\n\t- likely `/etc/postgresql/<VERSION>/main/`\n\n### base/\n- each directory within `base/` is used by a database node in our cluster. Each directory is named after the database's OID\n\t- the files inside are the actual data of the relations (ie. tables, indexes, sequences)\n- database examples: `postgres`, `template0`, `template1`, `neverforget`, which can be seen from psql with `\\l`\n\t- the `postgres` database is not required to exist. It is simply a default that is connected to if no database is specified in the connection string.\n\t- `CREATE DATABASE` works by copying the `template1` database (`template0` can be thought of as a more primitive and stripped down version of `template1`\n\n### pg_hba.conf\n- a file that controls the authentication of clients to connect to the postgres database\n\t- HBA stands for host-based authentication.\n- after making changes to this file, to put them into effect run `SELECT pg_reload_conf();` or `pg_ctrl reload` with superuser.\n- In `psql`, run `SHOW hba_file;`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/6bfcd8cc-9dc2-4a19-8038-b2d225a390b4.html","relUrl":"notes/6bfcd8cc-9dc2-4a19-8038-b2d225a390b4.html"},{"doc":"Auth","title":"Auth","hpath":"postgres.admin.auth","content":"\n# Authentication\n- to connect to a PG databse, there are a few different ways to have users authenticate themselves\n- the default authentication method can be found in the `pg_hba.conf` file.\n\t- therefore, if we want to change the default method from peer authentication to md5 (password), we change it here (remember to restart the service)\n\n## Peer Authentication\n- by default, `psql` tries to connect to the postgres database over UNIX sockets. The default authentication method is *peer authentication*, which requires the current UNIX user to have the same username as `psql`\n\t- spec: Therefore, to connect with peer authentication, we need to be logged in on UNIX as the same username as the postgres username we are trying to connect with\n\t- ex. if on UNIX we are logged in as user `kyletycholiz`, then simply executing `psql` without arguments will try and log us in as the postgres user `kyletycholiz`. If this user doesn't exist in postgres, then we will get a peer authentication error.\n- works by obtaining the client's OS username from the kernel, and using it as the allowed database username.\n- Only supported for local connections.","url":"https://tycholiz.github.io/Digital-Garden/notes/d7ae827f-7104-4b55-b9e2-42af4801d484.html","relUrl":"notes/d7ae827f-7104-4b55-b9e2-42af4801d484.html"},{"doc":"Acid","title":"Acid","hpath":"postgres.acid","content":"\n### How Postgres handles Transactions and Concurrency\nPostgres handles these 2 problems with MVCC (multi-version concurrency control)\n- When you update or delete a row, Postgres doesn't actually remove the row. When you do an UPDATE or DELETE, the row isn't actually physically deleted. For a DELETE, the database simply marks the row as unavailable for future transactions, and for UPDATE, under the hood it's a combined INSERT then DELETE, where the previous version of the row is marked unavailable. These new versions of rows are generally referred to as the \"live\" rows, and the older versions are referred to as \"dead\" rows.\n\n## UE Resources\n[Concurrency control](https://www.postgresql.org/docs/current/mvcc-intro.html)","url":"https://tycholiz.github.io/Digital-Garden/notes/8e17d677-231b-48f1-a493-bb106dd900fc.html","relUrl":"notes/8e17d677-231b-48f1-a493-bb106dd900fc.html"},{"doc":"Postgres Xl","title":"Postgres Xl","hpath":"postgres-xl","content":"\n- a horizontally scalable SQL database cluster\n- It is equipped to handle write-intensive workloads that depend on atomic transactions which are potentially distributed\n\t- OLTP\n- PostgresXL allows you to either partition tables across multiple nodes, or replicate them. \n\t- Partitioning (or distributing) tables allows for write scalability across multiple nodes as well as massively parallel processing (MPP) for Big Data type of workloads.\n\t\t- MPP is using many computers to perform a set of coordinated computations in parallel\n\n## Components of Postgres-XL\n### Global Transaction Monitor\n- ensures cluster-wide transaction consistency\n\t- responsible for issuing transaction ids and snapshots as part of its Multi-version Concurrency Control.\n\n### Coordinator\n- manages the user sessions and interacts with GTM and the data nodes\n\t- parses and plans queries, and sends down a serialized global plan to each of the components involved in a statement.\n\n### Data node\n- where the actual data is stored. \n\t- The distribution of the data can be configured by the DBA.\n- warm standbys of the data nodes can be configured to be failover-ready.","url":"https://tycholiz.github.io/Digital-Garden/notes/4eeb4df6-2842-4db4-84ce-1992975f36a5.html","relUrl":"notes/4eeb4df6-2842-4db4-84ce-1992975f36a5.html"},{"doc":"Postgraphile","title":"Postgraphile","hpath":"postgraphile","content":"\nPostGraphile automatically detects tables, columns, indexes, relationships, views, types, functions, comments and more. It builds a GraphQL server that is highly intelligent about your data, and that automatically updates itself without restarting when you modify your database.\n\nPostGraphile provisions, sets up and tears down a PostgreSQL client automatically for each GraphQL query\n- Setup involves beginning a transaction and setting the relevant session variables, e.g. using your JWT or the pgSettings function\n\n### Smart Comments\n- in postgres, we can make user-friendly remarks on a table called `comments` by using the COMMENT statement\n- Postgraphile leverages this feature to be able to alter functionalities by adding smart comments as comments on a table\n\n## Function\n**Computed Column vs. Custom Query**\n- We can differentiate between computed columns and custom queries by observing that computed columns must accept the table they belong to as a first arg. Of course, to be able to create a computed column we need a table to attach it to.\n- Consider the expansive nature of computed columns. They allow us to augment our existing tables with data that doesn't normally belong there.\n\n### Computed column\n- def - a psuedo column that we can attach to a table that will automatically be reflected in the graphql schema\n- in a function, running `setof nugget` will return a connection\n\n### Custom Queries\n- similar to computed columns, but instead of the function being callable as a node's field (ex. the `buckets` field on `nugget`), the function is callable from the root-level\n- ex. `all_nuggets_with_bucket_id` function\n\n## Auth\n- Postgraphile can generate JWTs easily from inside your PostgreSQL schema.\n\t- to do this, we define a `jwtToken` composite type and we pass it to `jwtPgTypeIdentifier`, and now every time that type is returned from a postgres function, it will be signed with the jwtSecret, and return it as a jwt token as part of the graphql response.\n- when the server receives a jwtToken from the request's authorization headers, like so:\n```\n{\n  \"aud\": \"postgraphile\",\n  \"role\": \"app_user\",\n  \"user_id\": 27\n}\n```\nit will automatically run this code:\n```\nset local role app_user;\nset local jwt.claims.role to 'user_login';\nset local jwt.claims.user_id to '27';\n```\n\n## Exposing HTTP requests to Postgres\n- `pgSettings` lets us set the jwt within postgres' `current_setting` while having access to the request\n\t- this function fires on each request, and everything returned by it will be applied to `current_setting` (with `set_config`)\n\t- ex. we can get the userId from the request and update the value of `user_id` within `current_setting(...)`\n\t- `pgSettings` is a function that can be async\n- instead of passing an object, we can pass `pgSettings` function that will get executed on each request.\n- Everything returned by `pgSettings` is applied to the current session with `set_config($key, $value, true)`\n- You can use `pgSettings` to define variables that your Postgres functions/policies depend on\n\t- When adding variables for your own usage, the keys must contain either one or two periods (`.`)\n\t\t- Variables without periods will be interpreted as internal Postgres settings, such as role, and will be applied by Postgres\n\t\t- All settings are automatically reset when the transaction completes\n\t- Here's an example of switching the user into the Postgres 'visitor' role, and applying the application setting jwt.claims.user_id:\n\t```\n\tpgSettings: async req => ({\n\t    'role': 'visitor',\n\t    'jwt.claims.user_id': req.user ? req.user.id : undefined,\n\t    //...\n\t}),\n\t```\n- role is overridden after pgSettings is applied\n\t- But only if `pgDefaultRole` is set or there's a role property in the claim of the JWT\n\n## Exposing HTTP requests to resolvers\n- `additionalGraphQLContextFromRequest` is an optionally asynchronous function that has access to the req and res objects from your HTTP library (Express)\n\t- The result returned from the function is merged into the GraphQL context object which is passed as the third argument to every GraphQL resolver\n\t- `additionalGraphQLContextFromRequest` let us perform asynchronous actions if we need to, for example looking up the current user in the database with Passport. Once all that is done, we can return an object from this function that will merge with the existing `context` object so the resolvers can access it.\n\n## Graphile Workers\n- allows you to run jobs (e.g. sending emails, performing calculations, generating PDFs, etc) \"in the background\" so that your HTTP response/application code is not held up\n[repo](https://github.com/graphile/worker)\n\n## Extending Graphql Schema (makeExtendSchemaPlugin)\n- when we use `makeExtendSchemaPlugin`, we can define types and resolvers that will get merged into the existing ones generated by Postgraphile\n- the callback returns an object with 2 keys:\n\t1. `typeDefs`\n\t2. `resolvers`\n\t\t- an object whose keys are graphql types, which resolve to an object with `key`-`value` pair of `field`-`resolver function`\n- the `build` argument is supplied to the `makeExtendSchemaPlugin` callback, and it contains lots of information and helpers defined by various plugins\n\t- includes the introspection results, inflection functions, and SQL helper (`build.pgSql`, an instance of `pg-sql2`, a query builder)\n\n* * *\n### Custom mutations/queries\n- By default Postgres assumes all functions will mutate the database. Therefore, if we want the postgres function to show up as a query, we need to mark it `stable`\n- when naming custom functions that get back the user some data, we need to name it as if it were a simple property on an object. We don't want to name is something like `getUsers`. Instead, we want to simply call it `users`. This makes more sense when viewing it from the graphiql perspective and querying via graphql.\n\n* * *\n\n### Pooling\n- if we are using postgraphile, `rootPgPool` Postgraphile doesn't know about it, as we don't pass it to the postgraphile engine. Instead, it is used in the `additionalGraphQLContextFromRequest` callback.\n\n* * *\n\n### Scalar Types\nPostGraphile generates the following scalar types:\n- BigFloat, BigInt, BitString, Boolean, CidrAddress, Date, Datetime, Float, Int, InternetAddress, Interval, JSON, KeyValueHash, MacAddress, MacAddress8, String, Time, UUID\n\n## Resources\n### Quality Repos\n[graphile/starter - lots of really quality code. check out how some of the lower level db config functions work](https://github.com/graphile/starter)\n### Quality Docs\n[lots of overall good info, inc high level setting up the SQL](https://github.com/graphile/postgraphile/blob/v4/examples/forum/TUTORIAL.md)","url":"https://tycholiz.github.io/Digital-Garden/notes/20206ec8-0739-464e-b41d-65307a0c5aa6.html","relUrl":"notes/20206ec8-0739-464e-b41d-65307a0c5aa6.html"},{"doc":"Utils","title":"Utils","hpath":"postgraphile.utils","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3e0dad08-376c-45b9-a08e-5062aa683e7c.html","relUrl":"notes/3e0dad08-376c-45b9-a08e-5062aa683e7c.html"},{"doc":"Gql","title":"Gql","hpath":"postgraphile.utils.gql","content":"\nThe gql helper is responsible for turning the human-readable GraphQL schema language you write into an abstract syntax tree ([[AST|graphql.ast]]) that the application can understand.\n\nthe `gql` helper differs slightly from the one from `graphql-tag`, namely in how the placeholders work.\n- This `gql` function is designed to work with PostGraphile's inflection system, so you can embed strings directly\n- We can also embed other gql tags directly\n\n```js\nconst nameOfType = \"MyType\"; // Or use the inflection system to generate a type\n\n// This tag interpolates the string `nameOfType` to allow dynamic naming of the\n// type.\nconst Type = gql`\n  type ${nameOfType} {\n    str: String\n    int: Int\n  }\n`;\n\n// This tag interpolates the entire definition in `Type` above.\nconst typeDefs = gql`\n  ${Type}\n\n  extend type Query {\n    fieldName: Type\n  }\n`;\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/25864130-d786-4fee-a831-f24ab9949472.html","relUrl":"notes/25864130-d786-4fee-a831-f24ab9949472.html"},{"doc":"Embed","title":"Embed","hpath":"postgraphile.utils.gql.embed","content":"\nIf we want to embed a raw js value (function, string, object etc.) into a document, we can use the `embed` function within the `gql` tagged template literal\n- most common use case is to pass the value to a directive","url":"https://tycholiz.github.io/Digital-Garden/notes/3da21477-83f6-4725-bac6-858fdcb3f9a0.html","relUrl":"notes/3da21477-83f6-4725-bac6-858fdcb3f9a0.html"},{"doc":"Schema","title":"Schema","hpath":"postgraphile.schema","content":"\n## makeExtendSchemaPlugin\nAllows us to extend the graphql schema that is generated by Postgraphile\n- It does this by allowing us to define additional Graphql Types and Resolvers and merge them into our schema.\n- To accomplish this, the callback we pass to `makeExtendSchemaPlugin` should return the `typeDefs` (schema definition) and the `resolvers` function\n\n### typeDefs\nThis allows us to specify how our Graphql schema will be extended.\n- ex. Will we be adding a new subscription on it? A new InputType?\n\n### build\nThe `build` argument to the `makeExtendSchemaPlugin` contains information and helpers defined by various plugins in the Postgraphile ecosystem\n- most importantly, it includes:\n\t1. introspection results (`build.pgIntrospectionResultsByKind`)\n\t2. inflection functions (`build.inflection`)\n\t3. an SQL helper (`build.pgSql`, an instance of `pg-sql2`\n\n### selectGraphQLResultFromTable\nThis helper populates data that is returned from our resolver\n- It should *not* be used to retrieve data for our resolver to process.\n\t- Instead use `context.pgClient` directly.\n\nThis helper should not be called more than once per resolver (which wouldn't make sense anyway)","url":"https://tycholiz.github.io/Digital-Garden/notes/c9de9dac-6b16-45fb-8485-8be569b23c8e.html","relUrl":"notes/c9de9dac-6b16-45fb-8485-8be569b23c8e.html"},{"doc":"Views","title":"Views","hpath":"postgraphile.schema.views","content":"\nWe can use postgres views in such a way that it is exposed on the graphql schema to be consumed, as if it were an actual table.\n\nImagine we have the following:\n```sql\nCREATE TABLE app_public.films (\n  id serial PRIMARY KEY,\n  name text,\n  release_year int,\n  kind text\n);\n\nCREATE VIEW comedies AS\n    SELECT *\n    FROM app_public.films\n    WHERE kind = 'Comedy';\n```\n\nThe view `comedies` is able to be queried directly on the Graphql schema:\n```\n{\n  comedies(first: 20) {\n    name\n    releaseYear\n  }\n}\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/440c6320-a997-4055-ad3c-f67a136263b5.html","relUrl":"notes/440c6320-a997-4055-ad3c-f67a136263b5.html"},{"doc":"Directives","title":"Directives","hpath":"postgraphile.schema.directives","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/fdaddd78-23a2-4421-a78d-a0aaf9097525.html","relUrl":"notes/fdaddd78-23a2-4421-a78d-a0aaf9097525.html"},{"doc":"Requires","title":"Requires","hpath":"postgraphile.schema.directives.requires","content":"\n# @requires\nWhen extending a schema, it's often because you want to expose data from Node.js that would be too difficult (or impossible) to access from PostgreSQL\n- in other words, we come to a situation where we don't want to rely on Postgres to process and give us data, but instead want to rely on Node to do the job for us.\n\n## USD to CAD example\n- Imagine in our database we store the price of a product in USD. However, we want to be able to query the `priceInCad`, directly as a field on the graphql schema:\n```\nquery Product {\n\tproduct {\n\t\tpriceInCad\n\t}\n}\n```\n\nWe can accomplish this by making a custom Postgraphile plugin, which would extend the Graphql schema to include the `priceInCadCents` field on the `Product` type:\n```\nconst MyForeignExchangePlugin = makeExtendSchemaPlugin(build => {\n  return {\n    typeDefs: gql`\n\t\t\textend type Product {\n\t\t\t\tpriceInCadCents: Int! @requires(columns: [\"price_in_us_cents\"])\n\t\t\t}`,\n    resolvers: {\n      Product: {\n        priceInCadCents: async product => {\n          // Note that the columns are converted to fields, so the case changes\n          // from `price_in_us_cents` to `priceInUsCents`\n          const { priceInUsCents } = product;\n          return await convertUsdToCad(priceInUsCents);\n        },\n      },\n    },\n  };\n});\n```\n\nwe include the `@require` directive to show that in order to calculate this field, we need the postgres column `price_in_us_cents`","url":"https://tycholiz.github.io/Digital-Garden/notes/079e5750-ec14-4d99-a730-58709b720630.html","relUrl":"notes/079e5750-ec14-4d99-a730-58709b720630.html"},{"doc":"Pgsubscription","title":"Pgsubscription","hpath":"postgraphile.schema.directives.pgsubscription","content":"\nProvided by the `@graphile/pg-pubsub` library\n\nThis directive allows us to embed a function that will calculate the PostgreSQL topic to subscribe to based on the arguments and context passed to the GraphQL field","url":"https://tycholiz.github.io/Digital-Garden/notes/6ab86334-7fb4-4f34-bf50-36bef151ce70.html","relUrl":"notes/6ab86334-7fb4-4f34-bf50-36bef151ce70.html"},{"doc":"Pubsub","title":"Pubsub","hpath":"postgraphile.pubsub","content":"\nGraphile defines a library `pg-pubsub`\n\nThe main high-level purpose of this library is provide our custom plugins with realtime data, which allows us to add subscription fields to our API.\n- The library uses Postgres' `LISTEN`/`NOTIFY` to provide realtime features\n\nThis library also gives us the `@pgSubscription` directive that we can put on graphql fields\n\n\n[[Pub-Sub|general.architecture.pub-sub]]","url":"https://tycholiz.github.io/Digital-Garden/notes/0eb20a5a-2ad4-4a56-8e9d-a28b8676c19e.html","relUrl":"notes/0eb20a5a-2ad4-4a56-8e9d-a28b8676c19e.html"},{"doc":"Philosophy","title":"Philosophy","hpath":"philosophy","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d1aaf074-9c4c-4926-9106-44b71fd92529.html","relUrl":"notes/d1aaf074-9c4c-4926-9106-44b71fd92529.html"},{"doc":"Data","title":"Data","hpath":"philosophy.data","content":"\n# Push vs Pull data creation\n- Pull and Push are two different protocols that describe how a data Producer can communicate with a data Consumer.\n![](/assets/images/2021-03-07-22-37-12.png)\n\nNote:\n- A regular function is a lazily evaluated computation that synchronously returns a single value on invocation.\n- A generator function is a lazily evaluated computation that synchronously returns zero to (potentially) infinite values on iteration.\n- A Promise is a computation that may (or may not) eventually return a single value.\n- An Observable is a lazily evaluated computation that can synchronously or asynchronously return zero to (potentially) infinite values from the time it's invoked onwards.\n\n**Pull**\n- Consumer determines when it receives data from the data Producer\n- The Producer itself is unaware of when the data will be delivered to the Consumer.\n- ex. every javascript function is a pull system\n\t- The function itself is the producer, and the calling code is the consumer. The reason the calling code is called the consumer is because the calling code \"pulls\" out a *single* return value\n\n**Push**\n- Producer determines when to send data to the consumer, and the consumer is unaware of when it will receive that data.\n- ex. Promises and Observables are a push system, since the promise (a producer) delivers a resolved value to the registered callbacks (the consumers).\n\t- it is the Promise which is in charge of determining precisely when that value is \"pushed\" to the callbacks.\n\t- ex. An Observable is a Producer of multiple values, \"pushing\" them to Observers (Consumers).\n","url":"https://tycholiz.github.io/Digital-Garden/notes/4104df31-9454-4551-bc65-a846a6646578.html","relUrl":"notes/4104df31-9454-4551-bc65-a846a6646578.html"},{"doc":"Automation","title":"Automation","hpath":"philosophy.automation","content":"\n# Automation\n![](/assets/images/2021-03-07-22-40-00.png)\n- benefits of automation (not related to time savings):\n\t- prevents you from having to context switch often\n\t- provides you with self-documented code that demonstates how to accomplish a certain task\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8434aeb0-eb45-418b-8bef-80b4a39a00d9.html","relUrl":"notes/8434aeb0-eb45-418b-8bef-80b4a39a00d9.html"},{"doc":"Paradigm","title":"Paradigm","hpath":"paradigm","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3babc3d2-79ae-470a-9c06-ab8bba2e684e.html","relUrl":"notes/3babc3d2-79ae-470a-9c06-ab8bba2e684e.html"},{"doc":"Oop","title":"Oop","hpath":"paradigm.oop","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/12a2891c-e26b-47db-9e2e-7a3de90b672b.html","relUrl":"notes/12a2891c-e26b-47db-9e2e-7a3de90b672b.html"},{"doc":"Polymorphism","title":"Polymorphism","hpath":"paradigm.oop.polymorphism","content":"\n# UE Resources\n- [Good explanation](https://stackoverflow.com/questions/154577/polymorphism-vs-overriding-vs-overloading)","url":"https://tycholiz.github.io/Digital-Garden/notes/35a45ef6-ad39-4bcb-ad57-4c2727236af7.html","relUrl":"notes/35a45ef6-ad39-4bcb-ad57-4c2727236af7.html"},{"doc":"Overriding","title":"Overriding","hpath":"paradigm.oop.overriding","content":"\nWhen we have a method in the child class and a method in the parent class, and they both have the same name. In this case, the method defined in the child would override the parent's version.\n\nOverriding implements Runtime Polymorphism","url":"https://tycholiz.github.io/Digital-Garden/notes/3a82a414-e1ac-4e54-8cb5-8b8cc1a6de27.html","relUrl":"notes/3a82a414-e1ac-4e54-8cb5-8b8cc1a6de27.html"},{"doc":"Overloading","title":"Overloading","hpath":"paradigm.oop.overloading","content":"\nWhen two or more methods in the same class have the same name but different parameters, it’s called Overloading.\n- This is distinct from `Overriding` which is when we have a method in the child class and a method in the parent class, and they both have the same name. In this case, the method defined in the child would override the parent's version.\n\nOverloading implements Compile time polymorphism","url":"https://tycholiz.github.io/Digital-Garden/notes/109bced0-70d0-4f2d-9ca1-f6bc2fa0602e.html","relUrl":"notes/109bced0-70d0-4f2d-9ca1-f6bc2fa0602e.html"},{"doc":"Interface","title":"Interface","hpath":"paradigm.oop.interface","content":"\nAn interface fulfills the principle of \"duck typing\"\n\n```ts\ninterface inputObjectI = {\n\tlabel: string\n\toptionalVal: string?\n\treadonly readOnlyVal: number // value is immutable post-declaration\n}\n```\nOnce we define the interface, it can then be used as a type.\n\nFunction interfaces can also be created\n- ex. we have a fn with 2 params of type `String` and `String`, and the function returns `Boolean`\n```ts\ninterface search {\n\t(source: string, substring: string): boolean\n}\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/e01644e0-dbe0-4760-aac4-d44f67558164.html","relUrl":"notes/e01644e0-dbe0-4760-aac4-d44f67558164.html"},{"doc":"Imperative","title":"Imperative","hpath":"paradigm.imperative","content":"\nA language is imperative when they make explicit references to the state of the execution environment\n- think about how calling .open() on a modal relates to this, versus setting a variable isOpen that is listened to by the modal. .open() is explicitly referencing the state of the environment \n","url":"https://tycholiz.github.io/Digital-Garden/notes/6e3f6aa9-2633-4244-b56f-8faab503d560.html","relUrl":"notes/6e3f6aa9-2633-4244-b56f-8faab503d560.html"},{"doc":"Functional","title":"Functional","hpath":"paradigm.functional","content":"\n# Overview\n\"function\" is a reference to how functions work in math.\nEx. f(x) = 2x² + 5 can be plotted on a graph to make a parabola. In this sense, it can be thought of as a map. An x value of 5 maps to a return value of 55. \nThough FP rarely deals with graphs like this, think of it as input values mapping to output\n![](/assets/images/2021-03-09-09-35-55.png)\n\nBased on idea of Referential Transparency\n\n### Referential transparency\nThe notion that a function could be replaced by its return value and it wouldn't impact the functionality of the program.\nIf satisfied, this is a clear sign that a function is pure.\nas you're reading a program, once you've mentally computed what a pure function call's output is, you no longer need to think about what that exact function call is doing when you see it in code, especially if it appears multiple times.\nThat result becomes kinda like a mental const declaration, which as you're reading you can transparently swap in and not spend any more mental energy working out.\n\n\n## Abstracting (Generalizing) Functions\nSimilar to how partial application and currying (see Chapter 3) allow a progression from generalized to specialized functions, we can abstract by pulling out the generality between two or more tasks. The general part is defined once, so as to avoid repetition. To perform each task's specialization, the general part is parameterized.\n\nconsider:\n```\nfunction saveComment(txt) {\n    if (txt != \"\") {\n        comments[comments.length] = txt;\n    }\n}\n\nfunction trackEvent(evt) {\n    if (evt.name !== undefined) {\n        events[evt.name] = evt;\n    }\n}\n```\nthe repetition (generality) between the 2 functions is: storing a value in a data source. the uniqueness (specialty) of them is that one sticks the value on the end of an array, while the other sets the value as a property on an object.\n\nabstracting:\n```\nfunction storeData(store,location,value) {\n    store[location] = value;\n}\n\nfunction saveComment(txt) {\n    if (txt != \"\") {\n        storeData( comments, comments.length, txt );\n    }\n}\n\nfunction trackEvent(evt) {\n    if (evt.name !== undefined) {\n        storeData( events, evt.name, evt );\n    }\n}\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/dfb11aa2-d9cf-4d30-b6b6-eaa1615b7540.html","relUrl":"notes/dfb11aa2-d9cf-4d30-b6b6-eaa1615b7540.html"},{"doc":"Transducer","title":"Transducer","hpath":"paradigm.functional.transducer","content":"\n# Transducer\n[read more](https://www.jeremydaly.com/transducers-supercharge-functional-javascript/)\ndef: A transducer is a function which takes in a reducer, and returns another reducer\ndef: A transducer takes an object or array, iterating through each value, transforming each element with a composition of transformer functions\n\nThe transduce function is really just a reduce function with an additional argument upfront:\n```js\n// With reduce\nR.reduce(R.flip(R.append), [], autobots)\n\n// With transduce\nR.transduce(transform, R.flip(R.append), [], autobots)\n```\nthe above reduce function iterates the elements of our `autobots` array and appends them to the accumulator\n\nWhen we use `transduce()`, we are passing each item from our list into our transformation function before passing it to our reducing function. So basically, transduce is just a way for us to transform items while reducing them. We are, in fact, *transducing*\n\ntransducers are a generic and composable way to operate on a collection of values, producing a new value or new collection of new values\n\nIn this example, we can see the power of `transduce`. The first way, we need to iterate over `autobots` a total of 3 times. However, when we use `transduce` as in the second way, we only need to iterate over it once.\n- The traditional way, is that we perform `map` on each element of the array, then `map` on each element again, then `filter` on each element. With transduce, we perform `map` on the first element, then `map` again on the first element, then `filter` on the first element. The resulting value is then mapped to the output array, then the second element is performed on.\n```js\nlet autobots = ['Optimus Prime','Bumblebee','Ironhide','Sunstreaker','Ratchet']\n \n// Filter for autobots that contain 'r', uppercase, then reverse\nlet transform = R.compose(\n  R.filter(x => /r/i.test(x)),\n  R.map(R.toUpper),\n  R.map(R.reverse)\n)\n \n// BEFORE\ntransform(autobots)\n// => [ 'EMIRP SUMITPO', 'EDIHNORI', 'REKAERTSNUS', 'TEHCTAR' ]\n\n// AFTER\nR.transduce(transform, R.flip(R.append), [], autobots)\n// => [ 'EMIRP SUMITPO', 'EDIHNORI', 'REKAERTSNUS', 'TEHCTAR' ]\n```\n\nThe word ‘transducer’ itself can be split into two parts that reflect this definition: \n- `transform` — to produce some value from another (ex. `reduce`/`map`).\n- `reducer` — to combine the values of a data structure to produce a new one.\n- this is a really cool concept, because it allows us to abstract away any implementation detais that we\n\n`.map` and `.filter` can be implemented with `.reduce`. Therefore, we see that `.reduce` is a more general function, and `.map` and `.filter` are more specific implementations of `.reduce`. Transduce allows us to treat a reduce-like construct as a *higher order function*, meaning we can pass in the *reducing function* (which is the function found in a reducer)\n- This means we could create `.map`/`.filter` with a transducer\n[more info](https://www.jeremydaly.com/transducers-supercharge-functional-javascript/)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/284556b1-3f05-4eb7-a93f-26bcef058391.html","relUrl":"notes/284556b1-3f05-4eb7-a93f-26bcef058391.html"},{"doc":"Recursion","title":"Recursion","hpath":"paradigm.functional.recursion","content":"\n# Recursion\nMentally, what's happening is similar to when a mathematician uses a Σ summation in a larger equation. We're saying, \"the max-even of the rest of the list is calculated by maxEven(...restNums), so we'll just assume that part and move on.\"\n```js\nfunction maxEven(num1, ...restNums) {\n\tvar maxRest = restNums.length > 0 ? maxEven(...restNums) : undefined;\n\n\treturn (num1 % 2 != 0 || num1 < maxRest) ? maxRest : num1;\n}\n\nconsole.log(maxEven(1, 6, 3))\n```\notherwise hacking would be easier. \n\nalmost every operation you'll do with trees is implemented most easily with recursion\nOnce the base case is met, the final return value makes its way back up all layers of the call stack to give us that value\n```js\nfunction foo(x) {\n    if (x < 5) return x;\n    return foo( x / 2 );\n}\n```\n\n![](/assets/images/2021-03-09-09-41-09.png)\nRecursion is declarative for algorithms in the same sense that Σ is declarative for mathematics.\n\n\n## Simulating reduce with recursion\neach call of the function separates the first argument from the rest, takes something from that first argument, then calling the function again with the remaining arguments (all but first)\n```js\nfunction sum(num1,...nums) {\n    if (nums.length == 0) return num1;\n    return num1 + sum( ...nums );\n}\n```\n```js\nfunction maxEven(num1,...restNums) {\n    var maxRest = restNums.length > 0 ?\n            maxEven( ...restNums ) :\n            undefined;\n\n    return (num1 % 2 != 0 || num1 < maxRest) ?\n        maxRest :\n        num1;\n}\n```\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/321388fd-fcad-4f32-b902-20c0ee043f9f.html","relUrl":"notes/321388fd-fcad-4f32-b902-20c0ee043f9f.html"},{"doc":"Purity","title":"Purity","hpath":"paradigm.functional.purity","content":"\n## Purifying functions\n- Use immutable data structures (ex. Immutable.js)\n- Sometimes you can just shift the side effects out of a function to the part of the program where the call of that function happens. The side effect wasn't eliminated, but it was made more obvious by showing up at the call-site.\n\n```js\nfunction addMaxNum(arr) {\n    var maxNum = Math.max( ...arr );\n    arr.push( maxNum + 1 );\n}\n\nvar nums = [4,2,7,3];\n\naddMaxNum( nums );\n\nnums;       // [4,2,7,3,8]\n```\nto\n\n```js\nfunction addMaxNum(arr) {\n    var maxNum = Math.max( ...arr );\n    arr.push( maxNum + 1 );\n}\n\nvar nums = [4,2,7,3];\n\naddMaxNum( nums );\n\nnums;       // [4,2,7,3,8]\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/62383843-32b7-4e1d-b2b3-9261ae5800ac.html","relUrl":"notes/62383843-32b7-4e1d-b2b3-9261ae5800ac.html"},{"doc":"Lens","title":"Lens","hpath":"paradigm.functional.lens","content":"\n# Lenses\nThink of a lens as something that focuses (zooms in) on a specific part of a larger data structure\n- Similar to stores  in Redux\n\nAnother way is to see lenses as little drones that copy a part of an object for us, and delve into it to change the properties, all without mutating the original \n\nGiven a lens there are essentially three things you might want to do\n- view the subpart\n- modify the whole by changing the subpart\n- combine this lens with another lens to look even deeper\n\nLenses can be handy if we have a somewhat complex data structure that we want to abstract away from calling code. Rather than exposing the structure or providing a getter, setter, and transformer for every accessible property, we can instead expose lenses. Client code can then work with our data structure using view, set, and over without being coupled to the exact shape of the structure.\n\nIt is a special `type` that combines a *getter* and a *setter* function into a single unit\n\nThe first fn is the getter, while the second is the setter\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5d269d8b-3323-4089-9fac-052909343d09.html","relUrl":"notes/5d269d8b-3323-4089-9fac-052909343d09.html"},{"doc":"Currying","title":"Currying","hpath":"paradigm.functional.currying","content":"\n# Currying\nTransforming a function of N arity (num of args) into N functions of 1 arity.\n\nPartial application\nApplying a certain number of args toward completion of the function (when the last argument is called)\nApplication === calling a function and applying it's return value\n\nJavaScript engine does its job in two phases: memory creation (declaring variables/functions and hoisting them), and execution (initializing variables and actually running through code)\n\n## Why use these techniques?\nThe first and most obvious reason is that both currying and partial application allow you to separate in time/space (throughout your codebase) when and where separate arguments are specified, whereas traditional function calls require all the arguments to be present at the same time. If you have a place in your code where you'll know some of the arguments and another place where the other arguments are determined, currying or partial application are very useful.\n\nAnother layer to this answer, specifically for currying, is that composition of functions is much easier when there's only one argument. So a function that ultimately needs three arguments, if curried, becomes a function that needs just one, three times over. That kind of unary function will be a lot easier to work with when we start composing them. But the most important layer is specialization of generalized functions, and how such abstraction improves readability of code.\n\nR.partial says \"you give me a function and any number of arguments you want, then I'll just keep letting you add in as many arguments as you want either indefinitely or until all parameters of the function have been satisfied with arguments\"\n![](:/7548f55bd87c4c279f38a15ebac02ed5)\n\n### Partial application of composed functions\nallows us to compose multiple functions together, while returning a function that will accept more. This has the benefit of allowing us to compose more and more specific functions.\nexpl. `unique` and `words` will get partially applied to `compose`\n\n```\nconst filterWords = partialRight( compose, unique, words )\nconst biggerWords = filterWords(skipShortWords)\n```\n\n![Untitled Diagram.jpg](:/593b0123095a48c98e17db790864665f)\n\n### Currying composed functions\nsince compose has right-to-left ordering, we normally `R.curry(R.reverseArgs(R.compose), ..)`\n\n*Using partials with `.pipe()`*\n`var filterWords = partial( pipe, words, unique )`\n\n#### Functor\nsomething that can be mapped over\nex. array, object\n- a functor that holds values of type A, when mapped over with a function that takes a value of type A and returns a value of type B, the result must be a functor that holds values of type B\n\n#### Applicative\na subtype of functors for which additional functions are defined.\n[more info](https://medium.com/@JosephJnk/an-introduction-to-applicative-functors-aea966799b1d)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/670976d5-df19-413a-8563-0433b0f0d707.html","relUrl":"notes/670976d5-df19-413a-8563-0433b0f0d707.html"},{"doc":"Open API","title":"Open API","hpath":"open-api","content":"\nOpenAPI is a specification for describing REST API formats. Swagger provides tools for implementing that specification. These tools can be used at different stages of the API lifecycle.\n\nAn OpenAPI file (json or yml) allows you to describe your entire API, including:\n- Available endpoints (`/users`) and operations on each endpoint (`GET /users`, `POST /users`)\n- Operation parameters Input and output for each operation\n- Authentication methods\n- Contact information, license, terms of use and other information.\n\nThe value of OpenAPI is that it allows APIs to describe their own structure. This allows us to:\n- Design-first users: use Swagger Codegen to generate a server stub for your API. The only thing left is to implement the server logic – and your API is ready to go live!\n- Use Swagger Codegen to generate client libraries for your API in over 40 languages.\n- Use Swagger UI to generate interactive API documentation that lets your users try out the API calls directly in the browser.\n- Use the spec to connect API-related tools to your API. For example, import the spec to SoapUI to create automated tests for your API.\n\nDefinition example:\n```\nopenapi: 3.0.0\ninfo:\n  title: Sample API\n  description: Optional multiline or single-line description in [CommonMark](http://commonmark.org/help/) or HTML.\n  version: 0.1.9\nservers:\n  - url: http://api.example.com/v1\n    description: Optional server description, e.g. Main (production) server\n  - url: http://staging-api.example.com\n    description: Optional server description, e.g. Internal staging server for testing\npaths:\n  /users:\n    get:\n      summary: Returns a list of users.\n      description: Optional extended description in CommonMark or HTML.\n      responses:\n        '200':    # status code\n          description: A JSON array of user names\n          content:\n            application/json:\n              schema: \n                type: array\n                items: \n                  type: string\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/bb4b4e79-9835-4284-bcac-d3de9b203e1a.html","relUrl":"notes/bb4b4e79-9835-4284-bcac-d3de9b203e1a.html"},{"doc":"Objection","title":"Objection","hpath":"objection","content":"\nObjection is a relational query builder. \n- get all the benefits of an SQL query builder but also a powerful set of tools for working with relations.\n\n# Parts\n- define models declaratively, and map the relationship between models (1:many etc)\n- QueryBuilder - Every method that allows you to fetch or modify items in the database returns an instance of the QueryBuilder\n\t- therefore most important component in Objection\n\nAll instance methods start with the character $ to prevent them from colliding with the database column names.\n\n## Model\nconsider that in an app, there are different layouts of data:\n1. it could be the layout that exists in the database itself\n2. it could be the layout that exists when the user gets that data back from the database (ie, on the client)\ntherefore, any time we read or write to a database, we are converting data\n\nthere are 4 methods on a model that are used to transform data. They exist by default, but can be overridden. \nthese \"converter methods\" will be called each time data is tranformed: \n1. when we are writing data, we are converting it to the database layout, therefore `$formatDatabaseJson`\n2. when we are reading data, we are converting it to our internal data layout, therefore `$parseDatabaseJson`\n3. when we give data for a query, (for example `query().insert(req.body)`) or create a model explicitly using `model.fromjson(obj)` the `$parsejson` method is invoked\n\t- When you call `model.toJSON()` or `model.$toJson()` the `$formatJson` is called.\n- Note: Most libraries like express and koa automatically call the toJSON method when you pass the model instance to methods like response.json(model). You rarely need to call toJSON() or $toJson() explicitly.\n- All properties that start with $ are also removed from database and external layouts.","url":"https://tycholiz.github.io/Digital-Garden/notes/e6ff7608-3de4-46e3-93a7-24ba07f70f42.html","relUrl":"notes/e6ff7608-3de4-46e3-93a7-24ba07f70f42.html"},{"doc":"Nosql","title":"Nosql","hpath":"nosql","content":"\nRelational databases assume that the relationships are of similar importance, document databases assume that relationships form a hierarchical structure and relationships between documents are less important\n\nIf your data cannot be represented on literally a sheet of paper, NoSQL is the wrong data store for you. And I don't mean sheets of paper with references that say \"now turn to page 64 for the diagram\", no, I mean a sheet of paper per document. That is what a normalized record looks like in a document store.\n\nHorizontal scaling is a distinct benefit of NoSQL, which is why companies like Netflix and Spotify use document databases.\n\nGoing from SQL to NoSQL is easier than from NoSQL to SQL","url":"https://tycholiz.github.io/Digital-Garden/notes/dfab2e99-6155-482d-a598-8b8cc11f7ddc.html","relUrl":"notes/dfab2e99-6155-482d-a598-8b8cc11f7ddc.html"},{"doc":"Node","title":"Node","hpath":"node","content":"\n## Resources\n### UE Resources\n[Tutorial: creating a native node module with C++](https://medium.com/@marcinbaraniecki/extending-node-js-with-native-c-modules-63294a91ce4)\n[Tutorial: creating streams in node](https://github.com/substack/stream-handbook)","url":"https://tycholiz.github.io/Digital-Garden/notes/60f0e948-b809-40de-aac1-180423cbebcd.html","relUrl":"notes/60f0e948-b809-40de-aac1-180423cbebcd.html"},{"doc":"Serial Port","title":"Serial Port","hpath":"node.serial-port","content":"\nSerial Port is a Node library that allows us to connect and communicate with an external device via the device's serial ports\n\n![Serial Port](/assets/images/2021-03-20-19-47-20.png)\n\n### Raspberry Pi + Roomba/Arduino\nRoomba has a serial port, meaning we can connect it with a Raspberry Pi via the Pi's GPIO pins\n\nOn the Pi, we would install Node SerialPort, and run `npx @serialport/list` to scan the devices that are connected via the GPIO pins.\n- From there, we open a serial port, we open a REPL, and we define a new port, which uses one of the connected devices and specifies some options\n```\n$ npx @serialport/terminal -p /dev/tty.usbmodem14301                                                                    \nOpening serial port: /dev/tty.usbmodem14301 echo: true\n```\n```\n$ npx @serialport/repl                                                                                                  \nport = SerialPort(\"/dev/tty.usbmodem14301\", { autoOpen: false })\nglobals { SerialPort, portName, port }\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/af02d251-a14c-484d-931e-a9175bef81a2.html","relUrl":"notes/af02d251-a14c-484d-931e-a9175bef81a2.html"},{"doc":"Modules","title":"Modules","hpath":"node.modules","content":"\n### node_modules\n- in `node_modules`, we have a folder `.bin/`. This is where the binaries (executables) of our dependencies (listed in `node_modules`) are stored.\n\t- normally these binaries are symlinks to the binaries that are stored within each package's directory in `node_modules`\n\t- when are within a project and run a command that is not globally installed, it looks in the `.bin/` directory for that executable\n\t\t- ex. running `jest test` in the project will look for an executable called `jest` within `node_modules/.bin` and execute it.\n- the [node module resolution algorithm](https://nodejs.org/api/modules.html#modules_loading_from_node_modules_folders) is recursive, meaning when looking for package A, it looks in local `node_modules/A`, then `../node_modules/A`, then `../../node_modules/A`, and so on. \n- the version listed in package.json is not the version our package uses. Rather, it defines a range that is allowed to be installed. To see the actual version, check yarn.lock","url":"https://tycholiz.github.io/Digital-Garden/notes/e55a6013-2d03-4ac9-9917-2d468bb27d78.html","relUrl":"notes/e55a6013-2d03-4ac9-9917-2d468bb27d78.html"},{"doc":"Winston","title":"Winston","hpath":"node.modules.winston","content":"\nWinston is a logger library designed with support for multiple transports (containers for the logs)\n- you can store errors in one transport, and normal logs in another. This is nice if you want to integrate other services. For instance what if we want our database to consume and store these errors, but don't care about storing other log levels?","url":"https://tycholiz.github.io/Digital-Garden/notes/ca7e83ec-1f06-4027-8097-00135a10e2db.html","relUrl":"notes/ca7e83ec-1f06-4027-8097-00135a10e2db.html"},{"doc":"Stream","title":"Stream","hpath":"node.modules.stream","content":"\nThere are 4 types of streams in Node.js:\n1. **Writable**: streams to which we can write data. For example, fs.createWriteStream() lets us write data to a file using streams.\n2. **Readable**: streams from which data can be read. For example: fs.createReadStream() lets us read the contents of a file.\n3. **Duplex**: streams that are both Readable and Writable. For example, net.Socket\n4. **Transform**: streams that can modify or transform the data as it is written and read. For example, in the instance of file-compression, you can write compressed data and read decompressed data to and from a file.\n\nIn a Node.js based HTTP server, `request` is a readable stream and `response` is a writable stream\nThe fs module lets you work with both readable and writable file streams\n\nWhenever you’re using Express you are using streams to interact with the client, also, streams are being used in every database connection driver that you can work with, because of TCP sockets, TLS stack and other connections are all based on Node.js streams.\n\n### How to create a readable stream\nWe first require the Readable stream, and we initialize it.\n```js\nconst Stream = require('stream')\nconst readableStream = new Stream.Readable()\n```\n\nNow that the stream is initialized, we can send data to it:\n```js\nreadableStream.push('ping!')\nreadableStream.push('pong!')\n```\n\n#### Async iterator (`for await`)","url":"https://tycholiz.github.io/Digital-Garden/notes/b3323d7c-ad18-45dc-ba24-2797c6169dda.html","relUrl":"notes/b3323d7c-ad18-45dc-ba24-2797c6169dda.html"},{"doc":"Path","title":"Path","hpath":"node.modules.path","content":"\n# Path\n## Path.join\n- concatenate each argument to get a full URL\n\t- therefore, it can be relative or absolute (using `__dirname`) depending on what args we pass \n- depending on if the OS is Unix-based or Windows, different delimiters will be used, abstracting this away from us. \n\n## Path.resolve\n- attempts to resolve a sequence of paths from RTL, with each subsequence path prepended until an absolute directory is formed. \n\t- if an absolute directory is not able to be formed, then the args will be put on the end of the current working directory. \n- this method will treat the first argument as the root directory \n- when called without arguments, it will return the working directory (which may happen to be equivalent to `__dirname`)\n- it will always result in an absolute URL ","url":"https://tycholiz.github.io/Digital-Garden/notes/4e682b80-44c8-4a66-9b64-03dcb0fefa43.html","relUrl":"notes/4e682b80-44c8-4a66-9b64-03dcb0fefa43.html"},{"doc":"Morgan","title":"Morgan","hpath":"node.modules.morgan","content":"\nMorgan is a HTTP request logger middleware for Node.js.\n\n### Token\nWhen morgan logs to the console, the structure of the content that gets logged in determined by the tokens used\n\nA standard way to use morgan is to use the preset tiny:\n`app.use(morgan('tiny'))`, which is equivalent to:\n`morgan(':method :url :status :res[content-length] - :response-time ms');`\n- The part following `:` is the token.\n\n#### Custom Tokens\nWe can create our own tokens with `morgan.token(nameOfToken, callback)`, where the callback returns the value that will stand in for the token name.\n\nTokens can be configured to accept custom arguments:\n```js\napp.use(morgan(':method :host :status :param[id] :res[content-length] - :response-time ms'));\n\nmorgan.token('param', function(req, res, param) {\n    return req.params[param];\n});\n```\n\nMorgan can be combined with Winston to great effect","url":"https://tycholiz.github.io/Digital-Garden/notes/b4f233c4-7129-4bbd-a90b-78da7a16eac5.html","relUrl":"notes/b4f233c4-7129-4bbd-a90b-78da7a16eac5.html"},{"doc":"Ngrok","title":"Ngrok","hpath":"ngrok","content":"\n# Ngrok\nNgrok will create a secure tunnel on the local machine from a given port (ex. `8000`) to a url hosted on the internet at their domain (ex. `https://e2210e647fe4.ngrok.io`)\n- when a request somewhere on the internet hits an endpoint of that remote url, ngrok will forward that request on through the tunnel to the local machine.\n\t- ex. Stripe sends a webhook post request to `https://e2210e647fe4.ngrok.io:8000/webhooks/stripe`. Ngrok sees this, and passes it along to `localhost:8000/webhooks/stripe`, where the `/webhooks/stripe` endpoint defined in the application server can then handle the request","url":"https://tycholiz.github.io/Digital-Garden/notes/9c56d34a-84ee-481e-ac5a-13fa56750a3c.html","relUrl":"notes/9c56d34a-84ee-481e-ac5a-13fa56750a3c.html"},{"doc":"Nginx","title":"Nginx","hpath":"nginx","content":"\n## Overview\n- Nginx has 1 master process, and multiple worker processes.\n\t- The master's job is to read configuration files, and to manage the worker processes.\n\t- The worker processes handle the requests.\n- Nginx uses an event-based model to distribute requests among workers\n- The # of workers is specified in the config file, and may either be fixed, or adjustable based on how many cores the CPU has. \n\n### Config file\n- The nginx config file `nginx.conf` is stored either in `/usr/local/nginx/conf`, `/etc/nginx`, or `/usr/local/etc/nginx`.\n- nginx consists of modules which are controlled by directives specified in the configuration file\n\t- Directives can either be simple directives or block directives\n\t\t- simple ends with `;`, block uses `{}`\n- If a directive can have other directives inside, it is called a Context \n\t- ex. `events`, `http`, `server`, `location`\n- If a directive is not placed within a Context, then it is considered to be in the Main Context. \n\t- The `events` and `http` directives reside in the Main Context, `server` in `http`, and `location` in `server`.\n\n## Blocks\n- Nginx divides the configurations meant to serve different content into Blocks, which live in a hierarchical structure.\n- Each time a client request is made to the server, Nginx begins a process of determining which hierarchical block should be used to handle the request.\n\n### Server Block\n- Defines a virtual server used to handle requests of a defined type\n\t- each Server Block functions as a separate virtual web server instance\n- Based on the domain name, port and IP address requested, we can configure multiple server blocks to handle each combination.\n- The `server_name` and `listen` directives are used to determine which server block should be used to fulfill a request. They are used to bind to tcp sockets.\n\t- With `listen`, we can use a lone IP, a lone port, or a combo of the two. If we only specify one, then defaults are used\n\t\t- default port: 80\n\t\t- default IP: 0.0.0.0\n\t- the `server_name` directive is only evaluated when nginx needs to distinguish between server blocks that match to the same level of specificity in the `listen` directive. Put another way, it is a \"specificity tie-breaker\" \n\t\t- in other words, if `example.com` is hosted on `192.168.1.10:80`, a request will always be served by a server block that specifies `listen 192.168.1.10`, even if there is another server block that specifies `server_name example.com`\n\t- Finally, if further specificity is needed, then the Host header from the request (which contains the URI that the client was trying to reach) is used. \n\t\t- When using wildcard matching, the longest match beginning with a wildcard is used\n\t\t\t- ex. if the request has a Host header of `www.example.com`, and we have 3 server blocks with `server_name` of `*.example.com`, `www.example.*` and `*.com`, `*.example.com` would win out.\n- With server blocks, we can run more than one website on a single host\n- in Apache, called *VirtualHost* \n\n### Location Block\n- Lives within a Server Block (or nested in other location blocks).\n- Determine how Nginx should handle the part of the request that comes after IP:port (ie. the URI).\n- Similar to how Nginx has a specificity-based process for determining which server block will process the request, Nginx has an algorithm to determine which location block within the server should be used for handling requests.\n- Location blocks take the following form:\n```\nlocation <optional_modifier> <location_match> {\n}\n```\n- The `location_match` defines what Nginx should check the request URI against.\n- The `optional_modifier` affects the way Nginx will attempt to match the location block.\n\t- ex. check for prefix match (default), check for exact match (`=`), check for case-sensitive Regex (`~`)\n- The URI specified after `location` will be added to the path specified in the *root directive*\n\t- ex. if we specify `root /var/www/` and the location block specifies `/images/`, then the path to the requested file on the local FS will be `/var/www/images`\n- ex. Imagine we had a server block:\n```\nserver {\n    location / {\n        root /data/www;\n    }\n\n    location /images/ {\n        root /data;\n    }\n}\n```\nin response to a request with URI starting with `/images/`, the server will send files from the `/data/images` directory. \n\n## Tasks of Nginx\n### Serving Static Content\n- Nginx can be configured to serve static content, such as HTML and images.\n- this involves setting up of a server block inside the http block with two location blocks.\n\t- multiple server blocks are able to be added, each specifying a different port.\n```\nhttp {\n\tserver {\n\t\t\n\t}\n}\n```\n\n### Reverse Proxy Server\n- When Nginx proxies a request, it sends the request to a specified proxied server, fetches the response, and sends it back to the client\n\t- it is possible to proxy requests to another HTTP server (eg. another Nginx server) or to a non-HTTP server (eg. express.js)\n\t\t- We use a specified protocol like FastCGI to do this\n- We can establish a proxy server by using the `proxy_pass` directive within a *location block*.\n\t- The value of `proxy_pass` is the address of the proxy server:\n```\nlocation /some/path/ {\n    proxy_pass http://www.example.com/link/;\n}\n```\n- In this config, all requests processed to `/some/path/` to be sent to the proxy server at `http://www.example.com/link/`.\n\t- ex. the request with the URI of `/some/path/page.html` will be proxied to `http://www.example.com/link/page.html`\n- to pass a request to a non-HTTP server, the appropriate `*_pass` directive should be used\n\t- ex. `fastcgi_pass`\n\n## Debugging\n- upon changing nginx.conf, we need to reload the nginx server with `nginx -s reload`.\n- logs are stored at either `/usr/local/nginx/logs` or `/var/log/nginx`\n\n### Pitfalls\n- A *root directive* should occur outside of the location block. We can then use another *root directive* within a *location block* if we want to override it.\n\t- Conversely, if you were to add a root to every location block then a location block that isn’t matched will have no root. Therefore, it is important that a root directive occur prior to your location blocks, which can then override this directive if they need to.\n- [source](https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/)","url":"https://tycholiz.github.io/Digital-Garden/notes/c299d2dd-7cbd-4b81-976a-37b808ee41a8.html","relUrl":"notes/c299d2dd-7cbd-4b81-976a-37b808ee41a8.html"},{"doc":"Conf","title":"Conf","hpath":"nginx.conf","content":"\nnginx.conf building blocks \n  - worker process    : should be equal to number cores of the server (or auto)\n  - worker connection : 1024 (per thread. nginx doesn't block) \n\n  - rate limiting     : prevent brute force attacks.\n  - proxy buffers     : (when used as proxy server)limits how much data to store as cache\n                         gzip /brotil or compression\n  - upload file size  : it should match php max upload size and nginx client max body size.\n  - timeouts          : php to nginx communication time.\n  - log rotation      : error log useful to know the errors and monitor resources\n  - fastcgi cache     : very important to boost the performance for static sties.\n  - SSL Configuration : there are default setting available with nginx itself \n                        (also see ssl performance tuning).\n\nExample nginx.conf: \n```\nuser www-data;                                   \nload_module modules/my_favourite_module.so;      \npid /run/nginx.pid;\n                                                    | Alternative global config for \n                                                    | [4 cores, 8 threads, 32GB RAM] \n                                                    | handling  50000request/sec\n                                                    |\nworker_processes auto;                           | worker_processes 8;\n                                                    | worker_priority -15;\ninclude /etc/nginx/modules-enabled/*.conf;       | \nworker_rlimit_nofile 100000;                     | worker_rlimit_nofile 400000;                                  \n                                                    | timer_resolution 10000ms;\n                                                    |\nevents {                                         | events {\n    worker_connections 1024;                       |     worker_connections 20000;                       \n    multi_accept on;                               |     use epoll;\n}                                                |     multi_accept on;\n                                                    | }\n\nhttp {               ←  global config            \n    index index.php index.html index.htm;          \n    # Basic Settings                               \n\n    sendfile on;                                   \n    tcp_nopush on;\n    tcp_nodelay on;\n    sendfile_max_chunk 512;\n    keepalive_timeout 300;\n    keepalive_requests 100000;\n    types_hash_max_size 2048;\n    server_tokens off;\n\n    server_names_hash_bucket_size 128;\n    # server_name_in_redirect off;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    ##\n    # SSL Settings\n    ##\n\n    #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE\n    #ssl_prefer_server_ciphers on;\n    #rate limit zone\n\n    limit_req_zone $binary_remote_addr zone=one:10m rate=3r/m;\n    #buffers\n\n    client_body_buffer_size 128k;\n    client_max_body_size 10m;\n    client_header_buffer_size 32k;\n    large_client_header_buffers 16 256k;\n    output_buffers 1 32k;\n    postpone_output 1460;\n    #Porxy buffers\n    proxy_buffer_size 256k;\n    proxy_buffers 8 128k;\n    proxy_busy_buffers_size 256k;\n    proxy_max_temp_file_size 2048m;\n    proxy_temp_file_write_size 2048m;\n\n    ## fast cgi PHP\n    fastcgi_buffers 8 16k;\n    fastcgi_buffer_size 32k;\n    fastcgi_connect_timeout 300;\n    fastcgi_send_timeout 300;\n    fastcgi_read_timeout 300;\n    #static caching css/js/img\n\n    open_file_cache max=10000 inactive=5m;\n    open_file_cache_valid 2m;\n    open_file_cache_min_uses 1;\n    open_file_cache_errors on;\n    #timeouts\n\n    client_header_timeout 3m;\n    client_body_timeout 3m;\n    send_timeout 3m;\n\n    # Logging Settings\n\n    log_format main_ext ‘$remote_addr – $remote_user [$time_local] “$request” ‘\n    ‘$status $body_bytes_sent “$http_referer” ‘\n    ‘”$http_user_agent” “$http_x_forwarded_for” ‘\n    ‘”$host” sn=”$server_name” ‘\n    ‘rt=$request_time ‘\n    ‘ua=”$upstream_addr” us=”$upstream_status” ‘\n    ‘ut=”$upstream_response_time” ul=”$upstream_response_length” ‘\n    ‘cs=$upstream_cache_status’ ;\n\n    #access_log /var/log/nginx/access.log main_ext;\n    error_log /var/log/nginx/error.log warn;   Read more on nginx error log&common errors\n\n    ##\n    # Gzip Settings #brotil\n    ##\n\n    gzip on;\n    gzip_disable “msie6”;\n\n    gzip_vary on;\n    gzip_proxied any;\n    gzip_comp_level 6;\n    gzip_buffers 16 8k;\n    gzip_http_version 1.1;\n    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript application/x-font-ttf font/opentype image/svg+xml image/x-icon;\n    ##\n    # Virtual Host Configs\n    ##\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;   \n}\n\nserver {             ← Domain level \n    listen 0.0.0.0:443 rcvbuf=64000 sndbuf=120000 backlog=20000 ssl http2;\n    server_name example.com www.example.com;\n    keepalive_timeout         60;\n    ssl                       on;\n    ssl_protocols             TLSv1.2 TLSv1.1 TLSv1;\n    ssl_ciphers               'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS:!RC4';\n    ssl_prefer_server_ciphers on;\n    ssl_session_cache         shared:TLSSL:30m;\n    ssl_session_timeout       10m;\n    ssl_buffer_size           32k;\n    ssl_certificate           /etc/letsencrypt/live/example.com/fullchain.pem;\n    ssl_certificate_key       /etc/letsencrypt/live/example.com/privkey.pem;\n    ssl_dhparam           /etc/ssl/certs/dhparam.pem;\n    more_set_headers          \"X-Secure-Connection: true\";\n    add_header                Strict-Transport-Security max-age=315360000;\n    root       /var/www;\n\n    location {         ← Directory level \n        root /var/www;\n        index index.php index.html;\n    }\n\n    location ~ .php$ {  \n    fastcgi_keep_conn on;\n    fastcgi_pass   unix:/run/php5.6-fpm.sock;\n    fastcgi_index  index.php;\n    fastcgi_param  SCRIPT_FILENAME /var/www$fastcgi_script_name;\n    include fastcgi_params;\n    fastcgi_intercept_errors off;\n    fastcgi_buffer_size 32k;\n    fastcgi_buffers 32 32k;\n    fastcgi_connect_timeout 5;\n    }\n\n    location ~* ^.+.(jpg|jpeg|gif|png|svg|ico|css|less|xml|html?|swf|js|ttf)$ { \n        root /var/www;\n        expires 10y;\n    }\n\n}\n```\n\n- /etc/nginx/conf.d/*: user defined config files\n\nSee also:\nhttps://github.com/trimstray/nginx-admins-handbook\nhttps://github.com/tldr-devops/nginx-common-configuration","url":"https://tycholiz.github.io/Digital-Garden/notes/485747d0-4ad3-4453-b400-515c8833535f.html","relUrl":"notes/485747d0-4ad3-4453-b400-515c8833535f.html"},{"doc":"Nextjs","title":"Nextjs","hpath":"nextjs","content":"\ncreate pre-rendered react websites, offered from SSR\n\njsx is rendered already into html on the server, and is sent to the client to be displayed\n- vanilla-react will do everything at runtime.\n\nhelps with SEO\n\nnext takes care of routing for us\n- we define new pages in the `pages/` directory, and next picks up on them automatically, giving us the url out of the box.\n\t- next defines a component `<Link />` we can use to handle routes\n\nnext is smart and will not re-render the same content, if it has rendered it already\n\nIf we had a function in a Next.js component that referred to the `window` object, we would get errors. This is because that code is being run on the server, which of course has no concept of the browser's global variables. However, if we were to use `window` in the `useEffect` call, we would indeed get access to the `window` object. This shows that `useEffect` is still done client side, even though functions are understood server-side","url":"https://tycholiz.github.io/Digital-Garden/notes/f4355062-fdda-4c6f-a969-3f8a10c15475.html","relUrl":"notes/f4355062-fdda-4c6f-a969-3f8a10c15475.html"},{"doc":"Network","title":"Network","hpath":"network","content":"\n# Physical Networks vs. Overlay Networks\n## Physical Network\n- physically, there are connections between nodes in a network. The network essentially is the sum of its physical components that connect computers to one another.\n\t- physical components include: cables, wires, routers, repeaters\n\t- multiple physical networks can make up one big virtual network.\n\t\t- ex. a university spread across many buildings will have many different phsyical networks, though all computers will be able to see and communicate with one another because of the overlayed virtual network that exists on top of the physical one.\n\n## Overlay Network\n- the overlay network is created using logic at the software level to determine where the connections between the nodes are.\n- a network may have multiple virtual layers built on top of the backbone physical one. \n- while in a physical network, each node is connected to another through physical means, such as a cable. In an overlay network, nodes are connected to one another using network addresses. \n- overlay networks *encapsulate* the data before sending it over the network, and unwrapped upon reaching the destination.\n- an overlay network needs to employ a protocol that determines the rules that the hosts of a network must all abide by (IP, VPN, P2P)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f7c590dc-921d-4b6f-8f8e-e075b0c16035.html","relUrl":"notes/f7c590dc-921d-4b6f-8f8e-e075b0c16035.html"},{"doc":"Vpn","title":"Vpn","hpath":"network.vpn","content":"\n# VPN\n- a VPN connects your PC, smartphone, or tablet to another computer (called a server) somewhere on the internet, and allows you to browse the internet using that computer’s internet connection\n    - so is it basically a network with a dedicated server that executes the web searches, and delivers that content to the client\n    - the VPN is the thing on the server than enables the network to exist\n- similar to a proxy server, but where a proxy server can only redirect web requests, a VPN connection is capable of routing and anonymising all of your network traffic.\n- ex. If a company were to have 2 branches, they would not be able to communicate with one another with only their private IP addresses. They could choose to connect through the internet, but that would not be preferable, since traffic should be limited to those in the company. Instead, the company can use a VPN to bridge the two private networks. \n\t- alternatively, an IP tunnel can be used.\n\t- with either method, the result is that we are encapsulating the packets within a protocol layer during trasmission across the public network.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/85142be9-d0c4-4386-bdf1-be1f2aa606f9.html","relUrl":"notes/85142be9-d0c4-4386-bdf1-be1f2aa606f9.html"},{"doc":"Tools","title":"Tools","hpath":"network.tools","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/eba8ed1e-bd06-47e6-9d80-6998d0e8dfe1.html","relUrl":"notes/eba8ed1e-bd06-47e6-9d80-6998d0e8dfe1.html"},{"doc":"Traceroute","title":"Traceroute","hpath":"network.tools.traceroute","content":"\n# Traceroute\n- see a list of all nodes (routers and end node) that your packets traveled through on their way to an origin server. Each step is a *hop*\n\t- Also tells us how long each jump took (the response time b/w nodes) (RTT; round trip time)\n- RTT tells us how long it took to get to that node (from the previous one) and return to your computer\n\t- There are three RTT columns because the traceroute sends three separate signal packets so that we may be able to spot inconsistencies in the route.\n- The final column has the router IP\n- `*` in the traceroute means that packets were lost\n- consistency of RTTs between columns is what we are looking for when analyzing a traceroute.  \n- when we use traceroute, the packet gets sent to the first router, which sends a packet back to the source. Then the packets continue on to the next router, which again sends a packet back to the source. This pattern continues until we reach the destination node.\n\t- ex. if there are 5 routers between a source node and destination node, then the source will send 6 packets into the network, which each packet addressed to the destination node.\n\t\t- The difference of course with traceroute over a regular packet, is that when an individual router along the chain receives the packet, it does not forward it along. Instead, it sends it back to the source. In this way, the source can determine the route that the packets took to reach the destination.\n\nIssues:\n- if the RTT from one hop to another greatly increases, and continues to increase until the destination, this indicates a problem with the node that first took a long time to respond. \n\t- This often accompanies packet loss (`*`)\n- if the RTT spikes on only one node, then subsequent hops have lower RTT, that doesn't indicate an issue. It just means the slow router gave your packets a lower priority.\n- if the RTT jumps then remains consisistent at that levels, this does not indicate an issue\n- By default, traceroute uses high UDP ports for tracing hosts. Sometimes firewalls block these UDP ports. \n\t- use `-P` flag to use different protocols: (`-P ICMP`, `-P TCP`, `-P UDP`)","url":"https://tycholiz.github.io/Digital-Garden/notes/ef7a5f17-abe4-47a6-add7-d750ffc0f383.html","relUrl":"notes/ef7a5f17-abe4-47a6-add7-d750ffc0f383.html"},{"doc":"Servers","title":"Servers","hpath":"network.servers","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c99fb13f-92e1-4edb-8300-025605b98359.html","relUrl":"notes/c99fb13f-92e1-4edb-8300-025605b98359.html"},{"doc":"Proxy","title":"Proxy","hpath":"network.servers.proxy","content":"\n# Proxy server\n- A proxy server is a gateway from one network to another \n\t- Ex. the Cloudflare proxy server gives the local network access to the network that holds that cached information.\n- a proxy acts on behalf of the client(s), while a reverse proxy acts on behalf of the server(s)\n\t- When you see \"proxy\" think: \"something that stands-in for something else.\"\n\n## Caching\nWhen you think about it, HTTP requests have the same benefits that pure functions do: for a given request, there is a given response. This means that if we remember all of the details of the request, we may be able to store that corresponding response, and serve that data to all subsequent identical requests. \n- The proxy server figures out whether or not it should cache data based on the headings that are found in the request. \n- a cache is used by a proxy server. Think of it as storage that the proxy server returns to the client making the request. ","url":"https://tycholiz.github.io/Digital-Garden/notes/0a32739d-b43a-4dd9-8f92-a20e76f1deff.html","relUrl":"notes/0a32739d-b43a-4dd9-8f92-a20e76f1deff.html"},{"doc":"Reverse","title":"Reverse","hpath":"network.servers.proxy.reverse","content":"\n# Reverse Proxy\n- Reverse proxies are servers that sit between the request-response process that retrieve resources from a server. These resources are then returned to the client, appearing as if they originated from the proxy server itself.\n\t- ex. This might include services such as caching, security (inc. SSL), load balancing, and content optimization\n- Proxying is typically for load balancing, to seamlessly show content from different websites, or to pass requests for processing to application servers over protocols other than HTTP.\n- It's called a reverse proxy because the client makes a request, and before that request can hit the server, it's intercepted by a server (the proxy server), which gives the request what it wants, and sends (reverses) it back to the client. \n- A common scenario is that we run our REST API behind a reverse proxy. Among other reasons, we might want to do this so our API server is on a different network/IP than our front-end application. Therefore, we can secure this network and only allow traffic from the reverse proxy server.\n- A reverse proxy like one from Nginx can be used to implement SSL\n\t- [guide](https://nginx.org/en/docs/http/configuring_https_servers.html)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/2fda5052-673c-4ad2-8103-2c3cc83696b3.html","relUrl":"notes/2fda5052-673c-4ad2-8103-2c3cc83696b3.html"},{"doc":"Forward","title":"Forward","hpath":"network.servers.proxy.forward","content":"\n# Forward Proxy (or simply Proxy)\n- From the server's perspective, requests came from the proxy server (in this case, the proxy server is just passing along the request made by the client)\n- It's called a forward proxy because when a request is detected, the proxy server intercepts it, and forwards it on to the next destination\n\n## Web Proxy Server\n- It's common to run web apps behind a proxy such as NGINX\n\t- Nginx listens on port 80, then forwards traffic to the app on another port, like 4000\n- An early limiting factor of Nginx that will be encountered will be related to the number of open sockets that the Nginx server can handle. The result is delays on the client side. \n\t- the next limiting factor would be the number of ports available (there are only 64k). These 64,000 ports can handle 500 requests/second \n","url":"https://tycholiz.github.io/Digital-Garden/notes/b32f561b-199e-4ffc-852d-c3bd89bb01e7.html","relUrl":"notes/b32f561b-199e-4ffc-852d-c3bd89bb01e7.html"},{"doc":"Edge","title":"Edge","hpath":"network.servers.edge","content":"\n# Edge Server\n- def - a server that acts as a gateway so that one network can access another\n\t- In other words, the edge server enables the two networks to communicate.\n- Therefore, the ability for clients to make a network connection to other clients is bottlenecked by the number of edge servers between the two clients;\n- Generally speaking, the farther the connection must travel, the greater the number of networks that must be traversed.\n![](/assets/images/2021-03-11-15-51-29.png)\n- \"edge\" refers to the philosophy of geographically placing the data close to the server (or proxy server) that requests it \n- Edge servers are contrasted with Origin Servers, which is your actual web server (ex. Express).\n- spec: caches want to live as close as possible to the client, while edge servers tend to live further out, closer to the internet (but still closer to the client than the origin server)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5639e60c-a19e-436d-b60c-c972d2edfb11.html","relUrl":"notes/5639e60c-a19e-436d-b60c-c972d2edfb11.html"},{"doc":"Clusters","title":"Clusters","hpath":"network.servers.clusters","content":"\n# Cluster\n- a group of computers that function so closely together that you may consider them to be a single computer\n- a cluster will designate each task to a different node so that the responsibility is split\n- Each node would run its own instance of the OS\n\n### Stretched cluster\nA stretched cluster is a deployment model in which two or more host servers are part of the same logical cluster but are located in separate geographical locations.","url":"https://tycholiz.github.io/Digital-Garden/notes/ce571eaa-539b-4518-8082-6a410c6d92d2.html","relUrl":"notes/ce571eaa-539b-4518-8082-6a410c6d92d2.html"},{"doc":"Osi Model","title":"Osi Model","hpath":"network.osi-model","content":"\n- The OSI model is a framework for understanding how communications work in a computing system. It is an abstract representation, since no attention is paid to the implementation details of each layer. Instead, each layer simply describes its function and purpose. Put another way, it defines what input it expects, and what output it gives. \n\t- Since each layer interfaces directly with the layers above and below it, the consistence of the input and output offered by each layer is the only thing that matters (ex. as long as L1 receives 0's and 1's and delivers frames, all other details are inconsequential)\n\n## The Protocol Layers\n### L7 Application\t\n- consists of network applications and their application-layer protocols\n- primary user interface with communication system.\n- PDU - messages\n- HTTP, FTP, DNS, SMTP, POP3, SSH, IRC, TLS/SSL, NFS (network FS)\n### L6 Presentation \n- Supports the functionality of the application layer by providing services such as formatting and translation of data.\n- provide data encryption and data compression.\n- Data representation (compression, decompression) and encryption\n- ex. SSL, SSH, IMAP, FTP, TLS, MPEG, JPEG\n### L5 Session\n- Maintains the transmission path by synchronizing packets and controlling access to the medium by the Application layer.\n- controls the connections between computers\n- provides for delimiting and synchronizing of data exchange, including the means to build \n- ex. API, sockets, HTTP sessions\n### L4 Transport\n- Ensures the quality of transmission and determines the best route for transmission of data using the Network layer below.\n- concerned with providing reliable communication over an unsecured network\n- PDU - segments (TCP), datagrams (UDP)\n- goal: deliver the data to the right software application\n\t- ex. TCP, UDP\n### L3 Network \n- Finds a route for transmission of data (packets) between 2 routers, and establishes and maintains the connection between two connected nodes.\n- goal: pass data chunks over multiple connected networks\n- PDU - packet\n- ex. IPv4, IPv6, ICMP, ARP, NAT\n### L2 Data Link \n- Creates, transmits, and receives packets. Controls the Physical layer.\n- Concerned with sharing multiple access channels\n- They are used to deliver frames on a LAN\n- PDU - frames\n- goal: organize the 1s and 0s into chunks of data, and get them to the right place on the wire.\n- ex. wifi, ethernet, bluetooth, VLAN, port forwarding procol\n### L1 Physical \n- Converts data into bits for transmission and converts received bits into usable data for the layers above it.\n- PDU - bits\n- goal is to send 0s and 1s across a wire\n- ex. fiber optic, copper wire, coaxial cable, wireless, modem, repeaters, ethernet (physical portion), USB\n\n### The Internet Protocol\n- The IP stack consists of L1, L2, L3, L4, L7\n- When an HTTP request is sent, the protocol is established by piggybacking on the TCP connection that had already been made. This TCP connection is enabled by following the internet protocol. This is the point at which the internet protocol determines which routes datagrams\n- The fact that there are 2 layers that are openly missing from the internet stack poses an interesting question: why are they not there? The reason is that the internet leaves these layers up to the application developer. The application developer can use any implementation of L5 and L6 that they choose in order to achieve their goals.\n\n#### Routers\n- As data is sent out, it starts at L7 and makes its way down to L1. At L1, it is connected to the link-layer switch and goes up to L2, before going back down to L1. Then it reaches the router, which goes up to L3, then back down to L1, to be repeated depending on the number of subsequent routers. At the final router, the the router's L1 communicates with the L1 of the destination host, as it makes its way back up to L7.\n\n### Airplane trip analogy\n- As we look at the process of planning and taking an airplane, it becomes apparent that there are different layers to the entire process. In fact, each layer appears two times in the whole process— in reverse order:\n1. buy ticket\n2. check baggage\n3. load at gate\n4. takeoff on runway\n5. airplane routing (travel)\n6. land of runway\n7. unload at gate\n8. pickup baggage\n9. complain about ticket.\n\n- each layer implements some functionality, and we can see that the opposite action was performed in reverse order.\n- We can also see that each layer provides service to the layer below it.\n\t- ex. the act of checking baggage only makes sense to a ticketed person. \n\t- ex. the idea of unloading at a gate only makes sense to a person on a landed plane.\n- we notice that we can replace any layer in the model, as long as the functionality remains the same. In this way, layers are interfaces to each other, and don't care about each other's implementation— only the outcome (ie. output) it provides.\n\n### Encapsulation\n- as data travels each layer from L7 down to L1, additional information is added. As we pass the information encapsulated in the HTTP request down to L4, the transport layer takes the information of L7 and adds its own information to it. This information that was added is then used by the transport layer of the next node in the chain (likely the destination host). This process of encapsulation continues on down layer by layer until L1.\n\t- This idea of encapsulation thus demonstrates what is fundamentally different between each PDU: a message (L7) is an encapsulated datagram (L4). In other words, the datagram encapsulates the message. A datagram is fundamentally a message, plus some other information (provided by the layer). Therefore, at each layer, the PDU has 2 types of fields: header fields, and payload fields (the payload is just a packet from the layer above).\n\n### Protocol Data Unit (PDU)\n- each layer has the concept of a Protocol Data Unit, which is the format that the data exists in within the current Layer. In other words, it is what an atomic unit of data is called at each layer.\n\t- All PDUs are composed of a header and payload\n\t\n### Miscelaneous\n- the very fact that there are layers means that we can treat it as a modular chain, and swap out one L3 implementation for another (such as wifi for ethernet)\n\t- therefore L2 doesn't care if we are using IP or IPX on L3, just like L3 doesn't care if L2 uses wifi or ethernet\n- The price we pay by layering is that we have to map between 32 bit IP addresses (L3) and 48 bit MAC addresses (L2).\n\t- The ARP protocol exists to solve this very problem\n- The OS may be a participant in any or all of the layers\n\t- ex. at L1, signal processing can be offloaded to a host CPU and that requires a driver which interfaces with the operating system","url":"https://tycholiz.github.io/Digital-Garden/notes/61c96e0c-ee3c-4d9f-bbf0-dc3ff9066b09.html","relUrl":"notes/61c96e0c-ee3c-4d9f-bbf0-dc3ff9066b09.html"},{"doc":"Lan","title":"Lan","hpath":"network.lan","content":"\n### Subnet\n- a subnet is a logical division of an IP network\n\t- an IP network is any network that uses IP address to communicate to other devices.\n\t\t- ex. LAN, the internet, enterprise network.\n\t- you might do this for logical reasons (ex. firewalling) for physical reasons (ex. smaller broadcast domains) \n- to be useful, a router is connected to two or more IP subnets\n- IP subnets exist to allow routers to choose appropriate destinations for packets.\n- Traffic is exchanged between subnetworks through routers\n- subnetting is the process of breaking down a single IP address block into smaller subnetworks called subnets\n\t- the reason we need to subnet is to efficiently distribute IP addresses to reduce wastage\n- machines on the same subnet will have the same first 3 digits of their IP address (known as the *most significant bit-group*)\n\t- this means that the IP address is made up of 2 parts: the subnet number and the host identifier. \n- subnetting is analogous to the concept of *zoning* in city planning \n\n#### Subnet Mask\n- a subnet mask allows a computer/router to determine the portion of the IP address which refers to the network, and the portion that refers to the host machine\n\t- the network portion of an address is represented by 1s (255) in the subnet mask, and the host portion is represented by 0s\n\t- a subnet mask can also tell us the number of hosts within a network\n\t- anal. just like our home address consists of a street name (network ID) and a number (network host), the mask's job is to determine where in the IP address one begins and the other ends. \n- each octet of a subnet mask can either be 255 or 0. When the octet on the mask is 255, that means that the when trying to connect to another node, it is going to go through the router to try and resolve that IP address\n\t- ex. if the host has IP=`168.25.4.6` and mask=`255.255.255.0`, that means that it will only try and hit nodes within the LAN if the IP address starts with `168.25.4`. If the host tried to connect to `168.25.8.2`, it would default to going through the gateway (router)\n\t\t- If the mask=`255.255.0.0`, that means the host will try to hit the node locally only if the IP address starts with `168.25`\n\t\t- if at any point the 255 is \"triggered\" (ie. the external node doesn't satisfy the mask's requirement for enabling local searching), then the external node is said to be \"outside the mask\"\n- When we apply the subnet mask to the IP address, we get the *routing prefix*\n- ex. we have a network with IP=`135.68.2.0`. In that network are 2 host computers with IP=`135.68.2.1` and IP=`135.68.2.2`. The network portion of each host's IP is `135.68.2`\n\t-  a 0 at the end of an IP address indicates that it is a network address\n- anal. house addresses are composed of a streetname and a number \n- with a mask of 255.255.255.0, since only one of the octets of bits refers to the hosts in a network, there can only be 256 nodes within that network\n\t- if the mask were 255.255.0.0, there could be 65,536\n\t- in reality we have to subtract 2 from that total, since there are 2 reserved IP addresses: the Network ID and the Broadcast IP address.\n- a subnet mask is needed to tell us how many computers a node within a subnet has to go through before it gets to another node on that same network (ex. the computers at a LAN party).\n\t- When trying to send a message across the network, the subnet mask will tell us if we can access that node via the current network, or if we can only connect to it through the router (ie. it is on the internet) \n- Another way of looking at the subnet mask is that it tells us which octets of an IP address are devoted to telling us which network the nodes are located in.\n\t- when a mask is 255.255.255.0, then anything is on the same network as the host if the first 3 octets of the IP address are identical. \n\n*Default Subnet Masks*\n- There are 3 classes:\nClass A - 255.0.0.0\nClass B - 255.255.0.0\nClass C - 255.255.255.0\n\n### Modem\n- portmanteau of \"modulator-demodulator\"\n- purpose is to convert data from a digital format to a format that is conducive to transmission over a physical layer\n- Modems can be used with almost any means of transmitting analog signals, from light-emitting diodes to radio\n- Any communication technology sending digital data wirelessly involves a modem. \n\t- ex. satellite, WiFi, WiMax, mobile phones, GPS, Bluetooth and NFC.\n\n### Demilitarized Zone (DMZ)\n- a subnetwork that sits between the network and the router. the DMZ exists so that we can control exposure to certain parts of a network. Anything in it is exposed to untrusted networks (like the internet). The idea is that the DMZ is all that can be accessed externally, so the purpose is added security\n\n### Dynamic Host Configuration Protocol (DHCP)\n- a server hosted within a network that dynamically assigns IP addresses\n\t- it is therefore the service that manages the IP address pool in a network\n- **DHCP Reservation** - set aside an IP address and map it to a specific MAC address. Whenever a device with the specified MAC address enters the network, it is assigned with the specified IP address.\n\n#### Static IP\n- on the device receiving IP assignment (ie. not the router), we can bypass the dynamic assigning of IP addresses by creating a entry in `/etc/dhcpcd.conf` (Linux)\n- Generally accepted best practice is to assign a static address on the device that needs it - ex. a NAS - rather than rely on a DHCP server to give you the address you are expecting. \n\t- This shows that there are 2 ways to achieve a predictable IP address for devices on a LAN\n\n### Localhost vs LAN\n- localhost (an alias for `127.0.0.1`) is an IP address that is used to test the computer's networking protocols without actually using the LAN that the computer is attached to.\n\t- Called a loopback address, and it is analogous to hooking up an outbound cable out of one end of a machine and into the other (as opposed to that cable being hooked up to a router).\n- On the other hand, the private IP (`192.168.X.X`) is created by your network (the router), and allows us to communicate with other devices in the same network. \n- anal. Imagine there were 2 postal services: 1 for your street (local), and one for the whole world (global). When you write a letter, you give it to your local postman, and he connects it to the global postal network. If you are sending your letter to the localhost network, then it is like writing a letter and handing it to yourself. If you are sending your letter to the `192.168` address, then it is like putting it in a mailbox, where your local postman proceeds to deliver it to you.\n\t- In this analogy it's important to note that the letter never reaches the global post network (ie the internet).\n\n## Routing Schemes\n### Broadcast\ntransfer a message to all recipients simultaneously. broadcasting refers to transmitting a packet that will be received by every device on the network\n- can exist aso low as L2\n\t- meaning we can broadcast on ethernet\n- broadcasting is not implemented on IPv6, since it is considered wasteful to broadcast a message to all nodes, when perhaps only a few need to know about it.\n\t- Instead, IPv6 uses multicast\n\n### Multicast\ngroup communication where data transmission is addressed to a group of destination computers simultaneously.\n- can be 1:many or many:many distribution\n- may exist on L7 (application) or L3 (network assisted).\n\t- if done on L3, the sender of the data can send it in a single transmission. \n- multicasting limits the pool of receivers to those that join a specific multicast receiver group.\n\n* * *\nthe combination of IP and port is an interesting thing. We have the IP address of a node (L3), and a TCP port that corresponds to a particular service running on that machine. This demonstrates how both are addresses for two different layers: L3 and L4. In other words, when the data is heading for the IP address, it is a packet. When it is heading for the port, it is a segment.\n\n\n* * *\n## UE Resources\n[explanation of VLAN](https://serverfault.com/questions/188350/how-do-vlans-work)","url":"https://tycholiz.github.io/Digital-Garden/notes/bf451c2b-2ca5-4424-b723-2b3f8ee8d3ac.html","relUrl":"notes/bf451c2b-2ca5-4424-b723-2b3f8ee8d3ac.html"},{"doc":"Router","title":"Router","hpath":"network.lan.router","content":"\n# Routers\n- routers are unique in that they have 2 IP addresses: a public WAN-facing one, and a private LAN-facing one.\n- Routers perform the \"traffic directing\" functions on the Internet, forwarding packets from one network to another\n\t- in other words, data packets are forwarded through the networks of the internet from router to router until they reach their destination computer (with routing algorithms determining the choice of route.)\n- Each router has a prior knowledge only of networks attached to it directly\n- routers gain knowledge of the topology of the network when the routing protocol shares the information of who the router's neighbors are. Each neighbor then shares this information with *their* neighbors, and so on until the whole network is revealed. \n- routing protocols are layer management protocols for L3, regardless of their transport mechanism\n\t- in other words, data may very well travel over L2 or L4\n- **DSL router** - a residential-grade router designed to create LANs and connect them to a WAN (which is provided by the ISP)\n\t- aka residential gateway\n- a residential router uses a modem to connect the LAN to the WAN\n\n### How routers determine which route to take\n- Each packet contains an address in its header, which has a hierarchical structure (like a postal code)\n\t- Each router has a forwarding table which is used to compare against a part of the address to determine the next step in the packet's journey\n\t- Specifically, the routing table is like a hashmap that makes portions of the destination address to outbound links.\n- each time a packet arrives at a router, the router consults its routing table.\n\t- This routing table contains the network ID and the host ID\n\n### Routing Table\nWhenever a node needs to send data to another node on a network, it must first know where to send it. If a direct connection can't be made, then the data has to be sent via other nodes along a route to the destination node.\n- Imagine a node somewhere along this chain receives a packet of data. It has no idea where it came from or where it's going. A routing table solves this problem, as it gives each node in the chain the address for the destination node. \n\t- Effectively, the router says \"I don't know how to deal with 192.168.0.34, but I know that 192.168.0.254 (a router) knows, so if I get a packet destined for that address, I'll just pass it along to that router, since he knows how to deal with it.\" \n- A routing table is a database that keeps track of paths and uses these to determine which way to forward traffic.\n- A routing table is a data file in RAM that is used to store route information about directly connected and remote networks.\n\n* * *\n\n### Core routers\n- core routers are the supercomputers of the internet\n- designed to operate on the internet backbone, as opposed to on the edge of a network (edge router, ex. home network).\n- the core router's purpose is to forward ip packets along. \n- edge routers connect to core routers\n\n* * *\n\nSeeing all nodes on a local area network:\n- `sudo nmap -sn 192.168.1.0/24`\n\t- the 24 is CIDR notation, signifying that we will scan from 192.168.1.0 to 192.168.1.255\n\t\t- the inclusion of 24 means we are scanning an *address block*\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c5d89932-0b27-4068-b450-6b33bb0a73ba.html","relUrl":"notes/c5d89932-0b27-4068-b450-6b33bb0a73ba.html"},{"doc":"Port Triggering","title":"Port Triggering","hpath":"network.lan.router.port-triggering","content":"\n# Port Triggering\n- Generally, port triggering is used when the user needs to use port forwarding to reach multiple local computers. \n\t- port triggering is also used when applications need to open incoming ports that are different from the outgoing port.\n- Port triggering is used by network administrators to map a port or ports to one local computer. \n\t- Port triggering is considered to be dynamic because ports are opened when they are needed and closed when they aren’t in use.\n- When using port triggering, the router is used to monitor traffic within the network. The user specifies a trigger port that sends outbound data. The router then logs the IP address of computers that have sent traffic to that port. The router then opens an incoming port or ports before forwarding the traffic to that location.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/40000bf9-77dd-4468-bb6d-991c726e5a66.html","relUrl":"notes/40000bf9-77dd-4468-bb6d-991c726e5a66.html"},{"doc":"Port Knocking","title":"Port Knocking","hpath":"network.lan.router.port-knocking","content":"\n# Port Knocking\n- port knocking is a technique to externally open a port on a firewall.\n- This is done by externally generating a connection attempt on a set of prespecified closed ports. Once the correct sequence of connection attempts is received, the firewall rules are dynamically relaxed to allow the external host to connect over the specified port. \n\t- this is similar to a secret handshake\n- to implement port knocking, we implement a daemon that watches the firewall log for connection attempts. If the attempted sequence is correct, then the daemon sends instructions to modify the firewall rules for the external host.\n\t- daemon examples: *knockd*\n- anal. Imagine we had a 9 pane glass window in a house, and we have a friend on the inside, while we remain on the outside. In private, my friend and I agree that if I tap each pane in a sequence, then he will open up the 3rd pane. For instance, let's imagine the code is: 1, 5, 9, 2, 2, 3. If I don't tap the panels in order, then my friend will simply ignore me. If I am correct however, he will open the 3rd panel\n\t- here, each panel represents a port, I represent an external host, and my friend represents the router/firewall of the network I am trying to access.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1569e66d-be6a-4376-96d7-b95971c509d3.html","relUrl":"notes/1569e66d-be6a-4376-96d7-b95971c509d3.html"},{"doc":"Port Forwarding","title":"Port Forwarding","hpath":"network.lan.router.port-forwarding","content":"\n# Port Forwarding\n- If I were to try and ssh into my home computer from halfway around the world by using my home's public IP address, my home router would receive the request, and not know what to do with it, since the private IP address would then be needed to complete the send.\n- We can specify to a router \"hey, when you receive requests from the internet with ssh (port 22), I need you to pass them along to 192.168.1.74\" \n\t- We give the router a forwarding IP address (the internal IP that the packets are destined for) and a port. The router external IP and port number together serve as a unique identifier, and we specify what happens when the router receives the combination  \n\t\t- \"upon receiving a request on port 8000, forward that request on to 196.168.1.74\"\n- we need to specify an **external port**, which is the port on the router that is open and facing the internet.\n\t- some numbers will be in use already, such as services from email and web server.\n\t- pick port above 5000\n- Port forwarding settings are found in the router (using the modem's IP) \n\n## External port range\n- normally identical to the internal port range. For security purposes, we may change this.\n- We may not like the fact that people on the outside can see what ports are being used. \n\t- ex. 5432 is the normal port for postgres. Recognizability like this introduces security issues.\n- To get around this issue, we can employ external port ranges. We can specify 11111 as the external port, and 5432 as the internal port, and now whenever a request comes in with port 11111, it will get forwarded to the specified internal IP and the internal port. \n\n## Internal port\n- port of the service running on the internal IP host. \n\t- ex, we are running an express server on port 8000\n- we may want to expose port 22 (SSH), port 21 (FTP) \n","url":"https://tycholiz.github.io/Digital-Garden/notes/c682de7a-47eb-4831-8fb5-5c02e83caac8.html","relUrl":"notes/c682de7a-47eb-4831-8fb5-5c02e83caac8.html"},{"doc":"Nat","title":"Nat","hpath":"network.lan.router.nat","content":"\n# Network Area Translation (NAT)\n- a NAT allows us to map a router's public internet-facing IP address to 1+ local IP addresses on a LAN\n\t- the NAT lives on the router.\n- the NAT was invented to solve a problem presented by the IPv4 protocol – a shortage of IP addresses\n- when you are at a home network and access a webpage, the request goes from your local machine to the router, where the router translates your machine's IP address to the router's address. Therefore, the server that you connected to only sees your router's IP address as the origin of the request. \n- The NAT is the precise reason why if you search something on Google, the results don't show up on your dad's computer. \n- A side effect of NAT is that machines on the internet cannot initiate communications to local machines; they can only respond to communications initiated by them\n\nNAT Table\n- allows devices on a private network to access a public network\n- each row in the table maps one private address to one public address.\n- When the router receives an outbound request from a host in the LAN, it changes the request headers to have the public IP of the router, and it creates an entry in the NAT table.\n\t- When an external request comes in from the internet to the private network, the router needs to know where to forward those packets, so it looks in the NAT Table to find out which host to send it to. \n- each pairing of our private host IP and external public IP is called a *connection*\n\n## Hole Punching\n- hole punching is a technique to establish a direct connection between two parties, where one or both are behind firewalls using NAT\n- To punch a hole, each client connects to an unrestricted third-party server that temporarily stores private and public IP address and port information for each client. The server then relays each client's information to the other, and using that information, each client tries to establish direct connection; \n\t- once there is a successful connection using valid port numbers, each router accepts and forwards the incoming packets on to the host node on the LAN\n- Hole punching is agnostic to which layer the hole punching is performed at. Therefore, there are different types of hole punching occurring at different layers:\n\t- ICMP hole punching\n\t- UDP hole punching\n\t- TCP hole punching\n- Technologies that use hole punching include:\n\t- VoIP, online games, P2P networking, Skype\n\nWhy is this needed?\n- networked devices with privately/publicly available IP addresses can connect easily. However, when one or both of the hosts are behind different firewalls, we need to implement hole punching to make the connection \n","url":"https://tycholiz.github.io/Digital-Garden/notes/c8bfa422-3fa2-479e-8e84-8e7642dd77a1.html","relUrl":"notes/c8bfa422-3fa2-479e-8e84-8e7642dd77a1.html"},{"doc":"Isp","title":"Isp","hpath":"network.isp","content":"\n## Networking at ISP Level\n### Peering\nPeering involves two IPSs coming together to exchange traffic with each other freely, and for mutual benefit.\n- Normally, ISPs of competing size have peer-agreements in place, ensuring that all traffic is forwarded freely, since each ISP will make money from their customers anyway\n\n### Transit\nContrasted with Peering, this is when a bigger ISP charges a smaller ISP for the right to use its network in order to have access to the larger internet\n\n* * *\n\nAn ISP is not necessarily a telecommunications company. In fact, it can be a university, or a private company providing internet access directly to its employees.\n\n## Tiers\n![](/assets/images/2021-03-11-17-31-26.png)\n### Tier-1 ISP\n- known as Internet Backbone networks.\n\t- ex. Sprint, Verizon, AT&T etc.\n- link speeds are often higher (sometimes as fast as 2.5-10 Gbps)\n- They are directly connected to all other tier-1 ISPs\n- They are connected to a large number of tier-2 ISPs\n- They are international in coverage.\n\n### Tier-2 ISP\n- usually have regional or national coverage\n- connects to only a few of the tier-1 ISPs.\n- routes traffic through one of the tier-1 ISPs to gain access to global internet.\n\t- pay tier-1 ISPs for this right.","url":"https://tycholiz.github.io/Digital-Garden/notes/182bfcdf-f8e7-4221-9ddb-3e737468a135.html","relUrl":"notes/182bfcdf-f8e7-4221-9ddb-3e737468a135.html"},{"doc":"Internet","title":"Internet","hpath":"network.internet","content":"\n# What is the Internet?\nThe internet is a network of networks. \n- the first \"network\" in this phrase are the ISPs\n\nThe Internet is a collection of separate and distinct networks\n\t- The relationship between these networks is defined by one of the following:\n\t\t1. Transit (or pay) – The network operator pays money (or settlement) to another network for Internet access (or transit).\n\t\t2. Peer (or swap) – Two networks exchange traffic between their users freely, and for mutual benefit.\n\t- Therefore, in order for a network to reach any specific other network on the Internet, it must either:\n\t\t1. Sell transit (or Internet access) service to that network (making them a 'customer'),\n\t\t2. Peer directly with that network, or with a network which sells transit service to that network\n- the internet is based on the principle that any Internet user can reach any other Internet user as though they were on the same network (*global reachability*)\n\t- Therefore, any Internet connected network must by definition either pay another network for transit, or peer with every other network which also does not purchase transit.\n- Think of the internet as a huge tree with lots of leaves and branches. Your router is the stem of a single leaf (your home). It connects that leaf to the rest of the tree. It doesn't need to know where everything else is, it just needs to know how to transfer data between the twig it is on and the leaf it is connected to. The twig knows what it's connected to, and so on. Some parts of the tree connect directly to many other parts. For example, the trunk is connected to every branch. But, it still has to pass information through those branches and rely upon them to get it to the right twig and leaf.\n\n## How data moves through a network of links and switches\nThere are two approaches to moving data through a network of links and switches: **circuit switching** and **packet switching**\n\n### Circuit Switching\n- with *circuit-switched networks*, the resources needed along a path are reserved for the duration of the communication session. this is less efficient, since the circuit is still reserved even when no data is transmitted (ex. on phone call, the line is reserved even when no one is talking)\n\n### Packet Switching\n- with *packet-switched networks*, the resources are not reserved, and the session's messages use the resources on demand, and therefore may have to wait for access to a communication link\n\t- here, resources would be buffers and the link transmission rate\n- **Packet Switching** is the process of grouping data to be placed into a packet and sent over a network. \n- a **packet switches** comes in two types: routers and link-layer switches.\n\t\n### Examples\n- anal: We have 2 restaurants: one that takes reservations and one that doesn't. The first has more initial setup, since we have to call to make the reservation— though there is less work to do once we arrive (circuit-switched). Meanwhile, the second takes less time to setup since we don't have to phone ahead, but we have to wait longer once we actually arrive at the restaurant (packet-switched).\n\t- In the non-reservable restaurant, the time spent waiting to get a table is analogous to a *queueing delay* that affects the packets. *Packet loss* would be analogous to arriving at the restaurant and encountering a big line and subsequently being asked to leave.\n- ex: telephone networks are circuit-switched networks, since of you want to send data over a line, you must first establish a connection between sender and receiver. also, once a connection is made, a constant transmission rate is reserved for the duration of the connection\n- ex: the internet on the other hand is a packet-switched network, since when data is sent over a network no bandwidth is reserved. if one of the links is congested because other packets need to be transmitted at the same time, and our packet will have to wait in a buffer at the sending side of the transmission link\n","url":"https://tycholiz.github.io/Digital-Garden/notes/69fb0757-3b16-4fb7-8f26-939d759859f0.html","relUrl":"notes/69fb0757-3b16-4fb7-8f26-939d759859f0.html"},{"doc":"Tcp","title":"Tcp","hpath":"network.internet.tcp","content":"\n# TCP\n- IP is unreliable, in that we can't guarantee that the data that's sent is received without being corrupted. IP cannot even guarantee that the data arrives. TCP is reliable. Interestingly, TCP is built on top of IP. The question is, how is a reliable technology built on top of an unreliable one? The answer is that TCP acts as sort of the manager of a transaction of data from one place to another. If data arrives corrupted, it will repeat the request until data comes in fully-formed\n\t- Since the end-user never knows about the data-loss that might have occurred somewhere along the way, TCP is effectively an abstraction\n","url":"https://tycholiz.github.io/Digital-Garden/notes/16fc044f-168d-4abc-aad6-57999a86f76f.html","relUrl":"notes/16fc044f-168d-4abc-aad6-57999a86f76f.html"},{"doc":"Switch","title":"Switch","hpath":"network.internet.switch","content":"\n## Network Switch\n- While routers server as an interface for devices at L3, a network switch uses MAC addresses to forward packets at L2 to a device.\n- ex. Switches for Ethernet are the most common form of network switch\n- switches are built into residential routers\n","url":"https://tycholiz.github.io/Digital-Garden/notes/ac92269d-e1ed-420a-a0c3-4d9fbeab593b.html","relUrl":"notes/ac92269d-e1ed-420a-a0c3-4d9fbeab593b.html"},{"doc":"Smb","title":"Smb","hpath":"network.internet.smb","content":"\nSMB is a communication protocol for providing shared access to files, printers, and serial ports between nodes on a network\nSBM is a proprietary protocol used by the Microsoft Windows network file system.","url":"https://tycholiz.github.io/Digital-Garden/notes/00cdabd9-e970-41d2-956e-b6ea2a6167a6.html","relUrl":"notes/00cdabd9-e970-41d2-956e-b6ea2a6167a6.html"},{"doc":"Repeater","title":"Repeater","hpath":"network.internet.repeater","content":"\n## Repeater\n- a networked device that broadcasts the same data out of each of its ports and lets each device decide what data they need\n- since repeaters amplify a signal, they need a source of electric power\n- Repeaters operate on L1, since they do not try to interpret the data being transmitted\n- anal. in Lord of the Rings, Gondor has a series of beacons that can be lit to communicate to the whole kingdom that war has been declared. \n\t- Therefore, each repeater (beacon) receives a signal and retransmits it (by lighting up their own beacon)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/7511c654-c551-4ec4-b1be-39765ef36eff.html","relUrl":"notes/7511c654-c551-4ec4-b1be-39765ef36eff.html"},{"doc":"P2p","title":"P2p","hpath":"network.internet.p2p","content":"\n# P2P\n- P2P networks works don't have a server, which would provide centralized coordination in a traditional client-server model Instead, it is a distributed application architecture that partitions tasks or workloads between peers\n- Each peer has equal privileges and each has equal power.\n- each peer designates a porion of their resources as available for consumption by other nodes in the network. \n\t- ex. can share resources like processing power, disk storage or network bandwidth\n- Peers are both suppliers and consumers of resources, in contrast to the traditional client-server model in which the consumption and supply of resources is divided. Put another way, each node acts as both a client and a server.\n- P2P networks implement an overlay network, whose nodes are a subset of the nodes found in the physical network.\n- data is still exchanged over TCP/IP, but P2P nodes can communicate directly at L7 by using the logical overlay links (each logical overlay link corresponds to a physical link of the underlying network.)\n- ex. WiFi Direct, Skype\n- Because there is no server, other strategies must be taken to accomplish tasks that would otherwise be handled by the server, such as login\n\t- To carry out login duties, a P2P network has what's called a Login Server\n\n## Place in history\n- P2P networks became popular with Napster, because the more traditional way to use the internet is more heavily regulated and corporately-owned. This means that Napster could skirt a lot of the authorities on the matter. \n","url":"https://tycholiz.github.io/Digital-Garden/notes/1bee20e3-d413-4c33-8c28-969a2ba526d6.html","relUrl":"notes/1bee20e3-d413-4c33-8c28-969a2ba526d6.html"},{"doc":"Ip","title":"Ip","hpath":"network.internet.ip","content":"\n# Internet Protocol\nIP addresses consist of 32 bits, which is why the address is broken down into 4 parts of 8 bits each.\n- 255 represents 8 bits.\n\n### IP Address Spaces\n- there are 2 main IP Address Spaces: public and private.\n\t- public are routable on the internet, meaning every device on the internet needs its own pubic IP\n- the public address space is further divided into 5 classes:\nClass A\t0.0.0.0 – 126.255.255.255\nClass B\t128.0.0.0 – 191.255.255.255\nClass C\t192.0.0.0 – 223.255.255.255\nClass D\t224.0.0.0 – 239.255.255.255\nClass E\t240.0.0.0 – 255.255.255.255\n\t- A,B,C - devices directly connected to internet\n\t\t- ex. L3 switches, routers, firewalls, servers\n\t- D - multicast traffic\n\t- E - experimental\n- The private address space is divided into 3 classes:\nClass A—10.0.0.0/8 network block\t10.0.0.0 – 010.255.255.255\nClass B—172.16.0.0/12 network block\t172.16.0.0 – 172.31.255.255\nClass C—192.168.0.0/16 network block\t192.168.0.0 – 192.168.255.255\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/bc34ba4c-148f-427a-bcb0-ac74e4c48679.html","relUrl":"notes/bc34ba4c-148f-427a-bcb0-ac74e4c48679.html"},{"doc":"Bridge","title":"Bridge","hpath":"network.internet.bridge","content":"\n## Bridging\n- A network bridge is a networking device that creates a single network from multiple networks.\n\t- This process is called *network bridging*\n- Bridging differs from routing, since routing allows multiple networks to communicate independently and yet remain separate.\n\t- On the other hand, bridging connects two separate networks as if they were a single network\n- Occurs at L2","url":"https://tycholiz.github.io/Digital-Garden/notes/6d1e294f-b063-45a6-9395-2c99bcc8ca58.html","relUrl":"notes/6d1e294f-b063-45a6-9395-2c99bcc8ca58.html"},{"doc":"CDN","title":"CDN","hpath":"network.cdn","content":"\n# Content Delivery Network (CDN)\n- A website server normally has a caching server along with it, which will cache the webpage. All requests to the URL will retrieve the content from the cache, until the expiry time is hit. At that point, the user will make a fetch, which will see that the cached content is expired, and will then hit the server, which will retrieve the content, pass it to the caching server, which passes it to the user. All subsequent calls (from other users) will be intercepted by the caching server until it is expired (warm caching)\n![](/assets/images/2021-03-11-15-50-18.png)\n- A CDN provider will place servers in many locations, but some of the most important are the connection points at the edge between different networks\n- Without a CDN, transit may take a slower and/or more convoluted route between source and destination\n- Originally, a CDN was like a cache for static assets that don't change, like images, logos, stylesheets. Nowadays, that benefit only make up ~10% of everything a CDN can do. \n- while CDNs perform caching, not everything that performs caching is a CDN\n- CDNs reduce the importance of where your web server is located, since this would theoretically only impact the timecost of the first fetch of data of each expiry cycle.\n\t- In practice, this is only true for data that doesn't change often. If we are talking about a Facebook news feed that updates frequently, then you are still going to be interfacing a lot with the server, making the location of your server ultimately still important. \n- The DNS plays a central role in the CDN \n- a CDN is known as a distributed internet service\n\n![](/assets/images/2021-03-11-15-50-32.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/e4a5c3d0-ce93-4e42-9786-0da38389b2ce.html","relUrl":"notes/e4a5c3d0-ce93-4e42-9786-0da38389b2ce.html"},{"doc":"Mysql","title":"Mysql","hpath":"mysql","content":"\n### Users\n- list all users - `SELECT User FROM mysql.user;`\n- connect as user \"monica\" - `mysql -u monica -p`\n- change user password - `ALTER USER 'userName'@'localhost' IDENTIFIED BY '<my-password>';`\n\n### Databases\n- change database - `use monica;`\n\n### Tables\n- show tables - `show tables;`\n- drop table - `drop table <table-name>`\n- show columns - `show columns from <table-name>`","url":"https://tycholiz.github.io/Digital-Garden/notes/4866aa20-2068-47f0-afd1-c2b96a24f951.html","relUrl":"notes/4866aa20-2068-47f0-afd1-c2b96a24f951.html"},{"doc":"Mongoose","title":"Mongoose","hpath":"mongoose","content":"\nNote: do not use arrow functions in mongoose, since it prevents binding\n# Hooks (a.k.a. *Middleware*)\nHooks are useful for atomizing model logic and avoiding nested blocks of asynchronous code.\nOther use cases:\n- complex validation\n- removing dependent documents\n    - (removing a user removes all his blogposts)\n- asynchronous defaults\n- asynchronous tasks that a certain action triggers\n    - triggering custom events\n    - notifications\n## pre-hooks\nThere are two types of pre-hooks:\n### Serial\nThe MW functions are executed one after another\n- Note: calling `next()` does not immediately stop execution of the function. To do this, we would need to call `return`:\n```\nvar schema = new Schema(..);\nschema.pre('save', function(next) {\n  if (foo()) {\n    console.log('calling next!');\n    // `return next();` will make sure the rest of this function doesn't run\n    /*return*/ next();\n  }\n  // Unless you comment out the `return` above, 'after next' will print\n  console.log('after next');\n});\n```\n### Parallel\nThe hooked method does not get executed until `done` is called by each middleware:\n```\nvar schema = new Schema(..);\n\n// `true` means this is a parallel middleware. You **must** specify `true`\n// as the second parameter if you want to use parallel middleware.\nschema.pre('save', true, function(next, done) {\n  // calling next kicks off the next middleware in parallel\n  next();\n  setTimeout(done, 100);\n});\n```\n## post-hooks\nFunctions that are executed *after* the hooked method and all of its `pre` middleware have completed.\n\nDo not directly receive control flow\n- i.e. no `next` or `done` callbacks are passed to it\n\n`post` hooks are a way to register traditional event listeners for these methods\n- ex. when the `save` pre-hook happens, this function will execute:\n```\nschema.post('save', function(doc) {\n  console.log('%s has been saved', doc._id);\n});\n```\n\n## Types of middleware\n### Document MW\n`this` refers to the document itself\n\nHook methods:\n- `init`\n    - initialize a document without setters\n    - Called internally after a document is returned from mongodb.\n- `validate` (a.k.a. *pre-save*)\n    - Executes registered validation rules for this document.\n    - if a validation rule is violated, save is aborted\n- `save`\n    - Saves this document\n    - calling this (as a pre-hook) will trigger `validate`, though `validate` will execute before `save`\n- `remove`\n    - Removes the document from database  \n### Model MW\n`this` refers to the model (schema)\n### Query MW\n`this` refers to the query\n- `count`\n- `find`\n- `findOne`\n- ...\nMongoose will not execute a query until `then` or `exec` has been called on it. The power of this comes when we want to build complex queries (ex. using `populate`/`aggregate`)\n- Note: `.then()` in Mongoose are not actually `promises`. If you need a fully-fledged promise, use `.exec()`\n```\nUser.find({ username }) // will not execute\n\n// callback\nUser.find({ name: 'John' }, (err, res) => {}) // Will execute\n\n// .then()\nUser.find({name: 'John'}).then(); // Will execute\n\nPromise.all([User.find({name: 'John'}), User.find({name: 'Bob'})]) // Will execute all queries in parallel\n\n// .exec()\nUser.find({name: 'John'}).exec(); // Will execute returning a promise\n```\n### Aggregate MW\nAggregate MW executes whe you call `exec()` on an aggregate object\n`this` refers to the aggregation object (`<Model>.aggregate`)\n\nIf any middleware calls `next()` or `done()` with an argument of type `Error`, the flow is interrupted and the error is passed to the callback\n\n# Populate\nLets us reference documents in other collections\n- similar to JOIN in SQL dbs\n\n*population* is the process of automatically replacing the specified paths in the document with document(s) from other collection(s).\n\nIn models, we give the `ref` option to a field (property within a document) to indicate which model to use during population.\n```\nconst userSchema = new mongoose.Schema({\n    username: String,\n    posts: [{\n        type: mongoose.Schema.Types.ObjectId, //an array of object ids\n        ref: 'Post' //which model to use\n    }]\n})\n\nconst postSchema = new mongoose.Schema({\n    content: String,\n    author: {\n        type: mongoose.Schema.Types.ObjectId,\n        ref: 'User'\n    }\n})\n```\nLater, when we are within our controller, we will be using `.populate()` on the fields that have `type = mongoose.Schema.Types.ObjectId`:\n```\nUser.findOne({ username: 'Kyle' })\n    .populate('posts') //we want populate to work with the posts field in the user collection\n    .exec((err, posts) => { // similar to .then()\n        if (err) return err\n        console.log('populated user:', posts)\n    }\n```\nThis query will return the specific document within the user collection where `username = 'Kyle'`, and it will *populate* the posts field with all posts made by 'Kyle':\n```\n{\n    _id: 123,\n    username: 'Kyle',\n    posts: [\n        \"s8fm39m\",\n        \"c83ncm8\",\n        \"w822m02\"\n    ]\n}\n```\n# Schemas\n## Instance Methods\nHere, instance refers to the document (since a document is an instance of a model)\n\nInstance methods are defined like so:\n```\n// define a schema\nvar animalSchema = new Schema({ name: String, type: String });\n\n// assign a function to the \"methods\" object of our animalSchema\nanimalSchema.methods.findSimilarTypes = function(cb) {\n    return this.model('Animal').find({ type: this.type }, cb);\n};\n```\nNow, all `animal` instances will have a `findSimilarTypes` method available on them:\n```\nvar Animal = mongoose.model('Animal', animalSchema);\nvar dog = new Animal({ type: 'dog' });\n\ndog.findSimilarTypes(function(err, dogs) {\n    console.log(dogs); // woof\n});\n```\nImagine we were making a Medium.com clone. Our Article model would have a field called `claps`. We could define an instance method called `clap()`, which when executed, would increment the field `claps`\n## Static Methods\nWhereas instance methods are defined on the instance (document), static methods are defined on the Model itself.\n```\n//static method\nconst fido = await Animal.findByName('fido');\n\n//instance method\nconst dogs = await fido.findSimilarTypes();\n```\nThe previous two could not be swapped (ex. `Animals.findSimilarTypes()`), since it would not make sense. \n- Since `Animals` is a model and it has no `type`. This naturally would only exist on an instance of the model\n\n## Query Helper\nInstance methods for Mongoose queries:\n```\nanimalSchema.query.byName = function(name) {\n    return this.where({ name: new RegExp(name, 'i') });\n};\nvar Animal = mongoose.model('Animal', animalSchema);\n\nAnimal.find().byName('fido').exec(function(err, animals) {\n    console.log(animals);\n});\n\nAnimal.findOne().byName('fido').exec(function(err, animal) {\n    console.log(animal);\n});\n```\n## Virtual\nLets us define `getters` and `setters` that don't get persisted to the database\n- Imagine we need a variable `fullName`, but on the `User` model, we only store `first` and `last`. The naive way would be to concatenate these 2 variables each time. Instead, lets define a virtual so that we can use this \"pseudo-field\" in our application:\n```\npersonSchema.virtual('fullname').get(function() {\n    return `${this.first} ${this.last}`\n}\n```\n\n### Accessing the parent documents from the child document\n- ex. Tour has *1:many* relationship with reviews\n- We can add a pre-hook MW function onto the tour model\n```\nparentSchema.virtual('reviews', { //the name of the virtual field\n  ref: 'Review', //the child model\n  foreignField: 'tour', //name of the field in the child model that contains the reference to the current (parent) model\n  localField: '_id' //name of the field where the ID is stored on the current (parent) model.\n});\n```\nThen, to actually populate this virtual field, we just have to use the `populate()` method within our `getTour` handler.\n\n## Nested routes\n- Having 2+ resources at the same url\n- \n\n## Improving Read Performance with Indices\n- see bottom of https://medium.com/@SigniorGratiano/modelling-data-and-advanced-mongoose-175cdbc68bb1\n- this also allows us to carry out logic such as \"each user can only review a tour one time\" \n\t- ` reviewSchema.index({ tour: 1, user: 1 }, { unique: true });`\n\n# E Resources\n[Data modeling and Referencing other collections](https://medium.com/@SigniorGratiano/modelling-data-and-advanced-mongoose-175cdbc68bb1)","url":"https://tycholiz.github.io/Digital-Garden/notes/1b6e8550-252e-4f9f-a586-05044cb97f0a.html","relUrl":"notes/1b6e8550-252e-4f9f-a586-05044cb97f0a.html"},{"doc":"Mongo","title":"Mongo","hpath":"mongo","content":"\n# Normalizing data\n- `normalized` - when data is normalized, it means that instead of having documents called `Folders` that has a field called `files` (an array of actual objects— not just the references to the objects), we have documents that are completely separated from one another, and they just make reference to one another.\n\t- a.k.a Referencing and Embedding\n- denormalized:\n```\nfolder = {\n  name: \"tagA\",\n  files: [\n\t\t\t  {\n\t\t\t    title: \"Tut #1\",\n\t\t\t    author: \"bezkoder\"\n\t\t\t  },\n\t\t\t  {\n\t\t\t    title: \"Tut #2\",\n\t\t\t    author: \"zkoder\"\n\t\t\t  }\n\t\t\t]\n}\n```\n- normalized:\n```\nfolder = {\n  name: \"tagA\",\n  tutorials: [\"238fnnc3\", \"edn38bsn39\"]\n}\n```\n\n## When to Normalize/Denormalize\nif we were to break \"one to many\" relationships down, we could have:\n\t- *1:few* - ex. 1 movie has a few awards\n\t- *1:many* - ex. 1 movie has potentially 1000s of reviews\n\t- *1:tonnes* - ex. 1 chat app has millions of messages\n- We need to distinguish between these relationships when we need to determine whether to normalize or denormalize\n\t- note: This granularity isn’t necessary in relational databases.\n\n### Deciding based on relationship type\n- For *1:few*, we always use denormalized (embedded) data\n- For *few:few* we can just embed\n\t- what about *few:many*?\n- For *1:tonnes* and *many:many*, almost always use normalized (referenced) data\n- *1:many* could go either way\n\n### Deciding based on how data will be interacted with\n- If the data is mostly read and does not change quickly, we should probably embed\n\t- ex. photo gallery related to a movie: once we gather the photos, we probably won’t be updating them too frequently.\n- If the data is updated frequently, we should probably reference\n\t- ex. user generated reviews and ratings of movies. Those vote counts are going to be changing all the time, and we don’t want to query the whole movie document every time a vote is cast. for this reason, it makes sense for reviews to be its own collection with its own documents that reference its parent\n- Imagine Instahop, where each tour guide is also a user. If we were to embed the tourguide directly into the `tour` document, changing any fields in the guide's document would be a pain, since we'd have to update it both in the `tour` document and the `user` document\n\t- Child referencing would be best used here\n\n- in Mongoose, `populate()` is how to denormalize the data as we are sending it to the client\n\n# Referencing other collections\n## Types of referencing\n### Child referencing (child ignorance/parent holds the child reference)\n- best for *1:Few* relationships\n\n### Parent referencing (parent ignorance/child holds the parent reference)\n- best for *1:Many*/*1:Tonnes* relationships\n\n\n### Two-way referencing\n- best for *Many:Many* relationsips\n\n- If we have a Nuggets collection and a Buckets collection, we have a many-to-many relationship between the two. We could Have a field in the Buckets model called `nuggets`, which would be an array of nugget ids. This would be sufficient for some cases, but what if we wanted to get a list of the buckets that a particular nugget belongs to, while we are looking at a nugget? To make this easier, we should have a field in the Nuggets model called `buckets`. This makes things easier in a way, but means that if we want to put the nugget in another bucket, we need to update both the document in the Buckets collection and the document in the Nuggets collection. In other words, using this schema design means we can no longer change the bucket field from the Nugget collection in a single atomic update.\n\n# Aggregation pipelines\n\n# Transaction\n- allows us to execute a series of actions, and have all changes roll back if one of the actions cannot be executed\n\t- ex. in a banking app we have 2 users. one user sends money to another. one action would be to debit the first user's account, and credit the other user's account. Without transactions, if the second action failed, then the first user would be debited while the second would not be credited.\n\n# Indexing\n- Indexing a property will greatly speed up queries where the property is compared for equality at the cost of slower insertions.\n\t- indexes come with a performance cost, so you need to be confident that you’re building the right ones to support your application.\n- When deciding what indexes to create, you need to consider a number of factors, including your query shapes, query volume, read-to-write ratio, and the size of your database\n\n## Purpose\n- Mongo implements indexes to support efficient execution of queries.\n- If there weren't indexes, Mongo would have to scan each document in a collection, and select the ones that match the query statement.\n- Since indexes exist, Mongo is able to limit the amount of documents that it must inspect as part of the querying process.\n\n# Sharded Cluster\n- spec: instead of Mongo deploying a single instance of your database (on a single computer), your database is deployed on different machines (the cluster), and are sharded\n\t- sharded means that not all of the data related to the application will be stored in a single database. Multiple will be used, each one storing different data\n- the `mongos` provides the interface between the client and the sharded cluster\n\t- From the perspective of the application, a mongos instance behaves identically to any other MongoDB instance\n- MongoDB shards data at the collection level, distributing the collection data across the shards in the cluster.\n\t- In other words, the data of a collection is partitioned and stored in different clusters\n- MongoDB uses a *Shard Key* to determine where the data should be split for storage across the different shards.\n\t- Therefore, the shard key consists of a field or multiple fields in the documents.\n\n\n# UE Resources\n- [$ variable](https://docs.mongodb.com/manual/reference/operator/projection/positional/)\n- [design patterns](http://thetechnick.blogspot.com/2016/06/mongodb-design-patterns.html)","url":"https://tycholiz.github.io/Digital-Garden/notes/05537701-5721-425c-a0d5-8ec5da941220.html","relUrl":"notes/05537701-5721-425c-a0d5-8ec5da941220.html"},{"doc":"Realm","title":"Realm","hpath":"mongo.realm","content":"\n## Partitioning\nMongo needs a way to know which data it should give the user access to. Of course, any one user should not receive all of the data in the entire application, only what is owned by them. When using RealmDB, each partition is composed of Realms.\n- A realm is a collection of Realm objects that all share a *partition key*. Each client subscribes to a different set of realms, which passively synchronize changes as network availability allows.realm\n- each partition value (ex. one userId) maps to a different realm. Therefore, Documents that share the same partition value belong to the same realm\n- Most common realms would be defined by userId or teamId, meaning all the data with the same userId will be partitioned and synced with the RealmDB on the user's phone.\n- You can combine these partitioning strategies to create applications that make some data publicly available, like announcements or an organizational chart, but restrict other data to privileged users.\n\t- To combine strategies, you can use a generic partition key field name like `_partitionKey` with a partition value structured like a query string. For example: `_partitionKey: \"user_id=abcdefg\"`\n\t\t- Structuring your partition values like a query string lets you take advantage of multiple partitioning strategies at once, providing the power of user, team, and public realms all at once\n\nSince all documents that share the same partition value also share the same permissions for each user, you should select a key whose unique values correspond with permissions within your application. Consider which documents each user needs to read, and which documents each user needs to write. What separates one user’s data from another? The concepts of ownership (which users can change which data?) and access (which users can see which data?) decide how you should partition your data.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e2f5f81d-e51e-4eca-b9f5-a3f3b24bf3b0.html","relUrl":"notes/e2f5f81d-e51e-4eca-b9f5-a3f3b24bf3b0.html"},{"doc":"CLI","title":"CLI","hpath":"mongo.cli","content":"\n# Shell commands\n- `show dbs` - list all dbs\n- `use <never-forget>` - use the Never Forget db\n- `show collections` - show all collections in the current db\n- `db.users.find()` - get all documents from the collection users\n- `db.insertOne({})`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c5daa598-d7eb-4204-87aa-beee5f79c5fb.html","relUrl":"notes/c5daa598-d7eb-4204-87aa-beee5f79c5fb.html"},{"doc":"Atlas","title":"Atlas","hpath":"mongo.atlas","content":"\n# Atlas\n- the cloud-based database-as-a-service (DBaaS) of Mongo. Basically, this solution lets Mongo take care of all the deployment details. The MongoDB instance is hosted on their servers, and we just need to use those servers, and pay a service charge for doing so. This is an alternative to self-hosting those MongoDB instances and taking care of all of those details yourself.\n\t- This is comparable to how Firebase works. You delegate your having to create a backend to Firebase, meaning Google will host your servers, and all you need to do is communicate with them.\n- Atlas will use a cloud-provider, like AWS, Azure, or Linode","url":"https://tycholiz.github.io/Digital-Garden/notes/25c21a85-7c1d-4115-86f6-abf04a1fc3fa.html","relUrl":"notes/25c21a85-7c1d-4115-86f6-abf04a1fc3fa.html"},{"doc":"Aggregation","title":"Aggregation","hpath":"mongo.aggregation","content":"\n## Aggregation\nprocess data records and return computed results.\ngroup values from multiple documents and give the ability to perform operations on the grouped data, returning a single result\n\nCan be done in 3 ways:\n[more info](https://docs.mongodb.com/manual/aggregation/#aggregation-map-reduce)\n- Aggregation pipeline\n- Map-reduce function\n- single purpose aggregation methods\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1fabb74e-6218-4a11-8b7d-38ca532104bc.html","relUrl":"notes/1fabb74e-6218-4a11-8b7d-38ca532104bc.html"},{"doc":"Memory","title":"Memory","hpath":"memory","content":"\n### Memory Hierarchy\n- Because of physical laws, larger storage devices are slower than smaller storage devices.\n\t- ex. a disk drive on a typical system might be 100 times larger than the main memory, but might take 10,000,000x longer to read a word from disk than from memory. Similarly a typical register file (CPU) stores only a few hundred bytes of information, as opposed to millions of bytes in main memory. However, the processor can read data from the register file almost 100x faster than from memory.\n- The main idea of memory hierarchy is that the storage at one level serves as a cache for storage at the next lower level\n\n### Memory Leak\nin languages without garbage collection, memory leaks occur when we fail to free up memory (deallocate) after a pointer no-longer serves its purpose (such as when we never reference that variable again until end of execution)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/27f2c279-9420-4375-9487-5493c1dbbefc.html","relUrl":"notes/27f2c279-9420-4375-9487-5493c1dbbefc.html"},{"doc":"Stack","title":"Stack","hpath":"memory.stack","content":"\n# Stack\nContrast with [[heap|memory.heap]]\n- A place in memory where variables declared by a function are stored\n- Any time we create a variable In a normal way, we are putting it on the stack\n- Every time a function declares a new variable, it is \"pushed\" onto the stack. Then every time a function exits, all of the variables pushed onto the stack by that function are deleted.\n- when a function exits, all of its variables are popped off of the stack. Thus stack variables are local in nature\n- variables are declared, stored and initialized during runtime.\n- Storage is temporary, and when The computing task is complete, the memory location will be erased\n- Data structure is linear\n- Data is physically located together (contiguous blocks)\n- Variable de-allocation is automatic\n","url":"https://tycholiz.github.io/Digital-Garden/notes/2f077ee6-21c4-4791-9167-77b7e97a4fef.html","relUrl":"notes/2f077ee6-21c4-4791-9167-77b7e97a4fef.html"},{"doc":"Pointer","title":"Pointer","hpath":"memory.pointer","content":"\n# Pointer\nA pointer is a variable that stores both a memory address and the type of data that resides at that memory location.\n- Obtaining the value stored at that location is known as *dereferencing the pointer*\n    - anal: The index within a textbook has page numbers which reference pages in the book. Dereferencing the pointer would be done by flipping to that page and reading the text.\n\t- the type of the pointer tells the compiler what operations can be performed through that pointer\n- a pointer is a very thin abstraction built on top of a language's own addressing capabilities\n- Each unit of memory in a system is assigned a unique address (memory location).\n\t- A pointer is an object that stores this address.\n\t\t- This is similar in concept to how we dont store images in a database, we just store its url location\n\t- Conceptually, the fact that memory is just stored in blocks makes memory itself a very large array.\n\t- arrays work a bit differently in that the variable points to the address (location) of the first character in the array \n- pointers are used partly because they are sometimes the only way to express a computation, and partly because they usually lead to more compact and efficient code than can be obtained in other ways\n\t- They are especially efficient when used in repetitive operations\n- pointers are used for constructing *references*, which in turn are fundamental to constructing nearly all data structures\n- using a pointer to retrieve the value that is stored at the relevant memory location is called **dereferencing the pointer**\n- anal:if storage were a reference book, then the page number in the index would be the pointer, and flipping to that page and reading the text would be dereferencing that pointer.\n- the format of a pointer is dependent on the underlying computer architecture (since pointer's pertain to physical memory locations)\n- Pointers and arrays are closely related\n\t- Any operation that can be achieved by array subscripting can also be done with pointers\n\t- consider that when we declare an array `x` of length 10, we are reserving 10 contiguous memory cells. We can create a pointer that points to the first element with `ptr = &x[0];`. Now, we can copy the contents of that first element to `y` with `y = *ptr;`.\n\t\t- Since elements in an array are contiguous, by defintion `ptr+1` points to the second element in the array, and `*(ptr+1)` refers to the *contents* of `x[1]` \n\t- ex. a char occupies a single byte, a short occupies 2 contiguous bytes, a long occupies 4 contiguous bytes, and so on.\n\t\t- Naturally, this means that contiguous groups of memory cells can be manipulated\n- A pointer is a group of cells (often two or four) that can hold an address\n\n## Example\nImagine we executed the following code:\n```\nint a = 5;\nint *ptr = NULL;\n```\nassuming that `a` is stored at `0x8130` and `ptr` at `0x8134`, our memory would look like:\n| Address | Value |\n|---------|------------|\n| 0x8130  | 0x00000005 |\n| 0x8134  | 0x00000000 |\n\nnow this code is run:\n```\nptr = &a;\n```\nAnd our memory looks like this:\n| Address | Value |\n|---------|------------|\n| 0x8130  | 0x00000005 |\n| 0x8134  | 0x00008130 |\n\nnow we can dereference `ptr`:\n```\n*ptr = 8;\n```\nAnd our computer takes the contents of `ptr` (0x00008130), locates the address, and assigns 8 to that location, yielding this memory:\n| Address | Value |\n|---------|------------|\n| 0x8130  | 0x00000008 |\n| 0x8134  | 0x00008130 |\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3d5f377a-991a-46b5-b514-d2be625f62bd.html","relUrl":"notes/3d5f377a-991a-46b5-b514-d2be625f62bd.html"},{"doc":"Heap","title":"Heap","hpath":"memory.heap","content":"\n# Heap\nContrast with [[stack|memory.stack]]\n- A place to store global variables. \n- The heap is not automatically managed for you\n- Data structure is hierarchical\n- Variables need to be deallocated manually\n- Garbage collection runs on the heap\n- The heap is slower, but it can also store much more data than the stack\n- In C, `malloc` and `calloc` are methods used to interact with the heap. Once memory has been allocated on the heat, we must use `free()` to deallocate that memory. Failure to do this results in what’s called memory leaks. \n- Pointers must be used to access memory on the heap\n- allocating memory is done on the heap, not the stack (as with other variables)\n- dynamic memory allocation can only be made through pointers and names (variable names) can't be given\n\t- `malloc()` is for allocating memory blocks from the heap in C\n","url":"https://tycholiz.github.io/Digital-Garden/notes/ba4f6bd5-86f2-469e-8d3d-42ca78727caf.html","relUrl":"notes/ba4f6bd5-86f2-469e-8d3d-42ca78727caf.html"},{"doc":"Buffer","title":"Buffer","hpath":"memory.buffer","content":"\nA Buffer is an in-memory collection of raw bytes\n- It can be thought of as a temporary place to put things that need to be worked on or processed, \n    - ex. like a to-do pile of work on your desk: Instead of just doing work the moment it’s given to you, you have people put it in a folder on your desk so you can work on everything at a steady pace\n- Buffering is a *technique*, not one specific place in the computer.\n\nBecause computers store data in bytes, a simple way to think of a Buffer is to think of it as an array of bytes.\n\nThe point of a Buffer is to simply store anything as an array of bytes. The reason this is important is because every thing in computing communicates in bytes.\n\na Buffer is similar to an array of integers, but corresponds to a raw memory allocation outside the V8 heap.\n- This is specific to node\n\nIn Javascript, a String is a collection of characters in UTF16 encoding. Under UTF16 encoding a single character may consist of multiple bytes (at least one, usually not more than four). To help convert between Strings and Buffers the Buffer class has various means to set its encoding so you can put a String into the buffer and it will translate from a String of UTF16 characters into an array of bytes. Once you have a String in byte form you can use it in computing communication.\n\n### with Streams\na Buffer is what you get from or put to a Stream. The stream then reads or writes the buffer to the input or output target that the Stream is connected to.\n\n### Examples\n\n#### Movies\nThe rate at which you download a file may fluctuate but if the playback of that video did too it would be very awkward to watch. A buffer is a temporary storage place for some of that video so the downloading process can put it somewhere as it comes in and the playback process can play it at a steady rate.\n- For video, what you do is to first retrieve (for example 10 seconds) and then start to play. If the network drops some packet (and the data along with it) you can ask for it to be retransmitted to fix the problem.\n- if the buffer empties without having an end-of-file marker (a special flag designed to tell the playback device that no more data is needed), the program won’t know what data to display/playback next. Thus in the context of video/audio, buffering is when the video/audio buffer is filled with data when it has processed all the data in the buffer without receiving an end-of-file marker.\n- When you see a “buffering” message in the middle of playing a video, that means that the internet got so slow that the player ran out of video to display. A smart player might notice that 10 seconds (or whatever) of buffered video wasn’t enough, given how flaky the internet is at this location is, so maybe it’ll bump up the buffer to 20 or 30 seconds.\n\n#### Keyboard\nWhen you type on your keyboard, the keystroke data is put into a buffer. Then depending on what program, text box, etc. you are using, the processor processes the data and makes the appropriate updates to where it is needed (like showing it on screen, applying any commands or special processes to it like copy and paste functions, storing the information in RAM/ROM, etc.) Typically each process has a separate buffer which your CPU can access, handle reading from, writing to, and processing, as well as share information between other buffers. i.e. your keyboard has one buffer, your mouse another, your display another, your web browser another etc. and your processor handles information sharing between all of them.","url":"https://tycholiz.github.io/Digital-Garden/notes/8f390b6d-b765-4ebd-b9d5-9da24332d994.html","relUrl":"notes/8f390b6d-b765-4ebd-b9d5-9da24332d994.html"},{"doc":"Mac","title":"Mac","hpath":"mac","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/ea29efc8-12ac-4797-8ca6-5f797210996b.html","relUrl":"notes/ea29efc8-12ac-4797-8ca6-5f797210996b.html"},{"doc":"Os","title":"Os","hpath":"mac.os","content":"\n### Launch Agent\n- a launch agent is a simple script used by `launchd` that causes the system to run programs at system startup.\n\t- ex. postgres, Dropbox","url":"https://tycholiz.github.io/Digital-Garden/notes/dada39cf-7db2-4a91-acfe-eabb9571a6b0.html","relUrl":"notes/dada39cf-7db2-4a91-acfe-eabb9571a6b0.html"},{"doc":"Cmds","title":"Cmds","hpath":"mac.cmds","content":"\n## Utility\ncmd shift 4 - take a screen shot\ncmd shift 5 - take video recording\n\n## Navigation\nctrl + cmd + f - toggle fullscreen","url":"https://tycholiz.github.io/Digital-Garden/notes/42a67c2e-d3ab-4ee1-afa7-1ec4b2e68598.html","relUrl":"notes/42a67c2e-d3ab-4ee1-afa7-1ec4b2e68598.html"},{"doc":"Lsp","title":"Lsp","hpath":"lsp","content":"\n# Language Server Protocol (LSP)\n- the problem is that there is a matrix of code editors and languages to support. It doesn't make sense that every editor has extensive support for js, ruby, python, C... Why not just make dedicated servers that will handle all things for a specific language (syntax checking, autocomplete, go-to-file etc).\n- For instance, we have vim and a language server that handles everything for javascript. This same language server can be used for VSCode, Sublime, IntelliJ etc.\n![](/assets/images/2021-03-11-19-53-45.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/3bcd5b24-cdfa-45ad-b249-55ae92119ed3.html","relUrl":"notes/3bcd5b24-cdfa-45ad-b249-55ae92119ed3.html"},{"doc":"Linux","title":"Linux","hpath":"linux","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/93de42ca-53ea-460a-baa7-b9ec5c47cb1e.html","relUrl":"notes/93de42ca-53ea-460a-baa7-b9ec5c47cb1e.html"},{"doc":"Os","title":"Os","hpath":"linux.os","content":"\n### Systemd\n- systemd is a collection of programs that provides system components for Linux.\n- Its purpose is to unify system configuration across different Linux distributions.\n- Systemd's primary component is a system and service manager, which is an init (boot) system used to manage user processes.\n\t- systemd also provides replacements for various daemons and utilities of the Linux system, including device management, login management, network connection management.\n- Effectively, systemd is used on most Linux systems, and has replaced distribution-specific init systems. \n- below is the systemd startup log.\n![74a24a20baffe9c81ed4a14a5d4c398a.png](:/3d7b66b1422e473096a6481247f59393)\n\n- `systemctl` is a utility used to control systemd \n\t- ex. we can issue a command to restart the `ssh` server\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5ddfbac4-5c06-4bce-836d-d3a36b8c3e7a.html","relUrl":"notes/5ddfbac4-5c06-4bce-836d-d3a36b8c3e7a.html"},{"doc":"Lerna","title":"Lerna","hpath":"lerna","content":"\nLerna allows us to have multiple packages within our project. Complete with each package, will be a package.json and a corresponding node_modules directory\n\nWe can list one of our packages as a dependency of another one by including the package in the `dependencies` section of the dependent's `package.json` \n\nThe root of a project with Lerna will have a `lerna.json`, as well as a `packages/` directory\n- Each sub-package of our project must be within the `packages/` directory \n\n- To run `lerna bootstrap` is to install all dependencies in sub-modules\n- When we run `bootstrap`, lerna will call `yarn` in each module, then create symlinks between the packages that refer to each other in the dependent's `node_modules`\n\t- ex. If we have 3 modules: Addition, Subtraction, and Calc (which performs the add/subtract operations), then Calc's `node_modules` will contain symlinks to the Addition and Subtraction modules. \n- if we use hoisting, then symlinks don't play a part. Instead, if we have `react` in 2 different submodules, then hoisting will allow us to remove `react` from those submodules, install it at the root level, then allow the node recursive resolver to handle resolution for us. In other words, from within the submodule, when we require `react`, it will look for the package in its nearest `node_modules`, not find it, then continue upwards until it does, which will be at the root.\n\t- The reason why hoisting works is due to the resolve algorithm of node require\n\n### Duplication\n- naturally, having multiple sub-packages in a project will result in duplicate package listings. Lerna offers hoisting, which allows us to effectively list the same package in multiple places, but have the packages install at the root level\n\t- therefore, the duplicated package (ex. React) will be in the root directory's `node_modules`, even though it is listed in the sub-package's `package.json` \n- If we have a project with 2 sub-modules: A and B, and both have React as a dependency, we can run `lerna bootstrap --hoist` at the root, which will remove React (and all of its dependencies) from `A/node_modules` and `B/node_modules`, and move them to the root level `node_modules`. Because of the recursive nature of how `node_modules` are resolved, this will cause the sub-module's package.json to look upwards in the directory hierarchy until the react binary is found. \n\t- Lerna will warn us when running this command if we have version mismatches\n\n### Commands\n- `bootstrap` - generate `node_modules` for packages.\n- `clean` - remove all `node_modules` that are not in root directory\n- `create` - add a new sub-package to your project\n- `run` - run the script that is listed in each sub-`package.json`\n\t- Therefore, it will run only npm or yarn commands (with the script that is listed in package.json)\n- `exec` - run a command inside each package\n\t- similar to `run`, but is not restricted to running scripts in `package.json` \n\n### Misc\nYou don't actually need to run lerna bootstrap if you're using yarn workspaces.","url":"https://tycholiz.github.io/Digital-Garden/notes/38d933b3-770f-49df-b1ac-f2ae4f6061fd.html","relUrl":"notes/38d933b3-770f-49df-b1ac-f2ae4f6061fd.html"},{"doc":"Lan","title":"Lan","hpath":"lan","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a8edc4ac-028b-40ec-872a-e4a005b04b2a.html","relUrl":"notes/a8edc4ac-028b-40ec-872a-e4a005b04b2a.html"},{"doc":"Mac","title":"Mac","hpath":"lan.mac","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/eec7f6b4-c966-483c-bb1a-b9aacc031692.html","relUrl":"notes/eec7f6b4-c966-483c-bb1a-b9aacc031692.html"},{"doc":"Karabiner","title":"Karabiner","hpath":"karabiner","content":"\n# Complex Modifications (json)\n- found in `~/.config/karabiner/assets/complex_modifications`\n\nFrom\n- this field specifies the key combination that we want to press\n\nTo\n- this field specifies the key combination that we want the system to understand\n\n# Modifiers\nMandatory\n- this key must be pressed\n\nOptional\n- this key can be pressed and the mapping will be registered, though it is not necessary\n\n# Tools\n[Complex rules generator](https://genesy.github.io/karabiner-complex-rules-generator/)\n\npackage.json has a pre/post version of every script","url":"https://tycholiz.github.io/Digital-Garden/notes/c56bb2bc-4c2b-4bce-87e7-f1e655aa449a.html","relUrl":"notes/c56bb2bc-4c2b-4bce-87e7-f1e655aa449a.html"},{"doc":"Kafka","title":"Kafka","hpath":"kafka","content":"\n- Traditionally, we use only databases as sole data storage, and think of that data in terms of \"things\", along with their state.\n    - Kafka encourages us to think of events first, and things second.\n- Kafka stores data in a distributed log instead of a database.\n    - log is an ordered sequence of events, along with state and a description of what happened\n    - Kafka is a system for managing these logs (in Kafka, a log is called a `Topic`)\n- logs are easy to build at scale\n- Implementations of Kafka are declarative\n\n### Topic\nAn ordered collection of events stored in a durable way.\n- Durable - written to disk, and replicated\n- Can be thought of as a *real-time stream*\n\nKafka Services can be used as a sort of glue between microservices of an application. A microservice can consume a message from the Kafka Topic, and produce an output which gets registered to another Topic.\n- Since it can act as glue between many services, we can produce an output from these Topics that can be consumed by a new service to perform real-time analysis of that data\n    - This is contrast to the old-school method of running a batch-process overnight\n\n### Kafka Connect\nImagine we have multiple databases, a legacy service, and a SaaS product, and we want a way to get the data that they produce into Kafka.\n- Kafka Connect helps us get that data into Kafka, and back out again.\n- Kafka Connect is a general term to refer to 100's of pluggable modules that handle the I/O of the whatever service we are connecting to.\n    - ex. There would be a connector to capture row-level changes in a Postgres database.\n\n### Kafka Streams\n- A Java API that performs helps us perform grouping, aggregating, filtering, enrichment (table joining)\n- the API would be used from within the services\n- This is available to us out of the box as a consequence of using Kafka.\n\n#### KSQL\n- a language that allows us to to use SQL-like syntax to query data from one Topic, and output it into another Topic.\n- Solves the problem statement: imagine we want to perform some analysis on data kept in Kafka, but we don't want to stand up a separate service to consume that data.\n\n### Confluent\n- a distribution of Kafka.\n- open source, but offers a paid managed service (similar to Docker)\n\n### Kafka vs Logstash\n[Kafka is a cluster; Logstash is a single instance](https://stackoverflow.com/questions/40864312/how-logstash-is-different-than-kafka)\n\n# UE Resources\n[The place to start learning Kafka](https://kafka-tutorials.confluent.io/)\n[Getting Started](https://www.confluent.io/blog/getting-started-with-kafkajs/)\n[Kafka with Azure Functions](https://github.com/Azure/azure-functions-kafka-extension)","url":"https://tycholiz.github.io/Digital-Garden/notes/6412cc53-4648-46c1-8be6-c6d1c301baa1.html","relUrl":"notes/6412cc53-4648-46c1-8be6-c6d1c301baa1.html"},{"doc":"Jest","title":"Jest","hpath":"jest","content":"\n- the data entering a component via props is the data, whose stability we want to test during unit tests. For instance, if we have a `<Table />` component, we are chiefly concerned with the data coming in and how it impacts `<Table />`. In this case, it would be: How long is the array of data that comes in (which ultimately translates to amount of rows)? What is the 'title' prop that we are expecting (which renders the title on the Table), etc. Given that the data is what we are interested in, begin to think of ways that data can fail on us. What happens if we get no data? Well, then I guess we expect the amount of <Row /> components to be 0. This would mean a passing test, meaning that paticular piece of logic is guarded, and will remain robust unless the test breaks.\n- Keep in mind we are forming our tests around: what is expected given this circumstance? In tests, an error case and happy-path are both equivalent in terms of value. They are simply scenarios that may happen, and we want to just clarify it for the record that \"this is what should happen, given this circumstance.\"\n- clarify this point: one level deep means\n- Example:\n    - in `const wrapper = shallow(<FeedbackSummary />);`, imagine we are the parent, and `FeedbackSummary` is the child we are testing. In other words, `FeedbackSummary` is the input, and `wrapper` is the output. In jest, we are basically saying \"hey FeedbackSummary, I'm going to give you some props and render you. The output will be called wrapper, and I will take my magnifying glass (aka enzyme library) and take a close look to ensure that the everything looks like it should\"","url":"https://tycholiz.github.io/Digital-Garden/notes/6c54da8e-be6f-4ce3-b642-ef3108533df9.html","relUrl":"notes/6c54da8e-be6f-4ce3-b642-ef3108533df9.html"},{"doc":"CLI","title":"CLI","hpath":"jest.cli","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/88750514-d657-49ef-96c8-e479bce21c6f.html","relUrl":"notes/88750514-d657-49ef-96c8-e479bce21c6f.html"},{"doc":"Mock","title":"Mock","hpath":"jest.cli.mock","content":"\n# jest.mock\n- `jest.mock` - replaces one module (or component) with either just `jest.fn` (if only first arg is provided), or the return value of the function provided (if there is a second arg)\n    - in other words, if there is 2 args, `jest.mock` says \"replace all occurrences of arg1 with arg2\" \n    - ***ex.*** `jest.mock('7g-components/box/box.jsx', () => 'Box');`\n        - this is saying \"for the component we are testing, take all instances of `Box` and replace them with the text `Box`\"\n        - what's happening underlying is that since components are just functions, we are saying \"replace our component function with the function `() => 'Box'`\"\n        - we do this when we don't really care about all the props that come with box, but are just interested in the structure. `<Box />` doesn't really matter, since that isn't what we are testing.\n        - we do this basically when our proptypes fails, because we aren't supplying data in the way proptypes would expect (ex. passing to `Box` margin as `25px` rather than the expected `25`)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/fcbdd53f-fabe-4121-af34-8b4b4767e25d.html","relUrl":"notes/fcbdd53f-fabe-4121-af34-8b4b4767e25d.html"},{"doc":"Fn","title":"Fn","hpath":"jest.cli.fn","content":"\n- `jest.fn()` - we want to mock a fn, and don't really care about its original implementation (often just mocking the return value)\n    - returns a spy (which remembers each call that was made on it)\n    - creates a stub\n    - useful in removing dependencies to some backend (eg. server, API)\n    - if we pass an argument, then it means we are passing a mock implementation\n    ```\n    const queryMock = jest.fn(() => Promise.resolve({ data: 1 })\n    console.log(queryMock()) // Promise { pending }\n    ``","url":"https://tycholiz.github.io/Digital-Garden/notes/3927f8c8-e1f6-4ad3-9054-2a65b72a6196.html","relUrl":"notes/3927f8c8-e1f6-4ad3-9054-2a65b72a6196.html"},{"doc":"JavaScript","title":"JavaScript","hpath":"javascript","content":"\n# Terms\n## expression\n- def - a piece of code that will be evaluated to produce a value, or a\npiece of code that is already at its furthest level of evaluation (string,\nnumber etc) \n    - ex. `5`\n    - ex. `\"hello\"`\n    - ex. `1 / 2` evaluates to `0.5`\n    - ex. `i++`\n    - ex. `'A' + 'string'`\n    - ex. `a && b`\n    - ex. `a ? b : c`\n    - ex. `function() {}`\n    - ex. `window.resize()`\n\n## Assignment\n- any time there is a LHS and RHS separated by `=`\n\n`__proto` is the connection between object instances and the prototypes that they \"inherit\" from\n\n* * *\n\n## Event loop\n\"all we're going to do is we're going to create a simple program that waits for things to happen, then reacts by notifying the correct parties. For instance, we need this program to listen for when I click this button. Every time I click this button, I want you to add an item to the user's cart. Now this program (called the event loop) knows about the click handler that we have already declared. This because when we create a new click handler, we are registering it with the event loop, and it is now \"listening\" for each event that we tell it to watch for. Every time the event loop witnesses the event happen, it causes a certain snippet of code to be executed\"\n- this is partially speculation and is meant to be understood at a high level for what the purpose of existence is.\n\nAn event loop must exist because of the fact Javascript is an asynchronous language. What happens is that we put what we need to do in the queue (eg. fetch data), and tell it \"when you're finished, do this\". The result is non-blocking code.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/77ebb3d5-30b3-44e0-badc-02314bfd4f12.html","relUrl":"notes/77ebb3d5-30b3-44e0-badc-02314bfd4f12.html"},{"doc":"Node","title":"Node","hpath":"javascript.node","content":"\n# How it works\nNode.js is a JavaScript runtime environment that processes incoming requests in a loop, called the event loop.\nNode runs as a single process. Other languages may handle \"concurrency\" by having multiple threads handle tasks. However, Node achieves this by having async. When an async action happens, the javascript code will not get blocked, and will continue to execute until the response has come. This makes node a very event-based language (hence its usefulness in the web word, which is fundamentally made up of requests). \n- concurrency here is used liberally, since Node does not run its code concurrently at all. Alas, this is how Node solves the problem, which may be solved using multiple threads (concurrency) at a time. \n\nNode is built on top of v8, the engine that converts javascript to native machine code. These core javascript features form the basis of what Node is. Because Javascript is missing native features that server languages typically have, there comes a need of having to fulfill those tasks. Web server functions (such as I/O, networking, streams etc.) are fulfilled by having isolated modules that each have a single responsibility. Node.js is made up of many of these modules, all built on top of the base language of Javascript and its underlying v8 engine.\n- ques:does this mean that all the language features of javascript are also available in node? Is there a global object? what about all of the javascript features that are considered useless, like exec? \n\nNode.js connects the ease of a scripting language (JavaScript) with the power of Unix network programming\n\nNode.js uses an event loop for scalability, rather than processes or threads\n\n## V8\nWhen it was first introduced, javascript was a simple scripting language, and processed commands in real time. This had performance implications, and made it very slow. This was the inspiration for V8, which did just in time compiling, making it a much faster and more capable language.\n\nWith Node, the idea was \"what if we took the V8 engine, and instead of using it in the context of the web, we allowed it to run terminal applications\"","url":"https://tycholiz.github.io/Digital-Garden/notes/af15188a-8357-4af5-b79f-3d9c81787bc2.html","relUrl":"notes/af15188a-8357-4af5-b79f-3d9c81787bc2.html"},{"doc":"Lang","title":"Lang","hpath":"javascript.lang","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/4ac4398f-3812-4770-94c8-4877f5a98ca9.html","relUrl":"notes/4ac4398f-3812-4770-94c8-4877f5a98ca9.html"},{"doc":"Type","title":"Type","hpath":"javascript.lang.type","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e4763fd0-a59f-4803-8cd5-3ccc5a6fe960.html","relUrl":"notes/e4763fd0-a59f-4803-8cd5-3ccc5a6fe960.html"},{"doc":"Numbers","title":"Numbers","hpath":"javascript.lang.type.numbers","content":"\nyou should not do javascript math on non-integers. If you want to perform math on decimal numbers, you should first convert all values to an integer, do the math, then convert back\n- reason because javascript does math in binary, there are rounding errors when doing math.","url":"https://tycholiz.github.io/Digital-Garden/notes/73dfe170-3a93-4fd7-b2a4-f6c3b00afa8c.html","relUrl":"notes/73dfe170-3a93-4fd7-b2a4-f6c3b00afa8c.html"},{"doc":"Scope","title":"Scope","hpath":"javascript.lang.scope","content":"\n# Lexical scope\n- def - scope that is determined by the JS engine before any code has even been executed (ie. it only looks at the source code— known as [lexing time](https://en.wikipedia.org/wiki/Lexical_analysis))\n- when you see lexical, think \"static\"\n- lexical scoping is analogous to prototypical inheritance, in a sense that the engine will walk up the chain to find a variable, if one is not found in the local scope.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/961961b1-a4c5-4e89-a57e-3ed06f237703.html","relUrl":"notes/961961b1-a4c5-4e89-a57e-3ed06f237703.html"},{"doc":"Promises","title":"Promises","hpath":"javascript.lang.promises","content":"\n# Promises\nusing promises effectively factors time out of the picture.\n- In other words, it is abstracted away\n- ex. race conditions\n\nAn asynchronous function returns a promise immediately, so the client isn’t blocked\n\nThere are 2 sides to promises: the `executor` (the one doing the actions) and the `consumer` (the one waiting for the actions to be done so it can consume the result.\nExecutor ex. (resolve, reject) =>\nConsumer ex. .then, .catch, .finally\n\n- executor is usually defined as part of a library, so it's not often that we write this code\n\nExecutor gets run as soon as it's defined. Therefore, its state will either be resolved or rejected\n\nThink of `.then`, `.catch` and `.finally` as the way that consumers subscribe to the executor\n\n`.then` appears only on promises\n\n`.then` is analogous to adding another \"fan\" onto the subscription mailing list\n\nA executor sends its material (the data) with resolve(stuff)\nA consumer receives it with .then(stuff)\n\n`promise.then` returns a promise\n\nspec:when we wrap a function in `Promise(..)`, we are promisifying it\n- The new function that returns this promisified function now returns a Promise that resolves to its original return value\n\nThe return value from a .then() method becomes the resolved value of the promise (and gets fed into the next .then() if there is one)\n\nThink of async actions atomically, and make each step a .then() that returns the input for the next .then()\nEx. First .then() returns the JSON data as an obj, second returns a subset of that data, third acts on it\n\n(Inside promises) if a promise is returned (eg. return Promise.resolve('stuff')), the next .then() will execute only when that promise has resolved (with 'stuff').\nIf the return value is anything else besides a promise, then it will be passed immediately to the next .then()\n\n.resolve(value)\nIf the value passed to it is a promise itself, this will automatically \"follow\" that promise chain and wait to pass back the final resolved value.\n- Good to use if you are unsure if a value is a promise or not\n\nPromise.all is a server at a restaurant waiting to bring everyone's food at the same time, even though one meal may be ready before the others\n\n## Promises are only ever resolved once per creation\n- In the following code it seems that we would connect to the database 2 times, but Promises don't work like that.\n- since `databasePromise` is only defined one time, it can by definition only resolve one time.\n```js\nconst databasePromise = connectDatabase();\nconst booksPromise = databasePromise\n  .then(findAllBooks);\nconst userPromise = databasePromise\n  .then(getCurrentUser);\nPromise.all([\n  booksPromise,\n  userPromise\n])\n  .then((values) => {\n    const books = values[0];\n    const user = values[1];\n    return pickTopRecommentations(books, user);\n  });\n```\n\n## Sleep example\n- We can use promises make a function who's purpose is to simply wait, before executing further code. We define it as such:\n```js\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n```\n- as soon as we call `sleep(1000)`, a promise is returned to us. This means that javascript will say \"ok, since we're waiting on that promise to resolve (or reject), I'm going to go do some other stuff, and once your promise resolves, I'll be back to execute the `.then()` code\". Once\n\t- This is precicely why promises are said to \"handle asynchronous things synchronously\". It is because promises help us manage 2 different lines of execution at a time\n\n## Async await\n- every function annotated with async returns an implicit promise\n\t- *\"The async function declaration defines an asynchronous function, which returns an AsyncFunction object. An asynchronous function is a function which operates asynchronously via the event loop, using an implicit Promise to return its result.\"*\n- spec: every time we see `await`, it means the promise must resolve before moving on.\n\n### Top-level Await\n- Top-level await allows us to use await outside of a function marked `async`\n\t- It acts like a big async function causing other modules who import them to wait before they start evaluating their body.\n- In the latest ECMAScript, we can run `await` at the top level\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d3a1e788-2d66-44ce-915c-37da3191d319.html","relUrl":"notes/d3a1e788-2d66-44ce-915c-37da3191d319.html"},{"doc":"Op","title":"Op","hpath":"javascript.lang.op","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8c927587-8d0b-445d-8116-70a435be3628.html","relUrl":"notes/8c927587-8d0b-445d-8116-70a435be3628.html"},{"doc":"Optional Chaining","title":"Optional Chaining","hpath":"javascript.lang.op.optional-chaining","content":"\n# `?.`\nJust as `??` doesn't abide by JS falsy (instead, only `undefined`/`null`), so too does the `?.` operator","url":"https://tycholiz.github.io/Digital-Garden/notes/a06ec285-1732-4167-bf8a-0b94804bc6f7.html","relUrl":"notes/a06ec285-1732-4167-bf8a-0b94804bc6f7.html"},{"doc":"Coalesce","title":"Coalesce","hpath":"javascript.lang.op.coalesce","content":"\nIn Javascript we often reach for `||` when we want to get the RH value if the LH value is *any* falsy value\n- ex.\n```js\nconst valA = nullValue ?? \"default for A\";\n```\n\nThis way of doing it works most of the time, but if our business logic considers falsy values like `0` or `''` as legitimate values, then our program will have unexpected behavior. Using `??` gives us more predictable results, and should be used most of the time instead of `||`\n\n### Assigning a default value to a variable\nIn the past, when one wanted to assign a default value to a variable, a common pattern was to use the logical OR operator (`||`):\n```js\nlet foo;\n\n//  foo is never assigned any value so it is still undefined\nlet someDummyText = foo || 'Hello!';\n```\n\nHowever, this is kind of hacky when we consider what we are doing. `||` is a boolean logical opertor (ie. it performs \"math\" on booleans). In order to do boolean math on 2 variables, it has to make sure both values are booleans. If they are not, they have to be coerced (Of course, JS is notorious for its sometimes unpredictable coercion behaviour).\n- With boolean logical operators, the only two values that are considered are truthy values and falsy values. This might work most of the time, but sometimes our business logic dictates that we would expect `0` to be a truthy value. If this is the case, then\n\nExample:\n\nBelow, `quantityOfApples` will not be `0` as we want. Rather, the default value of `10` is being used, because `0` is considered falsy in the eyes of the boolean logical operator.\n```js\nconst count = 0\n\nconst quantityOfApples = count || 10 // 10 — unexpected!\nconst actualQuantityOfApples = count ?? 10 // 0 — totally expected\n```\n\n### Misc\n- It is not possible to combine both the AND (`&&`) and OR operators (`||`) directly with `??`\n\t- To accomplish this, we need to use parentheses (`(null || undefined) ?? \"foo\";`)","url":"https://tycholiz.github.io/Digital-Garden/notes/c085f381-9986-433e-b1a3-851df2f874f3.html","relUrl":"notes/c085f381-9986-433e-b1a3-851df2f874f3.html"},{"doc":"Objects","title":"Objects","hpath":"javascript.lang.objects","content":"\nAs of ES6, objects have a predictable order, determined in one of 2 ways:\n1. *Keys are numbers* - The key-value pair being inserted will obey numerical order\n\t- ex. if we have existing keys `4`, `8`, and `33`, then inserting a key-value pair with the key as `1` will put it in first position of the object\n2. *Keys are strings* - The key-value pair being inserted will be appended to the end of the object\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a1675f7f-3434-4103-b9ac-01a29b717072.html","relUrl":"notes/a1675f7f-3434-4103-b9ac-01a29b717072.html"},{"doc":"Proxy","title":"Proxy","hpath":"javascript.lang.objects.proxy","content":"\n## Proxy Object\n- def - an object that wraps another object. The proxy object intercepts the fundamental operations of the wrapped object, including property lookup, assignment, function invocations etc\n\t- In other words, we can have a regular object that we interact with normally. We can also wrap that regular object with a proxy object, that will intercept those interactions that we have with the object.\n- Think of a proxy object like an enhanced (wrapped) component in React. It can do everything that the original can, but has some added functionality. This added functionality might be as simple as logging to the console every time a property is read (this would be implemented using a **get trap**. \n- The wrapped object is called the *target*\n- The custom functionality that is added is called the *handler*\n- *trap* - a method defined on the proxy that will intercept some interaction we have with the object (ie. read, write)","url":"https://tycholiz.github.io/Digital-Garden/notes/fb1f98cf-60ea-415c-a32b-73a124d32b6f.html","relUrl":"notes/fb1f98cf-60ea-415c-a32b-73a124d32b6f.html"},{"doc":"Cloning","title":"Cloning","hpath":"javascript.lang.objects.cloning","content":"\n# UE Resources\n[Cloning objects in Javascript](http://www.zsoltnagy.eu/cloning-objects-in-javascript/)","url":"https://tycholiz.github.io/Digital-Garden/notes/4f146884-c723-46e7-bcd0-c794df4931a9.html","relUrl":"notes/4f146884-c723-46e7-bcd0-c794df4931a9.html"},{"doc":"Imports","title":"Imports","hpath":"javascript.lang.imports","content":"\nModules import one another using a module loader. At runtime the module loader is responsible for locating and executing all dependencies of a module before executing it. Well-known module loaders used in JavaScript are Node.js’s loader for CommonJS modules and the RequireJS loader for AMD modules in Web applications.\n\nIn TypeScript, just as in ECMAScript 2015, any file containing a top-level import or export is considered a module. Conversely, a file without any top-level import or export declarations is treated as a script whose contents are available in the global scope (and therefore to modules as well).\n\n# ES Modules (ES6 Import)\nSince imports aren't a part of the Javascript standard yet, imports are just a specification— it is up to the implementation (Babel, Typescript, Metro Bundler(?)) to carry out the operation of linking modules.\n\nESM are required as they are needed, rather than there being a bundle created beforehand (as with CommonJS)\n\nwhen you tell the JS engine to run a module, it has to behave as though these four steps are happening:\n1. Parsing: The implementation reads the source code of the module and checks for syntax errors.\n2. Loading: The implementation loads all imported modules (recursively). (This is the part that isn’t standardized yet)\n3. Linking: For each newly loaded module, the implementation creates a module scope and fills it with all the bindings declared in that module, including things imported from other modules.\n\t- This is the part where if you try to import {cake} from \"paleo\", but the “paleo” module doesn’t actually export anything named cake, you’ll get an error. And that’s too bad, because you were so close to actually running some JS code. And having cake!\n4. Run time: Finally, the implementation runs the statements in the body of each newly-loaded module. By this time, import processing is already finished, so when execution reaches a line of code where there’s an import declaration… nothing happens!\n- when we import a module like `import 'firebase/storage'` (ie. without declaring a variable), it means we are executing the module `firebase/storage`, but not bothering to assign the default export to a variable.\n\t- This implies we are doing side-effects.\n\n- When you `import *`, what’s imported is a *module namespace object*. The properties of this object are the module’s exports:\n```\n[Module] {\n  default: '[Function]', // the default export\n  first: 'Kyle' // named export `first`\n  last: 'Tycholiz', // named export `last`\n}\n```\n- if we wanted to import the named exports, we could `import { first } from _____`. spec: also, we could import the default by `import default from _____` (or `import { default }`?)\n\n- `import _ from \"lodash\"` is an alias for `import { default as _ } from \"lodash\"`\n* * *\nImported ES6 modules are executed either asynchronously or synchronously, depending on the module loader (ie. the implementation) we use. Therefore, to be safe we must assume async. However, all imports are executed prior to the script doing the importing. This makes ES6 modules different from Node.js modules or `<script>` tags without the `async` attribute\n\n### Importing without name\nex. `import './bootstrap'`\n- this will execute the target module (ie. run the module's code), without importing anything. It will not affect the scope of the active module\n\t- There may be side-effects, such as declaring global variables.\n- This method of importing is described as \"importing a module for its side-effects only\"\n\n### Aggregating modules (Re-exporting)\n- We can import modules and immediately export them again by aggregating the import and export commands:\n```\nexport * from './atoms'\n```\n- If any name exported by “atoms” happened to collide with the other exports, that would be an error, so use export * with care.\n\nUnlike a real import, this doesn’t add the re-exported bindings to your scope, meaning we can't use the exports from \"atoms\" within that file.\n\n# CommonJS Imports\n## module.exports\n- `module.exports` is an object that is included in every `.js` file in a Node application.\n\t- `module` represents the current module\n\t- `exports` is an objects that will be exposed as a module\n\t- Therefore whatever we assign to `module.exports` is exposed as a module.\n- like `exports` below, `module.exports` can also be extended by including more properties/methods on the object.\n\nbefore a module's code is actually executed, Node will wrap it in a function that looks something like this:\n```\n(function(exports, require, module, __filename, __dirname) {\n// Module code actually lives in here\n});\n```\nThis gives us the benefit of:\n- scoped variables, rather than global variables.\n- ability to use `module` and `exports` objects.\n- ability to reference the module's absolute filename and directory path with `__filename` and `__dirname`\n\n## exports\n- `exports` is an object that we can attach properties and methods to.\n- when we import a module, we must then call the same property/method:\n```\n// dependency\nexports.name = 'Kyle'\nexports.phone = '7788713377'\n\n// dependent\nconst person = require('./information')\n\nconsole.log(person.name) // Kyle\n```\n\n# CommonJS vs ES6 Modules\n- Under the hood, we need something like Babel to convert from ES6 modules to CommonJS.\n\n[explanation](https://stackoverflow.com/questions/40294870/module-exports-vs-export-default-in-node-js-and-es6)\n\n# Circular Dependencies\n- Not always a problem, but they introduce tight coupling.\n\t- These kinds of modules are harder to understand and reuse, as doing so might cause a ripple effect where a local change to one module has global effects.\n\t- As such, it might indicate lack of a larger context or proper architecture, since a good architecture imposes uni-directional flow between modules and entire layers.\n\nusing `const` over `function` while defining functions prevents function hoisting within a single module and ensures the absence of circular dependencies within a single module.\n\nCircular dependencies with Function calls would not cause problems when the cycle is asynchronous, meaning that directly referenced functions are not called immediately.\n- ex. Cycle of function calls when one continues chain through a DOM event listener being async, i.e. waiting for user click.\n\n# Dynamic Imports\n`import(module)` loads the module and returns a promise that resolves into a module object that contains all its exports. `import` can be called from anywhere in the code.\n\nexample\n```\nimport(modulePath)\n  .then(obj => <module object>)\n  .catch(err => <loading error, e.g. if no such module>)\n```\nor\n```\n// 📁 say.js\nexport function hi() {\n  alert(`Hello`);\n}\n\nexport function bye() {\n  alert(`Bye`);\n}\n```\nthen\n```\nlet {hi, bye} = await import('./say.js');\n\nhi();\nbye();\n```\nNote: Although `import()` looks like a function call, it is specified as syntax that just happens to use parentheses (similar to super()). That means that import doesn’t inherit from Function.prototype so you cannot call or apply it.\n\n# UE Resources\nhttps://v8.dev/features/modules","url":"https://tycholiz.github.io/Digital-Garden/notes/a3276f47-b61d-4e3d-b1d2-d420d54ae988.html","relUrl":"notes/a3276f47-b61d-4e3d-b1d2-d420d54ae988.html"},{"doc":"Functions","title":"Functions","hpath":"javascript.lang.functions","content":"\n## Tagged Template Literals\n```js\n// These are equivalent:\nfn`some string here`\nfn(['some string here'])\n```\n- - -\nThe rest of the arguments will be the interpolations, in order.\n```js\nconst aVar = 'good'\n\n// These are equivalent:\nfn`this is a ${aVar} day`\nfn(['this is a ', ' day'], aVar)\n```\n\n## Identity function\n```\nfetchBook()\n  .then((book) => formatBook(book))\n  .then((postscript) => print(postscript))\n```\nis equivalent to (verify this)\n```\nfetchBook()\n  .then(formatBook)\n  .then(print);\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/c391b986-7a79-4731-971a-a34f52965542.html","relUrl":"notes/c391b986-7a79-4731-971a-a34f52965542.html"},{"doc":"Ttl","title":"Ttl","hpath":"javascript.lang.functions.ttl","content":"\n[explanation using styled-components](https://mxstbr.blog/2016/11/styled-components-magic-explained/)","url":"https://tycholiz.github.io/Digital-Garden/notes/42152323-36d0-4222-a830-dc7138fefa00.html","relUrl":"notes/42152323-36d0-4222-a830-dc7138fefa00.html"},{"doc":"Generator Functions","title":"Generator Functions","hpath":"javascript.lang.functions.generator","content":"\n# Generator functions\n- can be exited and later re-entered\n- like closures, variables inside the generator function maintain state.\n- when calling a generator function, an iterator object is returned. When we call `next()` on that object, all the code up until the first `yield` will be executed. Calling `next()` again will then execute all the code up until the second `yield`, and so on.\n\t- The function that calls the generator function is the **iterator**\n- the generator function can pass values to the iterator object (`yield`). Anything that occurs after `yield` gets stored in the iterator's `next()` value\n\t- The generator function can also retrieve values from the iterator object (`next(___)`)\n- `yield` returns execution to outside the generator function (ie. the context from which the gen fn was called), it's possible to use `while(true)`, as long as there is a yield inside\n\t- This way, `next()` can keep getting called\n- spec: `next` is like async/await in the sense that it will execute code up until a point (`yield`), then stop and wait for the availability of that data before continuing on\n","url":"https://tycholiz.github.io/Digital-Garden/notes/0b4a7970-c5ca-4db8-9780-7e802af9ce28.html","relUrl":"notes/0b4a7970-c5ca-4db8-9780-7e802af9ce28.html"},{"doc":"Feat","title":"Feat","hpath":"javascript.lang.feat","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/2eaeaf98-4b8e-409a-99c0-e1bced47480e.html","relUrl":"notes/2eaeaf98-4b8e-409a-99c0-e1bced47480e.html"},{"doc":"While","title":"While","hpath":"javascript.lang.feat.while","content":"\n### Use while(true) and break\n- we can make a while loop that will continuously execute code until `break` is reached.\n```js\nwhile(true) {\n\ttry {\n\t\tawait client.query('select true as \"Connection test\";')\n\t\tbreak //if the promise above resolves, then break will be run and we will exit the while-block \n\t} catch(e) {\n\t\tawait sleep(1000)\n\t}\n}\n```\n","url":"https://tycholiz.github.io/Digital-Garden/notes/0697a188-a9a6-4c4e-9ffc-fc5e40d8bb89.html","relUrl":"notes/0697a188-a9a6-4c4e-9ffc-fc5e40d8bb89.html"},{"doc":"This","title":"This","hpath":"javascript.lang.feat.this","content":"\n# This\n- `this` only cares about execution context (where the fn was called). it doesn't care about the scope chain\n- `this` is essentially an implicit input to a function, thereby negatively impacting function purity. Think of `this` as an implicit parameter that gets passed into the function. \n\t- ie. `this` shouldn't be used in FP\n\nThe scope chain encompasses the prototype chain\n- Expl. Imagine 2 functions: inner() and outer(). If a variable is used within inner() and it does not exist within that function's context, it will look at its prototype to see if it exists (prototype chain). If it goes all the way up the chain and still doesn't exist, then the scope of outer() will be considered.\n- Expl. If this were illustrated as 2 for loops, the scope chain would be the i iterator, while prototype would be j\n\n'return' is a keyword which returns us to the immediate outer execution context (to continue parsing at the point directly after where the function was called)","url":"https://tycholiz.github.io/Digital-Garden/notes/834871cd-72f3-4a7f-99d1-9893c17f1cc7.html","relUrl":"notes/834871cd-72f3-4a7f-99d1-9893c17f1cc7.html"},{"doc":"Spread","title":"Spread","hpath":"javascript.lang.feat.spread","content":"\n### Spread Syntax\nSpread syntax is like zooming. When you see '...', you should think \"expand\"\n- what's important is the receiver (what we are spreading it out into (eg. [], {}, () )\n- this gives the ability for us to spread it out into an array, object, or function (as args). Notice these all accept iterables\n- the copying process is shallow\nhttps://zhenyong.github.io/react/docs/jsx-spread.html\nhttps://stackoverflow.com/questions/31048953/what-do-these-three-dots-in-react-do\nTo remove a property from an object, use rest operator\nEx. Const { id, ...rest } = obj\nThis will remove id from the obj","url":"https://tycholiz.github.io/Digital-Garden/notes/02e49d43-0183-4925-8aa0-0fecc93dafdc.html","relUrl":"notes/02e49d43-0183-4925-8aa0-0fecc93dafdc.html"},{"doc":"Destructuring","title":"Destructuring","hpath":"javascript.lang.feat.destructuring","content":"\n### Destructuring\n- With this technique we can pull out values from an array or object declaratively\n```js\nconst arr = [1, 2, 3, 4]\nconst [ a, , b ] = arr\n// a = 1\n// b = 4\n```\n- when spreading, we are unpacking a variable one level deeper (into an array, object etc)\n    - the `...` is unpacking, the `{}` is putting it into an object\n    - ***ex.*** - if we had an array of objects, the unpacking by itself would result in us having an invalid javascript, since it would just be a group of objects with no \"bucket\" to house them (such as an array or object). This is why `{ ...arrayOfObjects }` will result in those \"stray\" objects having found a home in the surrounding `{}`\n```js\nnewObj = {\n    0: {},\n    1: {},\n    ...\n}\n```\n```js\n<Component a={obj.a}, b={obj.b} c={obj.c} /> === <Component {...obj} />\n```\nwe can rename the object keys of a destructured object like so:\n- an object is returned from calling `useQuery` with fields `loading` and `error`, which we rename.\n```js\nconst { loading: queryLoading, error: queryError, data } = useQuery()\n```\n\nDestructuring creates a shallow copy\n\n# UE Resources\n[advanced destructuring](https://dmitripavlutin.com/javascript-object-destructuring/)","url":"https://tycholiz.github.io/Digital-Garden/notes/a706b0e8-7ef0-4238-b690-4f3b34e29ad2.html","relUrl":"notes/a706b0e8-7ef0-4238-b690-4f3b34e29ad2.html"},{"doc":"Coercion","title":"Coercion","hpath":"javascript.lang.feat.coercion","content":"\n# Object::String\nThe default conversion from an object to string is `[object Object]`\n- when we see `[object Object]` somewhere, it would seem to imply that the object is getting coerced into a string somewhere along the line, thus resulting in us seeing `[object Object]` \n- check to see if you didn't properly stringify json\n- output from `console.log` is captured at the application level, meaning that we won't get access to the logs if we are using multiple applications\n\t- ex. if we are using Azure Functions in our app, we are using multiple applications, since our codebase exists in a different application than our Azure Functions. ","url":"https://tycholiz.github.io/Digital-Garden/notes/319f6437-8185-48cd-9749-82f3f865f433.html","relUrl":"notes/319f6437-8185-48cd-9749-82f3f865f433.html"},{"doc":"Closures","title":"Closures","hpath":"javascript.lang.closures","content":"\n### Closure\nFrom the context of an inner scope, there is a: local scope, any number of closure scopes, and a global scope. The closure scopes represent the different scopes of the surrounding code. If our current scope is nested 3 levels deep then there are 2 closure scopes. Within these scopes, there may exist variables.\n\nbecause of how lexical scope works, when we call a function in js that accesses a variable from outside its scope, it will capture it at the very time the function is created\n\n- A closure is a function that encloses its surrounding state by referencing fields external to its body. The enclosed state remains across invocations of the closure.\n- Occurs when the function we are calling returns another function, so the second function lives on, while the initial function dies (because it has already returned). Put another way, a closure is formed any time an inner function is made available from outside of the scope it was defined within.\n```\nVar outer = () => () => {}\nVar innerFunc = outer()\n```\n- above, innerFunc causes outer() to execute, returning a function and setting its value to it. innerFunc has access to the local variables of its containing object (normally a containing function). Therefore, these \"sibling\" local variables are changeable from outside the function.\n- Think of a closure as the lifeline that an inner function extends to the variables (that the inner function has used) defined in the outer function. They continue to exist because the closure exists. In other words, the inner function closes over (ie. captures/remembers) the variables defined in the outer function.\n- Conceptually (but not actually), the closed over function (`outer`) has all of its variables put into an object. That is how `inner` is able to access those values. Something like this is happening:\n```js\nfunction outer() {\n    var x = 1;\n\n    return function inner(){\n        return x;\n    };\n}\n```\nmakes 2 objects like this:\n```js\nscopeOfOuter = {\n    x: 1\n}\n\nscopeOfInner = {};\n```\nthen `scopeOfOuter` is set as the prototype of `scopeOfInner`, so when we try to access the value of x with `return scopeOfInner.x`, we see that `scopeOfInner` doesn't have an `x` property, so it goes up the prototype chain and finds an `x` property on `scopeOfOuter`\n\n```js\nObject.setPrototypeOf( scopeOfInner, scopeOfOuter );\n```\n\n- Conceptually, the structure of a closure is not mutable. In other words, you can never add to or remove state from a closure\n- closures are a subset of lambdas\n\n#### How scope enables closures to happen\nIn JS, a scope is created by a function or code block.\n- When we have 2 separate functions at the same level of the code, both can use the same variable names and not have collisions. But what happens when one fn (`inner`) goes within another (`outer`)?\n\nIn the following example, `myInnerFunc` is an instance of `innerFunc`, with the enhanced benefit of having access to `outerVar`\n- The reason it has access is because of lexical scope, which (importantly) is defined before any javascript code has run (ie. analyzed just by the source code)\n![dad83091392736b4fc218299b2073d6d.png](:/8e496538fa28463a9e908e9164c39882)\n- Therefore, a closure is a function that has access to its lexical scope, *even though* that function was executed from outside of that lexical scope.\n\t- Simpler, the closure is a function that remembers the variables from the place where it is defined (and not where it was executed)\n- A rule of thumb to identify a closure: if you see in a function an alien variable (not defined inside the function), most likely that function is a closure because the alien variable is captured.\n\n#### Analogy\nImagine a magical paintbrush with an interesting property. If you paint with it some objects from real life, then the painting becomes a window you can interact with.\n\nPainting as a model of JavaScript closures\n\nThrough this window, you can move the painted objects with your hands.\n\nMoreover, you can carry the magical painting anywhere, even far from the place where you’ve painted the objects. From there, through the magical painting as a window, you can still move the objects with your hands.\n\nThe magical painting is a closure, while the painted objects are the lexical scope.\n![c40e3c1034d769d6bc5aa8f2100a83e7.png](:/cee3345f41c44ceca614faea5e5cc400)\n\n#### Stale closures\n- stale closures capture variables that have outdated values.\n```js\nfunction createIncrement(i) {\n  let value = 0;\n  function increment() {\n    value += i;\n    console.log(value);\n    const message = `Current value is ${value}`;\n    return function logValue() {\n      console.log(message);\n    };\n  }\n\n  return increment;\n}\n\nconst inc = createIncrement(1);\nconst log1 = inc(); // logs 1\nconst log2 = inc();             // logs 2\nconst log3 = inc();             // logs 3\nlog1();             // logs \"Current value is 1\"\nlog2();             // logs \"Current value is 2\"\nlog3();             // logs \"Current value is 3\"\n```\n- `log{1,2,3}()` are stale closures, because it has already captured the value *at the time* that `inc()` was called. What's important to note here is that `inc()` is called 3 times. Every time it is called, it runs through the `increment` function that was closed over. It then returns that value, and holds it (within a function called `logValue` that prints out the held value). In other words, it does not get updated with each subsequent call of `inc()`. It has already held onto that value, and there is nothing it can do to change that fact.\n- Therefore, if we want to capture the freshest value, we have to figure out which closure it is that has those freshest variables.\n\t- Here, that variable would be the *latest* call of `inc()`.\n- in React\n\n#### Closures vs Objects\nclosure offers granular change control and automatic privacy.\nobject offers easier cloning of state\n\nClosures are made every time we create an event handler, a promise, setTimeout, and even within `useEffect` in React.\n\n# Node Debugger\nclosure scope is outside of local scope\nthere are multiple layers of closure state\n","url":"https://tycholiz.github.io/Digital-Garden/notes/76c0e35a-0e0d-467a-8275-57b94629267e.html","relUrl":"notes/76c0e35a-0e0d-467a-8275-57b94629267e.html"},{"doc":"Callbacks","title":"Callbacks","hpath":"javascript.lang.callbacks","content":"\n## Callbacks\n- A callback is a function you provide as an argument to another function. When that other function has completed its task, it will execute the provided function. In the meantime, however, the code coming after the request will be executed regularly.\n- spec: we see that `bcrypt.genSalt` takes in salt as its argument. given its position, this tells us that the function it is a part of will ultimately return that item. looking at the inner one, `hash` is now in that position. that function `bcrypt.hash` returns a hash. \n```\nbcrypt.genSalt(10, (err, salt) => {\n    if (err) return next(err)\n    bcrypt.hash(user.password, salt, (err, hash) => {\n      if (err) return next(err)\n      user.password = hash\n      next()\n    })\n  })\n```\n\nThis is also the same value that will be used as the parameter taken in with `.then` when working with promises\n```\n  bcrypt.genSalt(10)\n    .then(salt => {\n      return bcrypt.hash(user.password, salt)\n    })\n    .then(hash => {\n      user.password = hash\n      next()\n    })\n    .catch(err => {\n      console.error(err)\n      return next(err)\n    })\n```\n","url":"https://tycholiz.github.io/Digital-Garden/notes/dea25a9c-76a4-46b1-84dd-499318736d21.html","relUrl":"notes/dea25a9c-76a4-46b1-84dd-499318736d21.html"},{"doc":"Arrays","title":"Arrays","hpath":"javascript.lang.arrays","content":"\n## When to use Arrays for lists of data\nIf you have a list of data that you are going to be transforming somehow, or mapping over it somehow, you should try to structure that data as an array\n- expl: if you had an object of keys, and each key was a list item, then to operate on that might get a little messy and imperative. Imagine if we stored taxes in an object like this:\n```js\nconst taxes = {\n    GST: 0.05,\n    PST: 0.07\n}\n```\nWe can loop over this with `Object.entries` without problem, but now imagine that we want to map over each entry, and display it in the UI in React. That might look something like this:\n```jsx\nObject.entries(taxes).map(tax =>\n    <>\n        <p>Tax type: {tax[0]}</p>\n        <p>Amount: {tax[1]}</p>\n    </>\n)\n```\n\nAlternatively, imagine we structured taxes like this:\n```js\nconst taxes = [\n    {\n        jurisdiction: 'PST',\n        percentage: 0.05,\n    },\n    {\n        jurisdiction: 'GST',\n        percentage: 0.05,\n    },\n]\n```\n\nAlthough this is less succinct, there is a benefit to doing it this way. This is how we would handle that React task:\n```js\ntaxes.map(tax => {\n    <>\n        <p>Tax type: {tax.jurisdiction}</p>\n        <p>Amount: {tax.percentage}</p>\n    </>\n})\n```\n\n\n\n### Array methods\nAnytime you need to do filter and map on same array, use reduce\n\nThink of reduce as \"transform\", since it can be used to accomplish any data transformation\n\nOperating on arrays offers O(1) search speed if the index is known, but adding/removing an element is slow since size of array cannot change once it's created\n\nforEach is almost always used for side-effects at the end of a chain\n\nWhen manipulating a string (ex. Algorithm) remember that a string is nothing but a character array\nEx. ['c', 'h'...]\n\nOnly declarations are hoisted, not initializations\nEx. 'Const name' is hoisted, 'name = \"Kyle\"' is not.\n\n# Techniques\n### Surgically remove one item from an array\n```js\n    return [\n        ...bigArray.slice(0, itemToRemoveIndex),\n        ...bigArray.slice(itemToRemoveIndex + 1)\n    ]\n```\n\n### Surgically replace one item in an array\n```js\n    return [\n        ...bigArray.slice(0, itemToReplaceIndex),\n        itemToInsert //if item is not object\n        { ...array[itemToReplaceIndex], ...itemToInsert } //if item is object\n        ...bigArray.slice(itemToReplaceIndex + 1)\n    ]\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/1d6ea169-4e13-49ce-a747-ae38bc0bc709.html","relUrl":"notes/1d6ea169-4e13-49ce-a747-ae38bc0bc709.html"},{"doc":"Java","title":"Java","hpath":"java","content":"\n### javac\n- the main compiler included within the JDK\n- converts source code into Java bytecode\n\n# Commands\n### List Java versions installed\n- `/usr/libexec/java_home -V`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1310c9cf-0160-4ad5-aad7-cf355c9313ac.html","relUrl":"notes/1310c9cf-0160-4ad5-aad7-cf355c9313ac.html"},{"doc":"Ios","title":"Ios","hpath":"ios","content":"\n### Podfile\n- holds our ios-written dependencies.","url":"https://tycholiz.github.io/Digital-Garden/notes/76420b75-f886-43ad-a876-c4fd4ad09ff2.html","relUrl":"notes/76420b75-f886-43ad-a876-c4fd4ad09ff2.html"},{"doc":"Cmds","title":"Cmds","hpath":"ios.cmds","content":"\n# iOS\n- open developer menu - `cmd+d`\n- enable on-screen keyboard - `cmd+shift+k`","url":"https://tycholiz.github.io/Digital-Garden/notes/94dceeea-43f4-4210-8f50-aa38c7da9222.html","relUrl":"notes/94dceeea-43f4-4210-8f50-aa38c7da9222.html"},{"doc":"HTTPS","title":"HTTPS","hpath":"https","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3bb25f58-2b50-4fa3-af55-48ea9f88a081.html","relUrl":"notes/3bb25f58-2b50-4fa3-af55-48ea9f88a081.html"},{"doc":"Ssl","title":"Ssl","hpath":"https.ssl","content":"\nTLS (Transport Layer Security) replaced SSL (Secure Sockets Layer), which is deprecated\n\nspec:TLS is an agreement (protocol) between 2 IP addresses (your own and the web server you are connecting to).\n\n### SSL Termination\n- the act of data reaching the end of the SSL chain and getting decrypted (or offloaded) so the recipient can read the data.\n\t- happens at the server end of an SSL connection\n- SSL termination helps speed the decryption process and reduces the processing burden on backend servers.\n![](/assets/images/2021-03-09-09-46-03.png)\n\n### Wildcard SSl Certificate\na single ssl certificate that let's us have SSL on any `*.mydomain.com`\n\n### UE Resources\n- [self-signed certificates](https://medium.com/@jonatascastro12/understanding-self-signed-certificate-in-chain-issues-on-node-js-npm-git-and-other-applications-ad88547e7028)\n\n### E Resources\n- [setting up a private CA](https://www.digitalocean.com/community/tutorials/how-to-set-up-and-configure-a-certificate-authority-ca-on-ubuntu-20-04)","url":"https://tycholiz.github.io/Digital-Garden/notes/b0fe1452-ee8c-4633-8524-fcfa24216b2f.html","relUrl":"notes/b0fe1452-ee8c-4633-8524-fcfa24216b2f.html"},{"doc":"San Certificate","title":"San Certificate","hpath":"https.san-certificate","content":"\n### SAN Certificate (Subject Alternate Name)\n- allows multiple hostnames to be protected by a single certificate.\n\n- SSL Termination - The process of decrypting SSL-encrypted data\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f77750b0-0453-4fdc-a75d-2ae482d07191.html","relUrl":"notes/f77750b0-0453-4fdc-a75d-2ae482d07191.html"},{"doc":"Lets Encrypt","title":"Lets Encrypt","hpath":"https.lets-encrypt","content":"\n# Let's Encrypt\n## Plugins\nCertbot supports 2 types of plugins for obtaining and installing certificates: authenticators and installers\n- some plugins can do both, such as the Apache and Nginx plugins\n\n### Authenticator\n- Authenticators are plugins used with the `certonly` command to obtain a certificate, validating that we own the domain we are requesting a certificate for. It then obtains the certificate for that domain, and places the certificate in the `/etc/letsencrypt` directory on your machine\n\t- The authenticator does not install the certificate (it does not edit any of your server’s configuration files to serve the obtained certificate)\n- If we list multiple domains to authenticate, they will all be included in a single certificate by default.\n\n### Installer\n- Installers are Plugins used with the `install` command to install a certificate.\n- These plugins modify the webserver's configuration in order to server the site over HTTPS \n\n## Certificates\n- All generated keys and certificates can be found on the host that serves the application. \n\t- found in `/etc/letsencrypt/live/$domain` if using Let's Encrypt\n- note: `pem` is a type of encoding\n\n### privkey.pem\nThis is the private key for the certificate \n- This is what Apache needs for `SSLCertificateKeyFile`, and Nginx for `ssl_certificate_key`\n\n### fullchain.pem\nThis is the full list of certificates, including the server certificate (a.k.a Leaf Certificate or End-Entity Certificate)\n- the server certificate is the first one listed. It is followed by intermediary certificates. \n- This is what Apache needs for `SSLCertificateFile`, and what Nginx needs for `ssl_certificate`.\n\n## Concepts\n### ACME\n- ACME is a communications protocol for automating interactions between CAs and their users' webservers.\n\t- This allows automated deployment of public key infrastructure.\n- Certbot is an example of an ACME client\n\n### Challenge\n- Challenges are a way for the Let's Encrypt servers to validate that you own the domain.\n- ex. HTTP-01 Challenge, DNS-01 Challenge\n- We only need one.\n\nHTTP-01\n- The webserver proves it controls the domain by provisioning resources on its filesystem. The ACME server then challenges the webserver to provision a file at a specific path. If the webserver is able to do that, it is proof that the domain is under the webserver's control.\n- When our webserver gets a token from Let's Encrypt, the webserver creates a file at `http://<YOUR_DOMAIN>/.well-known/acme-challenge/<TOKEN>`\n\t- this file also includes a thumbprint of your account key\n- Once our webserver tells Let’s Encrypt that the file is ready, Let’s Encrypt tries retrieving it. If successful in doing so, then we are able to issue the certificate.\n- This is the most common type of challenge.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/bf997d25-108b-4f45-8fe1-071409c078bc.html","relUrl":"notes/bf997d25-108b-4f45-8fe1-071409c078bc.html"},{"doc":"Ca","title":"Ca","hpath":"https.ca","content":"\n## Certificate Authority (CA)\n- the CA is an entity responsible for issuing digital certificates that verify identities on the internet.\n- a CA is an organization that stores public keys and their owners, and every party in a communication trusts this organization (and knows its public key)\n\t- When we navigate to a trusted site, the browser will have already had the public key of the CA, which it can then use to verify the signature, implying that the certificate and its contained public key are trustworthy\n- CA's are used to sign certificates that enable HHTPS\n- In essence, the CA is responsible for saying \"yes, this person is who they say they are, and we, the CA, certify that\"\n- CA servers can manage certificate enrollment requests from customers, and are able to issue and revoke digital certificates\n- CA servers support various certificate templates, such as SSL (both server and client), email signing/encryption, etc\n- to authenticate the recipient of the certificate, HTTPS servers typically use a technique called \"domain validation\"\n\t- This is where the domain name of the applicant is validated by proving that it owns and has control over a DNS domain. After this validation, a Domain Validated Certificate (DV) is issued.\n- A CA issues digital certificates that contain a public key and the identity of the owner\n\t- the certificate being issued is confirmation (by the CA) that the public key within the certificate belongs to the entity noted in the certificate.\n\t- spec: the CA has the private key\n- the CA market is very fragmented, and government-related activities (govt forms that verify identity) will have their own CA. For commercial use, the CA market is dominated by a few players (Let's Encrypt, GoDaddy, VeriSign are two of them)\n- ex. when using DigitalOcean managed Postgres, we are given a `ca-certificate.crt` from DigitalOcean. This is the certificate we must pass from our application server. The reason this works, is because the certificate is signed by one of the CAs that is trusted by the DigitalOcean server.\n\n### Chain of Trust\n- example chain:\nGlobalSign → Google CA → neverforgetapp.co\n\n- The idea of Chain of Trust is that the Root CA can \"trust\" other Intermediate CAs (ICA) to issue certificates (which are signed by the Root CA). Though the ICAs themselves are not trustworthy, they are trusted by a trustworthy entity (the Root CA), so the certificates they issue are also considered trustworthy.\n- The Root CA is kept behind many layers of security\n\t- if private keys are compromised, then all certificates based on the Root CA are compromised as well. For this reason, we use an Intermediate CA\n- In the browser, if we click on the lock to the left of the URL, we can see the certificate chain \n\t- The one at the bottom (leaf node) of the tree is the SSL certificate (`*.neverforgetapp.co`), which was issued by the ICA directly above it (`Google CA`)\n\t\t- The SSL certificate is signed with the private key of the ICA (`Google CA`). Once this is done, it is sent back to `neverforgetapp.co`.\n\t\t- All certificates contain the public key of the entity that the certificate is for, and the private key of the entity that signed the certificate.\n\t\t\t- ex. the SSL certificate (the end of the chain) contains the public key of `neverforgetapp.co`, and is signed by Google CA; Google's CA certificate contains Google's public key and is signed by GlobalSign.\n\t- In the browser's Certificate Manager, we can see that GTS CA 101 is not listed as a trusted CA. Therefore, this Intermediate CA needs to be verified by a Root CA, which it is (GlobalSign). In the Certificate Manager, we can see that this is indeed trusted. This is the chain of trust. \n- In our example chain, there are 2 certificates issued: the Google ICA certificate, and the SSL certificate (to `neverforgetapp.co`). These both get sent to `neverforgetapp.co` and are installed on the webserver.\n\t- When someone visits `neverforgetapp.co` with HTTPS, both certificates are sent back to the browser. It starts by verifying the certificate that is higher up in the chain (closer to the root), which is the Google ICA certificate. It sees that GlobalSign has signed this certificate, whom it recognizes as a trusted Root CA.\n\t\t- The browser has the public keys (and the certificates, of which they are a part) of all Root CAs, so it can unencrypt the Google ICA certificate. This proves that this ICA is trustworthy.\n\t\t- the browser trusts all Root CAs in its list. The fact that a Root CA in that list has signed the Google ICA certificate means that the browser automatically trusts that ICA. \n\t\t\t- As a consequence of unecrypting the certificate, the browser gets the public key from Google ICA. \n\t\t- Now that Google ICA has been considered trustworthy, the browser then looks to the `neverforgetapp.co` SSL certificate. It uses the public key issued by Google ICA to decrypt the certificate. It sees that Google ICA has signed this certificate, making it trustworthy.\n\t\t- Now, the browser gets the public key from `neverforgetapp.co`, generates a symmetric key, encrypts it (using the public key), and sends it to the `neverforgetapp.co` webserver.\n\t\t\t- since the `neverforgetapp.co` webserver has the private key, it is able to decrypt it, which yields the same symmetric key. Now, both the browser and the `neverforgetapp.co` webserver have the same symmetric key that is used to exchange information securely.\n\n### Private CA\n- There are 2 versions, public and private CA\n\t- public are for verifying the identity of websites and other services that are provided to the general public\n\t- private are for closed groups and private services.\n- spec:While in production we will use a public CA, we may want to configure a private CA in development and staging environments so those environments match the production environment, and also have HTTPS connections. \n- we need to build a private CA when our programs require encrypted connections between client and server.\n\t- with the private CA, we can issue certificates to users, servers, or individual programs and services within your infrastructure.\n\t\t- ex. OpenVPN would use its own private CA\n\t\t- ex. we can configure our web server to use certificates issued by a private CA to make development and staging environments match production servers that use TLS to encrypt connections.\n\n### Bank Example\nWhen a user logs into an HTTPS enabled bank, they receive a public key. The public key is used to create a temporary shared symmetric encryption key, which is used to encrypt all messages sent from the client to the server. These messages are enciphered with the bank's public key in such a way that only the bank has the private key required to decrypt them. For the rest of the session, the (disposable) symmetric key is used to encrypt messages from client to server.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/476bc675-229b-445e-8fad-a43abc39de46.html","relUrl":"notes/476bc675-229b-445e-8fad-a43abc39de46.html"},{"doc":"HTTP","title":"HTTP","hpath":"http","content":"\n### Credentials policy\n*Only relevant to browsers*\n- Credentials are cookies, authorization headers or TLS client certificates.\n\nCredentials\n- `include` - send a request with credentials included, even if it's cross-origin\n- `same-origin` - only send credentials if the request URL is on same origin as the calling script\n- `omit` - ensure browsers don’t include credentials in the request\n\n- we can use the `Access-Control-Allow-Credentials` response header to tell browsers to expose the response to frontend JavaScript code.\n\t- to do this, the client will need to set the credentials to `include` (`Request.credentials`)\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/68bea64d-8840-4e0d-aa52-033ae3ee4db2.html","relUrl":"notes/68bea64d-8840-4e0d-aa52-033ae3ee4db2.html"},{"doc":"Webdav","title":"Webdav","hpath":"http.webdav","content":"\n# WebDAV\n- an extension of HTTP\n\t- Therefore, gets all of the benefits that HTTP offers, such as encryption, if HTTPS\n\t- Also can use HTTP tools like cURL\n- Since it is an extension of HTTP, it gets access to HTTP verbs. Additionally, it extends these base verbs, giving additional functionality\n\t- ex. COPY, MOVE, MKCOL (make collection, aka directory)\n- a protocol that allows us to create, update, and move documents on a server.\n\t- these are known as *remote web content authoring operations*\n- WebDAV provides a coherent set of methods and headers, and has a system like Express that involves request and response objects\n- ex. perform CRUD operations on information\n   about Web pages, such as their authors, creation dates, etc.\n- The WebDAV protocol enables a webserver to behave like a fileserver","url":"https://tycholiz.github.io/Digital-Garden/notes/017d767e-0127-4ef8-b5bf-7616ad5a7a9a.html","relUrl":"notes/017d767e-0127-4ef8-b5bf-7616ad5a7a9a.html"},{"doc":"Requests","title":"Requests","hpath":"http.requests","content":"\n## HTTP Requests\n### GET vs. POST\n- there are some limitations to GET requests as follows:\n    - GET requests can be cached\n    - GET requests remain in the browser history\n    - GET requests can be bookmarked\n    - GET requests should never be used when dealing with sensitive data\n    - GET requests have length restrictions\n    - GET requests should be used only to retrieve data\n- Some + points on POST requests:\n    - POST requests are never cached\n    - POST requests do not remain in the browser history\n    - POST requests cannot be bookmarked\n    - POST requests have no restrictions on data length","url":"https://tycholiz.github.io/Digital-Garden/notes/9fe0ede4-885d-4380-b1b1-7a3a84184e7e.html","relUrl":"notes/9fe0ede4-885d-4380-b1b1-7a3a84184e7e.html"},{"doc":"Cors","title":"Cors","hpath":"http.cors","content":"\n# Cross-Origin Resource Sharing (CORS)\n- a mechanism to use HTTP headers to tell browsers to give a web application running at one origin, access to selected resources from a different origin\n\t- a cross-origin HTTP request is executed when it requests a resource that has a different origin (domain, protocol, or port) from its own\n- ex. the front-end JavaScript code served from `https://domain-a.com` uses XMLHttpRequest to make a request for `https://domain-b.com/data.json`\n\nyour server should enable the cross origin requests, not the client\n- [Adding CORS support to server](https://enable-cors.org/server.html)","url":"https://tycholiz.github.io/Digital-Garden/notes/7813ad76-5620-4b0b-9062-3cd8f318007a.html","relUrl":"notes/7813ad76-5620-4b0b-9062-3cd8f318007a.html"},{"doc":"Hosting","title":"Hosting","hpath":"hosting","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5f61c751-7930-497f-a705-93b3bfbb4898.html","relUrl":"notes/5f61c751-7930-497f-a705-93b3bfbb4898.html"},{"doc":"Vps","title":"Vps","hpath":"hosting.vps","content":"\n## VPS Hosting vs Shared Hosting\n### Shared Hosting\n- share space on a server along with other websites.\n- best utilized for static sites.\n\n### VPS Hosting\n- have superuser access to system\n- for practical purposes, they are functionally equivalent to a dedicated physical server.\n- underlying hardware is shared with other VPSes, which may result in lower performance \n- the virtualization aspect provides a high level of security.\n- Because each virtual server is isolated from the others, it can run its own OS\n- the physical server runs a hypervisor, which creates, manages, and allocates resources to the guest OSes (VMs)","url":"https://tycholiz.github.io/Digital-Garden/notes/4e90f147-9f26-4d43-8ed1-f666852ca829.html","relUrl":"notes/4e90f147-9f26-4d43-8ed1-f666852ca829.html"},{"doc":"Hiring","title":"Hiring","hpath":"hiring","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d0147e41-fa29-4727-aba5-f21d4e9eec69.html","relUrl":"notes/d0147e41-fa29-4727-aba5-f21d4e9eec69.html"},{"doc":"Interviewee","title":"Interviewee","hpath":"hiring.interviewee","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/696e4dad-afc7-4cef-be76-ec49887fbf36.html","relUrl":"notes/696e4dad-afc7-4cef-be76-ec49887fbf36.html"},{"doc":"Software","title":"Software","hpath":"hiring.interviewee.software","content":"\n\"I'm a fullstack developer. I'm not trying to be the best at any one technology. I'm not trying to be best React developer, or the best SQL database admin. Rather I'm trying to be effective in a large amount of areas. My goal is to be able to go in and get things done in a reasonable amount of time. This doesn't mean that I am not eager to become proficient with any given technology, but I am more inclined to allow those growth opportunities to come passively, rather than purposefully seeking them out. For instance, if I've made a PR that does some React work, there is a good chance that someone who knows React better than I do will PR it, and give me good feedback which will allow me to grow. This lets me leverage the time they've spent to become proficient in React, and learn these concepts in a fraction of the time. My familiarity with the problem allows me to more easily understand the new proposed solution. I presumably already had run into some scenarios where things just did not feel right\".\n\nI'm passionate about documentation and good design","url":"https://tycholiz.github.io/Digital-Garden/notes/ca0e3871-5876-4860-a44a-100e15c0c1ca.html","relUrl":"notes/ca0e3871-5876-4860-a44a-100e15c0c1ca.html"},{"doc":"Hardware","title":"Hardware","hpath":"hardware","content":"\n### What happens when we run an executable?\n1. as we type the characters `./hello` into the shell program, each letter is read into a register (in the CPU), and then stores it in memory\n2. When we hit `<enter>`, the shell loads the executable `hello` file by executing a sequence of instructions that copies the code and data in the `hello` object file from disk to main memory (included is the string of characters `\"hello, world\\n\"`)\n3. the data then travels directly from disk to main memory, without passing through the processor (a technique known as *direct memory access*)\n5. Once the code and data of the `hello` file are loaded into memory, the processor begins executing machine-language instructions in the `hello` program's `main` function.\n\t- These instructions copy the bytes in the `\"hello, world\\n\"` string from memory to the register file (CPU), and from there to the display device (monitor).","url":"https://tycholiz.github.io/Digital-Garden/notes/67265fd9-7fbd-4bf7-b3c9-ced802b8a830.html","relUrl":"notes/67265fd9-7fbd-4bf7-b3c9-ced802b8a830.html"},{"doc":"Ram","title":"Ram","hpath":"hardware.ram","content":"\n### Main memory (RAM)\n- the temporary storage device that holds both a program and the data it manipulates while the processor is executing the program.\n- physically, main memory consists of a collection of DRAM chips.\n- logically, memory is organized as a linear array of bytes, each with its own array index starting at 0.\n- ex. Imagine every spot of RAM having its own address. \n\t- If your computer is 32 bit (4,294,967,296), then our computer can't talk to any address that is higher than that. \n\t\t- This much memory is commonly known as 4gb.","url":"https://tycholiz.github.io/Digital-Garden/notes/609a4c09-80ee-45c3-80fb-3ad44caad709.html","relUrl":"notes/609a4c09-80ee-45c3-80fb-3ad44caad709.html"},{"doc":"Interfaces","title":"Interfaces","hpath":"hardware.interfaces","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1afb5b6f-6d69-44cc-bf96-102e7a673e2d.html","relUrl":"notes/1afb5b6f-6d69-44cc-bf96-102e7a673e2d.html"},{"doc":"Serial Port","title":"Serial Port","hpath":"hardware.interfaces.serial-port","content":"\nWith this type of port, information transfers in or out sequentially one bit at a time\n![](/assets/images/2021-03-20-19-15-42.png)\n\nThroughout most of the history of personal computers, data has been transferred through serial ports to devices such as modems, terminals, various peripherals, and directly between computers.\n- Modern consumer PCs have largely replaced serial ports with higher-speed standards, primarily USB\n- serial ports are still frequently used in applications demanding simple, low-speed interfaces, such as industrial automation systems, scientific instruments, point of sale systems and some industrial and consumer products.\n- Server computers may use a serial port as a control console for diagnostics\n\n## UE Resources\n[Connect Pi to Arduino with Serial Ports (and using Node's serialport.js)](https://medium.com/@machadogj/arduino-and-node-js-via-serial-port-bcf9691fab6a)","url":"https://tycholiz.github.io/Digital-Garden/notes/2675ac4b-80c0-4545-b6a8-5ad44dd1ceba.html","relUrl":"notes/2675ac4b-80c0-4545-b6a8-5ad44dd1ceba.html"},{"doc":"Parallel Port","title":"Parallel Port","hpath":"hardware.interfaces.parallel-port","content":"\nmultiple bits transfer in or out simultaneously\n\nnetworking hardware (such as routers and switches) commonly use serial console ports for configuration, diagnostics, and emergency maintenance access\n- Today, the parallel port interface is virtually non-existent because of the rise of Universal Serial Bus (USB) devices\n\nContrasted with [[Serial Ports|hardware.interfaces.serial-port]]","url":"https://tycholiz.github.io/Digital-Garden/notes/5d07ec8a-5072-4f86-aabd-95e10778253a.html","relUrl":"notes/5d07ec8a-5072-4f86-aabd-95e10778253a.html"},{"doc":"Cpu","title":"Cpu","hpath":"hardware.cpu","content":"\n# CPU\n- CPU is the engine that interprets (or executes) instructions that are stored in main memory.\n- at its core, there is a word-sized storage device (or register) called the program counter (PC).\n\t- At any given point in time, the PC points at (contains the address of) some machine-language instruction in main memory.\n- From the time the system is powered on until it is powered off, the processor blindly and repeatedly performs the same basic task over and over again:\n\t1. reads the instruction from memory pointed at by the PC\n\t2. interprets the bits in teh instruction\n\t3. performs some simple operation, as per the instruction\n\t4. updates the PC to point to the next instruction (which may or may not be in contiguous memory) \n- Different CPU architectures understand different instruction types, which is why we can't take a program from one architecture (ex. x86) and run it on another (ex. ARM)\n\n- A socket is the physical socket where the physical CPU capsules are placed\n- Cores are the number of CPU-cores per CPU capsule\n- some CPUs can run more than one parallel thread per CPU-core\n- If you multiply the number of socket, cores and threads, then you get the number of \"CPUs\": 24.\n\t- These aren't real CPUs, but the number of possible parallel threads of execution your system can do.\n\n\nThe CPU reads instructions in, does the instruction, and then reads the next instruction. These instructions need addresses (RAM address) to know where to go to get data, or put data. But it also needs to know what to do with that data. These two parts compose a basic \"instruction\", usually divided into op-codes(operation code) and addresses.\n- So a 32 bit CPU has 32 bits for an instruction, while a 64 bit CPU has 64 bits to hold an instruction. This means we can have many more opcodes, and also have many more places in memory we can address.\n- ex. Let's say numbers 1 through 20 are addresses in RAM. If we have, for example, a 3 bit processor, an instruction would look like so: S-1-2 (something like subtract 1 from 2, assuming S is subtract op-code). Now what if we want to subtract something from memory address 17? We can't, we don't have enough room, since we only have 3 bits(1 for opcode, one for first address, one for second address). There are ways to work around this but for simplicity, it's impossible.\n\n### Hardware Acceleration","url":"https://tycholiz.github.io/Digital-Garden/notes/62089692-4caf-4303-90d0-4b18648d5efd.html","relUrl":"notes/62089692-4caf-4303-90d0-4b18648d5efd.html"},{"doc":"Register","title":"Register","hpath":"hardware.cpu.register","content":"\n# Register\na quickly accessible storage available to the computer's processors\nmost computers load data from a larger memory into registers where it is used for arithmetic operations\n- manipulated data is then stored as RAM\nregisters are at the top of the memory hierarchy, meaning they provide the fastest access to data\nregisters measure how much data they can hold in terms of bits (ex. 64-bit register)\n- ARM instructions operate only on registers with a few instructions for loading and saving data from / to memory while x86 can operate directly on memory as well.\n- when discussing 32bit or 64-bit architectures, we are referring to the size of the register\n","url":"https://tycholiz.github.io/Digital-Garden/notes/4b239f2c-9dd1-422b-9c9b-5e845552592a.html","relUrl":"notes/4b239f2c-9dd1-422b-9c9b-5e845552592a.html"},{"doc":"Clock","title":"Clock","hpath":"hardware.cpu.clock","content":"\n# Clock Generator\nCPUs have electronic clocks in them, in fact it’s a fundamental part of how they operate as the electronics within a CPU have to operate in a synchronised way.\n\nThe clock is a crystal that oscillates a predictable number of times each second when electricity is passed through it. Counting these oscillations allows the computer to measure the passing of time.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/6f3560cc-264d-4b60-88d3-850312749a9f.html","relUrl":"notes/6f3560cc-264d-4b60-88d3-850312749a9f.html"},{"doc":"Architecture","title":"Architecture","hpath":"hardware.cpu.architecture","content":"\n# CPU Architecture\n## 32 bit vs 64 bit architecture\n- a memory address that's 32 bits long can only refer to 4.2 billion unique locations (i.e. 4 GB).\n- anal: we can count on our fingers up to 10 in the decimal system. Since computers are binary, a bit is the number of \"digital fingers\" that the computer can count on.\n\t- ex. If I said a computer has 4 bits, the biggest number I could put in it is 4 places long, or 1111.\n- the biggest number a 32 bit system could handle is `11111111111111111111111111111111` (4,294,967,296)\n\t- this would be the theoretical highest number that our computer could do calculations with (though there are workaround tricks that are slower)\n\t- The practical use of this \"largest number\" is in determining how much memory we can have.\n- At a high level, a higher bit CPU allows the machine to run faster by having advanced operations supported by opcodes(rather than using simple opcodes to build up to a complex algorithm) and utilize more memory by having access to more address spaces(it can save stuff in RAM instead of hard-disk, which is slower than RAM, even if it is flash memory also).\n- ex. In legacy versions of Excel, the max number of rows is 65,537, which corresponds to 16 bits (`1111111111111111`)\n- ex. in the original zelda, the max number of rupees was 255, which corresponds to 8 bits (`11111111`)\n- anal: imagine a paper letter. More bits means more possible addresses, which would correspond to a bigger envelope. The other side of the equation is how much RAM we have. Going from a 32-bit system to a 64-bit system would be like going from a standard sized envelope to an envelope that wraps around the world 3 times (since: 2<sup>64</sup> is more than 4 billion times larger than 2<sup>32</sup>).\n- a 32-bit system would only be able to understand 4GB worth of RAM, making any RAM beyond that amount redundant on a 32-bit system.\n\t- This is not entirely true because of the existence of PAE (Physical Address Extension), which allows a 32-bit system access more than 4GB of RAM. However it is still true that individual programs are limited to the 4GB.\n\n## ARM vs. x86\nARM and x86 are 2 different families of CPU architecture\n- ARM is more efficient than regular x86 processors, which is made possible by a simplified instruction set, and having a stripped down hardware.\n- unlike x86, ARM wasn't made with backward compatability in mind, so it doesn't repeat a lot of the inefficient designs that were made by its antecedents.\n- known as *reduced instruction set computing* (RISC), while x86 uses *complex instruction set* (CISC)\n- anal: imagine that we have 2 general-purpose factories that can make a wide array of things: the x86 Factory, and the ARM Factory\n\t- **x86 Factory** - There is a receptionist at the front that receives mail (requests) one at a time. Each time he gets a piece of mail, he passes it on to a group of people who open (decode) the messages to figure out what action should be taken. Each person in this group can figure out his own request, and figure out where it should go next. Also, these requests can vary wildly in size and purpose. For instance, one request might be \"take data from this warehouse and put it in bucket X (load to register)\". Another might be \"take data from this warehouse and put it on the conveyor to this trucking station (bus to I/O)\". Another might be \"take 3 different pieces of data from the warehouse, add them together, then put them back in a different spot of the warehouse\". Since each request varies in the amount of time is takes to process, the factory gets more efficient if more people are hired to handle these requests. This is why this type of factory can be so expensive: to get more efficient, more decoders must be hired.\n\t- **ARM Factory** - There is a receptionist at the front that receives mail (requests) **and** opens them. These requests differ from the ones accepted by the x86 Factory, in that they are more atomic and simpler to understand. The implication of this is that the receptionist knows exactly which department to send the request to, without needing another department of decoders to figure everything out. One type of request the receptionist might receive is \"load this one piece of data from the warehouse into this bucket\". Another might be \"add these two buckets together and store the result in the first bucket\". However, this means that the compiler (which converts the program to machine code) needs to write out all of these smaller instructions. Instead of the x86 method of decoding large instructions into smaller ones on the fly, the ARM method is to do it all ahead of time.\n- Since the ARM architecture has less hardware doing things, it is a lot cheaper to design and make, and is more energy efficient.\n- Originally, ARM was meant for small embedded systems. This meant that there was little incentive to improve performance. Since ARM chips have expanded to phones, performance is a more recent focus of ARM chips.","url":"https://tycholiz.github.io/Digital-Garden/notes/b798b442-d719-4b50-bfae-94ccb6a74d11.html","relUrl":"notes/b798b442-d719-4b50-bfae-94ccb6a74d11.html"},{"doc":"Circuit Board","title":"Circuit Board","hpath":"hardware.circuit-board","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1f518d9d-ec2d-47b1-9cc6-1027835d5e67.html","relUrl":"notes/1f518d9d-ec2d-47b1-9cc6-1027835d5e67.html"},{"doc":"Transistor","title":"Transistor","hpath":"hardware.circuit-board.transistor","content":"\n### Transistor\n- amplifies charge.\n- transistors have 3 terminals\n- transistors can regulate current or voltage flow while also acting as a switch for signals of the electronic variety\n- Semiconductors make up three layers of a transistor.\n\t- Therefore, each layer can carry current\n","url":"https://tycholiz.github.io/Digital-Garden/notes/9cc988d3-7882-4ebf-a12c-6a5dcfb11a66.html","relUrl":"notes/9cc988d3-7882-4ebf-a12c-6a5dcfb11a66.html"},{"doc":"Transformer","title":"Transformer","hpath":"hardware.circuit-board.transformer","content":"\n### Transformer\n- converts high AC voltage like 230VAC to 24VAC low voltage to be used by bridge rectifier (aka diode bridge)","url":"https://tycholiz.github.io/Digital-Garden/notes/7b22e6c3-ac6a-43fe-b2e2-c34b3f5a1aab.html","relUrl":"notes/7b22e6c3-ac6a-43fe-b2e2-c34b3f5a1aab.html"},{"doc":"Resistor","title":"Resistor","hpath":"hardware.circuit-board.resistor","content":"\n### Resistor\n- control the electric current as it passes through them. \n\t- The colour-coded lines are used to determine their value.\n- resistors have many uses including: \n\t1. reduce current flow\n\t2. adjust signal levels\n\t3. divide voltages. This happens when resistors are lined up in series with each other. \n- resistors dissipate electrical power as heat\n\t- For bigger applications like a motor, this is exactly what they are used for.\n![](/assets/images/2021-03-22-22-24-31.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/056264aa-dcca-467a-be25-e9c3dfb1edf5.html","relUrl":"notes/056264aa-dcca-467a-be25-e9c3dfb1edf5.html"},{"doc":"Microchip","title":"Microchip","hpath":"hardware.circuit-board.microchip","content":"\n### Microchip (Integrated Circuit)\n- IC is contrasted with Discrete Circuit (such as a PCB)\n- an IC is a set of electronic circuits on one small flat piece (or \"chip\") of semiconductor material (normally silicon)\n- The integration of large numbers of tiny MOS transistors into a small chip results in circuits that are orders of magnitude smaller, faster, and less expensive than those constructed of discrete electronic components.\n- ICs can be seen as a building block approach to integrated circuit design \n\t- This has resulted in discrete transistors being far more rare on the PCB itself\n- a modern chip (year 2020) may have many billions of MOS transistors in an area the size of a human fingernail\n","url":"https://tycholiz.github.io/Digital-Garden/notes/667e0bd1-0cf8-4034-ac0e-6416f891ee7c.html","relUrl":"notes/667e0bd1-0cf8-4034-ac0e-6416f891ee7c.html"},{"doc":"Diode","title":"Diode","hpath":"hardware.circuit-board.diode","content":"\n### Diode\n- Diodes only allow current to flow in a single direction\n\t- This is done by having zero resistance in one direction, and very high resistance in the other.\n- Diodes have 2 terminals, each of which is an electrode: an *anode* and a *cathode*\n\t- anodes allow current to flow in from outside a circuit\n\t- cathodes allow current to flow out of a polarized device.\n![](/assets/images/2021-03-22-22-26-11.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/7723b3be-dae2-40f2-9268-0a689b5c8ec1.html","relUrl":"notes/7723b3be-dae2-40f2-9268-0a689b5c8ec1.html"},{"doc":"Capacitor","title":"Capacitor","hpath":"hardware.circuit-board.capacitor","content":"\n### Capacitor\n- basically a tiny battery\n- potentially carries just enough energy to keep something like memory chip running for a few second\n\t- This is why when we restart devices to try and fix an issue, we need to leave it off for about 10 seconds. That timespan will ensure that the capacitors are fully drained and the memory chips are all wiped. \n![e0122960d37020f2ce639a7495f0bb15.png](:/f350c9ead44241b5b40df7a983279d93)\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/4ab65793-b9f5-4918-9fc9-3e136d83060f.html","relUrl":"notes/4ab65793-b9f5-4918-9fc9-3e136d83060f.html"},{"doc":"Bus","title":"Bus","hpath":"hardware.bus","content":"\n# Bus\n- A bus carries byes of information back and forth between the hardware components of a computer system (or even between computers, if connected physically)\n- Buses are normally designed to transfer fixed-sized chunks of bytes known as *words*. The number of bytes in a word (word size) is a fundamental system parameter that varies across different systems.\n\t- ex. word size could be 4 bytes long, while others could be 8. Typically smaller systems (such as embedded controllers) have word sizes of 1 or 2.\n\n### I/O Device\n- An I/O device is the system's connection to the outside world\n- ex. keyboard and mouse for input, monitor for output, and disk drive for long-term data storage.\n- Each I/O device is connected to the I/O bus by either a controller or adapter.","url":"https://tycholiz.github.io/Digital-Garden/notes/d0f14e71-8e8b-4b7e-80a4-239cdd6a6808.html","relUrl":"notes/d0f14e71-8e8b-4b7e-80a4-239cdd6a6808.html"},{"doc":"Alu","title":"Alu","hpath":"hardware.alu","content":"\n# Arithmetic Logic Unit (ALU)\n- made up of the Arithmetic Unit and the Logic Unit \n\t- Arithmetic handles numeric operations, like add/subtract, increment\n- the numbers operated on are in binary format ","url":"https://tycholiz.github.io/Digital-Garden/notes/8d5e33ab-f7f5-4390-8964-13ae3d9fdae0.html","relUrl":"notes/8d5e33ab-f7f5-4390-8964-13ae3d9fdae0.html"},{"doc":"Graphql","title":"Graphql","hpath":"graphql","content":"\n### UE Resources\n[Graphql concepts visualized](https://www.apollographql.com/blog/the-concepts-of-graphql-bc68bd819be3/#.hfczgtdsj)\n[security (preventing DDOS)](https://www.apollographql.com/blog/securing-your-graphql-api-from-malicious-queries-16130a324a6b/)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/55a2161d-d415-4fbd-98cb-ff8aed19e6ee.html","relUrl":"notes/55a2161d-d415-4fbd-98cb-ff8aed19e6ee.html"},{"doc":"Variables","title":"Variables","hpath":"graphql.variables","content":"\nTechnically, we could do string interpolation to make our queries dynamic. However, this wouldn't be a good idea because our client would be tasked with manipulating that string at runtime. Instead, Graphql provides us with first-class variables that allow us to factor-out the dynamic parts of a query (eg. userId, first: 10)\n\nVariables must be either scalars, enums or input object types.\n- Input Object Type: Passing in an object might be a more sensible solution than strings, if there is related data. For instance, imagine we are running a `createUser` mutation. Instead of passing `first_name`, `last_name`, as so on to the mutation, why not just define a type, and pass an object?:\n```\nmutation createUser($userInfo: UserInfoInput!) {\n\tcreateUser(userInfo: $userInfo)\t{\n\t\tfirst_name\n\t\tlast_last\n\t}\n}\n```\n","url":"https://tycholiz.github.io/Digital-Garden/notes/14ebe4e7-f6f4-4a5f-9e8e-efd02827b8cc.html","relUrl":"notes/14ebe4e7-f6f4-4a5f-9e8e-efd02827b8cc.html"},{"doc":"Directives","title":"Directives","hpath":"graphql.variables.directives","content":"\n# Directives(`@`)\n- Using variables, Directives let us dynamically change the structure and shape of our queries.\n- This allows us to either `@include` or `@skip` over fields.\n- ex. In Never Forget, consider the BrowseNugget screen. Our list component allows us to switch between just showing the nugget name, and showing a summary of the nugget. To determine whether or not we want the summary, we can use a variable:\n```\nquery Nugget($details: details) {\n\tnugget {\n\t\ttitle\n\t\tmediaItems @include(if:  $details) {\n\t\t\ttitle\n\t\t}\n\t}\n}\n```\nDirectives are useful when we find ourselves needing to do string manipulation in order to modify (ie. add/remove fields from) our query structure.\n\nTo be Graphql spec-compliant, a server must implement only the `@include` and `@skip` directives. This leaves each server implementation open to extending their own directives.\n\n### Use with Inline Fragments\nDirectives can also be applied with inline fragments like so:\n```\nquery inlineFragmentNoType($expandedInfo: Boolean) {\n  user(handle: \"zuck\") {\n    id\n    name\n    ... @include(if: $expandedInfo) {\n      firstName\n      lastName\n      birthday\n    }\n  }\n}\n```\n\n### Deprecation\nFields in an object may be marked with the `@deprecated` directive like so:\n```\ntype ExampleType {\n  oldField: String @deprecated\n}\n```\n\nThese fields can still be queried (to prevent breaking changes).","url":"https://tycholiz.github.io/Digital-Garden/notes/3c2327df-b246-4cff-b2f4-ca8f2e929412.html","relUrl":"notes/3c2327df-b246-4cff-b2f4-ca8f2e929412.html"},{"doc":"Types","title":"Types","hpath":"graphql.types","content":"\n# Types\nIn the same way a schema defines the shape of the total response, the type of an individual field defines the shape of that field's value.\n- The shape of the data we return in our resolver must likewise match this expected shape. When it doesn't, we frequently end up with unexpected nulls in our response.\nGraphQL is based on its type system, and depending on what type the GraphQL server receives, it will determine what it should do next.\n- If we are trying to execute a query to get all nuggets, then the GraphQL server will see we are querying an object (nuggets). Since it is an object (and therefore not a scalar, which would end the execution cycle), GraphQL server knows that it needs to \"fetch\" those fields on the object. It then checks the type of each field. If they are non-scalar, then it checks for more data. This continues until every field is a scalar type. \n\n# User-defined Types\nIn GraphQL we deal with 8 different types:\n- scalars, enums, lists, objects, input objects, interfaces, unions, non-null\n\n## Scalar\nScalar types are:\n1. Int\n2. Float\n3. String\n4. Boolean\n5. ID\n\nA GraphQL object type has a name and fields, but at some point those fields have to resolve to some concrete data. These basic types of data are called scalars, and they are the \"atomic type\" of GraphQL\n- At each type, graphql will keep resolving until it gets to the scalar\n\nScalar types resolve to a scalar object. By definition, they cannot have sub-selections (fields) in the query.\n\nAll Graphql scalars are representable as strings\n\nGraphql provides built-in scalars, but type systems can add additional scalars\n- ex. Imagine making a scalar called `Time`, which we define as conforming to ISO-8601. When we query a field of type `Time`, we can rely on the ability to parse the result with an ISO-8601 parser. On the client, we can use a primitive (such as Date) to represent the value.\n- ex. Imagine a scalar called `Url`. It would still be serialized as a string, but would be guaranteed by the server to be a valid URL.\n\n### ID\nID is a wrapped type of kind `NON_NULL`\n\n## List\nRepresents are sequence of values in Graphql.\n- A list type is a *type modifier*, meaning it wraps another type, found in the `ofType` field. This field defines the type of each item in the list.\n\nWe can request paginated data from list fields with the `first` argument\n\n## Object\nWhile scalars represents the leaf values of a query hierarchy, objects represent the intermediate levels.\n\nRepresent concrete instantiations of sets of fields. the introspection types (e.g. `__type`, `__field`, etc) are examples of objects.\n\n## Input Object\nA composite type used for inputs into queries, defined as a list of named input values.\n- often, an update and create operation on a db record will take in the same inputs (ex. title, mediaitems...).\n- an input type is simply a type that includes all of these same inputs so we can reuse it in multiple places.\n- input types can't have fields that are other objects, only basic scalar types, list types, and other input types.\n```\ntype nugget {\n\tid: id!\n\ttitle: string\n\tmediaitems: json\n}\n\ninput nuggetinput {\n  title: string\n  mediaitems: json (?)\n}\n\n// then...\n\ntype mutation {\n  createnugget(input: nuggetinput): nugget\n  updatenugget(id: id!, input: nuggetinput): nugget\n}\n\n```\n\n## Null\na null result on a Non-Null type bubbles up to the next nullable parent. If this bubbling never stops because everything is of Non-Null type, then the root data field is null.\n- in other words, if we are trying to query a field that is `NONNullable` and the result happens to be `null`, that `null` will bubble, and the parent \"object\" will result in null. If however the field we are querying is `nullable` and the result happens to be `null`, it will remain at that field and not bubble\n- When fields are nullable, you still get partial data: even if you don't have the name, you can still show the age and other fields. When you mark fields as Non-Null, as in the second example, you forfeit your right to partial data.\n[source](http://spec.graphql.org/June2018/#sec-Errors)\n\n## Interface\nSimilar to how fragments allow us to DRY our queries, interfaces can DRY our type definitions.\n\nInterfaces represent a list of named fields and their arguments. These interfaces can then be implemented by objects.\n\n## Union\nUnions represent an object that could be one of many specified types. However, there is no guarantee that any of the fields on each of those types will be provided (whereas interfaces guarantee that a field will be available)\n\nUnlike interfaces, Unions do not implement fields of their own\n\nThey differ from interfaces in that Object types declare what interfaces they implement, but they are not aware of what unions contain them.\n\n```\nunion SearchResult = Photo | Person\n\ntype Person {\n  name: String\n  age: Int\n}\n\ntype Photo {\n  height: Int\n  width: Int\n}\n\ntype SearchQuery {\n  firstSearchResult: SearchResult\n}\n```\n\n## Non-null\nGraphql fields are nullable, making `null` a valid response for field type.\n- Like lists, a non-null type is a type modifier (it wraps another type instance in the `ofType` field).","url":"https://tycholiz.github.io/Digital-Garden/notes/4915ac5e-b823-42e1-83f6-6226b9b93616.html","relUrl":"notes/4915ac5e-b823-42e1-83f6-6226b9b93616.html"},{"doc":"Tools","title":"Tools","hpath":"graphql.tools","content":"\n# Client-side\n[Periqles: Generate a React form from a Graphql mutation](https://github.com/oslabs-beta/periqles)\n\n# Server-side\n## The Guild Libraries\n[Graphql-Tools: Graphql-first mentality to building Graphql Schema](https://www.graphql-tools.com/)\n- graph-tools provides a thin convenience layer on top of graphql.\n- we can get schema validation\n- biggest benefit of using graphql-tools is its nice API for connecting your declarative schema with resolvers\n\n[Graphql-Inspector: Maintain and improve Graphql API thru validation (+receive notifications) (runs in CI/CD pipeline)](https://graphql-inspector.com/)\n\n[Graphql-Scalars: A library of custom scalar types, ready to be imported](https://github.com/Urigo/graphql-scalars)","url":"https://tycholiz.github.io/Digital-Garden/notes/0cd5ff13-57e6-4885-b681-3deef4e8cc68.html","relUrl":"notes/0cd5ff13-57e6-4885-b681-3deef4e8cc68.html"},{"doc":"Testing","title":"Testing","hpath":"graphql.testing","content":"\n### UE Resources\n[mocking graphql](https://graphql.org/blog/mocking-with-graphql/)","url":"https://tycholiz.github.io/Digital-Garden/notes/410bc914-ffde-461e-a7ab-451ef6ec0247.html","relUrl":"notes/410bc914-ffde-461e-a7ab-451ef6ec0247.html"},{"doc":"Structures","title":"Structures","hpath":"graphql.structures","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e8c0cb48-6247-4280-8717-5c389ee66659.html","relUrl":"notes/e8c0cb48-6247-4280-8717-5c389ee66659.html"},{"doc":"Node","title":"Node","hpath":"graphql.structures.node","content":"\n# Node\nA node represents the actual object you were looking for.","url":"https://tycholiz.github.io/Digital-Garden/notes/43045943-d023-4427-8e1d-7887dd3b6b65.html","relUrl":"notes/43045943-d023-4427-8e1d-7887dd3b6b65.html"},{"doc":"Edge","title":"Edge","hpath":"graphql.structures.edge","content":"\n# Edge\nAn edge describes the relationship between a parent object and the target node. In our graph above, we have a central node (the parent) that points to other objects. The relationship (edge) is that of friendship. The entity on the other end of the line is the node.\n- An edge has metadata about one object in the paginated list, and includes a cursor to allow pagination starting from that object.\n- Each edge has a node and a cursor \n\nAn edge type may look like so:\n```\ntype UserFriendsEdge {\n  cursor: String!\n  node: User\n}\n```\nEdges are not just valuable for pagination purposes.\nIn graph theory an edge can have properties of its own, which act effectively as metadata.\nIt has become common to think of the `edges` field as boilerplate, and simply a container for the cursor, but it actually can be quite powerful. When we consider that an edge is a field that represents the relationship between 2 nodes, we can start to realize there is a lot of potentially appropriate fields to describe that connection.\n- ex. In the friend graph above, we can keep metadata about the friendship, such as when it started.\n- ex. Relevancy score when searching (e.g the relevancy of this result to the input search query is 0.7).\n- ex. Distance when searching/sorting by distance from a certain location.\n\nIn the above cases, the field doesn't belong on the node because its value changes depending on the connection parameters. Remember, you should be able to get to the same record (e.g. node) via multiple routes within a single query and the values for its fields should be the same, so contextual data has to go somewhere else -- the edge types.\n\n## Pagination\nEdges enable us to perform cursor-based pagination. Since the cursor is (usually) just an `id`, we can use the `after` argument on a list, passing that `id`. This allows us to specify the starting point that data should be retrieved from\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8c1514ed-0488-4dc8-a459-40b94f45260b.html","relUrl":"notes/8c1514ed-0488-4dc8-a459-40b94f45260b.html"},{"doc":"Connection","title":"Connection","hpath":"graphql.structures.connection","content":"\n# Connections\n- a connection is a collection of objects with metadata such as `edges`, `pageInfo`...\n\t- It is effectively the edges, and the metadata associated with their environment.\n- `pageInfo` will contain `hasNextPage`, `hasPreviousPage`, `startCursor`, `endCursor`\n\t- `hasNextPage` will tell us if there are more edges available, or if we’ve reached the end of this connection.\n\n- A connection is a paginated field on an object — for example, the friends field on a user or the comments field on a blog post.\n- very similar to cursor-based pagination\n\nConnections are made up of edges, but a connection is more than just that, since it also contains metadata about the group of edges, such as `pageInfo`, which gives us pagination info (ex. what the most recent cursor was, if there is another page or not, etc)\n![](/assets/images/2021-03-09-21-57-34.png)\nIn the above image, the connections would be all of the grey lines together as one. \n\nWe might define our `User` type like this:\n```\ntype User {\n  id: ID!\n  name: String\n  friendsConnection(\n    first: Int,\n    after: String,\n    last: Int,\n    before: String\n  ): UserFriendsConnection\n}\n```\n\nThis might lead to a query like so:\n```\n{\n  user(id: \"ZW5jaG9kZSBIZWxsb1dvcmxk\") {\n    id\n    name\n    friendsConnection(first: 3) {\n      edges {\n        cursor\n        node {\n          id\n          name\n        }\n      }\n    }\n  }\n}\n```\nA connection is a way to get all of the nodes that are connected to another node in a specific way\n- In this case we want to get all of the nodes connected to our users that are friends. Another connection might be between a user node to all of the posts that they liked.\n\nA connection is by nature an abstract concept, and it is difficult to think about. An edge makes sense, because we can think of a user having a friendship with another user, or we think of a user authoring a post.\n\nA connection type may look like so:\n```\ntype UserFriendsConnection {\n  pageInfo: PageInfo!\n  edges: [UserFriendsEdge]\n}\n```\n","url":"https://tycholiz.github.io/Digital-Garden/notes/27b0e95b-7b60-4ea9-8bbb-fb9a8b5bce64.html","relUrl":"notes/27b0e95b-7b60-4ea9-8bbb-fb9a8b5bce64.html"},{"doc":"Server","title":"Server","hpath":"graphql.server","content":"\nA GraphQL server essentially takes in your API and exposes your GraphQL API via an endpoint. It has two core parts:\n- A Schema, which includes type definitions.\n- Resolve functions, which hold all functions that define how to get the data\n\n# Type Coercion\nwhen preparing a field of a given scalar type, a GraphQL server must uphold the contract the scalar type describes, either by: \n- coercing the value\n- producing a field error if a value cannot be coerced or if coercion may result in data loss.\n\nA GraphQL server may decide to allow coercing different internal types to the expected return type. Unless the coercion is sensical (ie. no information is lost), the Graphql server will raise a field error.\n- For example when coercing a field of type `Int`, a boolean `true` value may produce `1`. Alternatively, a string value \"123\" may be parsed as base‐10 123.\n\nThe Graphql server will also coerce input values it receives as arguments to fields, as long as those arguments are a scalar type.\n- For example, if we pass a string `\"4\"` or int `4` to an ID input type, the value should be coerced to an ID format (as expected by the Graphql server)\n\n# Examples\n`express-graphql`, `apollo-server`, `graphql-yoga`\n\t- In Express, these are nothing but middleware functions that act as glue between the request and GraphQL engine provided by graphql.js (the tool that provides functionality for resolving queries)\n- express-graphql has 2 responsibilities:\n\t1. Ensure that the GraphQL query (or mutation) contained in the body of an incoming POST request can be executed by GraphQL.js.\n\t2. Attach the result of the execution to the response object so it can be returned to the client.\n- apollo-server is more generic than express-graphql, so it works in other frameworks, in addition to express.\n\t- it can also be configured to work as FaaS, like AWS Lambda\n- graphql-yoga\n\t- like create-react-app for GraphQL servers","url":"https://tycholiz.github.io/Digital-Garden/notes/9f92498a-01e7-4009-8766-1c1ed38f766c.html","relUrl":"notes/9f92498a-01e7-4009-8766-1c1ed38f766c.html"},{"doc":"Schema","title":"Schema","hpath":"graphql.schema","content":"\n# Schema\n- specifies the capabilities of the API and defines how clients can request the data. It is often seen as a contract between the source of data (ie. server) and its destination (ie. client), and it defines what data can be queried and how it ought to be queried\n- the schema gives the app's backend the ability to say \"here's the data that I want to make available to the client\"\n- when queries come in, they are validated and executed against the schema\n- Our schema, along with the requested query, defines the \"shape\" of the data object in the response returned by our endpoint\n\t- By shape, we mean what properties objects have, and whether those properties' values' are scalar values, other objects, or arrays of objects or scalars\n- Prefer building a GraphQL schema that describes how clients use the data\n\nThe schema is a mode of the data that can be retrieved through the Graphql server. It specifies:\n- It specifies what queries clients are allowed to make\n- what types (scalar, object, query, mutation) of data can be fetched from the server\n- what the relationships between these types are.\n\nThe Schema says \"Here's the data you can look at\". If we were to imagine our database as a complex graph, then the schema would be the graph. The schema defines how we can query and alter the data of the underlying database.\n\n- ex. a simple user and post app `schema.gql`:\n```\n// schema may be implicitly defined\nschema {\n\tquery: Query\n\tmutation: Mutation\n}\n\ntype Query {\n\tgetUser(uuid: String!): User\n}\n\ntype Mutation {\n\tcreateUser(input: UserInput!): User\n}\n\ntype User {\n\tUUID: String\n\tName: String\n\tPosts: [Post]\n}\n\ntype Post {\n\tUUID: String\n\tText: String\n}\n\ninput UserInput {\n\tName: String\n\tPosts: [PostInput]\n}\n\ninput PostInput{\n\tText: String\n}\n\n```\n\nA field does not become queryable until it exists on the `Query` type:\n```\ntype Query {\n\tmyName: String\n}\n```\n\n## Descriptions\nDescriptions are a first-class way of documentation in Graphql. To implement, we only need to include comments before the definition we are describing. There are 2 different ways to add documentation: Comment blocks for constructs, and inline comments for fields:\n```graphql\n\"\"\"\nA simple GraphQL schema which is well described.\n\"\"\"\ntype Query {\n  \"\"\"\n  Translates a string from a given language into a different language.\n  \"\"\"\n  translate(\n    \"The original language that `text` is provided in.\"\n    fromLanguage: Language\n\n    \"The translated language to be returned.\"\n    toLanguage: Language\n\n    \"The text to be translated.\"\n    text: String\n  ): String\n}\n\n\"\"\"\nThe set of languages supported by `translate`.\n\"\"\"\nenum Language {\n  \"English\"\n  EN\n\n  \"French\"\n  FR\n\n  \"Chinese\"\n  CH\n}\n```\n\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f5ad3d97-57cb-439e-9d38-928f6363855f.html","relUrl":"notes/f5ad3d97-57cb-439e-9d38-928f6363855f.html"},{"doc":"Resolver","title":"Resolver","hpath":"graphql.resolver","content":"\n# Resolver\nResolvers specify how the types and fields in the schema are connected to various backends. Through these functions, you are able to answer questions such as “How do I get the data regarding Course Authors?” and “Which backend do I need to call with what arguments to get the data regarding Courses?”.\n\n- A Resolver is a collection of function whose responsibility is fetching the data for a particular field. They are responsible for generating a response to the Graphql query.\n\t- the function is mapped to a schema. In other words, each type in the schema has a corresponding resolver\n\t- The schema (made up of queries and mutations) says \"here's what you can look at\", and the resolver says \"here's how you get it\"\n- therefore, the resolver is the part that initiates the SQL query\n- The resolvers map to your schema and are what GraphQL actually executes to retrieve each piece of data. The resolvers are like controllers in a regular REST API.\n- a resolver receives 4 args:\n\t1. obj\n\t2. args\n\t3. context\n\t4. info\n\n**Obj**\nthe previous object in the graphql \"tree\" (with the root being `query` or `mutate`). It contains the result returned from the resolver on the parent field.\n- sometimes aka `root`\n- when we are making a resolver function on the root Query type, we probably won't need to use `obj`.\n- All Graphql has to do in order to resolve a query is call the resolvers on the query's fields. This is being done level by level (in other words, from most outdented to most indented; ltr).\n- `obj` argument in each resolver call is simply the result of the previous call\n\t- ex. we are querying `getNuggetById`. `obj` is the `query` type at this point. When the backend receives that request, the resolver executes a db query and returns to us `{ id: 1, title: 'first nugget' }`. With the first field resolved, the value of `obj` on the second iteration is the `nugget` type, since that is what was returned by the first iteration. This is precicely the reason why we don't have to explicitly write resolvers for every single field.\n\t- In fact, if we were to console.log `obj`, on the second iteration we'd have the nugget object.\n``\nquery {\n\tnugget(id: $id) {\n\t\tid\n\t\ttitle\n\t}\n}\n```\n\n**args**\nThe arguments provided to the field in the GraphQL query\n\n**context**\nan object that gets passed through the resolver chain that each resolver can read from and write to (basically a means for resolvers to communicate and share information).\n- holds info like currently logged in user, current access to the database (which includes postgres user) etc.\n- therefore, we can use the context to provide access to the database\n\n**info**\nholds field-specific information relevant to the current query as well as the schema details\n- The way you form relationships is by defining custom types in your resolvers\n- In its most basic form, a GraphQL server will have one resolver function per field in its schema\n\t- Each resolver knows how to fetch the data for its field\n\n## How GraphQL resolves fields\nWhile you certainly can write a resolver for every field in your schema, it's often not necessary because GraphQL.js uses a default resolver when you don't provide one.\n- in most cases, the GraphQL library will just omit simple resolvers and will just assume that if a resolver isn't provided for a field, that a property of the same name should be read and returned.\n- what the default resolver does is simple: it looks at the value the parent field resolved to and if that value is a JavaScript object, it looks for a property on that Object with the same name as the field being resolved. If it finds that property, it resolves to the value of that property. Otherwise, it resolves to null.\n\t- This process is the reason why deeply nested queries are more computationally expensive.\n\n# E Resources\n[Good breakdown of how schema/resolvers work](https://www.prisma.io/blog/graphql-server-basics-the-schema-ac5e2950214e)","url":"https://tycholiz.github.io/Digital-Garden/notes/bd6bee10-c00f-4724-99bf-0d5143998407.html","relUrl":"notes/bd6bee10-c00f-4724-99bf-0d5143998407.html"},{"doc":"Pagination","title":"Pagination","hpath":"graphql.pagination","content":"\n# Pagination\nAt a high level, cursor-based pagination works like this:\n1. User requests some data (either through initial page load, or clicking some UI pagination button).\n2. Along with the list of data that to display, we also want to get pagination information `pageInfo` that is defined on the connection\n```\nquery OrganizationForLearningReact {\n  organization(login: \"the-road-to-learn-react\") {\n    name\n    url\n    repository(name: \"the-road-to-learn-react\") {\n      issues(first: 10, after: \"Y3Vyc29yOnYyOpHODAESqg==\") {\n        pageInfo {\n          endCursor\n        }\n        edges {\n          node {\n            author {\n              login\n            }\n          }\n        }\n       \n      }\n    }\n  }\n}\n```\n3. Get the `id` of the last item in the most previously-fetched data list (stored in `endCursor` variable)\n4. Store the cursor in local state (e.g. React, Apollo Client)\n5. pass as variable on subsequent fetches of data (ex. when user clicks \"Load more\", or scrolls more on infinite page)\n6. use that variable as input to the `after` parameter defined on the Connection (ex. in looking through a list of Github issues, the cursor is defined on the `IssuesConnection`)","url":"https://tycholiz.github.io/Digital-Garden/notes/f1d77378-ebd1-4c4c-a880-0b4369a8d1e2.html","relUrl":"notes/f1d77378-ebd1-4c4c-a880-0b4369a8d1e2.html"},{"doc":"Overview","title":"Overview","hpath":"graphql.overview","content":"\n# Overview\nGraphql is a protocol that gets implemented on a server, which gets communicated with from a client. The Graphql protocol defines a language for the client to use. The Graphql server is an expert at locating data. Through its resolvers, it knows exactly where every piece of data resides, and it knows how to get it through the most effective means. But in order to be able to know exactly which data you want, you have to speak to it in the graphql language.\n\nSince Graphql is just a spec, we need to have an implementation to use it (similar to how SQL is a spec for Postgres, MySQL etc). The 2 components of a GraphQL implementation are: the GraphQL server and the GraphQL client.\n\nGraphql has resolvers, each understanding where a certain resource of data is and how to get it. We might have data in a mongo database or a Postgres database, and as long as the resolver knows how to get it, Graphql doesn't really care. All it cares is that the data is retrievable.\n\nTo teach our Graphql server to be able to retrieve the data, we need to define a type system for it.\n\nGraphql uses `POST` requests for queries and mutations under the hood, and websockets for subscriptions\n\n![2bc1e31d73c0da1e8598d766424dd751.png](:/a10e5211520a457fb318c12aec62d1ea)\n\n### Arguments\nIn REST, there is one place to pass arguments, which is in query params or in the request body. With Graphql, every field and nested object can get its own set of arguments.\n\n\n- declarative data fetching where a client can specify exactly what data it needs from an API. Instead of multiple endpoints that return fixed data structures, a GraphQL server only exposes a single endpoint and responds with precisely the data a client asked for.\n- The `GraphQLSchema` object is the core of a GraphQL server\n\t- `GraphQLSchema` consists of 2 parts:\n\t\t1. schema definition (ie. the structure)\n\t\t2. resolver functions that implement the API (ie. the behaviour)\n\t- Query and Mutate are the rootTypes\n- Graphql is completely agnostic to the network layer (usually HTTP) and payload format (normally JSON). In fact, Graphql is not opinionated about the application architecture in general.\n- Graphql allows us to make relational queries that allow us to get all the data needed in one trip, instead of having to make multiple calls.\n\n## Between the client and app server\nA key thing to understand about GraphQL is that it’s actually agnostic to the way how data is transferred over the network\n- ie. it can work on protocols other than HTTP, like websockets\n\n* * *\n## Selection Sets\nA selection set in Graphql is similar to a result set in SQL, except the selection set is where we specify what we'd like to get back, while a result set in SQL is what we *actually* got back. \n\nAs the name suggests, a selection set is a list of Selections (type Selection)\n- a Selection has the following signature:\n```\ntype Selection {\n\tField\n\tFragmentSpread\n\tInlineFragment\n}\n```\n\nA selection set might look like:\n```\n{\n\tid\n\tfirstName\n\tlastName\n}\n```\nSome fields may describe more complex data or relationships to other data. To allow us to explore this data, a field itself may contain a selection set.\n- To remain unambiguous, the most nested fields must be scalars","url":"https://tycholiz.github.io/Digital-Garden/notes/5372fdf0-e38c-440c-96a1-d702b421cafa.html","relUrl":"notes/5372fdf0-e38c-440c-96a1-d702b421cafa.html"},{"doc":"Introspection","title":"Introspection","hpath":"graphql.overview.introspection","content":"\nIntrospection is about the links between types and scalars\nThe Graphql server is what supports introspection over its schema.\nTypes and fields required by the Graphql introspection systema are prefixed with `__`\n\nDetermining which types are available\n```\n{\n  __schema {\n    types {\n      name\n    }\n  }\n}\n```\n\n## Introspection Types\n`__Type` is at the core of the type introspection system. It represents scalars, interfaces, object types, unions, enums in the system.\n- All types in the introspection system provide a `description` field to allow type designers (ie. us, the developer) to publish documentation about a type.\n\n### __Field\nRepresents each field in an Object or Interface type.\n\n\n### __InputValue\nRepresents field and directive arguments as well as the inputFields of an input object.\n\nHas fields:\n- `name`— returns string\n- `description`— returns string or null\n- `type`— returns `__Type`, representing the type this input value expects\n- `defaultValue`\n\n## Introspection Queries\nThe schema introspection system is accessible from the meta‐fields `__schema` and `__type` which are accessible from the type of the root of a query operation.\n\nCan be of 3 types:\n```\n__schema: __Schema!\n__type(name: String!): __Type\n__typename: String!\n```\n\nThese fields are implicit and do not appear in the fields list in the root type of the query operation.\n\nThe schema of the GraphQL schema introspection system:\n\n```\ntype __Schema {\n  types: [__Type!]!\n  queryType: __Type!\n  mutationType: __Type\n  subscriptionType: __Type\n  directives: [__Directive!]!\n}\n\ntype __Type {\n  kind: __TypeKind!\n  name: String\n  description: String\n\n  # OBJECT and INTERFACE only\n  fields(includeDeprecated: Boolean = false): [__Field!]\n\n  # OBJECT only\n  interfaces: [__Type!]\n\n  # INTERFACE and UNION only\n  possibleTypes: [__Type!]\n\n  # ENUM only\n  enumValues(includeDeprecated: Boolean = false): [__EnumValue!]\n\n  # INPUT_OBJECT only\n  inputFields: [__InputValue!]\n\n  # NON_NULL and LIST only\n  ofType: __Type\n}\n\ntype __Field {\n  name: String!\n  description: String\n  args: [__InputValue!]!\n  type: __Type!\n  isDeprecated: Boolean!\n  deprecationReason: String\n}\n\ntype __InputValue {\n  name: String!\n  description: String\n  type: __Type!\n  defaultValue: String\n}\n\ntype __EnumValue {\n  name: String!\n  description: String\n  isDeprecated: Boolean!\n  deprecationReason: String\n}\n\nenum __TypeKind {\n  SCALAR\n  OBJECT\n  INTERFACE\n  UNION\n  ENUM\n  INPUT_OBJECT\n  LIST\n  NON_NULL\n}\n\ntype __Directive {\n  name: String!\n  description: String\n  locations: [__DirectiveLocation!]!\n  args: [__InputValue!]!\n}\n\nenum __DirectiveLocation {\n  QUERY\n  MUTATION\n  SUBSCRIPTION\n  FIELD\n  FRAGMENT_DEFINITION\n  FRAGMENT_SPREAD\n  INLINE_FRAGMENT\n  SCHEMA\n  SCALAR\n  OBJECT\n  FIELD_DEFINITION\n  ARGUMENT_DEFINITION\n  INTERFACE\n  UNION\n  ENUM\n  ENUM_VALUE\n  INPUT_OBJECT\n  INPUT_FIELD_DEFINITION\n}\n```\n\n### __schema\nThe most important field (query) in graphql, as it allows us to fetch the whole schema. It is also the primary source for Graphiql. The name for this query is `__schema`, and its Schema Definition Language (SDL) is:\n```\ntype __Schema {\n  types: [__Type!]!\n  queryType: __Type!\n  mutationType: __Type\n  subscriptionType: __Type\n  directives: [__Directive!]!\n}\n```\n\nAs we can see, when we query `__schema`, we get some information about the whole schema, including which operation types we have, and which types and directives we have. The result of that query might look like this:\n```\n{\n  __schema {\n    directives {\n      name\n      description\n    }\n    subscriptionType {\n      name\n      description\n    }\n    types {\n      name\n      description\n    }\n    queryType {\n      name\n      description\n    }\n    mutationType {\n      name\n      description\n    }\n    queryType {\n      name\n      description\n    }\n  }\n}\n```\n\n### __type\nAllows us to query for information about the exact type we are interested in. All we need to do is pass in the type as an argument like so:\n```\nquery introspectionUserType {\n  __type(name: \"User\") {\n\tname\n\tkind\n\tfields {\n\t\tname\n\t\ttype {\n\t\t\tname\n\t\t}\n\t}\n  }\n}\n```\nThis might return:\n```\n{\n  \"__type\": {\n    \"name\": \"User\",\n\t\"kind\": \"OBJECT\"\n    \"fields\": [\n      {\n        \"name\": \"id\",\n        \"type\": { \"name\": \"String\" }\n      },\n      {\n        \"name\": \"name\",\n        \"type\": { \"name\": \"String\" }\n      },\n      {\n        \"name\": \"birthday\",\n        \"type\": { \"name\": \"Date\" }\n      },\n    ]\n  }\n}\n```\n\nWhen the `name` field is `null`, it is because it is a wrapper type (ID, List). If we query for the `ofType` on these fields, we can see what the \"wrapped\" type is\n- ex. a list is a wrapper type, because each item in the list has its own type. The List is just a type that wraps them altogether. The same can be said for Non-null and ID.\n\n### __typename\nOffers type name introspection. `__typename` is a meta-field that allows us to get the type name at any point within a query. It is available through all types when querying.\n\nApollo client makes use of `__typename` to construct the apollo cache. It uniquely identifies an item by `id + __typename`","url":"https://tycholiz.github.io/Digital-Garden/notes/dbf21b14-3f46-4cd9-8f47-535e419c0bfe.html","relUrl":"notes/dbf21b14-3f46-4cd9-8f47-535e419c0bfe.html"},{"doc":"Operations","title":"Operations","hpath":"graphql.operations","content":"\n# Operation Types (ie. Queries and Mutations)\n- `query` types and `mutation` types are the entry point to a graphql query\n\t- It's important to remember that other than the special status of being the \"entry point\" into the schema, the Query and Mutation types are the same as any other GraphQL object type, and their fields work exactly the same way.\n\nScalars are primitive values in GraphQL. If we consider a graphql query as a hierarchical graph, the leaves would be the primitives\n\nGraphQL requires that you construct your queries in a way that only returns concrete data\n- Each field has to ultimately resolve to one or more scalars (or enums). That means you cannot just request a field that resolves to a type without also indicating which fields of that type you want to get back (hence why `graphiql` will auto-complete for us).\n\ntwo operations that occur in GraphQL servers are:\n1. **result coercion**: upholding the contract of a type which we receive from the server (basically upholding the primitive values or object type)\n\t- The type system knows what to expect and will convert the values returned by a resolver function into something that upholds the API contract\n2. **input coercion**: upholding the contract of a type for input arguments that we pass into the GraphQL query or mutation\n\t- if we pass in `5` for the `id` field, the type will be parsed into a string as `\"5\"`","url":"https://tycholiz.github.io/Digital-Garden/notes/1841ba79-84f7-411b-bfaa-ab6b6cb66a7a.html","relUrl":"notes/1841ba79-84f7-411b-bfaa-ab6b6cb66a7a.html"},{"doc":"Subscriptions","title":"Subscriptions","hpath":"graphql.operations.subscriptions","content":"\n## Subscriptions\nreal-time connection from the client to the server that allows the client to get immediately informed about events happening server-side\n- when a client subscribes to an event, it will hold a steady connection to the server. when this event happens, the server will push that corresponding data to the client\nTherefore, subscriptions are event-based, acting in response to what just happened.\n- We can see this in the subscription nomenclature: `commentAdded`, `paymentMethodAdded`\n- Unlike queries and mutations that follow a typical “request-response-cycle”, subscriptions represent a *stream* of data sent over to the client.\n\nYou should use subscriptions for the following:\n- Small, incremental changes to large objects.\n  - Repeatedly polling for a large object is expensive, especially when most of the object's fields rarely change. Instead, you can fetch the object's initial state with a query, and your server can proactively push updates to individual fields as they occur.\n- Low-latency, real-time updates.\n  - For example, a chat application's client wants to receive new messages as soon as they're available.\n\n```\nsubscription {\n  newPerson {\n    name\n    age\n  }\n}\n```\nWhenever a newer mutation is performed that creates a new `Person`, the server sends the information about this person to the client\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c1a58a3a-c74c-4466-8d7c-7254f3801645.html","relUrl":"notes/c1a58a3a-c74c-4466-8d7c-7254f3801645.html"},{"doc":"N1","title":"N1","hpath":"graphql.n1","content":"\n# GraphQL suffers from the N+1 problem\n- The number of queries grows exponentially with the depth of the query\n- N1 problem occurs when you have to retrieve the same information multiple times. In graphql, the resolver for that particular field would have to be hit every single time, instead of the system understanding that it's the same result, and being more efficient about it.\n\t- ex. We have a query shaped as below. imagine that the article has 5 `comments` and all are written by the same person. In Graphql, that would be 5 times that the resolver got called, instead of once.\n```js\nquery {\n  user(id: \"abc\") {\n\tname\n\tarticle(title: \"GraphQL is great\") {\n\t  comments {\n\t\ttext\n\t\twrittenBy {\n\t\t  name\n\t\t}\n\t  }\n\t}\n  }\n}\n```\n- N1 problem can be solved by DataLoader library\n[possibly redundant info](https://stackoverflow.com/questions/97197/what-is-the-n1-selects-problem-in-orm-object-relational-mapping)\n[using a batchloader (like joinMonster) to solve N+1](http://www.petecorey.com/blog/2017/08/14/batching-graphql-queries-with-dataloader/?from=east5th.co)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/9f27342e-3421-4952-a4f6-de978d8c9c7c.html","relUrl":"notes/9f27342e-3421-4952-a4f6-de978d8c9c7c.html"},{"doc":"Fields","title":"Fields","hpath":"graphql.fields","content":"\n# Fields\nEvery field on a GraphQL object type can have zero or more arguments:\n```\ntype Starship {\n  id: ID!\n  name: String!\n  length(unit: LengthUnit = METER): Float\n}\n```\n- REST endpoints are similar to GraphQL fields, as they are entry points into the data that call functions on the server.\n- sometimes you can have an entity that is both a field and a type. When I query for one nugget, I am querying on a field `nugget`, and getting back a Nugget type.\n\t- in this case, `nugget` is known as the root field\n- Each field on each type is backed by a *resolver*\n\t- When a field is executed, the corresponding resolver is called to produce the next value. If a field produces a scalar value like a string or number, then the execution completes. However if a field produces an object value then the query will contain another selection of fields which apply to that object. \n\t\t- This continues until scalar values are reached. GraphQL queries always end at scalar values.\n\nFields are conceptually functions which return values, and occasionally accept arguments which alter their behavior.\n- These arguments often map directly to function arguments within a GraphQL server’s implementation.\n- you can think of each field as a function of the previous type which returns the next type\n\t- therefore, fields can exist at different levels. Imagine we had a `User` type, and that `user` had an `id` and `name`. We would be looking at 3 fields in total here, since `user` is just a field defined on the `Query` type. \n\nSince fields are conceptually just functions, that means we may be able to pass arguments to them. \n- Arguments can also be passed into scalar values, which would allow the server to do data transformations before sending it to the client (as opposed to the client having to handle that). All we need to do is define an enumeration type:\n```\ntype Person {\n\tname: String\n\tpicture(size: Int): Url\n}\n\n{\n  person(id: \"1000\") {\n    name\n    picture(size: 600)\n  }\n}\n```\n\n## Alias\nBy default, the response of our query is an object with a key matching the field name(s) we queried\n\nSometimes we want to request data about 2 identical objects. (ex. querying the same field twice, but with different arguments). In cases like this, we need to use aliases, which changes the key of the object we get as our query result.\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/2bd27a5e-ec35-43c9-b74b-d0611bc571db.html","relUrl":"notes/2bd27a5e-ec35-43c9-b74b-d0611bc571db.html"},{"doc":"Structures","title":"Structures","hpath":"graphql.documents","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8884cd80-bc86-4bf0-9bc7-c40eb4292eb7.html","relUrl":"notes/8884cd80-bc86-4bf0-9bc7-c40eb4292eb7.html"},{"doc":"Queries","title":"Queries","hpath":"graphql.documents.queries","content":"\n## Queries\nEvery GraphQL service has a query type and may or may not have a mutation type\n\n### Understanding how a query works\n```\nquery {\n  hero {\n    name\n    appearsIn\n  }\n}\n```\n\n1. We query the special root Query type, and get back an object\n2. We select the hero field on that object\n3. For the object returned by hero, we select the name and appearsIn fields\n","url":"https://tycholiz.github.io/Digital-Garden/notes/0bed3e67-420c-417e-ad8e-42609a1afb30.html","relUrl":"notes/0bed3e67-420c-417e-ad8e-42609a1afb30.html"},{"doc":"Mutations","title":"Mutations","hpath":"graphql.documents.mutations","content":"\n## Mutations\n- While query fields are executed in parallel, mutation fields run in series, one after the other.\n\t- This means that if we send two `updateNugget` mutations in one request, the first is guaranteed to finish before the second begins\n### Input types\noften we will be using the same inputs to different mutations (ex. createNugget, updateNugget). Input types offers us a way to decalare these inputs once, and alias it as a type. \n","url":"https://tycholiz.github.io/Digital-Garden/notes/04ae4cae-8708-4c4b-b90b-603defd872e1.html","relUrl":"notes/04ae4cae-8708-4c4b-b90b-603defd872e1.html"},{"doc":"Fragments","title":"Fragments","hpath":"graphql.documents.fragments","content":"\nFragments are the primary unit of composition in Graphql. They...\n- must specify the type they apply to\n- cannot be specified on any input value (scalar, enumeration, or input object).\n- can be specified on object types, interfaces, and unions.\n\nBelow, the part after `on` is the type we are selecting from\n- so `people` is of type `Person`, and we want the `firstName` and `lastName` fields from `people(id: \"7\")`\n```\nfragment NameParts on Person {\n  firstName\n  lastName\n}\n\nquery GetPerson {\n  people(id: \"7\") {\n    ...NameParts\n    avatar(size: LARGE)\n  }\n}\n```\n## Conditional Fragments\n### Type Conditions \n- allow us to conditionally include fields based on their runtime type.\n\nOn Facebook, imagine we have 2 different fields that both specify a `count`: User Friends, and Page Likes. We have set up our schema in such a way that we define a `Profile` type, which can be either a `User` or `Page`. \n\nNow imagine that we want to write a query that gets back both the friends count of Kyle Tycholiz, and it gets back the like count of the Never Forget page. If we want to write this in a single query, we are presented with a problem: while both have a field `count`, the parent field is different, namely `friends` and `likes`. To solve this problem, we must use fragments, which will be applied *depending on* the type of profile that is returned in the query.\n\nWhen we query for this, the `profiles` root field returns a list where each element could be a `Page` or a `User`.\n\n```\n{\n\tprofiles(name: [\"kyletycholiz\", \"neverforget\"]) {\n\t\tname\n\t\t...userFragment\n\t\t...pageFragment\n\t}\n\t\nfragment userFragment on User {\n\tfriends {\n\t\tcount\n\t}\n}\n\nfragment pageFragment on Page {\n\tlikers {\n\t\tcount\n\t}\n}\n}\n```\n\n### Inline Fragments\nif querying a field that returns an interface or union type, we need to use inline fragments to access data on the underlying concrete type\n- These can be used to compose fields in a type-dependent way\n    \nThe above Facebook example can be accomplished using inline fragments. \n\nThe Graphql server will determine whether to return `homeAddress` or `address` at runtime, depending on whether the requested object is a `User` or `Business`\n\n```\nquery Foo {\n  profile(id: $id) {\n    url\n    ... on User {\n      homeAddress\n    }\n    ... on Business {\n      address\n    }\n  }\n}\n```\n\n#### Practical usage\n- Imagine we had an interface `Character` that represents a character from Star Wars:\n```\ninterface Character {\n  id: ID!\n  name: String!\n  friends: [Character]\n  appearsIn: [Episode]!\n}\n```\n- Imagine now we create two types that *implement* this interface:\n```\ntype Human implements Character {\n  id: ID!\n  name: String!\n  friends: [Character]\n  appearsIn: [Episode]!\n  starships: [Starship]\n  totalCredits: Int\n}\n\ntype Droid implements Character {\n  id: ID!\n  name: String!\n  friends: [Character]\n  appearsIn: [Episode]!\n  primaryFunction: String\n}\n```\n- As we can see, **Droid** has the field `primaryFunction`, while **Human** does not\n    - This means that if we were to make a query that wanted that field back, we would get an error:\n```\nquery HeroForEpisode($ep: Episode!) {\n  hero(episode: $ep) {\n    name\n    primaryFunction\n  }\n}\n// PRODUCES ERROR: cannot query field 'primaryFunction' on type Character\n```\n- To get around this, we need to use an inline fragment:\n```\nquery HeroForEpisode($ep: Episode!) {\n  hero(episode: $ep) {\n    name\n    ... on Droid {\n      primaryFunction\n    }\n  }\n}\n```\n","url":"https://tycholiz.github.io/Digital-Garden/notes/abe50863-d71e-4aeb-833a-2d529d2b6f97.html","relUrl":"notes/abe50863-d71e-4aeb-833a-2d529d2b6f97.html"},{"doc":"Client","title":"Client","hpath":"graphql.client","content":"\n# Graphql Client\nThe Graphql API has more underlying structure than a REST API. This means there are more things to handle and keep track of, such as batching, caching, and other features. ","url":"https://tycholiz.github.io/Digital-Garden/notes/fc241dd6-c547-4bc6-a14b-42a7392f8990.html","relUrl":"notes/fc241dd6-c547-4bc6-a14b-42a7392f8990.html"},{"doc":"Auth","title":"Auth","hpath":"graphql.auth","content":"\n### Authorization\n- all authorization should be handled by the business logic layer in the application, not in graphql\n\t- [source](https://graphql.org/learn/authorization/)\n- In a REST API, authentication is often handled with a header, that contains an auth token which proves what user is making this request. Express middleware processes these headers and puts authentication data on the Express request object. \n\nThere are two broad ways of handling authentication in GraphQL APIs:\n1. **Authentication via the GraphQL server**: All users have to be logged in by the GraphQL server before they can query the endpoint—purely GraphQL workflows. Authentication is implemented in the GraphQL Schema\n\t- It involves first getting a JWT token, then passing that token in subsequent requests\n\t- this is the method proposed by Postgraphile (a JWT-based approach). \n2. **Authentication via a Web Server** (ex. Express and Passport): Users can make queries to the GraphQL endpoint once they are logged in. Authentication is implemented with middleware.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/adf7faf0-7dec-4c6d-bf91-2af12739115f.html","relUrl":"notes/adf7faf0-7dec-4c6d-bf91-2af12739115f.html"},{"doc":"Ast","title":"Ast","hpath":"graphql.ast","content":"\n\nIn order to interpret and understand these unique and complex requests, GraphQL employs the use of an Abstract Syntax Tree (AST) to structure the incoming requests.\n- This makes it easier for the back-end gurus & frameworks to parse and construct the required response.\n\nAs a consequence of Graphql being data-layer-independent, libraries such as graphql-js and graphql-tools exist to abstract away the heavy lifting, removing our need to worry about having to interact with the underlying AST.\n- Sometimes however we must be able to perform our own custom operations based on the specifics of the user's request.\n\n### What is an AST?\nan AST is just a fancy way of saying \"heavily nested objects\"\n- It is a structure that is commonly used in compilers to parse the code we write, converting it into a tree structure that we can traverse programmatically.\n\nWhen a user makes a request, GraphQL combines the query document that the user requested with the schema definition that we defined for our resolver in the form of an AST. This AST is used to determine which fields were requested, what arguments were included and much more.\n\n### Rugby Player Example\nSuppose we wanted to expose a list of rugby players on our Graphql backend. We make a schema definition which models a new Graphql type `RugbyPlayer`, which has 2 fields: `full_name` and `club`:\n```js\nexport const RugbyPlayer = new gql.GraphQLObjectType({\n    name: 'RugbyPlayer',\n    fields: {\n        full_name: {\n            type: gql.GraphQLString,\n        },\n        club: {\n            type: new gql.GraphQLObjectType({\n                name: 'Club',\n                fields: {\n                    name: {\n                        type: gql.GraphQLString,\n                    },\n                },\n            }),\n        },\n    },\n})\n```\nImagine now that a client makes a request for that list of players. From the Graphql server's perspective, the schema knows nothing about the query document (ie. the request). Therefore, it has to parse the query document into AST format (ie. heavily nested objects) before it can traverse it. Now, it can perform any necessary validation, like throwing errors for having the wrong types for fields or arguments.\n- Following this validation, schema types are mapped onto the respective branches of the AST to provide a richer set of metadata.\n\n### What we can do with the AST\nBy understanding the AST, we can craft custom directives and optimise user requests\n- We do this by breaking the traditional Graphql lifecycle and inercepting a request before it gets passed onto another library to generate a response.\n\nBy traversing and augmenting the AST we can implement:\n- Schema stitching\n- Custom directives\n- Enriched queries\n- Layered Abstraction\n- More backend magic!\n\nIn a step to give more control to the front-end client, we often find it useful to implement a range of directives that can transform or filter out fields specified in the AST.\n- ex. In the Rugby example, imagine we want to solve this problem: \"Rugby player's names are being mispronounced too often\".\n\t- What we can do is implement a Graphql directive called `@pronounce`, which can be added onto the `full_name` field when we are querying our `RugbyPlayers` type:\n```\nquery {\n  RugbyPlayers {\n    full_name @pronounce\n  }\n}\n```\nlibraries like `graphql-js` can provide methods such as `gql.GraphQLDirective` that allow us to add these directives.\n\nWhenever we see code like this:\n```\nconst Type = gql`\n  type ${nameOfType} {\n    str: String\n    int: Int\n  }\n`;\n```\n\nWe can understand `gql` as a function that transforms our human-readable graphql schema language into an AST that the application can understand.","url":"https://tycholiz.github.io/Digital-Garden/notes/731d81d4-6c0d-4e1f-98bc-79e7ac3d8a05.html","relUrl":"notes/731d81d4-6c0d-4e1f-98bc-79e7ac3d8a05.html"},{"doc":"Graphile Worker","title":"Graphile Worker","hpath":"graphile-worker","content":"\n## Tasks\nWhen we run a task in Node (ex. with `run()`, we are returned a `Runner` object, which has the following methods/properties:\n- stop(): Promise<void> - stops the runner from accepting new jobs, and returns a promise that resolves when all the in progress tasks (if any) are complete.\n- addJob: AddJobFunction - see addJob.\n- promise: Promise<void> - a promise that resolves once the runner has completed.\n- events: WorkerEvents - a Node.js EventEmitter that exposes certain events within the runner (see WorkerEvents).\n\nex. we can add a job to the queue in response to another job being added to the queue:\n```js\nawait runner.addJob(\"testTask\", {\n  thisIsThePayload: true,\n});\n```\n\n### Events\nWe can listen to events like so:\n```js\nrunner.events.on(\"job:success\", ({ worker, job }) => {\n  console.log(`Hooray! Worker ${worker.workerId} completed job ${job.id}`);\n});\n```\n\nTo realize the importance and place of Workers, we have to break asynchronous tasks down into 2 types:\n- async tasks, where the process of execution is dependent on the response to continue on to execute the next lines of code.\n  - ex. When the Express server is making database queries, the it has to wait for the data to return before it can continue on with its operation. This is why we use `async`/`await`, because we need that data\n- async tasks, where the 2 processes are realistically distinct from one another, and can be completely decoupled.\n  - ex. When we want to send registration success emails to users, we don't want to tie up the Express server thread doing this task. It is something that can be handed off to some other service to handle for us.","url":"https://tycholiz.github.io/Digital-Garden/notes/e1c5eaef-0205-4e6d-ab9f-5a207d58cc12.html","relUrl":"notes/e1c5eaef-0205-4e6d-ab9f-5a207d58cc12.html"},{"doc":"Graphile Migrate","title":"Graphile Migrate","hpath":"graphile-migrate","content":"\n## Tracking Changes\n- GM has an .gmrc file, which allows us to define hooks. We can therefore define a hook that will run `pg_dump` on the shadow db after every migration. If we track this file in git, then we can see a clear history of what each migration has done.\n\n## Development\n- In Development, aside from the main database, there is a shadow db which is used internally by graphile-migrate and is mainly for testing consistency of the migrations, among other minor tasks\n- `commit`, `uncommit`, `watch` and `reset` are development-only commands.\n\n## Flow\n1. We write our new idempotent migration in `current.sql`\n\t- any seed data we have can be placed at the bottom, to be removed prior to committing.\n2. when ready, we run `commit`\n\t- this should only be done immediately before the branch is merged into master (see README###Collaboration). This it to ensure commits are linear.\n\t- all sql in `current.sql` is removed and catalogued into `committed/`\n\t- the shadow database is dropped, and recreated by running all migrations. This is to ensure that the migration works without a hitch.\n\n* * *\n\n### Pitfalls\n- when dropping tables, schemas, and functions, we should use CASCADE\n- use idempotent commands whenever possible.\n\t- ex. DROP ____ IF EXISTS\n\t- ex. CREATE OR REPLACE FUNCTION\n\t- The initial migrations don't need to be idempotent if this migration starts off by dropping all schemas (which implicitly drops all tables attached to those schemas)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d8118e61-b25f-4a6f-8f88-4e7ddd69827f.html","relUrl":"notes/d8118e61-b25f-4a6f-8f88-4e7ddd69827f.html"},{"doc":"GitHub","title":"GitHub","hpath":"github","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5d700782-fb81-416e-83f9-5dd88260e350.html","relUrl":"notes/5d700782-fb81-416e-83f9-5dd88260e350.html"},{"doc":"Pr","title":"Pr","hpath":"github.pr","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/b8d1aa39-4fdf-40aa-8df1-658229e3037a.html","relUrl":"notes/b8d1aa39-4fdf-40aa-8df1-658229e3037a.html"},{"doc":"Strategies","title":"Strategies","hpath":"github.pr.strategies","content":"\n### Push new changes to PR branch that has diverged.\n1. Before pushing any new changes, rebase `master` onto the feature branch, and resolve conflicts:\n`git rebase master`\n2. on feature branch, run `git push --force`\n\t- If we don't force push, we will get a warning that branches have diverged and we must first pull. This is a bad idea, spec: because we would be getting rewritten history, and would in effect be introducing many more commits than we are actually intending.","url":"https://tycholiz.github.io/Digital-Garden/notes/d2b60cb5-fd23-4bb9-a004-6298468bfd7e.html","relUrl":"notes/d2b60cb5-fd23-4bb9-a004-6298468bfd7e.html"},{"doc":"Pages","title":"Pages","hpath":"github.pages","content":"\n[Subdirectory of master branch at root of gh-pages branch](https://gist.github.com/cobyism/4730490)","url":"https://tycholiz.github.io/Digital-Garden/notes/ab160f3f-c43b-40af-ab29-133c9d39ac4c.html","relUrl":"notes/ab160f3f-c43b-40af-ab29-133c9d39ac4c.html"},{"doc":"Cmds","title":"Cmds","hpath":"github.cmds","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/21c70cce-ea24-41f6-9bc8-3fb04f943d68.html","relUrl":"notes/21c70cce-ea24-41f6-9bc8-3fb04f943d68.html"},{"doc":"Search","title":"Search","hpath":"github.cmds.search","content":"\n# Searching\nsearch all code bases with `@json`\n- `\"json;\"`\nexclude results from repository WatermelonDB\n- `-repo:Nozbe/WatermelonDB`\n\n- checkout a branch from github (ex. for CR)\n    - `hub pr checkout <issue number>`","url":"https://tycholiz.github.io/Digital-Garden/notes/c78b8b9f-2707-4bbd-93f6-0f88b50a9777.html","relUrl":"notes/c78b8b9f-2707-4bbd-93f6-0f88b50a9777.html"},{"doc":"Actions","title":"Actions","hpath":"github.actions","content":"\nA Github Action is installed as soon as we have a `.github/workflows` directory committed to our repo on Github.\n\n![](/assets/images/2021-04-15-21-56-11.png)\n\n## Terminology\n\n### Workflow\nA workflow is an automated procedure that exists as a part of our repo.\n- Workflows are made up of a collection of jobs which run is response to some event (on PR, on push etc)\n\nA Github Workflow can be used to build, test, package, release, or deploy a project on GitHub.\n\nWorkflows can be configured to run in response to a webhook call, so we can trigger workflows in response to events happening outside Github.\n\n### Job\nA job is a set of steps that execute on a single runner\nA workflow with multiple jobs will run those jobs in parallel by default (configurable)\nEach job in a workflow runs in a fresh virtual environment\n\n### Step\nA step is an individual task that can run commands in a job\n\nA step can either be:\n- a shell command\n- an action\n\nEach step in a job executes on the same runner, allowing the actions in that job to share data with each other.\n\n### Action\nActions are standalone commands that are combined into steps to create a job\nActions are the smallest portable building block of a workflow. You can create your own actions, or use actions created by the GitHub community\n\n### Runner\nA runner is a server that has the GitHub Actions runner application installed\n- meaning we could use the managed service from Github, or self-host the Runner.\n\nA runner listens for available jobs, runs one job at a time, and reports the progress, logs, and results back to GitHub\n\nGitHub-hosted runners are based on Ubuntu Linux, Microsoft Windows, and macOS\n\n* * *\n\n### Secrets\n`GITHUB_TOKEN` is passed to the runner when a workflow is triggered.\n- Aside from this, no other secrets are passed by default, and must be configured manually.\n\nTo provide an action with a secret (either as input or as an env variable), use the `secrets` context.\n- This allows us to access secrets we've created in our repo.\n- [more info](https://docs.github.com/en/actions/reference/context-and-expression-syntax-for-github-actions)\n\nSecrets should not be passed between processes via the command line, as these commands might be visible to other users with the `ps` command.\n- This is why we use `STDIN` or env variables to pass secrets.","url":"https://tycholiz.github.io/Digital-Garden/notes/2cbfa75d-0a0f-4cf8-9cc4-2428f61eb372.html","relUrl":"notes/2cbfa75d-0a0f-4cf8-9cc4-2428f61eb372.html"},{"doc":"Git","title":"Git","hpath":"git","content":"\n# Inner Working of Git\n- Git is a database of references. Essentially, it is just one giant acyclic graph\n\t- acyclic means that we can traverse the commit parents in one direction and there is no chain that begins and ends with the same commit. \n- Each time we create a file and begin tracking it, git compresses it and stores it into its own structure\n\n### How git stores the data\nCommits are snapshots, not diffs\n- Git takes snapshots (via commits), and stores it in something of a mini-filesystem. When Git stores a new version of a project, it stores a new tree (a bunch of blobs and a collection of pointers) that can be used to expand back out into the whole directory of files and directories.\n- Git stores content similar to how a UNIX filesystem does. All content is stored as tree and blob objects\n    - trees would correspond to directories\n        - Like UNIX filesystem, all a directory is is a file with its contents listed. (The relationship between parent-child is made via reference, not the child actually being inside parent)\n    - blobs would correspond to file contents or inodes\n- A single tree object contains one or more entries (which is a SHA of a sub-tree or blob (ie. directory or file))\n- ex. imagine we had the following project structure:\n```\n.\n|-- README\n`-- lib\n    |-- inc\n    |   `-- tricks.rb\n    `-- mylib.rb\n```\n- if we made a commit, we would create a tree object for each directory, and a blob object for each file. It would be represented like this:\n![](/assets/images/2021-03-06-16-14-27.png)\n\n- The reason we can represent the history of the codebase as a tree is because each snapshot (commit) specifies its parent\n- The object database is content-addressable, meaning that we can retrieve the data based on its content, rather than its location.\n\t- This is a major reason why Git is so performant ","url":"https://tycholiz.github.io/Digital-Garden/notes/0628397e-fef6-4f0c-8608-1638506393d3.html","relUrl":"notes/0628397e-fef6-4f0c-8608-1638506393d3.html"},{"doc":"Tree","title":"Tree","hpath":"git.tree","content":"\n### Tree\n- a tree is Git's representation of snapshots, meaning they store the state of a directory at a given point (without notion of time/author). To tie trees together into a coherent history, Git wraps each one in a commit object and specifies a parent commit. By following the parent of each commit, we can walk through the entire history of the project.\n\t- Each commit refers to only one tree object.\n- holds pointers to filenames and other trees, effectively allowing us to group files together (which is essentially what a directory is)\n- A tree object contains 1+ entries. Each of which is either a blob or subtree hash.\n- The tree object is what associates the filename (or directory name) with its content.\n\t- We can confirm this by running `git cat-file` on the tree object. It will give us back a list of blobs and their associated filenames\n\t- We can use a plumbing command `update-index`, which effectively allows us to associate an existing blob with a filename:\n\t\t- `git update-index --add --cacheinfo 100644 83baae61804e65cc73a7201a7252750c76066a30 test.txt`\n\t\t\t- add a file to the index (`--add`), get it from the object database (`--cacheinfo`)\n\t\t\t- upon executing this command, we have `test.txt` added to the staging area.\n- The tree is normally made by examining the state of the staging area","url":"https://tycholiz.github.io/Digital-Garden/notes/4cbbd98d-d4b9-40b8-88db-133ce0a20ece.html","relUrl":"notes/4cbbd98d-d4b9-40b8-88db-133ce0a20ece.html"},{"doc":"Tag","title":"Tag","hpath":"git.tag","content":"\n- if we are doing something a little risqué (like undoing a rebase), it's good to make a backup `git tag BACKUP`. Then if we ever need to go back to it, run `git reset --hard BACKUP`","url":"https://tycholiz.github.io/Digital-Garden/notes/d561a4bb-89f7-4062-b4e8-1fffb8d52c97.html","relUrl":"notes/d561a4bb-89f7-4062-b4e8-1fffb8d52c97.html"},{"doc":"Tag","title":"Tag","hpath":"git.tag.inner","content":"\n### Tag\n- The fourth (and less integral) object functions similar to a commit object, in that it contains a tagger, a message, a date, and a pointer. \n\t- The main difference is that a tag normally points to a commit rather than a tree. In this sense it is similar to a branch reference, but it never moves (ie. it always points to the same commit).\n- There are 2 types of tag, lightweight and annotated\n\t- *lightweight* - a reference to a commit that never moves\n\t\t- this type of tag does not create a tag object\n\t- *annotated* - when we create an annotated tag, Git creates a tag object and then writes a reference point to it (rather than directly to the commit). \n","url":"https://tycholiz.github.io/Digital-Garden/notes/a8f94481-a48b-4c82-b6b5-5fe199a59bd8.html","relUrl":"notes/a8f94481-a48b-4c82-b6b5-5fe199a59bd8.html"},{"doc":"Tag","title":"Tag","hpath":"git.tag.cli","content":"\n### Tag\n- simply a way to mark specific commits as special in some way (ex. tag a specific commit as a specific release, or something along those lines)\n- tags and branches are similar, in that they both point to a specific commit (ie. they are pointers to snapshots). Theoretically, branches could perform the role of tags. We could just keep around a branch called *Release 2.0*. Instead, Git allows us to separate concerns.\n    - A branch is a moveable pointer to a snapshot, while tags point at a single snapshot and never move. Also, tags are actually stored as objects, while branches are not. \n","url":"https://tycholiz.github.io/Digital-Garden/notes/39b962a9-36b2-4275-b0d9-0cba1ee422b2.html","relUrl":"notes/39b962a9-36b2-4275-b0d9-0cba1ee422b2.html"},{"doc":"Switch","title":"Switch","hpath":"git.switch","content":"\nDesigned to replace the branch-switching functionality of `git checkout`","url":"https://tycholiz.github.io/Digital-Garden/notes/451d244d-66ed-4929-bc25-208fc57f35ef.html","relUrl":"notes/451d244d-66ed-4929-bc25-208fc57f35ef.html"},{"doc":"Subtree","title":"Subtree","hpath":"git.subtree","content":"\n## Subtree Push\n- `git subtree push` allows us to cause a sub-directory of the current branch to be the root-level directory of another branch\n- ex. `git subtree push --prefix docs origin gh-pages`\n\t- this pushes just the `docs` directory to the `gh-pages` branch.\n\n## UE Resources\nhttps://www.atlassian.com/git/tutorials/git-subtree","url":"https://tycholiz.github.io/Digital-Garden/notes/ddc7bbcf-0877-408f-ac4d-32468c5c2d6c.html","relUrl":"notes/ddc7bbcf-0877-408f-ac4d-32468c5c2d6c.html"},{"doc":"Strategies","title":"Strategies","hpath":"git.strategies","content":"\n### Include some changes as part of a previous commit\nImagine we realized that we should have included a change (perhaps deleting some old comments) as part of a previous \"cleaning commit\"\n\nOf course, we will have to change history to do so, with `rebase`:\n1. Use git stash to store the changes you want to add.\n2. Use git rebase -i HEAD~10 (or however many commits back you want to see).\n3. Mark the commit in question (a0865...) for edit by changing the word pick at the start of the line into edit. Don't delete the other lines as that would delete the commits.[^vimnote]\n4. Save the rebase file, and git will drop back to the shell and wait for you to fix that commit.\n5. Pop the stash by using git stash pop\n6. Add your file with git add <file>.\n7. Amend the commit with git commit --amend --no-edit.\n8. Do a git rebase --continue which will rewrite the rest of your commits against the new one.\n9. Repeat from step 2 onwards if you have marked more than one commit for edit.","url":"https://tycholiz.github.io/Digital-Garden/notes/ed8be337-7a10-4c17-93a6-d1bd7c268ef8.html","relUrl":"notes/ed8be337-7a10-4c17-93a6-d1bd7c268ef8.html"},{"doc":"Stash","title":"Stash","hpath":"git.stash","content":"\n# Stash\n- A stash is simply a list of patches, that you can apply wherever you want.\n- Take all modified tracked files (that are unstaged) and staged changes, and save them onto a stack of unfinished changes\n\t- When you run git stash (alias of `git stash save`), git makes two commits that are not on any branch. One commit holds the state of the index, the second commit holds state of the work tree.\n- `git stash` takes uncommitted changes, stores them internally, then runs `git reset --hard` to give us a clean working directory.\n\t- This means that stashes can be applied to any branch, useful if we ever discover that we were developing on the wrong branch.\n\n### Apply vs Pop\n- pop will delete the stash after it is applied, while apply keeps it around for future use\n\t- this is why the below trick to revert the stash does not work if `pop` is used\n","url":"https://tycholiz.github.io/Digital-Garden/notes/7ae29bf5-b7cc-4323-acac-42ab61284c5e.html","relUrl":"notes/7ae29bf5-b7cc-4323-acac-42ab61284c5e.html"},{"doc":"Cook","title":"Cook","hpath":"git.stash.cook","content":"\nDiff between stash and HEAD (ie. view the changes tied to the stash)\n- `git stash show -p stash@{1}`\n\nRetain staged work of stashes\n- run `git stash pop --index` so that staged files return as staged when you pop the stash\n\nReverting `stash apply`\n- `git reset --hard`\n\t- assuming you had everything in a clean state when you started\n\nPopping stash onto a new branch\n- `git stash branch <branch-name> stash@{0}`\n\nAdding a message\n- `git stash save \"<message>\"`\n\nIncluding untracked files with stash\n- `git stash --include-untacked`\n\nDeleting a stash\n- `git stash drop stash@{1}`","url":"https://tycholiz.github.io/Digital-Garden/notes/dfee8410-fde4-4e35-a448-bda26d186563.html","relUrl":"notes/dfee8410-fde4-4e35-a448-bda26d186563.html"},{"doc":"Show","title":"Show","hpath":"git.show","content":"\n- `git show <SHA>` - show differences introduced by a commit","url":"https://tycholiz.github.io/Digital-Garden/notes/a03cacce-73d5-4f59-8021-d0f73e719bbd.html","relUrl":"notes/a03cacce-73d5-4f59-8021-d0f73e719bbd.html"},{"doc":"Rm","title":"Rm","hpath":"git.rm","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/ede963d4-1f8c-4d4a-b658-acd9b06e8029.html","relUrl":"notes/ede963d4-1f8c-4d4a-b658-acd9b06e8029.html"},{"doc":"Rm","title":"Rm","hpath":"git.rm.cli","content":"\n# rm\n- remove a file from the index as well as working directory so they will stop being tracked.\n- with `--cached`, only remove it from index.","url":"https://tycholiz.github.io/Digital-Garden/notes/fd06e025-8a4f-4967-9340-666b6c55f98c.html","relUrl":"notes/fd06e025-8a4f-4967-9340-666b6c55f98c.html"},{"doc":"Restore","title":"Restore","hpath":"git.restore","content":"\nRefreshes the current working version to be the HEAD version. No modifications are made to the staging area.\n- We can also pass `-s <branch-name>` to restore in the version on the specified branch\n\nDesigned to replace version-checking out functionality of the `checkout` command\n","url":"https://tycholiz.github.io/Digital-Garden/notes/57b08fb1-8baf-43c7-9c8d-a0890e8dd61d.html","relUrl":"notes/57b08fb1-8baf-43c7-9c8d-a0890e8dd61d.html"},{"doc":"Reset","title":"Reset","hpath":"git.reset","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/67eb415f-b5e5-4df7-b9c6-75e514789c81.html","relUrl":"notes/67eb415f-b5e5-4df7-b9c6-75e514789c81.html"},{"doc":"Reset","title":"Reset","hpath":"git.reset.cli","content":"\n# Reset\n- `reset` is the opposite of `add`\n- In general, git reset is used to move branch tips around (likely to another commit)\n- If we pass a filename to `git reset`, then the staging area will be updated to match the given commit instead of the working directory (the branch pointer does not move).\n\t- ex. if we have 3 files staged, we can remove one of them with `git reset HEAD index.js`, making `index.js` match the version found in HEAD. The working directory and current branch are left alone. The result is a staging area that matches the most recent commit and a working directory that contains the modified `index.js` file.\n\t\t- Put another way, we are unstaging the file.\n![8f6b8c503feeeceab0f2175850b7acbd.png](:/78999c9edf5c4f129b389d40acc423cc)\n\n## Soft\n- moves the HEAD to the provided `<SHA>`, while keeping working tree and staging area intact.\n- *soft* means the commit is canceled and moved before the `HEAD`\n- `git reset --soft HEAD^` - undo last commit\n\n## Mixed\n- moves the HEAD to the provided `<SHA>`, while keeping working tree intact.\n\t- Move the HEAD backward `<n>` commits, but don’t change the working directory.\n- reset the staging area\n- `git reset <file>` is the opposite of `git add <file>`\n- ex. if we are on c3 and do `git reset c1`, we will go back to c1, and the working directory and index will remain unchanged\n    - This is the default type of reset\n\n## Hard\n- moves the HEAD to the provided `<SHA>`, keeping neither the working tree nor staging area intact.\n\t- Move the HEAD backward `<n>` commits, and change the working directory to match\n- the only version of `reset` that actually results in a changed working directory file.\n- ex. if we are on c3 and do `git reset --HARD c1`, we will go back to c1 (i.e. our head will point to c1) and c2 and c3 will be \"destroyed\", and working directory wiped.","url":"https://tycholiz.github.io/Digital-Garden/notes/8c140a40-9d73-4380-b269-bb1799a9759b.html","relUrl":"notes/8c140a40-9d73-4380-b269-bb1799a9759b.html"},{"doc":"Remote","title":"Remote","hpath":"git.remote","content":"\n# Remote\n- a pointer to a branch on a copy of the same repository.\n\t- remote simply means a copy of the repo on someone else's machine.\n- *origin/master* means \"the master branch of the origin remote\"\n- When you clone a repository, Git automatically adds an origin remote pointing to the original repository, under the assumption that you’ll probably want to interact with it down the road\n\t- run `git remote -v` to see what origin is\n- we can run `git branch -r` to see the remote branches available to us. If there are none, then we can run `git fetch <remote-name>` to copy them over.\n- Checking out a remote branch takes our HEAD off the tip of a local branch, resulting in a detached HEAD:\n![efafe6e1a14641006f80ee5a895572b2.png](:/287b1c5127484a3b99356dfdfa60acbc)\n\n## Upstream\n- Imagine we forked a repo remotely, then forked it locally. `Upstream` would be the original repo that we forked, and `origin` would be the remote repo of our forked version\n- upstream means \"towards the trunk\" (ie. towards the single source of truth)\n- By default, `origin/master` is set as the upstream branch of `master`, so `git pull/push` will default there.\n- `git branch -vv` <-- show upstream branch of local version.\n\n## Tracking Branch\n- The local branch that is connected to a remote branch.\n- ***ex.*** `master` **==>** `origin/master`\n- checking out a remote branch from the local repo will create that branch.\n- `git branch --remotes`\n- `git remote -v` - list all remote repos you've connected to\n\n#### Get URL of remote\n`git remote get-url origin`","url":"https://tycholiz.github.io/Digital-Garden/notes/28ca9bfb-b84b-42b5-ad17-b09dbaba5daa.html","relUrl":"notes/28ca9bfb-b84b-42b5-ad17-b09dbaba5daa.html"},{"doc":"Refspec","title":"Refspec","hpath":"git.refspec","content":"\n### Refspec\n- Map a branch in local repo to branch in remote repo.\n    - Allows us to manage remote branches using local Git commands\n- Specified as `<src>:<dst>`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/70aefd3d-077b-4257-944e-bc71061daa5b.html","relUrl":"notes/70aefd3d-077b-4257-944e-bc71061daa5b.html"},{"doc":"Reflog","title":"Reflog","hpath":"git.reflog","content":"\n### Reflog\n- The reflog is a list of chronological hashes, which represent where you have been during commits, all without regard to branches.\n\t- since branches don't matter, we are able to recover dangling commits.\n\t- Each time a branch is updated to point to a new reference (ie. HEAD changes), an entry is written in the reflog to say where you were.\n\t- Since the branch is updated whenever you commit, the git reflog has a nice effect of storing your local developer's history.\n- It serves as Git's safety net, recording every change made in the repo (regardless of whether it was committed or not)\n- The listed commit hash represents the HEAD after that action.\n- While `git log` shows a history of the commits, we can think of `git reflog` as showing us a history of everything (what branch we checked out, what commands we ran etc.)\n- Becuase it keeps a full history, even if we were to `git reset`, we can still access the commit SHA\n- ***ex.*** we made some commits, then reset. If we decide that we want to *undo* the reset, all we need to do is checkout the commit we want with `git checkout HEAD@{1}`\n```\nad8621a HEAD@{0}: reset: moving to HEAD~3\n298eb9f HEAD@{1}: commit: Some other commit message\nbbe9012 HEAD@{2}: commit: Continue the feature\n9cb79fa HEAD@{3}: commit: Start a new feature\n```\n- This puts us in detached head state. All we need to do is create a new branch, and continue working on our feature.\n\n#### Data Recovery\n- Most of the time, the reflog is our friend in the circumstance where we want to recover data that has been \"lost\"\n\t- we can either run `git reflog`, or use `git log -g` which gives us normal log output for the reflog\n- Imagine we hard reset a number of commits, effectively \"erasing\" them from the git log. All we need to do is find that commit with `git log -g`, create a new branch with its SHA (`git branch recover-branch <SHA>`)\n- If the data we are looking for is not in the reflog, we can try using `git fsck --full`, which will list all objects that aren't pointed to by another object.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/b8a5010c-188c-4b0b-be95-ed19f6eb4c20.html","relUrl":"notes/b8a5010c-188c-4b0b-be95-ed19f6eb4c20.html"},{"doc":"Ref","title":"Ref","hpath":"git.ref","content":"\n### Relative Refs\n- we can use relative refs to move around.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/31f4f3e5-bab2-4d9c-a573-4125f49867d6.html","relUrl":"notes/31f4f3e5-bab2-4d9c-a573-4125f49867d6.html"},{"doc":"Refs","title":"Refs","hpath":"git.ref.inner","content":"\n### Ref\n- In a world where all we have is blobs, trees and commit objects, we would have to remember the commit SHAs so we have a starting point for our history. Instead it would be easier if we had a file where we could store that SHA under a simple name, so we could use the name instead of the SHA. These simple names are called *refs*.\n- A ref is anything pointing to a commit.\n    - ie. branches (local/remote) and tags, mostly.\n    - unlike objects, they are mutable and constantly change.\n\t\t- In Git, the immutable parts are the expensive ones (ex. an entire blob), while the mutable parts are just references, and therefore cheap (ex. branch, remote, HEAD)\n\t- Think of it like a user-friendly alias for commit SHA\n\t\t- ex. `my-feature-branch` is actually an alias for the branch. In reality, the branch name is a SHA\n- `git show-ref`\n\t- ex. if we look at `.git/refs/heads/master`, we will see a commit SHA, which is the latest commit on this branch. If this wasn't done automatically for us, we could manually create this file with the SHA, which would effectively be saying \"hey Git, when we are on master branch, the latest commit will be this SHA\"\n\t\t- Therein lies basically what a branch is: a simple pointer to the latest commit\n- stores pointers to commit objects that we would consider to be significant\n\t- ie. the ref is the location of the commit \n    - The head commit of each branch that exists (`refs/heads` directory)\n    - The head commit of each branch on the remote repos (mostly just `origin`)\n    - stash commits\n    - tags\n- if commits point to trees, and trees point to blobs, then refs conceptually point to commits","url":"https://tycholiz.github.io/Digital-Garden/notes/0c7b4dcf-7e88-432f-b6ac-bb12a1dedb7b.html","relUrl":"notes/0c7b4dcf-7e88-432f-b6ac-bb12a1dedb7b.html"},{"doc":"Rebase","title":"Rebase","hpath":"git.rebase","content":"\n# Overview\nRebase is an alternate way to get commits from a feature branch onto a master branch\n- with `git merge`, we take all of the commits on the feature branch and try and jam them together with the commits of the master branch, in order to make a new commit (the merge commit).\n- with `git rebase`, we take all of the commits from the current branch and move them on top of another. This involves rebasing first, then doing a regular merge (which will be fast-forward, since the base commit will now be a direct ancestor)\n\t- When we run `git rebase master` from our feature branch, we say \"hey master, I want to take all the work I've done so far and make it look like it was built *directly* on top of all of the work that *you* have\"\n\nIf you're `rebase`ing, you will always have to `--force` when you push, because rebase rewrites history— That's simply how it works. Rewritten history isn't going to fast-forward on top of that same history by definition.\n\nAs the name suggests, rebase exists to change the base of a branch (ie. the origin commit). We do this by replaying a series of commits on top of a new base.\n- This is mostly needed when a series of local commits is deemed to start from an obsolete base (put another way, our local master is very out of sync with origin/master)\n\n- behind the scenes, git is duplicating the commits of the feature branch, putting them on top of the master branch, and then blowing away those original feature branch's commits (hence why they are greyed out in the following image).\n\t- therefore, in a sense it is rewriting history, as evidenced by the fact that the duplicated commits have a different SHA than the originals\n\n- Because rebase rewrites history, it's important that we pull all remote changes to our local master branch before rebasing, so that we are reanchoring our feature branch's commits to the current version of the code.\n- ex. we are on branch `about`, which has diverged from `master`. We want to incorporate changes from master into `about`. From `about`, we run `git rebase master`:\n\n## Process\nGit always squashes a newer commit into an older commit or “upward” as viewed on the interactive rebase todo list, that is into a commit on a previous line.\n- This means if commit1 is a `WIP` commit, and commit2 is the one we want to keep (along with changes from commit1), then we must actually `squash` commit2 into commit1. Doing so will allow us modify the commit message (now a combination of the messages from commit1 and commit2) before rewriting the history.\n\n\n### Behind the Scenes\n1. Git will checkout the upstream branch and copy all the changes you've done since you last merged, placing them on the tip of the upstream branch.\n\t- ex. in the above image, to an outside observer it would seem that you had checked out the upstream branch from ***a***b and then done your changes.\n\t- note: here upstream most likely is origin/master or simply master, but it could be any branch we are \"merging\" into.\n2. Git produces a series of patch files of your work and starts applying them to the upstream branch.\n\t- consider that these commits are actual copies with different commit SHAs\n\n### Process\n1. when finished with feature branch, pull all remote changes onto master\n\t- if local master === origin/master, step 2 can be skipped, since it would have no effect anyway\n2. from feature branch, run `git rebase master`, which will cause our feature branch commits to be anchored against the updated master branch\n\t- consider that when we checkout a new branch, we have a common base with the branch which we checked out from. Rebasing master here ensures that the remote changes that happened and got merged into master (remotely) are included as part of that anchor.\n3. from master branch, run `git rebase feature-branch`, copying and placing the commits of the feature branch onto the main branch.\n\n#### Conflicts\n- say we are rebasing 8 commits onto the new branch — each one could cause a conflict, and we can resolve the conflicts introduced by each commit one by one.\n\t- fix the file, run `git add`, then run `git rebase --continue` (which moves us on to the next patch, until all are completed)\n\n### When to use rebase\n1. Any time we are working on a long term branch that needs to stay somewhat up to date with master. It is better to keep it as up to date as possible, rather than staying diverged for a long time.\n\t- ex. Imagine working on an experimental branch and getting blocked at some point. This is a scenario that would cause your branch to diverge from master more and more over time. When it does come time to pick up work again, it would be a good time to rebase to the master branch. The result is that it would look just like you started from there.\n2. Consider that in a perfect world, my coworker and I would have a linear commit history (even though we are developing asynchronously, it makes more sense looking back if we have a straight line of commits). In this ideal world, I would be developing my work off the base of my coworker's work, and vice versa.\n3. Imagine we have a *quick-fix* branch that we don't want muddying up the history. If master has not been touched since we branched, the ff merge is automatic. However, if master has indeed changed, then we need a way to tweak *quick-fix* so it becomes a direct descendent of *master* again.\n\t- In this scenario, we want our local master to have the same tip as origin/master. This would allow us to do a ff merge, thus avoiding muddying up history.\n\n### Drawbacks to rebase\n- doesn't play too well with open source projects, since it becomes hard to trace changes introduced to a codebase.\n- doesn't work well when working on a shared branch, since commits are rewritten.\n- only rebase when working on a local branch prior to pushing, or on remote repos where you are the only contributor (ex. for backup purposes).\n\t- In the second scenario, we'll need to force push (since we replaced its commit history with a fresh one).\n\t- Issues arise when other people pull in objects that were orphaned by the rebase process.\n\n#### Shared branches\nRebase is not a great candidate for shared branches. Because `git push --force` is a fact of life to the \"rebase-way\" of Git workflow, we would have to be careful to check if someone else has pushed to the remote branch first. This is why we should use `--force-with-lease`, so that we cannot overwrite commits that have been pushed already to that remote branch. If there have been, we will get errors, and we can `git pull --rebase` to incorporate those changes, before force pushing again.","url":"https://tycholiz.github.io/Digital-Garden/notes/e9956de4-5207-4565-a279-52ef4620e836.html","relUrl":"notes/e9956de4-5207-4565-a279-52ef4620e836.html"},{"doc":"Cook","title":"Cook","hpath":"git.rebase.cook","content":"\n### See the patch of the current commit\n`git rebase --show-current-patch`\n\n## Status of rebase\nNumber of commits in this rebase\n`cat .git/rebase-apply/last`\n\nOut of the number of commits in this rebase, which are we on?\n`cat .git/rebase-apply/next`\n\nWhich commit is currently being applied?\n`cat .git/rebase-apply/original-commit`","url":"https://tycholiz.github.io/Digital-Garden/notes/abbc7d34-b06d-42f9-976e-715428fb2386.html","relUrl":"notes/abbc7d34-b06d-42f9-976e-715428fb2386.html"},{"doc":"Push","title":"Push","hpath":"git.push","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c9f476c8-a7e6-4048-85f4-546fa334a8ca.html","relUrl":"notes/c9f476c8-a7e6-4048-85f4-546fa334a8ca.html"},{"doc":"Push","title":"Push","hpath":"git.push.cli","content":"\n- `git push origin master` moves the HEAD of the central repo:\n\t- This has the exact same result as if we went on the central repo and ran a fetch and fast-forward merge.\n![cbbedd3aff8b9bae62000af2a996468e.png](:/1f2474a7a067490f91335052cb37b7cf)\n\n## Avoiding race conditions with force push\nWhat if we want to force push, but don't want to run into problems if someone else has pushed to that branch in the meantime?\nTo get a warning when trying to force push to a branch that has been committed to in the meantime, run:\n`git push --force-with-lease`\n\n## Force push\n- Force pushing to feature branches is a fact of life. Force pushing to a `master` branch should be considered with extreme care.\n\t- Force pushing to feature branches allows us to have a clean history of commits on that feature branch. If we embrace `rebase`-`merge` instead of just `merge`, then we will encounter lots of scenarios where we must force push to that feature branch.\n\t\t- Note: this strategy should NOT be taken if multiple developers are working on the same branch. Rebase is not a good candidate for shared branches.","url":"https://tycholiz.github.io/Digital-Garden/notes/8b33ce0c-3d57-4c8a-a5b5-3088b60584f0.html","relUrl":"notes/8b33ce0c-3d57-4c8a-a5b5-3088b60584f0.html"},{"doc":"Prune","title":"Prune","hpath":"git.prune","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d4b37f24-9a40-4df0-b639-5b7356f84d8d.html","relUrl":"notes/d4b37f24-9a40-4df0-b639-5b7356f84d8d.html"},{"doc":"Prune","title":"Prune","hpath":"git.prune.cli","content":"\n# Prune\n- delete lost or \"orphaned\" objects (ie. those that are unaccessable by any ref). Any commit that cannot be accessed through a branch or tag is unreachable.\n- prune is a garbage collection command, and considered a child command of `git gc` (in other words, `gc` runs `prune`)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/af6927df-3bc7-4204-8f25-6fa950645a1d.html","relUrl":"notes/af6927df-3bc7-4204-8f25-6fa950645a1d.html"},{"doc":"Packfiles","title":"Packfiles","hpath":"git.packfile","content":"\n### Packfiles\n- since different versions of the same file create multiple blobs, it can get inefficient if we only difference between these versions were only a single line. What would be great is if we could store one version of the file, and just store the delta of the other with the first.\n\t- Git does exactly this using packfiles.\n- The initial format in which Git saves objects on disk is called a “loose” object format. Occasionally Git packs up several of these objects into a single binary file called a *packfile* for the purpose of being more efficient.\n- Packfiles are created under 3 circumstances:\n\t1. there are too many loose objects around (~7000)\n\t2. `git gc` is run manually\n\t3. we push to a remote repo \n- When we run `git gc`, we will notice that many of the objects in our object database disappear and are replaced by packfiles. The objects that remain are those blobs that aren't pointed to by any commit (in other words, the blobs were never added to any commits). Because of this, those blobs are considered dangling and as a result were never packed up into the new packfile.\n\t- The other files are the packfile and an index. The packfile contains the contents of all the objects that were removed from the filesystem. The index can be thought of as the index of a textbook, which helps us locate specific objects quickly.\n\t\t- if we inspect this packfile index with `git verify-pack`, we can see that 2 versions of the same file will have one version referencing the other, showing that we use deltas to be efficient. \n\t\t","url":"https://tycholiz.github.io/Digital-Garden/notes/95840bc3-6a00-42c5-9d27-474ad1308940.html","relUrl":"notes/95840bc3-6a00-42c5-9d27-474ad1308940.html"},{"doc":"Object","title":"Object","hpath":"git.object","content":"\n# Objects\n- Git is built around manipulating the structure of the 4 object types (blob, tree, commit, tag). In a way, it is like a mini-filesystem that sits upon the machine's filesystem\n- Nearly all git commands manipulate either the working tree, the staging area, or the commits.\n- objects are immutable\n- A git repo is a collection of objects, with each having its own hash\n    - A commit is a hashed object\n- Each object consists of 3 things:\n    - *type* - what type of object it is (blob, tree, commit, tag)\n    - *size* - the size of the contents\n    - *content*\n\n## SHA of Objects\n- The SHA of the object is generated by the content of the files, not the names\n\t- Therefore if we have 2 files `foo.txt` and `bar.txt` with identical content, then the SHA will be the same. But if we introduce an extra character in one of the files, then the SHA will be different. \n\t- the object is created as soon as it is known to git (ie. when added to index)\n- inside `.git/objects`, we see many hexadecimally named directories. The names of these are actually the first 2 digits of the SHA\n\t- ex. `.git/objects/77/a54737` is the commit with SHA `77a54737...`\n- The commits are encrypted, but we can see the contents of the object with `git cat-file -p <SHA>`\n\t- use `-t` to see what type the object is \n- The below image is a representation of our workflow. Here we have 3 commits (with the first commit on the left). It's important to note how the same blob is referred by different trees. This is because that file didn't change between commits, therefore the blob is identical \n![](/assets/images/2021-03-06-16-16-37.png)\n![](/assets/images/2021-03-06-16-17-02.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/4b305ad1-740e-43ad-9eb6-5c9ab34c6997.html","relUrl":"notes/4b305ad1-740e-43ad-9eb6-5c9ab34c6997.html"},{"doc":"Merge","title":"Merge","hpath":"git.merge","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a27714da-49f6-4f50-9b91-5ccbb6cf0115.html","relUrl":"notes/a27714da-49f6-4f50-9b91-5ccbb6cf0115.html"},{"doc":"Merge","title":"Merge","hpath":"git.merge.cli","content":"\n# Merge\n- to undo a merge, simply run `git reset --hard HEAD`\n\t- by default, only the index will be reset, meaning the partially merged files will remain in the working directory.\n\t- If we've merged and committed already but want to revert the merge, we can discard the commit with `git reset --hard ORIG_HEAD`\n- From a DX standpoint, the big difference between rebase and merge is that `merge` will preserve the \"railroad\" switch that was our branch when we look at the history, whereas `rebase` will show it linear.\n\t- \"Should this branch remain visible in the graph?\"\n\n[git merge with multiple parents](https://softwareengineering.stackexchange.com/questions/314215/can-a-git-commit-have-more-than-2-parents)\n\n### Fast-Forward Merge \n- Occurs when we tell git to merge our feature branch into master, but git realizes that there are no 2 branches to merge, but instead the master just needs to be brought up to speed to the current feature branch. This is known as *fast-forwarding*\n- effectively what is happening is Git engine is moving the current branch tip up to the target branch tip.\n- If the tip of master hasn't moved since we branched off, no merge commit will be made. The HEAD will simply move to the most recent commit of the feature branch.\n\t- In a ff merge scenario, since the master has not changed since we branched, a \"merging/melding\" of branches is not relevant here.\n\t\t- This ff may not be desirable. Imagine we want to merge in a branch called *oauth*. This is a pretty significant feature, and we'd like to keep the branch in our history. This is a scenario where we can merge with `--no-ff`.\n\n![240d716cd6708b64cbefb47f30773132.png](:/5edfa6a1c2a4442faff1e60705f87d32)\n\n- ***non fast-forwarding*** (a.k.a *3 way merge*) ex. - we create a feature branch from master. We do some work on feature, then decide to merge into master. Since we had branched off from master, other feature branches have been merged into master. This means the git engine has to figure out how to *merge* the master branch (which is now different from how our feature remembers it) and our feature branch.\n    - If this were a *fast-forward merge*, then there would have been no merges into master during the time that our feature branch existed.\n\n## When to use Merge\n1. If master remained untouched while we were working on our feature branch (in which case a fast-forward merge would be automatic).\n2. When we have work that is related to an agile ticket or bugfix (which would allow us to look back in history and clearly see it)\n\n## Terminology\nThe `ours` `theirs` terminology is from the perspective of the \"already existing branch\". Put another way, if we are on main and merging in a feature branch, main is `ours` and feature is `theirs`. Also in the same way (and using a different paradigm), `current change` would refer to main, and `incoming change` to feature.\n\nThis is also the same as with rebase. if we are rebasing main onto feature, then `ours` will refer to main (`git rebase main` from feature branch)\n\n## Merge Strategies\n- when merging (and pulling), we can pass the `-s` flag to specify which merge strategy we want to use.\n\t- When not specified, Git will select the most appropriate based on the provided branches.\n- merges can be either explicit or implicit\n\t- *explicit* means a new merge commit is created.\n\t- *implicit* means no new merge commit is made, and any evidence of a branch having existed is erased from history\n\t\t- These are triggered either by fast-forward merges or rebases.\n\nRecursive\n- operates on 2 heads.\n- default when merging one branch.\n- has additional sub-operation options\n\t- *ours* - auto-resolve conflicts by accepting all \"ours\" versions of the HEADS\n\t- *theirs* - accept all \"theirs\" versions of the HEADS\n\nResolve\n- operates on 2 heads using 3-way merge algorithm.\n\nOctopus\n- default when there are more than 2 heads.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e09ff813-ca8e-4e61-b0c6-bc76819153a5.html","relUrl":"notes/e09ff813-ca8e-4e61-b0c6-bc76819153a5.html"},{"doc":"Git","title":"Git","hpath":"git.md","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/6957fdb6-2c38-4d2c-9d5c-3073306e86c7.html","relUrl":"notes/6957fdb6-2c38-4d2c-9d5c-3073306e86c7.html"},{"doc":"CLI","title":"CLI","hpath":"git.md.cli","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a58dc841-63cc-4b9c-95ef-bd1e35aa1098.html","relUrl":"notes/a58dc841-63cc-4b9c-95ef-bd1e35aa1098.html"},{"doc":"Log","title":"Log","hpath":"git.log","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3c552b5b-6b8f-4442-9f94-1504e6a4ab6f.html","relUrl":"notes/3c552b5b-6b8f-4442-9f94-1504e6a4ab6f.html"},{"doc":"Log","title":"Log","hpath":"git.log.cli","content":"\n### Log\n- we can pass a filename to `git log` to only show the history of a given file\n- also, we can pass a commit SHA to see the revision history up until that point\n\t- This fact shows how by default, `git log` will run with `HEAD`, showing the revision history from the current branch's tip\n- We can run `git log -n 4` to show only the last 4 commits from HEAD\n\t- equivalent to `git log HEAD~4..HEAD`\n- running `git log master..origin/master` checks what exists on origin that doesn't on our local master. The opposite, `git log origin/master..master` checks what exists on our local master that doesn't on origin. If the output is empty, it means that our branches have not diverged.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/ee7eb0b4-a8e6-483b-a23c-5228b90e43ad.html","relUrl":"notes/ee7eb0b4-a8e6-483b-a23c-5228b90e43ad.html"},{"doc":"Inner","title":"Inner","hpath":"git.inner","content":"\n- At the core of Git is a simple key-value store, with the value being the content, and the key being a SHA representing that content. \n- With a fresh git repo, upon adding `foo.txt` to the index, a new object (in `.git/objects`) is created. If we run `git cat-file` on that object, we will see the contents of the file. \n- Upon committing, 2 more objects will be created:\n\t1. a tree object, which makes reference to `foo.txt`, which points to a blob object SHA\n\t\t- if we `git cat-file` that SHA, we will see the contents of `foo.txt`.\n\t\t\t- What this demonstrates is that we have a tree object which references a filename, which is associated with the content in the file. None of these things are glued together, but they merely make references to each other.\n\t2. a commit object, with reference to the tree (the same tree that was just created)\n\t\t- when we run `git commit`, the SHA of that commit object is output into the console\n\t- We have just demonstrated that a commit points to a tree, and a tree points to a blob. First a blob is created (when adding to index), then committing will create a commit and tree object.\n- Now with a clean tree, we create a new file `bar.txt`, and edit the contents of `foo.txt`. We add both those files to the index.\n\t- two more blob objects are created\n- With those 2 files in the staging area, we commit, and 2 more objects are created: a tree object and a commit object.\n\t- Since this is the second commit, it also references its parent commit \n\t- The tree object will reference both `foo.txt` and `bar.txt` \n- Again with a clean tree, we create a new file `baz.txt`, add it and commit it.\n\t- Interestingly, the tree object that just got created will reference all `foo`, `bar` and `baz`, even though only `baz` was changed in this commit.\n\t\t- This is because the commit object maintains the entire state of every file (and every version of the file) by SHA at that point in time.\n\t\t\t- This is precisely why we can cherry pick a commit and put it on disk\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5cfbcf7f-b9c7-4c59-9663-387ef61fd498.html","relUrl":"notes/5cfbcf7f-b9c7-4c59-9663-387ef61fd498.html"},{"doc":"Head","title":"Head","hpath":"git.head","content":"\n### HEAD\nThe pointer that points to the latest commit on the currently checked out branch.\n- Therefore the HEAD is what lets Git know which commit will be the parent for the next commit.\n\n- spec: While a branch simply points to a commit, a HEAD simply points to a branch (or a commit). If it points to a commit, it is because the commit is not at the tip of its branch, resulting in a detached HEAD.\n\t- Recall that when HEAD does not coincide with the tip of any branch, the result is a detached HEAD. From Git's point of view, this simply means that HEAD doesn't contain a local branch.\n- under normal circumstances, HEAD points to the branch we currently have checked out. However, we can also checkout a commit. Interestingly, we can checkout the same commit that our branch points to (which would be a detached head). In this case, both working trees would be identical. \n- normally, the HEAD file is a symbolic reference to the currently checked out branch. We can see this by logging out the contents of `.git/HEAD`\n\t- here, symbolic reference means that it contains a pointer to another reference.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/9f58b063-fce0-4a87-88ae-aba5b2092756.html","relUrl":"notes/9f58b063-fce0-4a87-88ae-aba5b2092756.html"},{"doc":"Gitignore","title":"Gitignore","hpath":"git.gitignore","content":"\n# Gitignore\n- When git comes across a file in your repo that is untracked, it will report it to you (in `git status`). However, if that file has been added to `.gitignore`, it will suppress it.\n- once a file is known to git (ie. it has been in the index), adding the file to `.gitignore` has no effect (ie. the file will continue to be tracked)\n- if you need to stop tracking a file (for example, we've already added the file to the index, but now want to start ignoring it), we need to run `git rm --cached <file>`\n\t- for folders, `git rm -r --cached <folder>`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8e7caa04-9b26-44bc-b884-d15ace9e7647.html","relUrl":"notes/8e7caa04-9b26-44bc-b884-d15ace9e7647.html"},{"doc":"Formulas","title":"Formulas","hpath":"git.formulas","content":"\n### Rewording/deleting commits\n- imagine we want to change a commit message from 2 commits ago, or imagine we want to delete it altogether\n\t1. We can run `git rebase -i HEAD~2`\n\t\t- meaning \"I want to operate on the last 2 commits\"\n\t2. vim will open and will list all of the commits that we have asked to change.\n\t3. change the command from `pick` to `reword` (or `drop` to delete) of the relevant commit, then save and close the file.\n\t4. change the message, save and close.\n\n### Changing contents of commit/Splitting commits\n1. run `git rebase -i HEAD~n`, where `n` is the commit immediately preceding the commit we want to edit.\n3. change the rebase command of the commit we want to edit from `pick` to `edit`\n4. run `git reset --mixed HEAD^`, which leaves the working directory unchanged, but reverses the commit\n\t- `HEAD^` is the parent of our current commit\n6. do our normal operations, adding files, deleting files, or modifying them as we please. `git add` them, `commit` them as per normal workflow.\n7. run `git rebase --continue`\n\n### Reverting commits\n- think of `git revert` as an inverse operation to `git commit`. Effectively, this command creates a new commit that undoes all of the changes introduced by a certain commit.\n- Imagine we made a commit that simply changed which port our app connects to. Later on down the line, imagine that we want to \"undo\" that, and go back to the original port. We could change that port in the code and commit it, or we could simply run `git revert <SHA>` to make a new commit, known as a `revert commit` whose sole purpose is to reverse the changes that that particular commit actually made.\n\n### Squashing commits\n- Imagine we made 3 commits that should logically only be one.\n\t1. run `git rebase -i HEAD~3`\n\t2. leave the commit that everything will get squished into alone, but change the commits that will be thrown away from `pick` to `fixup`\n\t\t- listed by oldest to newest\n\t\t- fixup and squish are similar, but fixup will discard the commit messages of the discarded commits.\n\t\t- if we want to retain the commit messages, then use `squash` instead of `fixup`\n- If we compare the git logs before and after, we will notice that the SHA of the commit with the same message will be different, showing that we are in fact rewriting history\n\n### Splitting commits\n- Imagine we have made a commit that realistically should actually be split into 2 commits (\"add navbar and fix bug\")\n\t1. run `git rebase -i HEAD~2`\n\t2. change the commit we want to split from rebase operation `pick` to `edit`\n\t3. after saving the file, git will put us onto a special rebasing branch\n\t4. unstage all files that were added during that commit by running `git reset HEAD^`\n\t5. perform normal workflow, by adding commit1 changes and committing, then adding commit2 changes and committing\n\t6. run `git rebase --continue`\n\n## Undoing Work\n- this will discard everything permanently\n\n### Restoring to the state of the last commit\n- if just a single file, we can simply run `git checkout HEAD <file>` to blow away all changes, and restore the file to what it was in the most recent commit.\n- if we want to blow away all changes and get the exact state of the last commit, we can run `git reset --hard HEAD`\n\t- This tells Git to replace the files in your working copy with the \"HEAD\" revision\n\t- we can replace HEAD with any SHA to go back to that previous version.\n\t- note: this will not produce any new commits (like revert), nor will it delete any old ones. Instead, it works by resetting your current HEAD branch to an older revision (also called \"rolling back\" to that older revision)\n\t- note2: since this doesn't rewrite history, the commits we \"erased\" are still available","url":"https://tycholiz.github.io/Digital-Garden/notes/879ac901-4561-4710-bed7-9edef2fadb91.html","relUrl":"notes/879ac901-4561-4710-bed7-9edef2fadb91.html"},{"doc":"Fetch","title":"Fetch","hpath":"git.fetch","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5d2311f4-0e78-4872-935e-86388d2ab38f.html","relUrl":"notes/5d2311f4-0e78-4872-935e-86388d2ab38f.html"},{"doc":"Fetch","title":"Fetch","hpath":"git.fetch.cli","content":"\n# Fetch\n- running fetch will pull all refs and objects that we don't already have from the remote repo, and will put them into the object database. \n- fetch is opposite of push, in the sense that fetch will import branches, while push will export them\n\t- when we run `git push origin master`, we are exporting our master branch to origin. If we were to run `git branch` on origin, we would see our local branch that we just pushed, listed as a local branch.\n\t\t- ex. Imagine we had a repo called `foo`, then cloned in locally into a different directory on our machine and called it `bar`. On `bar`, origin would default to `foo` (since it was copied from `foo`). `foo` would not have an origin by default, since it did not come into existence by being copied. We could manually add `bar` as a remote. Now, if we made a branch on `foo` and pushed it, that branch would be available locally on `bar`.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c124872f-b825-479f-985d-1af718c53e6b.html","relUrl":"notes/c124872f-b825-479f-985d-1af718c53e6b.html"},{"doc":"Diff","title":"Diff","hpath":"git.diff","content":"\n### Diff\n- `git diff` - show changes between index and working tree\n- `git diff --staged` - show changes between index and HEAD (ie. last commit)\n- `git diff master..feature-branch` - show changes between master and feature-branch \n\t- `git diff f733ed..` - show changes between a commit and HEAD\n- `git diff -- package.json` - show only changes in a specific file.\n\n#### Patch file\n- the default output of `git diff` is a valid patch file, meaning we can pipe its output into a file, give that file to someone else, and they can apply it with the `patch` command.\n\t- ex. `git diff master..experiment > experiment.patch`. The recipient can then run `patch -p1 < ~/experiment.patch`\n- Each patch represents a full commit, complete with metadata like author and date.\n\t- ex. If we have made 2 commits since master, then running `git format-patch master`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5280d1a2-8c19-476c-ace8-125f1fe5faa5.html","relUrl":"notes/5280d1a2-8c19-476c-ace8-125f1fe5faa5.html"},{"doc":"Deploy","title":"Deploy","hpath":"git.deploy","content":"\n## Centralized vs Distributed Git\n- we can run Git in a centralized system, where we have a git repo on a central server. All contributors must push and pull to the same repo for progress on the project to be made.\n\t- A concern with this is that for anyone to be able to contribute, they must have access to the entire project. Therefore you cannot maintain control with a centralized workflow.\n\t\t- This may not be a concern, if there are 2 or 3 people working on a project.\n- With an integrator workflow, everyone has their own private repo and public repo. When a contributor wants to add their changes, they push to their own public repo. From there, we can pull their changes into our local repo to ensure everything works as expected. If everything is good, we merge them into our local branch, then push that branch to the main repo. From there, everyone can pull those changes.\n\t- with this workflow, everyone pulls from a single official repo, but pushes to their own public repo.\n\nCentralized:\n![](/assets/images/2021-03-11-15-39-35.png)\nDistributed:\n![](/assets/images/2021-03-11-15-39-48.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/818eaa5c-d778-4b67-960a-b81c61c86ea1.html","relUrl":"notes/818eaa5c-d778-4b67-960a-b81c61c86ea1.html"},{"doc":"Conventional Commits","title":"Conventional Commits","hpath":"git.conventional-commits","content":"\n## About\nConventional Commits are designed to dovetail with SemVer\n- the CC `fix` corresponds to SemVer `PATCH`\n- the CC `feat` corresponds to SemVer `MINOR`\n- a `!` after the CC type/scope corresponds to SemVer `MAJOR` (ie. breaking change)\n\n## Types\n- fix\n- feat\n- build - changes that affect build components like build tool, ci pipeline, dependencies, project version etc.\n- ops - changes that affect operational components like infrastructure, deployment, backup, recovery\n- chore\n- ci\n- docs\n- style - changes to the code to do with formatting (white-space, semi-colons etc). Consistent with changes that a linter makes\n- refactor - a rewrite/restructure to the code that does not change any behaviour\n\t- perf - a subtype of `refactor` that improves performance.\n- revert\n- test\n\n## Structure:\n```\n<type>[optional scope]: <description>\n\n[optional body]\n\n[optional footer(s)]\n```\n\n### Subject (required)\nThe subject contains a succinct description of the change.\n- Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\n- Don't capitalize the first letter\n- No dot (.) at the end\n\n### Body (optional)\nThe body should include the motivation for the change and contrast this with previous behavior.\n- Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\n- This is the place to mention issue identifiers and their relations\n\n### Footer (optional)\nThe footer should contain any information about Breaking Changes and is also the place to reference Issues that this commit refers to.\n- optionally reference an issue by its id.\n- Breaking Changes should start with the word `BREAKING CHANGES:` followed by space or two newlines. The rest of the commit message is then used for this.\n\n## Example:\n```\nfeat(stripe)!: grant the ability to users to determine a primary payment method\n\nThis feature builds on database work that was previously done, to allow users to select their primary card from the payment-methods page.\nreference: JIRA-1337\n\nBREAKING CHANGES:\n```\n\ncommmit\nfeat(stripe): create `book_order` when user signals intent to buy.\n\nThis commit introduced a bit of a functionality change to how payments are processed through our backend and Stripe. Previously, rows in the invoices, invoice_items, and book_orders tables were only created after Stripe had verified our purchase (ie. by calling our webhook). Now, a row in the book_orders table will be inserted at the point that the user signals their intent to buy (ie. by entering their address info and hitting the 'Continue' button). At this point, our Express server makes an API call to get taxes (future implementation, though a placeholder solution included in this commit), and it is included on the Stripe PaymentIntent object. We retrieve this metadata in our webhook to insert it into the invoice table.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/542290d7-ff7f-4a93-b3f4-0e69931c17ce.html","relUrl":"notes/542290d7-ff7f-4a93-b3f4-0e69931c17ce.html"},{"doc":"Commit","title":"Commit","hpath":"git.commit","content":"\n# Commit\n- state of a folder structure at a given point in time\n- You tell Git you want to save a snapshot of your project with `git commit` and it basically records a manifest of what all of the files in your project look like at that point\n    - Most of the git commands interact with those manifests in some way\n- We can think about Git as a tool for storing, comparing and merging snapshots of our project\n- We can get a snapshot of an individual file with `git checkout <Commit SHA> <filename>`\n- A snapshot is basically a commit\n- Snapshot is to a repository as screenshot is to a video.\n    - since it represents one moment of time in that video\n\n## Ahead/Behind\n- If your current branch is 3 commits *ahead* of master, it means the current branch has 3 commits that don't exist on master\n- If the current branch is 3 commits *behind* master, it means there are 3 commits on master that don't exit on the current branch\n- The ahead number tells you roughly how much impact the current branch will have on the base branch should it be merged.\n- The behind number tells you how much work has happened on the base branch since this branch was started.\n    - If the number is high, it's an indication that there will not be a clean merge. This would be a good time to merge master (or other base branch) into the current branch, which would bring the \"behind\" number to 0\n- In the follow diagram:\n    - A is 2 commits behind and 0 commits ahead of B\n    - B is 0 commits behind and 2 commits ahead of A\n    - C is 1 commit behind and 2 commits ahead of A\n    - C is 3 commits behind and 2 commits ahead of B\n\n![](/assets/images/2021-03-06-16-45-56.png)\n\n## Detached State\n- Occurs when we check out a commit or a remote branch, as opposed to a local branch.\n    - Put another way, any ref that does not originate from your line of commits (and would thus be unable to trace any sort of history with the code you'd been working on)\n- If we were to develop in detached mode then try to merge it into master, git would complain to us, because by definition, detached mode means there is no path to get back into master. (ie. there is no way to reference that feature \"branch\")\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1a5afb2d-270f-47a0-a482-94df160eb804.html","relUrl":"notes/1a5afb2d-270f-47a0-a482-94df160eb804.html"},{"doc":"Strategies","title":"Strategies","hpath":"git.commit.strategies","content":"\n### Pull files from one branch to current branch\n- If we have a base branch that we want to keep, but several files from another branch that we want to pull over, from the base branch we can run:\n    - `git checkout <other-branch-name> -- file1.js file2.js`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/2851ea28-f302-415c-8aef-0582477bb975.html","relUrl":"notes/2851ea28-f302-415c-8aef-0582477bb975.html"},{"doc":"Merge Commit","title":"Merge Commit","hpath":"git.commit.merge","content":"\nWhen git creates a merge commit it will also by default append a list of files that had conflicts to the commit message:\n```\nConflicts:\n    src/foo-service.c\n    src/bar-client.c\n```\nThis is a piece of useful information for us when poring over the history to find out what went wrong\n- Even if your merge conflict was trivial, there is always a non-zero chance of introducing a bug when resolving a conflict, and seeing those lines in the merge commit message could be valuable information. They are basically a hint saying “Still confused? Maybe you should be extra careful when reviewing the changes in these files”.\n\n### no-ff\nWe can use `git merge --no-ff feature-branch` to force the creation of a merge commit, in circumstances where there would have been fast-forwarding","url":"https://tycholiz.github.io/Digital-Garden/notes/376eeee0-0701-4618-b924-cd37b559932e.html","relUrl":"notes/376eeee0-0701-4618-b924-cd37b559932e.html"},{"doc":"Commits","title":"Commits","hpath":"git.commit.inner","content":"\n# Commit Object\n- stores metadata about the commit, like who authored it, when it was made, what the previous commit is etc.\n- a commit points to a single tree, marking it as what the project looked like a certain point in time\n\t- Therefore, the commit object is what gives a project its sense of history in Git\n- While the tree and blob objects are the content of the Git, the commit objects allow us to use the data in a user-friendly way. Technically, Git could be used without commit objects. However, we would have to remember every SHA to recall the snapshots. Also, we wouldn't have important information like who saved the snapshots, when they were saved, and who saved them. These benefits are what commit objects bring to us.\n- When we run `git commit`, a commit object is created, and the parent commit is specified as the commit that the HEAD file points to (recall that a branch is just a pointer to a commit)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/af184fd1-0e7c-4344-806a-70e359cd5c37.html","relUrl":"notes/af184fd1-0e7c-4344-806a-70e359cd5c37.html"},{"doc":"Commit","title":"Commit","hpath":"git.commit.cli","content":"\n# Amend\n### Amend\n- amend the most previous commit, allowing us to add/remove files to that commit, or even to simply change the commit message.\n- if we forgot to stage some changes as part of the last commit, simply add them to the working tree (should be clean, considering we just committed), then run `git commit --amend` \n\n* * *\n\n## Operators\n\n### ~ Operator\n- allows us to reach parent commits\n\t- ex. `git show HEAD~2` shows the grandparents of HEAD\n- if the commit has more than 1 parent (ie. merge commit), then the first parent of the commit will be used by default.\n\t- Need to use `^` to specify a different commit\n\n### ^ Operator\n- allows us to specify which parent we want to refer to\n\t- spec: therefore, if there is only one parent, then `HEAD~1` refers to the same commit as `HEAD^1`\n![](/assets/images/2021-03-07-22-45-05.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/41babc40-d4e4-4b48-8020-2c71f674c1ef.html","relUrl":"notes/41babc40-d4e4-4b48-8020-2c71f674c1ef.html"},{"doc":"Boundary","title":"Boundary","hpath":"git.commit.boundary","content":"\n# Boundary Commit\nA boundary commit is the commit that limits a revision range but does not belong to that range. For example the revision range HEAD~3..HEAD consists of 3 commits (HEAD~2, HEAD~1, and HEAD), and the commit HEAD~3 serves as a boundary commit for it.\n\nMore formally, git processes a revision range by starting at the specified commit and getting at other commits through the parent links. It stops at commits that don't meet the selection criteria (and therefore should be excluded) - those are the boundary commits.\n[Source](https://stackoverflow.com/questions/42437590/what-is-a-git-boundary-commit)","url":"https://tycholiz.github.io/Digital-Garden/notes/d1e4ecc9-93d6-444f-a5c2-c0a7d6707606.html","relUrl":"notes/d1e4ecc9-93d6-444f-a5c2-c0a7d6707606.html"},{"doc":"Clean","title":"Clean","hpath":"git.clean","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/627056d4-5697-4f14-8b20-1425e0e6c8e9.html","relUrl":"notes/627056d4-5697-4f14-8b20-1425e0e6c8e9.html"},{"doc":"Clean","title":"Clean","hpath":"git.clean.cli","content":"\n# clean\n- remove untracked files and any patchfiles.","url":"https://tycholiz.github.io/Digital-Garden/notes/6eb21a6b-daf7-4572-98b2-b55a5deb3374.html","relUrl":"notes/6eb21a6b-daf7-4572-98b2-b55a5deb3374.html"},{"doc":"Cherrypick","title":"Cherrypick","hpath":"git.cherrypick","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c7bf1bb3-0f2a-4baf-a7e9-4d189fdf053c.html","relUrl":"notes/c7bf1bb3-0f2a-4baf-a7e9-4d189fdf053c.html"},{"doc":"Cherrypick","title":"Cherrypick","hpath":"git.cherrypick.cli","content":"\n`^` means inclusive\n`git cherry-pick 1234^..56789` will take commits from 1234 (inclusively) until 567789, and plop it on top of `HEAD`\n\nuse cherry-pick when you want to rebase, but have more power over what exactly you want to bring over from one branch to another","url":"https://tycholiz.github.io/Digital-Garden/notes/b57a43bf-5256-4914-913a-f964df110ba5.html","relUrl":"notes/b57a43bf-5256-4914-913a-f964df110ba5.html"},{"doc":"Checkout","title":"Checkout","hpath":"git.checkout","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e88c4341-6abf-4a29-9f22-58b83aea0165.html","relUrl":"notes/e88c4341-6abf-4a29-9f22-58b83aea0165.html"},{"doc":"Checkout","title":"Checkout","hpath":"git.checkout.cli","content":"\n# Checkout\n- The act of switching between different versions of a file, commit or branch\n- Think of it as switching between snapshots\n- When checking out a different branch, Git makes your working directory look like that branch. Any checked in content that is in your working directory but is not in the new tree will be removed. This is why Git only lets us checkout another branch if everything is checked in (ie. no uncommitted modified files).\n\t- The reason for this is that Git will remove files that are not necessary in the branch we are checking out.\n- if we add a file path to `git checkout`, only the specified file will be checked out, and the branch pointer will not be updated.\n\t- ex. `git checkout HEAD index.js` will check out the most recent (HEAD) version of `index.js`\n![0a0ed0120ebc145ec651db96de7b73c4.png](:/a7a7bd8f7ee646d4a308c17366095fad)\n\nChecking out a file\n- running `git checkout <file>` is similar to running `git reset <file>`, except checkout updates the working directory, while reset updates the staging area.\n\t- This has a similar effect to `git revert`, with an important difference: `revert` only undoes changes introduced the commit, while `checkout` undoes all changes *since* that commit.\n\t- ex. what if we want to change the file in the working tree to what it was 2 commits ago. We can run `git checkout HEAD~2 <file>`.\n- while running `checkout` on a branch/commit will move the HEAD reference, running `checkout` on a file will not, meaning we don't change branches.","url":"https://tycholiz.github.io/Digital-Garden/notes/e54fa79e-b3e2-41e3-a1f8-5606564ae94a.html","relUrl":"notes/e54fa79e-b3e2-41e3-a1f8-5606564ae94a.html"},{"doc":"Branch","title":"Branch","hpath":"git.branch","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a6b3c2fb-ebf7-486a-9a3c-04467f43415b.html","relUrl":"notes/a6b3c2fb-ebf7-486a-9a3c-04467f43415b.html"},{"doc":"Branch","title":"Branch","hpath":"git.branch.cli","content":"\n# Branch\n- Simply a pointer to a specific commit\n\t- Therefore, creating a branch is nothing more than writing 40 characters (a SHA) to a file.\n- The key difference is that it is moveable\n- It’s important to realize that Git uses the tip of a branch to represent the entire branch. That is to say, a branch is actually a pointer to a single commit—not a container for a series of commits\n\t- this is why Git diagrams show commits pointing to other commits.\n- If we make a new branch from master, that branch will point to the same commit as master. Once we commit, this new branch will then point to the new commit. Therefore, the branches will have diverged.\n- When we create a new branch, Git knows which commit to use because of the HEAD file.\n\n## Slashes in branch names\nImagine we had a branch named `stripe` already, and we wanted to make a new branch called `stripe/saved-methods`. This would cause an error, because as far as git knows, we have a file named `stripe`, and we are now trying to create a directory named `stripe`.\n- Because they are just directories, deleting `stripe/saved-methods` branch will result in the `stripe` directory still existing.\n\nIt's probably a good convention to stick to at most one level. Consider if we had a branch `wip/stripe`, and we then wanted to create `wip/stripe/saved-methods`, we would get an error because we are trying to create a directory `stripe`, when there is already a file called `stripe`\n\nWhen we have a branch with slashes in it, it gets stored as a directory hierarchy under `.git/refs/heads`.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f7367fbe-674f-4a2b-9233-92f09b3e1ade.html","relUrl":"notes/f7367fbe-674f-4a2b-9233-92f09b3e1ade.html"},{"doc":"Blob","title":"Blob","hpath":"git.blob","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/6d25fbd7-14b8-4fee-b9ed-0359fe242ba4.html","relUrl":"notes/6d25fbd7-14b8-4fee-b9ed-0359fe242ba4.html"},{"doc":"Blob","title":"Blob","hpath":"git.blob.inner","content":"\n### Blob\n- A particular version of one file\n\t- the only part of a file that is relevant to a blob is the file's contents. The object database does not care about the filename.\n\t\t- ex. if we write 2 different files with the same content to the object database, only one SHA will get recorded, since the same content hashed will always produce the same result.\n\t\t\t- the same can be said for a tree object, but not for a commit object, since the author and time of commit makes their content always unique.\n- the blob itself doesnt have a name, but it is referred internally by the hash of its content","url":"https://tycholiz.github.io/Digital-Garden/notes/d531f116-a2a9-4681-8920-6a33c2839197.html","relUrl":"notes/d531f116-a2a9-4681-8920-6a33c2839197.html"},{"doc":"General","title":"General","hpath":"general","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/29606f9d-5056-461c-9f4d-69d8759a1bf9.html","relUrl":"notes/29606f9d-5056-461c-9f4d-69d8759a1bf9.html"},{"doc":"Timezones","title":"Timezones","hpath":"general.timezones","content":"\n[Essential reading before working with Timezones](https://zachholman.com/talk/utc-is-enough-for-everyone-right)\n- Jensen recommends using https://date-fns.org/","url":"https://tycholiz.github.io/Digital-Garden/notes/13904d9f-bdef-48eb-813c-b6ed00be0b20.html","relUrl":"notes/13904d9f-bdef-48eb-813c-b6ed00be0b20.html"},{"doc":"Terms","title":"Terms","hpath":"general.terms","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/77d8d8d6-4f2a-4b0f-85f6-7f50f1175b3a.html","relUrl":"notes/77d8d8d6-4f2a-4b0f-85f6-7f50f1175b3a.html"},{"doc":"Indirection","title":"Indirection","hpath":"general.terms.indirection","content":"\n# Indirection \n- Indirection is the ability to refer to something through use of a *reference* (id). Normally, this involves manipulating the variable in question by using a pointer. Pointers exist to provide reference to an object (called indirection node)\n\t- *\"All problems in computer science can be solved by another level of indirection\"*\n\t- When the complexity of information becomes too much at one level, we simplify it with a layer of indirection that pushes the bulk of the complexity to a new level.","url":"https://tycholiz.github.io/Digital-Garden/notes/941b9de4-4dab-45e3-b767-f0b197c3d11e.html","relUrl":"notes/941b9de4-4dab-45e3-b767-f0b197c3d11e.html"},{"doc":"Idempotent","title":"Idempotent","hpath":"general.terms.idempotent","content":"\n## Idempotency\nIn mathematics, the an idempotent function is one that will yield the same output, even as its output feeds back as input indefinitely, such as:\n`Math.abs(Math.abs(Math.abs(Math.abs(-2))))`\n`String( String( x ) )`\na way to test mathematical idempotency is to check if the output of a single function is equal to multiple calls of that function:\n```\ncurrency( -3.1 ) == currency( currency( -3.1 ) )\n// true\n``\nWherever possible, restricting side effects to idempotent operations is much better than unrestricted updates.\n\nThe programming-oriented definition for idempotence is similar, but less formal. Instead of requiring `f(x) === f(f(x))`, this view of idempotence is just that `f(x);` results in the same program behavior (in other words, not just the output. *All* impacts of the function) as `f(x);` `f(x);`. In other words, the result of calling `f(x)` subsequent times after the first call doesn't change anything.\n\nIf you need to do side-effects, try to make it idempotent\n\nPure functions are idempotent in the programming sense\n\nPure functions can reference `free variables` (ie. those that are not defined within the function scope), as long as those variables are sure to not change during the execution of the program (ie. they are constant)\n\nAn example of idempotency is in Postgres, when we say `delete function if exists`, rather than just `delete function`\n- It is idempotent because running the command more than once has no additional effect than simply running it once.","url":"https://tycholiz.github.io/Digital-Garden/notes/560254d3-6e96-4d9d-85d9-389cc299a5d9.html","relUrl":"notes/560254d3-6e96-4d9d-85d9-389cc299a5d9.html"},{"doc":"Patterns","title":"Patterns","hpath":"general.patterns","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/37d21937-a6a6-4ec9-a37f-f8df6425006f.html","relUrl":"notes/37d21937-a6a6-4ec9-a37f-f8df6425006f.html"},{"doc":"Password Reset","title":"Password Reset","hpath":"general.patterns.password-reset","content":"","url":"https://tycholiz.github.io/Digital-Garden/notes/f31da66e-3105-41ee-8cb6-a60ad9aec9fc.html","relUrl":"notes/f31da66e-3105-41ee-8cb6-a60ad9aec9fc.html"},{"doc":"Feature Flags","title":"Feature Flags","hpath":"general.patterns.feature-flags","content":"\n### Using Feature Flags while undergoing a DBMS transition\n\"What if we want to go from Mongo to Postgres, without having to make the switch all at once?\"\n- https://featureflags.io/feature-flags-database-migrations","url":"https://tycholiz.github.io/Digital-Garden/notes/e2014920-f498-470e-a6e5-53216d45a803.html","relUrl":"notes/e2014920-f498-470e-a6e5-53216d45a803.html"},{"doc":"Event Sourcing","title":"Event Sourcing","hpath":"general.patterns.event-sourcing","content":"\nA system that allows us to query a database tells us the current state of everything. A system that allows Event Sourcing also tells us how we got here. In other words, the system keeps records of all changes that have happened.\n- Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.\n\nevery change to the state of an application is captured in an event object, and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.\n- To be clear, we are persisting two different things: an application state (ex. DB) and an event log.\n\nThe key to Event Sourcing is that we guarantee that all changes to the domain objects (ex. what is stored in the DB) are initiated by the event objects\n- This shows that when we want to change something in our database, our event system is first updated with the new log. Only once that log has been entered can our database be updated as well.\n\t- An implication of this is that we could dump the whole application state, and rebuild it from the log.\n\t- Another implication is that we can see the state of the application as a timeline, analogous to Git branching.\n\nA common example of an application that uses Event Sourcing is a version control system\n\n[[CQRS|general.patterns.CQRS]] and event sourcing often go hand-in-hand\n\n# UE Resources\nhttps://martinfowler.com/eaaDev/EventSourcing.html","url":"https://tycholiz.github.io/Digital-Garden/notes/ae8e6d6b-b0b0-484c-bd39-0b5a6bc46da3.html","relUrl":"notes/ae8e6d6b-b0b0-484c-bd39-0b5a6bc46da3.html"},{"doc":"CQRS","title":"CQRS","hpath":"general.patterns.CQRS","content":"\nCommand Query Responsibility Segregation (CQRS) is an architectural pattern that separates reading and writing into two different models. This means that every method should either be a Command that performs an action or a Query that returns data. A Command cannot return data and a Query cannot change the data\n- ex. Redux attempts to follow this pattern. Consider that to write to state we must dispatch an action, and to read state we must use selectors. The implementations of reading and writing to state are decoupled.\n\n# UE Resources\nhttps://martinfowler.com/bliki/CQRS.html","url":"https://tycholiz.github.io/Digital-Garden/notes/6b451d09-c1fd-4638-8f1a-0b943fc8fc76.html","relUrl":"notes/6b451d09-c1fd-4638-8f1a-0b943fc8fc76.html"},{"doc":"Best Practices","title":"Best Practices","hpath":"general.best-practices","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/7fce64d5-9600-4850-9e55-69c910f4c11d.html","relUrl":"notes/7fce64d5-9600-4850-9e55-69c910f4c11d.html"},{"doc":"Functions","title":"Functions","hpath":"general.best-practices.functions","content":"\nthe indent level of a function should not be greater than one or two.\n- If a function does only those steps that are one level below the stated name of the function, then the function is doing one thing.","url":"https://tycholiz.github.io/Digital-Garden/notes/8e1e1611-cfb1-4af2-b9db-a3445c87cbcf.html","relUrl":"notes/8e1e1611-cfb1-4af2-b9db-a3445c87cbcf.html"},{"doc":"Architecture","title":"Architecture","hpath":"general.architecture","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d9211a1d-ae93-441d-8c1a-0fcbd63e1958.html","relUrl":"notes/d9211a1d-ae93-441d-8c1a-0fcbd63e1958.html"},{"doc":"Workers","title":"Workers","hpath":"general.architecture.workers","content":"\nTask Queues/Worker Queues are used as a mechanism to distribute work across threads or machines.\nA task queue’s input is a unit of work called a task. Dedicated worker processes constantly monitor task queues for new work to perform.\n\nOften, a task queue will use a message broker to mediate between clients and workers\n- To initiate a task the client adds a message to the queue, the broker then delivers that message to a worker.\n\nTask Queues like Celery require a configured message broker like Redis broker transports or RabbitMQ to be functional\n- Realistically, you could roll your own using SQLite if you wanted to. You just need a service that will take care of the message queue for you.\n- Graphile-worker has this functionality built-in, as it manages its own queue in your Postgres db.","url":"https://tycholiz.github.io/Digital-Garden/notes/4a5060d8-6084-4cd9-b0e9-957d5813f3af.html","relUrl":"notes/4a5060d8-6084-4cd9-b0e9-957d5813f3af.html"},{"doc":"Webhooks","title":"Webhooks","hpath":"general.architecture.webhooks","content":"\n# Webhook\nA webhook is a callback living on a 3rd party server that sends an HTTP request upon some event.\n- ex. Twilio providing an Express server with SMS as they arrive to the Twilio server\nWebhooks are a way for an app to provide other applications with real-time information\n- ex. with Twilio, whenever their servers receieve an SMS, they will call the webhook endpoint that we provide to them. their webhook is a POST request, that will match up with a POST route that we define on our app server.\n- The key difference with a webhook is the use of callbacks. In traditional APIs, we manually have to poll for data in order to get a real-time experience. \n- Webhooks can be thought of as \"reverse APIs\", as they give you the API spec, and you must design the API for them to use.\n\t- Put another way, the webhook makes an API request to your app, which you must handle.\n- To make the connection, the webhook provider (ex. Twilio) must be provided with the address that it will send the requests to.\n\t- Therefore, your app must have a publicly accessible url for the webhook to work (ie. localhost will not work)\n\nMetaphorically, webhooks are like a phone number that Stripe calls to notify you of activity in your Stripe account\n- The webhook endpoint is the person answering that call who takes actions based upon the specific information it receives.\n\nA webhook on our server says \"hey, I want you to call this number (`/webhook` url) when *this* event happens\"","url":"https://tycholiz.github.io/Digital-Garden/notes/894e97c3-3f89-46b7-9c1b-e3f096c2d2c2.html","relUrl":"notes/894e97c3-3f89-46b7-9c1b-e3f096c2d2c2.html"},{"doc":"Pubsub","title":"Pubsub","hpath":"general.architecture.pubsub","content":"\n## Overview\nPub-Sub is a paradigm for getting real-time updates to a client. It is unlike webhooks, which would require the publisher of information to explicitly call someone who has \"signed up\" to receive it. Instead, with Pub-Sub the publisher determines that there is a certain set of information that it wants to send out, and it specifies a container (the topic) that it will be sent to. This leaves the topic open to be subscribed to, which a subscriber (a client) may do. The subscriber specifies that it is interested in receiving any new messages that arrive in a specific Topic (by name), and when new messages arrive, they are sent to the client.\n- This topic name would be scoped for a specific user, for instance we might call it `graphql:user:${userId}`\n\nA subscriber receives messages either by Pub/Sub pushing them to the subscriber's chosen endpoint, or by the subscriber pulling them from the service. Upon successful receiving of the message, the subscriber informs that it was received successfully.\n\nPub-sub is a messaging pattern whereby senders of messages (publishers) do not program the messages to be sent to the receivers (subscribers). Instead, the publishers categorize published messages into classes, without knowing who the subscribers are that will ultimately consume those messages\n- Similarly, subscribers express interest in one or more classes and only receive messages that are of interest to them (without any knowledge about who the publishers are)\n- This above explanation shows that the Pub-Sub paradigm results in the publisher service and the subscriber service being decoupled from one another.\n\nAn RSS feed might be a good analogy for the Pub-Sub paradigm, if we consider that we don't really care where each post/article came from. We just get them all lumped together, and consume them there, without any real consideration to where they came from. The process is decoupled.\n\nCommunication may be:\n1. 1:many (fan-out)\n\t- or, many clients subscribing to one topic (ie. multiple subscriptions to 1 topic)\n2. many:1 (fan-in)\n\t- or, one client subscribing to many topics (ie. multiple subscriptions to multiple topics)\n3. many:many\n![PubSub many:1 vs 1:many](/assets/images/2021-03-24-10-42-17.png)\n\nThe pub-sub pattern provides greater network scalability and a more dynamic network topology, with a resulting decreased flexibility to modify the publisher and the structure of the published data.\n\n## Core concepts\n### Topic\n- messages are sent to resources, which we call Topics.\n- topics have names that uniquely identify them\n\t- ex. if we are subscribing to the event \"user clicks this button\", then the topic might be named `\n- the topics live on a server controlled by the publisher\n\t- ex. Postgraphile server, Google Pub/Sub\n\n### Subscription\n- A subscription represents the stream of messages from a single specific topic.\n\n### Relation to Message Queue paradigm\nthe Pub-Sub paradigm is a sibling of the \"Message Queue\" paradigm.\n- Most messaging systems support both the Sub-Pub and Messaging Queue paradigms in their API.\n\n### Relation to Job Queues\nJob Queues only let one \"subscriber\" watch for new \"events\" at a time, and keep a queue of unprocessed events.\n- ex. Celery\n\nIt turns out that Postgres generally supersedes job servers as well. You can have your workers \"watch\" the \"new events\" channel and try to claim a job whenever a new one is pushed. As a bonus, Postgres lets other services watch the status of the events with no added complexity.\n\n### Examples\n- Kafka\n- RabbitMQ\n- Redis PUB/SUB","url":"https://tycholiz.github.io/Digital-Garden/notes/725e6582-3eee-452f-9a8a-4409f828afdf.html","relUrl":"notes/725e6582-3eee-452f-9a8a-4409f828afdf.html"},{"doc":"Offline First","title":"Offline First","hpath":"general.architecture.offline-first","content":"\nTo make an app truly offline first, we primarily need to do two things:\n1. Any code and assets used should be available offline\n2. Any data changes should be made locally first and then synced to the cloud.\n\n### using Apollo and resolver to serve offline-first\n- the local cache can be used to remember what queries resolved to. The resolver helps the cache remember which updates it made. Therefore if the app is offline, the cache will be able to remember updates that were made to the cache, so that up-to-date information can be returned \n![](/assets/images/2021-03-28-19-52-42.png","url":"https://tycholiz.github.io/Digital-Garden/notes/2d15f8ba-1225-463f-88c4-1cfdaa3fa0a2.html","relUrl":"notes/2d15f8ba-1225-463f-88c4-1cfdaa3fa0a2.html"},{"doc":"Message Broker","title":"Message Broker","hpath":"general.architecture.message-broker","content":"A message broker is an intermediary computer program module that translates a message from the formal messaging protocol of the sender to the formal messaging protocol of the receiver.\n\n# Examples\n- Kafka\n- RabbitMQ\n- AWS Kinesis\n- Google Cloud Pub/Sub\n- Redis (not a message broker per se, but rather has messaging brokering as one of its capabilities)","url":"https://tycholiz.github.io/Digital-Garden/notes/3460bf8b-2549-425f-ba93-fc8706e22f85.html","relUrl":"notes/3460bf8b-2549-425f-ba93-fc8706e22f85.html"},{"doc":"Edge","title":"Edge","hpath":"general.architecture.edge","content":"\n# Edge Compute\nEdge compute is an architecture where compute and storage resources are decentralized and distributed\n- for example it could mean compute environments are distributed all over the globe, located on every oil rig owned by an oil company or put into every truck in a fleet of food delivery vehicles.\n\nThe goal of edge compute is to minimize the amount of raw, unprocessed data sent to/from applications operating at edge locations (think CCTV cameras, or digital signage) and to reduce the time it takes for instructions to be sent from a central server to devices located at the edge\n- In an edge environment you can have sensors and devices talking directly to applications running on the network, with the application processing information in real time","url":"https://tycholiz.github.io/Digital-Garden/notes/d1db106c-f273-462a-8ab9-ed18432e48e7.html","relUrl":"notes/d1db106c-f273-462a-8ab9-ed18432e48e7.html"},{"doc":"Client Server","title":"Client Server","hpath":"general.architecture.client-server","content":"\nThe frontend aims to be stateful (that is, keep track of the state between requests). If the frontend wasn’t stateful, you would have to log in every time you navigated to a new page.\nThe backend, however, aims to be stateless. This means that the state must be provided on every new invocation. For instance, the API does not keep track of whether you are logged in or not. It determines your authentication state by reading the token in your API request.\n- If you used a Redux store on your Node.js server, the state would be cleared every time the node process stops\n- It becomes even more involved when you consider scaling. If you were to scale your application horizontally by adding more servers, you’d have multiple Node processes running concurrently, and each would have their own version of the state. This means that two identical requests to your backend at the same moment could easily get two different responses.","url":"https://tycholiz.github.io/Digital-Garden/notes/bf27c4df-64e8-4571-92a4-721a1018d419.html","relUrl":"notes/bf27c4df-64e8-4571-92a4-721a1018d419.html"},{"doc":"MVC","title":"MVC","hpath":"general.architecture.MVC","content":"\n## MVC (Front-end)\n### Model\n- Models represent knowledge. A model could be a single object (rather uninteresting), or it could be some structure of objects.\n    - There should be a one-to-one correspondence between the model and its parts on the one hand, and the represented world as perceived by the owner of the model on the other hand.\n- The classes which are used to store and manipulate state, typically in a database of some kind.\n- The model retrieves and populates the data.\n\n### View\n- A view is a visual representation of its model. \n    - It highlights certain attributes of the model and suppress others. It is thus acting as a presentation filter.\n\n### Controller\n- A controller is the link between a user and the system. It provides the user with input by arranging for relevant views to present themselves in appropriate places on the screen. It provides means for user output by presenting the user with menus or other means of giving commands and data. The controller receives such user output, translates it into the appropriate messages and pass these messages on to one or more of the views.\n- The brains of the application. The controller decides what the user's input was, how the model needs to change as a result of that input, and which resulting view should be used.\n- job is to provide a bit of orchestration between Models and Views\n","url":"https://tycholiz.github.io/Digital-Garden/notes/4b3731fa-102f-44c5-8e08-6c94a185a4ae.html","relUrl":"notes/4b3731fa-102f-44c5-8e08-6c94a185a4ae.html"},{"doc":"Fs","title":"Fs","hpath":"fs","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e65188a1-177e-4ab2-99f9-75f85d537621.html","relUrl":"notes/e65188a1-177e-4ab2-99f9-75f85d537621.html"},{"doc":"Virtual","title":"Virtual","hpath":"fs.virtual","content":"\n# Virutal FS (VFS)\n- a component of the kernel\n- handles all system calls related to files and file systems\n- serves as an interface between a user and a particular file system\n\t- In other words, it abstracts away the specific filesystem implementation and let's us access it on a command line.\n\t- it accomplishes this by specifying an interface (a contract) between the kernel and the underlying FS\n- we interact with the underlying FS by using the API provided by the VFS. \n- this abstraction allows us to bridge the differences between Windows filesystems, Mac filesystems, and Unix filesystems\n- when an external device attached to the system (such as a USB stick), Unix can run the `mount` command, which will create a new directory on the VFS.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/7007ba40-0284-4103-bef3-7bb645f79066.html","relUrl":"notes/7007ba40-0284-4103-bef3-7bb645f79066.html"},{"doc":"Network","title":"Network","hpath":"fs.network","content":"\n## Network FS\n- a filesystem that is distributed across a network\n\t- distributed means that the FS does not share block level access to the data, instead opting for a network protocol (likely IP)\n- the NFS mounts directly into the file system on mount points, so we can use regular unix commands like `cp`, `ls` etc.\n- ex. a NAS exposes its data via a NFS\n","url":"https://tycholiz.github.io/Digital-Garden/notes/716db7ee-05a1-465d-a7ea-59ffc7fac3a2.html","relUrl":"notes/716db7ee-05a1-465d-a7ea-59ffc7fac3a2.html"},{"doc":"Disk","title":"Disk","hpath":"fs.disk","content":"\n# Disk Filesystem\ndisk file systems are file systems which manage data on permanent storage devices. As magnetic disks are the most common of such devices, most disk file systems are designed to perform well in spite of the seek latencies inherent in such media.","url":"https://tycholiz.github.io/Digital-Garden/notes/ef0e246a-3834-4dc6-88e4-b997f837b066.html","relUrl":"notes/ef0e246a-3834-4dc6-88e4-b997f837b066.html"},{"doc":"Ntfs","title":"Ntfs","hpath":"fs.disk.ntfs","content":"\n# NTFS\nContrast with [[fat|fs.fat]]\n- unlike FAT, NTFS supports permissions, a change journal, and other features\n- Mac can only read NTFS, not write. \n","url":"https://tycholiz.github.io/Digital-Garden/notes/7951ec9d-ef4a-482a-9ca1-15f53def596b.html","relUrl":"notes/7951ec9d-ef4a-482a-9ca1-15f53def596b.html"},{"doc":"Fat","title":"Fat","hpath":"fs.disk.fat","content":"\n## FAT (File Allocation Table)\n- the FAT filesystem uses an index table stored on the device to identify chains of data storage that are associated with a single file. \n- the table is a linked list of entries for each cluster\n\t- a cluster is an atomic unit of space on a hard disk that can be allocated to hold files.\n\t\t- Storing small files on a filesystem with large clusters will therefore waste disk space (slack space)\n\t- being a linked list, each entry contains either the number of the next cluster in the file, or else a marker indicating the end of the file, unused disk space, or special reserved areas of the disk.\n\t\t- also, the root directory of the disk contains the number of the first cluster of each file in that directory\n\t\t- this enables the operating system to traverse the FAT, looking up the cluster number of each successive part of the disk file as a cluster chain until the end of the file is reached\n\t\t- Sub-directories are implemented as special files containing the directory entries of their respective files.\nFAT is a legacy system, and is supported for backward-compatability reasons\n","url":"https://tycholiz.github.io/Digital-Garden/notes/05bf930a-f6a3-4aa9-9804-240f050c775a.html","relUrl":"notes/05bf930a-f6a3-4aa9-9804-240f050c775a.html"},{"doc":"Exfat","title":"Exfat","hpath":"fs.disk.exfat","content":"\n# exFAT\nlightweight file system like FAT32, but without the extra features and over head of NTFS\n- not as compatible as plain FAT, but much more compatible than NTFS\n- Assuming every device we may want to mount to is compatible with exFAT, this is the filesystem we should probably use.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/9cfe8898-f32a-41cf-983d-ff8ac206809f.html","relUrl":"notes/9cfe8898-f32a-41cf-983d-ff8ac206809f.html"},{"doc":"Firebase","title":"Firebase","hpath":"firebase","content":"\n# Google Cloud\n## Terminology\n### Reference \n- a locally existing pointer that points to a location within the cloud where data is stored. The client (ex. mobile app) can interact with the reference in order to interact (CRUD) with the database.\n\n### Task \n- an object that is returned from calling a CRUD operation on a `reference` (ex. `.putFile()`. Holds info similar to `res` object in Express.js\n\n### Cloud functions\n- A service from Google that allows you to run snippets of code within their own infrastructure. If you were to build your own server, you would simply execute these functions in your backend code. Normally they are executed in response to some event. Since Firebase is a Backend as a Service, we don't have this ability. Cloud functions allow us to fulfill this need.\n- The functions can also be fired directly with a simple HTTP request.\n- Normally however, we have an event provider (the origin of the event) such as Firestore that will wait on a certain event to occur \n\t- ex. we can execute functions in response to databse writes, user creation etc.\n- can be used as a webhook - Via a simple HTTP trigger, respond to events originating from 3rd party systems like GitHub, Slack, Stripe, or from anywhere that can send HTTP requests.\n\n#### Features\n- these functions are stateless, meaning that if we were trying to make a counter function that holds the current value, we would get unpredictable results. This is because multiple instances of the same function will be created depending on how many people are using your app and executing that function. Therefore, all storage needs must be delegated to some other Google cloud service (like Firestore) \n\n# Firebase\n- When you create a new Firebase project in the Firebase console, you're actually creating a Google Cloud Platform (GCP) project behind the scenes\n\t- You can think of a GCP project as a virtual container for data, code, configuration, and services. A Firebase project is a GCP project that has additional Firebase-specific configurations and services (put another way, a firebase project is a wrapper around a GCP project). \n\t- therefore, a Firebase project ***is*** a GCP project. Everything that is possible in GCP is also possible in Firebase.","url":"https://tycholiz.github.io/Digital-Garden/notes/739907b4-b36d-4ad1-866d-c9f8050de776.html","relUrl":"notes/739907b4-b36d-4ad1-866d-c9f8050de776.html"},{"doc":"Express","title":"Express","hpath":"express","content":"\n- Express is an HTTP server\n- It is built on top of another framework callet Connect\n- express applications are request handler functions that you pass to http or http Server instances\n\n![](/assets/images/2021-03-07-22-10-51.png)\n\n## Express variables\n- we can set and get variables that are available throughout express with `app.set` and `app.get`\n- Express has the concept of the \"app settings table\", which is essentially a list of key-value pairs for configging Express that we can manipulate with `app.set`\n\n# ER\n[Using /services to encapsulate business logic](https://www.coreycleary.me/should-one-express-controller-call-another/)\n- [also](https://www.coreycleary.me/why-should-you-separate-controllers-from-services-in-node-rest-apis/)","url":"https://tycholiz.github.io/Digital-Garden/notes/53b47d5a-542b-4fab-8f6c-c70574ab7a48.html","relUrl":"notes/53b47d5a-542b-4fab-8f6c-c70574ab7a48.html"},{"doc":"Router","title":"Router","hpath":"express.router","content":"\n## Router\n- A router is essentially just a container for a set of middleware, grouped by the fact they all have to do with http methods and routes\n- The router is an isolated (meaning it operates independently of other routers) instance of middleware and routes. Therefore it can only perform middleware and routing functions.\n- The router can be thought of as a mini-application\n- A route is a combination of a path and a callback (called the route-handler)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/9cec97d0-52d5-4531-8a0b-5e96f12d80c8.html","relUrl":"notes/9cec97d0-52d5-4531-8a0b-5e96f12d80c8.html"},{"doc":"Middleware","title":"Middleware","hpath":"express.middleware","content":"\n# Middleware\nMiddleware is a stack of functions that gets called in a chain. However, the next function in that chain does not need to get called necessarily. The middleware that is currently handling the request decides if the request will complete right then and there, continue on to the next middleware in the stack with `next()`, or die instantly.\n- MWs are the key to flexibility and modularity in Express\n- A MW is a function that occurs in the lifecycle of a http request before the request hits the server, or before the response gets to the client\n- These functions have access to the request and response objects, and can modify/read however they want\n- When we add middleware to express with `app.use()`, we are appending items to `Server.prototype.stack` in Connect. When the server receives a request, it iterates over the stack, calling the `(req, res, next)` method.\n- Recall that each middleware item will either modify the request object, modify the response object, or call `next()` so the next middleware in the stack is called (?)\n- If a middleware does *not* modify the body or the header of the response, it should call `next()`. If it does, it should not call `next()`\n- If the current mw function does not end the request-response cycle, it must call `next()` so the next mw in the stack can take over.\nTechnically, Express itself is just middleware that occurs before the node.js server\n\n## Types of middleware\n### Application level\nusing `app.use()`, we can bind a piece of application-level MW to the app object\n- `app.use` will be called each time a request is sent to the server\n### Router level\nworks the same as application-level, except the middleware is bound to an instance of `express.Router()` instead of `express()`\n### Error handling\n### Built-in\nex. `express.static`, `express.json`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3ee66fcb-6156-49d9-9bb5-7cb319cb1020.html","relUrl":"notes/3ee66fcb-6156-49d9-9bb5-7cb319cb1020.html"},{"doc":"Body Parser","title":"Body Parser","hpath":"express.middleware.body-parser","content":"\n## body-parser\n- when we don't use body-parser, we get the raw request in the request body. In this format, the `body` and `headers` keys are not on the root object of the request parameter (ie. they are nested). This means that we must individually manipulate all the fields","url":"https://tycholiz.github.io/Digital-Garden/notes/9a29814e-98d6-4719-ae74-5dbfde6b28ba.html","relUrl":"notes/9a29814e-98d6-4719-ae74-5dbfde6b28ba.html"},{"doc":"Controller","title":"Controller","hpath":"express.controller","content":"\n## Controllers\n- define the actions to be taken when given a route\n- Performs the database interactions (using Mongoose or an SQL equivalent)\n- Determines the response that the server will give","url":"https://tycholiz.github.io/Digital-Garden/notes/dfb627a3-f5a3-45ec-af5c-e9036e553e83.html","relUrl":"notes/dfb627a3-f5a3-45ec-af5c-e9036e553e83.html"},{"doc":"Enzyme","title":"Enzyme","hpath":"enzyme","content":"","url":"https://tycholiz.github.io/Digital-Garden/notes/f32fab12-3c92-4e91-8d61-01e17e734910.html","relUrl":"notes/f32fab12-3c92-4e91-8d61-01e17e734910.html"},{"doc":"Terms","title":"Terms","hpath":"enzyme.terms","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/da8c38dc-9bca-4d24-b81d-467c9d1122a5.html","relUrl":"notes/da8c38dc-9bca-4d24-b81d-467c9d1122a5.html"},{"doc":"Unpack","title":"Unpack","hpath":"enzyme.terms.unpack","content":"\n### Unpack\n- render 1+ level deeper (ie. not shallow)\n- `dive()` - shallow render the component child of the wrapper","url":"https://tycholiz.github.io/Digital-Garden/notes/106ac5c1-0932-4fcd-9bb8-a857770da4fb.html","relUrl":"notes/106ac5c1-0932-4fcd-9bb8-a857770da4fb.html"},{"doc":"Ember","title":"Ember","hpath":"ember","content":"\n# Overview\n- provides MVC in the client-side\n- opinionated— follows convention over configuration\n- modules import other modules implicitly because they know exactly where to look. Therefore, following the conventional structure is paramount (like Ruby on Rails) \n\t- ex. data that's fetched in the `/routes/index.js` will be available in `/templates/index.hbs` as `@model`\n\n![](/assets/images/2021-03-10-22-39-10.png)\n\n# Decorators\n- they are special functions that make modifications to the following line\n- they can be thought of as wrapper functions in a sense, if they are before a\n  function.\n- ex. this decorator will cause the following getter to cache the result on the\n  first time.\n## Types\n- `@tracked` - in a class component, This annotation on a variable tells Ember to monitor this variable for updates. Whenever this variable's value changes, Ember will automatically re-render any templates that depend on its value.\n- `@action` - in a class component, define a function that is available to the component layout (handlebars html)\n```\n@cache\nget count() {\n  return this._count++;\n}\n```\n- can receive arguments: `@alias('fullName') name`\n\n# Anatomy of Ember app\n![](/assets/images/2021-03-10-22-39-28.png)\n- url determines the current state of the app\n\t- ex. are they looking at a list? a post? are they editing the post?\n- When the url is entered in the address bar, it...\n\t- connects to a route handler which loads a particular model.\n\t- It renders a template, which has access to the model.\n\n## Models\n- Models represent persistent state. \n\t- ex. in an airbnb app, the details of a rental (price, description, photos etc) would be stored in the `rental` model. we'd also have a `user` model to store the state of the currently logged in user\n- models persist information, whether it's to a web server or as local state\n- model layers can be swapped in, so we could use Redux or Apollo\n\n## Templates\n- similar to `ejs`\n- the route handler makes the data of the model available to the template\n\n### application.hbs\n- a wrapper around the whole application. This is where we can specify a footer and header, since they will appear on all pages.\n- use `{{outlet}}` to specify the whole application (think of it as `children`, which contains the rest of the app)\n\n## Components\n- essentially just a template that goes in another template\n- they can take args (just like passing props in React)\n![](/assets/images/2021-03-10-22-39-41.png)\n- You can think of components as Ember's way for letting you create your own HTML tags.\n- use the `{{yield}}` keyword to pass \"the rest of the data\" (similar to `children` in React)\n\n### Namespaced components\n- you can have a component that exists within another component by:\n\ta. creating a folder within `/components` with `ember generate component <parentDir>/<subComponent>`\n- invoked in templates like this `<Parent::Child>`\n- We can pass down HTML attributes just like props in react:\n```\n<div class=\"image\">\n  <img ...attributes>\n</div>\n```\n\n### Class components\n- enable us to add behavior to components (by using states)\n- run `ember generate component-class <nameOfComponent>\n- class components extend `Glimmer` components, giving it functionality similar to classes in React, such as state and lifecycle methods.\n- whenever a component is invoked, an instance of its related class component will be instantiated, allowing us to store state in it and call any relevant lifecycle methods.\n- initial state is stored in the constructor (in Ember, writing out constructor seems to be optional)\n- in the template part of the component (ie. the html) we get access to the instance variables (the component state) defined in the class component\n- Glimmer components have access to `this.args`, which is just like `this.props`\n\t- All arguments that can be accessed from this.args are implicitly marked as @tracked\n\n#### Block parameters\n- a block is any code that is between the opening and closing tags of a component (in react it would be called `children`)\n- What if there is a variable that we want to pass to the block content?\n\t- In this case we can the `as |results|` syntax, which would make `results` available to everything inside the block\n\t- this is similar to when we do a `items.map(item)`, and we have the current iteration available to us as `map`\n```\n  <ul class=\"results\">\n    <Rentals::Filter @rentals={{@rentals}} @query={{this.query}} as |results|>\n      {{#each @rentals as |rental|}}\n\t\t<li>\n\t\t  <Rental @rental={{rental}} />\n\t\t</li>\n      {{/each}}\n    </Rentals::Filter>\n  </ul>\n```\n- this also allows us to pass down the resultings data in the corresponding module that pertains to `rentals/filter.hbs` with `{{yield this.results}}` (see next section)\n\n#### Provider component\n- A pattern we use when we want to set up a piece of state for a component, but don't have any html to render for it. Instead, the html is just passed on down to the next level by using `{{yield this.results}}`\n- The child component then passes data up to it's parent\n\t- look at `rentals.hbs` and `rentals/filter.js`. \n\t\t- `@query={{this.query}} as |results|` passes the `query` variable down to the child, giving it access to it. The child (a class component) uses that variable to make computations, then returns a result, which gets put in the variable `results` (due to the `|results|` line above)\n\n## Routes\n### Model hook\n- The model hook is responsible for fetching and preparing any data that you need for your route. Ember will automatically call this hook when entering a route, so that you can have an opportunity to run your own code to get the data you need. The object returned from this hook is known as the model for the route.\n\t- Usually, this is where we'd fetch data from a server. Since fetching data is usually an asynchronous operation, the model hook is marked as async\n\t\n## Services \n- Services are just singleton objects (ie. they get instantiated only once) to hold long-lived data such as user sessions.\n- serve a similar role to global variables, in that they can be easily accessed by any part of your app\n- For example, we can inject any available service into components, as opposed to having them passed in as an argument. This allows deeply nested components to \"skip through\" the layers and access things that are logically global to the entire app, such as routing, authentication, user sessions, user preferences, etc.\n- A major difference between services and global variables is that services are scoped to your app, instead of all the JavaScript code that is running on the same page. This allows you to have multiple scripts running on the same page without interfering with each other.\n\n### Store service\n- can be injected into a route with `@service store`, making the Ember Data store available to use as `this.store`, and giving us `find` and `findAll` methods.\n```\nimport { inject as service } from '@ember/service';\n\nexport default class IndexRoute extends Route {\n  @service store;\n  async model() {\n    return this.store.findAll('rental');\n  }\n}\n```\n- store service might be compared to Redux in its role to fetch from the database and cache it\n\n## Controller\n- def - an object that receives the return value of the `model()` method (which is found in the associated route).\n- def - an object that receives one property when its associated route is hit: `model` (the return value of the Route's model method)\n- controller is only needed if we want to customize the properties or provide actions to the Route\n\t- in other words, they are an extension of the model loaded from the Route\n- if we don't make a `controller` file, one is generated for us by default (we just don't see it)\n- the controller name must match the route that renders it\n- controller is a singleton (ie. they get instantiated only once) \n\t- this means we shouldn't keep state in the controller \n\t\t- (unless it comes from either the Model or the Query params; since these would persist in between activations such as when a user leaves the Route and then re-enters it)\n- Controllers can also contain actions, Query Parameters, Tracked Properties, and more\n- Basically, use controllers when: \n\t1. we want to pass down actions or variables to the components found in a route. \n\t2. we want to support query params\n\t3. we want to compute a value (that we will ultimately pass down to the route's components) that depends on the model hook \n\t\t- in other words, the controller takes in the result of `model()` as its sole argument. What if we want to pass a variable down to the components that depend on the return value of `model()`?\n\n# Libraries\n## Ember Data\n- a library that helps manage data and application state in Ember applications.\n- built around the idea of organizing your app's data into model objects (in `/models` directory).\n\t- These objects represent units of information that our application presents to the user\n- The model represents the shape of the data\n```\nimport Model, { attr } from '@ember-data/model';\n\nconst COMMUNITY_CATEGORIES = [\n  'Condo',\n  'Townhouse',\n  'Apartment'\n];\n\nexport default class RentalModel extends Model {\n  @attr title;\n  @attr owner;\n  @attr city;\n  @attr location;\n  @attr category;\n  @attr image;\n  @attr bedrooms;\n  @attr description;\n\n  get type() {\n    if (COMMUNITY_CATEGORIES.includes(this.category)) {\n      return 'Community';\n    } else {\n      return 'Standalone';\n    }\n  }\n}\n```\n- Ember Data uses Adapters and Serializers. The idea is that, provided that your backend exposes a consistent protocol and interchange format to access its data, we can write a single adapter-serializer pair to handle all data fetches for the entire application.\n\t- Adapters deal with how and where Ember Data should fetch data from your servers, such as whether to use HTTP, HTTPS, WebSockets or local storage, as well as the URLs, headers and parameters to use for these requests. \n\t- Serializers are in charge of converting the data returned by the server into a format Ember Data can understand.\n\n# Structure\n- the root of the ember application is `templates/application.hbs`\n\n# Tests\n- integration tests - components that exist in isolation. They don't have to interact with the context in which they are placed. They can exist as a unit. Essentualy these are our unit tests.\n- acceptance tests - components that need to interact with other areas of the app (ex. navbar link functionality)\n\n# Mirage\n## Factories\n- allow you to create blueprints for your data. In other words, seed your development database","url":"https://tycholiz.github.io/Digital-Garden/notes/7cc5d9c3-20d8-4fef-aca2-7d6dcdc7b3d7.html","relUrl":"notes/7cc5d9c3-20d8-4fef-aca2-7d6dcdc7b3d7.html"},{"doc":"Email","title":"Email","hpath":"email","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/42e9a91c-4139-40cb-8bc0-356481aa19c7.html","relUrl":"notes/42e9a91c-4139-40cb-8bc0-356481aa19c7.html"},{"doc":"Protocol","title":"Protocol","hpath":"email.protocol","content":"\n## The Process\n\nThe MTA receives the email from the Mail Submission Agent (MSA), which it receives from the email client (MUA)\n- Once the MTA gets the email, relaying comes into play— with the MTAs acting as the relays (SMTP relay)\n\t- In relaying, the email is forwarded to other MTAs if the recipient of the email is not hosted locally on the same system as the sender.\n- Once the relaying process has finished, the email is sent to the Mail Delivery Agent (MDA)— which is the last stop before being delivered to a client's mailbox\n- The whole process uses SMTP, except for the final stage, which is between MDA and and MUA, which uses POP3 or IMAP\n\t- In other words, the process of getting mail from the \"mail server\" onto the email client uses POP3/IMAP.\n\n![](/assets/images/2021-04-07-10-12-35.png)\n- MUA - Mail User Agent (email client)\n- MSA - Mail Submission Agent\n- MDA - Mail Delivery Agent\n\n### Mail Transport Agent (MTA)\nA mail server can have many names: mail relay, mail router, Internet mailer. But the most common alias is an mail transport agent (MTA)\n- an MTA is responsible for transferring email among users on the internet\n- MTAs query the MX records and select a mail server to transfer emails\n- Usually, MTAs use a store-and-forward model of mail handling. This means that outgoing mail is put into a queue and waits for the recipient’s server response. An MTA will recurrently try to send emails. If the mail fails to be delivered during the established term, it will be returned to the mail client.\n\n#### MTA impact on email deliverability\nThere are three major factors that email deliverability is based on:\n- sender’s reputation\n- infrastructure & authentication\n- content\n\nThe reputation of the domain and IP address the email is sent from is the most important thing.\n- MTAs can protect and strengthen the reputation of the sender.\n\n##### Brand new IPs\nIf you’re building your reputation from scratch, you should not use your virgin IP address at full load. It has no email sending history and thus needs some warming up. An MTA will let you do this and later slowly increase sending capacity","url":"https://tycholiz.github.io/Digital-Garden/notes/684ca5ef-3f6d-4736-a5ca-c9450be9031e.html","relUrl":"notes/684ca5ef-3f6d-4736-a5ca-c9450be9031e.html"},{"doc":"Elastic Search","title":"Elastic Search","hpath":"elastic-search","content":"\nElasticsearch is an open-source, RESTful, distributed search and analytics engine built on Apache Lucene\n- You can send data in the form of JSON documents to Elasticsearch using the API. Elasticsearch automatically stores the original document and adds a searchable reference to the document in the cluster’s index. You can then search and retrieve the document using the Elasticsearch API\n- ES is NoSQL and is more powerful, flexible, and faster than SQL's LIKE\n\nES is typically used when you have :\n- high data volumes, and are likely to need multiple nodes to process the data\n- unstructured or semi-structured data (log files, text, ...). You ingest the raw data in its original form.\n- the data is never updated. It’s ingested once, queried, and then purged according to some bulk retention policy (e.g. older than 30 days)\n- you need to access aggregate data more than individual records\n\nWhen you're searching for text. ES ranks search results based on how close the phrase or words are. SQL doesn't do this nearly as well.\n- ES starts to shine when you start to do a lot of filtering\n\nElastic search scales horizontally with your requirements.\n\n### Tools\n[Kibana: a data visualization platform for Elasticsearch](https://www.elastic.co/kibana)","url":"https://tycholiz.github.io/Digital-Garden/notes/b3b7587a-fbd2-48a9-aef1-4efc90458e82.html","relUrl":"notes/b3b7587a-fbd2-48a9-aef1-4efc90458e82.html"},{"doc":"Docker","title":"Docker","hpath":"docker","content":"\n# Docker\nUsing Docker, we can abstract away the software, OS, and hardware configuration from our application, turning each service into a building block that we can run anywhere\n- when using containers you have to always think of dynamic vs. static parts of your application. You can use your host's file system to store your data and files. A more scalable and efficient way of thinking about this is to store your data to amazon rds and your files to amazon s3. This way you can spin up as many containers of your app/site as you want and have them all point to a single place where they store your dynamic stuff; namely, your data and files.\n\n## .dockerignore\n- Because being lean is a design principle of Docker, it is important to cut out the stuff from the image that is not necessary to running the code. \n\t- This includes git files, travis.yml, .vscode etc. To ensure these files do not become a part of the image, we put these in a .dockerignore file.","url":"https://tycholiz.github.io/Digital-Garden/notes/295ca45b-6851-4d38-9ecb-1d7e4a885849.html","relUrl":"notes/295ca45b-6851-4d38-9ecb-1d7e4a885849.html"},{"doc":"Tools","title":"Tools","hpath":"docker.tools","content":"\n## Watchtower\n- allows us to update an already running docker container by simply pushing to Docker Hub. Watchtower sees that a new image has been pushed, automatically runs `docker pull`, then starts the container back up again.\n- note: a message from calibre:\n\t- We do not endorse the use of Watchtower as a solution to automated updates of existing Docker containers. In fact we generally discourage automated updates. However, this is a useful tool for one-time manual updates of containers where you have forgotten the original parameters. In the long term, we highly recommend using Docker Compose.\n\n## Portainer\nan open source, platform agnostic tool for managing containerized applications. It works with Kubernetes, Docker, Docker Swarm, Azure ACI in both data centres and at the edge.\n```\ndocker container run -d \\\n  -p 9000:9000 \\\n  -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer\n```\n[Website](https://www.portainer.io/)","url":"https://tycholiz.github.io/Digital-Garden/notes/50569972-8494-41bd-aeb3-3dbe5434726c.html","relUrl":"notes/50569972-8494-41bd-aeb3-3dbe5434726c.html"},{"doc":"Registry","title":"Registry","hpath":"docker.registry","content":"\n# Registry\n- ***def*** - a stateless, scalable server-side application that stores and allows us to distribute docker images\n    - In other words, it allows us storage for our images\n- Think of it like github for docker images\n    - So we have commands like `docker pull`, `docker push`x etc.\n- ex. *Docker Hub*\n- Docker registry stores images as 2 parts, with a pointer between them:\n    - image:tag\n    - digest (i.e. the SHA id)\n- we use the registry as a place to store our images \n- Docker Hub is a an available registry that is hosted for us \n","url":"https://tycholiz.github.io/Digital-Garden/notes/2406a558-9c4a-49ac-913b-981303b3142b.html","relUrl":"notes/2406a558-9c4a-49ac-913b-981303b3142b.html"},{"doc":"Orchestrators","title":"Orchestrators","hpath":"docker.orchestrators","content":"\n# Orchestrators\n- Tools to manage, scale, and maintain containerized applications \n- ie. Kubernetes, Docker Swarm\n- Orchestration tools are designed to handle Docker containers running stateless applications\n\t- as such, you should not run stateful applications in orchestration tools which are built for stateless apps.\n\t- this makes sense, seeing as containers can be killed and restarted without any issue, multiple can run at once etc. \n\t\t- The same cannot be said for a database, which is why we typically don't dockerize DBs\n\nIn general, the development workflow looks like this:\n1. Create and test individual containers for each component of your application by first creating Docker images.\n2. Assemble your containers and supporting infrastructure into a complete application, expressed either as a Docker stack file or in Kubernetes YAML.\n3. Test, share and deploy your complete containerized application.\n\n# Kubernetes vs Docker Swarm mode\nIn Kubernetes, an application can be deployed using a combination of pods, deployments, and services (or micro-services).\n\nWhereas, in Docker Swarm, applications can be deployed as services (or micro-services) in a Swarm cluster. YAML files can be used to specify multi-container. Moreover, Docker Compose can deploy the app.\n\n# Kubernetes\nAll containers in Kubernetes are scheduled as pods\n- Kubernetes uses client-server architecture\n\n## pod \n- def: one or more containers deployed together on one host, thereby sharing the same resources (of the host)\n\t- ex. if we have 5 containers of a mongodb service deployed, and 3 of them were on the same host (ex. same machine), those 3 together would be called a Pod\n  - pod refers to a pod of whales or pea pod\n- they often share storage/network and namespaces\n- they are mortal. They are born and when they die, they are not resurrected. \n- Individual Pods are not intended to run multiple instances of the same application,\n- in a pre-container world, being executed on the same physical or virtual machine would mean being executed on the same logical host.\n  - logical host would contain relatively tightly coupled code\n- Containers within a Pod share an IP address and port space, and can find each other via localhost. \n\n## Deployment\n- def: scalable groups of pods\n- A Deployment provides declarative updates for Pods and ReplicaSets.\n  -You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate\n\n## Service\n- def: An abstract way to expose an application running on a set of Pods as a network service.\n- can be thought of as a logical set of pods and a policy by which to access them\n  - the front end shouldn't care which backend it uses. Services allow us to achieve this decoupling\n- since pods of a single service can exist on different machines, it makes sense for us to be able to interact with the service itself, so that we can orchestrate activities between all containers that are part of a service(?) \n\n### Motivation\nEach Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.\n\nThis leads to a problem: if some set of Pods (call them “backends”) provides functionality to other Pods (call them “frontends”) inside your cluster, how do the frontends find out and keep track of which IP address to connect to, so that the frontend can use the backend part of the workload?\n\n* * *\nYou should not run stateful applications (like a db) in orchestration tools which are built for stateless apps.\n- Orchestration tools are designed to handle Docker containers running stateless applications. Such applications don’t mind being terminated at any time, any number can run at the same time without communicating with each other and nobody will really notice if a new container will take over on a different machine.","url":"https://tycholiz.github.io/Digital-Garden/notes/5c26f633-7b25-427e-9811-bd39659de68d.html","relUrl":"notes/5c26f633-7b25-427e-9811-bd39659de68d.html"},{"doc":"Machine","title":"Machine","hpath":"docker.machine","content":"\n# Docker Machine\n\n- Docker engine is a tool that lets us install Docker Engine on virtual hosts and manage those hosts\n\t- Therefore, it is something we install on our own computer.\n\t- with the **driver** concept, we can deploy to 3rd party cloud services, like AWS, DigitalOcean, or even VirtualBox on your local machine \n- *docker-machine* commands let us start, inspect, stop, and restart hosts, as well as configure a docker client to talk to the hosts.\n- it allows us to control the docker engine of a VM created using docker-machine\n- The main reason you would use docker-machine is when you want to create a deployment environment for your application and manage all the micro-services running on it\n- To setup, all we need to do is point our `docker-machine` CLI at our managed host, which enables us to run docker commands directly on that host.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/bd927e8d-134e-4bf8-aca5-f559438d1ce3.html","relUrl":"notes/bd927e8d-134e-4bf8-aca5-f559438d1ce3.html"},{"doc":"Images","title":"Images","hpath":"docker.images","content":"\n# Images\n- an image is an immutable snapshot of a system\n\t- The fact that it is immutable gives it predictability. In other words, it will always work as-is\n- an image has a tree hierarchy. There is a base image (aka Parent Image), which is initiated with the `FROM` command in the Dockerfile. It sets the base for the rest of the images generated in the Dockerfile. \n\t- Therefore, every Dockerfile must have a `FROM` directive\n\n## Layers\n- Docker image is made up of a series of `layers`\n- Each line in the `dockerfile` creates a `layer`\n\t- Therefore, a layer contains only the differences between the preceding layer and the current layer (like consequent git commits).\n- layers are read-only\n- On top of the layers, there is a writable layer (the current one) which is called the container layer\n- layers may be shared between images\n\t- this means if the layer `COPY . /app` is used in multiple places, each iteration of it (from other Dockerfiles) will not contribute to Docker's overall footprint.\n\t- When the Dockerfile is run with `docker build`, each layer gets executed and its result cached\n- The following Dockerfile instructions create a layer and influence the size of the image:\n\t- RUN\n\t- COPY\n\t- ADD\n- The other Dockerfile instructions create intermediate layers, and do not increase the size of the image\n- When we build an image from Dockerfile, we'll notice that it says `removing intermediate container`, rather than what we might expect: `removing intermediate layer`. This is because a build step (a line in the Dockerfile) is executed in an intermediate container, which is no longer needed once the build step is done\n- if we run `docker history <image-id>`, we can recognize the intermediate containers as the ones having 0B size\n\t- There are also a lot of containers labelled `missing`, meaning that those layers are built on a different system and are not available locally.\n\nBelow, The FROM statement starts out by creating a layer from the ubuntu:15.04 image. The COPY command adds some files from your Docker client’s current directory. The RUN command builds your application using the make command. Finally, the last layer specifies what command to run within the container.\n```\nFROM ubuntu:15.04\nCOPY . /app\nRUN make /app\nCMD python /app/app.py\n```\nIn the previous example, we spun up a container whose basis is a Ubuntu server. We can just as easily spin up a continer whose basis is nodejs. \n\n## Dockerfile\n- Think of these Dockerfile commands as a step-by-step recipe on how to build up our image.\n\t- first step to containerizing an application\n- Dockerfiles describe how to assemble a private filesystem for a container, and can also contain some metadata describing how to run a container based on this image\n\t- A Dockerfile specifies the operating system that will underlie the container, along with the languages, environment variables, file locations, network ports, and other components it needs— and, of course, what the container will actually be doing once we run it.\n\nExample Dockerfile\n```\nFROM node:6.11.5 \nWORKDIR /usr/src/app \nCOPY package.json . \nRUN npm install \nCOPY . . \nCMD [ \"npm\", \"start\" ]\n```\n\nBuilding up our images takes the following steps:\n\n1. Start FROM the pre-existing node:6.11.5 image. This is an official image, built by the node.js vendors and validated by Docker to be a high-quality image containing the node 6.11.5 interpreter and basic dependencies.\n\n2. Use WORKDIR to specify that all subsequent actions should be taken from the directory /usr/src/app in your image filesystem (in other words, not the FS on your machine).\n\n3. COPY the file package.json from your host to the working directory in your image (so in this case, to /usr/src/app/package.json)\n\n4. RUN the command npm install inside your image filesystem (which will read package.json to determine your app’s node dependencies, and install them)\n\n5. COPY in the rest of your app’s source code from your host to your image filesystem.\n\nThese above commands effectively set up the filesystem of our image\n\nCMD specifies how to run a container based off *this* particular image\n- In this case, it’s saying that the containerized process that this image is meant to support is npm start.\n- i.e. it is a metadata specification\n- there can only be one `CMD` instruction per Dockerfile\n\nENTRYPOINT allows us to configure the container to run as an executable \n- the commands specified in ENTRYPOINT will always be run.\n- we also have CMD, whose commands will only run if we are spinning up a container and not explicitly setting any CLI arguments \n\t- if we specify arguments when spinning up a container, CMD is ignored, but ENTRYPOINT commands will still be executed \n\t- ex. `docker run -it <image> <arg1>`\n\t- CMD therefore are default arguments\n\n## Tags\n- an alias to the ID of an image. ie. they are just a a way to refer to a specific image\n- anal: git tags can be used to refer to a specific commit (ex. map tag SHA 3fhd883nnf9 to v1.4)","url":"https://tycholiz.github.io/Digital-Garden/notes/1d7e0393-5819-48fe-a830-e722189037fd.html","relUrl":"notes/1d7e0393-5819-48fe-a830-e722189037fd.html"},{"doc":"CLI","title":"CLI","hpath":"docker.images.cli","content":"\n### `docker images`\n- show all images\n\n### `docker build`\n- purpose is to build an image from a Dockerfile. The command will find the `Dockerfile` and will build an image based on it.\n\n## Build image\n- from your application folder:\n`docker image build -t <APP NAME> .`\n  - this causes each line of Dockerfile to be executed, building up the image as it goes\n- Images are static once they are created\n","url":"https://tycholiz.github.io/Digital-Garden/notes/24a408fb-544d-4c5f-bd93-b1e9f42fb842.html","relUrl":"notes/24a408fb-544d-4c5f-bd93-b1e9f42fb842.html"},{"doc":"Engine","title":"Engine","hpath":"docker.engine","content":"\n# Docker Engine\n- Docker engine is a client-server application made up of the Docker daemon, a REST API allowing us to interact with the daemon, and a CLI for talking to the daemon. \n- Docker Engine accepts docker commands from the CLI (`docker run` etc)\n- When people say “Docker” they typically mean Docker Engine\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a0c77d92-ffc0-4d98-b46d-c3bd974ef490.html","relUrl":"notes/a0c77d92-ffc0-4d98-b46d-c3bd974ef490.html"},{"doc":"Debug","title":"Debug","hpath":"docker.debug","content":"\n- `docker logs <container name>` - see what happened during container initialization\n- `docker inspect <container-name>` - get low-level info about a container, such as its IP, port mappings,  \n- remove all containers based on one image (ex. monica) - `docker ps -a | awk '{ print $1,$2 }' | grep monica | awk '{print $1 }' | xargs -I {} docker rm {}`","url":"https://tycholiz.github.io/Digital-Garden/notes/e87c9ba1-b9db-41db-97ef-4ad6050d3634.html","relUrl":"notes/e87c9ba1-b9db-41db-97ef-4ad6050d3634.html"},{"doc":"Containers","title":"Containers","hpath":"docker.containers","content":"\n# Containers\n- Container - the instance of an *Image*\n- Each container had its own filesystem (provided by a Docker image)\n- Image includes everything needed to run an application\n    - inc. the application code, runtimes, dependencies etc\n- Containers are designed to be transient and temporary, but they can be stopped and restarted, which launches the container into the same state as when it was stopped.\n- Containers can communicate with each other by attaching (`docker attach`). They do this by attaching stdin, stdout and stderr steams to one another so that one container's output can be piped into another container as their input \n\t- When we run `docker attach` with a specified container, we are piping our stdin/stdout/stderr to the container, so we are effectively able to write commands in the container's terminal\n\n## From the container's perspective \nThe container by default has no access to the outside world. When the docker host specifies the `-p` option when spinning up a new container, the container opens up the specified ports and maps them to the docker host's port.\n- For all the container knows, it is a regular computer. It has a network interface, including an IP address, a gateway, a routing table etc. \n\n![9ede10f99d18b464b0087150a5679308.png](:/26150f69cbf24c1583bf667d69c6ac9b)\n\n- Containers guarantee our applications will run the same anywhere, whether it's our own machine, a data centre, or anything else\n\n## Run container\n`docker run --name *<NAME OF CONTAINER>* -d -v /tmp/mongodb:/data/db -p 27017:27017 *<NAME OF IMAGE:IMAGE TAG>*`\n- `--name`: Name of the container.\n- `-v`: Attach the /tmp/mongodb volume of the host system to /data/db volume of the container.\n- `-p`: Map the host port to the container port.\nLast argument is the name/id of the image.\n\n## Running commands in the container\n- we can use `docker exec` to run commands.\n- by default, the command is executed in the WORKDIR variable that is declared in the Dockerfile\n- often, we see `docker exec -it <containerID> /bin/bash`\n\t- this executes the command `/bin/bash` within the specified container, opening a new bash session for us.\n\t- `-t` tells docker to open a terminal session\n\t- `-i` for interactive, ensures that our stdin input stream remains open \n\n## Actions\n- get IP of container - `docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <my-container-name>`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/0f9a227f-0abc-4077-a528-26d770611db6.html","relUrl":"notes/0f9a227f-0abc-4077-a528-26d770611db6.html"},{"doc":"Volumes","title":"Volumes","hpath":"docker.containers.volumes","content":"\n## Volumes\n- The primary way to persist data that exists in a Container\n- in a node app, we specify the following `volume` field attributes\n```\nvolumes:\n      - '.:/app'\n      - '/app/node_modules'\n```\n- This first line is saying \"map the host's PWD (.) to the `/app` directory in the container\". The next line says \"persist the node-modules directory in the container so that when we mount the host directory (the first line) at runtime, the node-modules won't get overwritten\"\n\t- In other words, this would happen:\n\t\t- Build - The node_modules directory is created in the image.\n\t\t- Run - The current directory is mounted into the container, overwriting the node_modules that were installed during the build.\n\n## Bind Mount\n- Instead of Docker creating a directory on the host that a container can have full access to (volume), we can make use of bind mounts. A bind mount lets us create something similar to a symbolic linked directory, whereby a directory or file on the host machine gets mounted into the container.\n\t- This means that we can make changes to the directory/file on the host machine, and those changes will reflected in the docker container's version of the directory/file. \n- bind mounts are created in `docker-compose.yml` with the `volumes` directive\n\n# UE Resources\n[Primer](https://docs.docker.com/storage/volumes/)","url":"https://tycholiz.github.io/Digital-Garden/notes/5f71e29a-9ca8-4eda-a9e6-333f55c47e46.html","relUrl":"notes/5f71e29a-9ca8-4eda-a9e6-333f55c47e46.html"},{"doc":"Networks","title":"Networks","hpath":"docker.containers.networks","content":"\n# Networks\n- Networks enable containers to be able to communicate with each other and with non-Docker processes (such as a host) \n- Networks are natural ways to isolate containers from other containers or other networks. As such, they provide complete isolation for containers\n- Docker’s networking subsystem is pluggable, made possible by having drivers\n\t- depending on which driver you are using, you will have different core networking functionality \n- `docker network ls` - show all networks\n- all containers within a network can communicate with each other. This can be shown by the fact that you can ping the IP address of one container from another (within the same network) \n\t- you can also simply `ping <container-name>`\n- Docker networking allows you to attach a container to as many networks as you like.\n- to see if 2 containers are properly on the same network, try pinging one container's IP from another.\n\nThere are 2 main types of network: bridge and overlay\n\n## Bridge\n- this is the default\n\t- Docker creates a bridge named `docker0`, and both the docker host and the docker containers have an IP address on that bridge.\n- Bridge networks are usually used when your applications run in standalone containers that need to communicate over the same docker host (ex. a pod?)\n- Limited to a single host running Docker Engine.\n- default type\n- if our `docker-compose.yml` does not explicitly specify a network to use, a special network called *bridge* will be the network that our containers are launched in \n\t- visible with `docker network ls`\n\n## Overlay\n- Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other.\n- can include multiple hosts and is a more advanced configuration\n\n## Host network\n- if we have a standalone container, network isolation between the container and the Docker host will be removed, and the container will use the host's network directly\n- host is only available for swarm services\n","url":"https://tycholiz.github.io/Digital-Garden/notes/ad2d5fb3-a347-4dd7-8028-f1e176f07240.html","relUrl":"notes/ad2d5fb3-a347-4dd7-8028-f1e176f07240.html"},{"doc":"CLI","title":"CLI","hpath":"docker.containers.cli","content":"\n## Commands\n### run a command in a new container\n`docker run`\n\n### show all running containers in docker engine\n`docker ps`\n- pass `-a` to see all containers\n- `docker-compose ps` will list containers in the docker engine that are related to the images specified in `docker-compose.yml` (therefore `dc ps` is a subset of `docker ps`)\n\n## Start a container\n`docker container run --publish 8000:8080 --detach --name bb <APP NAME>`\n- `--publish` asks Docker to forward traffic incoming on the host’s port 8000, to the container’s port 8080\n  - containers have their own set of ports\n- notice, we didn’t specify what process we wanted our container to run. We didn’t have to, since we used the CMD directive when building our Dockerfile; thanks to this, Docker knows to automatically run the process npm start inside our container when it starts up\n- visit application at localhost:8000\n","url":"https://tycholiz.github.io/Digital-Garden/notes/229649d8-bcc6-4784-81e1-2d635b256a9f.html","relUrl":"notes/229649d8-bcc6-4784-81e1-2d635b256a9f.html"},{"doc":"Compose","title":"Compose","hpath":"docker.compose","content":"\n# Docker Compose\n- compose is a tool for defining and running multi-container applications\n- use `docker-compose.yml` to configure the app's services, and start them with a single command\n\t- this file will be parsed everytime you run `docker-compose up`. Therefore, if you make changes to `docker-compose.yml`, all you need to do is `docker-compose down`, then `dc up` again. This is unlike the Dockerfile, which will only be run once when you are building an image\n- using compose is a 3 step process:\n    1. Define your app’s environment with a Dockerfile so it can be reproduced anywhere.\n    2. Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.\n    3. Run docker-compose up and Compose starts and runs your entire app.\n- Compose has commands for managing the whole lifecycle of your application:\n    - Start, stop, and rebuild services\n    - View the status of running services\n    - Stream the log output of running services\n    - Run a one-off command on a service\n\n## Anatomy of docker-compose.yml\n- this file will be run with `docker-compose up`, building up the services (and with it, the containers) of the whole application\n\t- if the image has already been built, then by default the build command will be ignored \n- The docker-compose.yml file is organized by service. The `build` field designates the location of the service's Dockerfile (this location is called the \"build context\") so that it can be run and the image can be built.\n\t- note: when deploying a stack in **swarm mode**, the `build` command is not run (since `docker stack` does not build images before deploying)\n- the docker-compose.yml file can specify environment variables to be executed during process of building the images with the `args` field under `build` \n\t- These variables are accessed in the Dockerfiles by specifying `ARG`:\nDockerfile:\n```\nARG buildno\nARG gitcommithash\n\nRUN echo \"Build number: $buildno\"\nRUN echo \"Based on commit: $gitcommithash\"\n```\n\ndocker-compose.yml\n```\nbuild:\n  context: .\n  args:\n    buildno: 1\n    gitcommithash: cdc3b19\n```\n\n### Publish vs. EXPOSE\n- these commands make the services inside the containers available to outside the containers\n- expose makes the processes in the container accessible to other containers within Docker (but not from *outside* Docker)\n\t- note: this is not entirely correct, but useful for understanding. [see](https://stackoverflow.com/questions/22111060/what-is-the-difference-between-expose-and-publish-in-docker/47594352#47594352)\n- publish (`-p`) makes the services available to outside Docker\n\t- Therefore, publish implicitly EXPOSES\n\n## Service\n- there is one image per service, but there can be multiple containers for a service. In other words, a docker \"service\" is one or more containers from one image\n- In our development set up, we are likely to have a single container per image. When we start to scale and need for additional containers per service arises, we can use `docker service` to create multiple containers from the same image\n\t- consider how this relates to the `docker run` command, which starts up 1+ containers from an equal amount of image (ie. one container per image)\n\t- what arises from running this command is Docker's swarm mode \n- Swarm mode is a container orchestration tool, giving the user the ability to manage multiple containers deployed across many different host machines\n\t- ex. imagine there are 5 containers deplayed across 5 different machines across USA. Swarm mode allows you to interact with all 5 containers at once, instead of having to manage one container at a time \n- This above distinction between a container and a service is precicely what differentiates `docker exec` and `docker-compose run`\n\t- naturally, `docker-compose` runs `docker` commands under the hood. \n\t- `docker exec` exectutes a command within a docker container\n\t- `docker-compose run` executes a command on a service (with the help of a swarm manager(?)\n\n# UE Resources\n[Definitive Guide to Docker Compose](https://gabrieltanner.org/blog/docker-compose)","url":"https://tycholiz.github.io/Digital-Garden/notes/2bc9d2a6-d2d3-46eb-83e9-061c57bf1bf0.html","relUrl":"notes/2bc9d2a6-d2d3-46eb-83e9-061c57bf1bf0.html"},{"doc":"dns","title":"dns","hpath":"dns","content":"\n## History\nDNS is based on a distributed database that takes time to update globally. In the early internet days, the database was small and could be updated by hand. As the internet grew, this became unfeasible, so a new management structure was introduced: the concept of *domain name registrars*. The idea was that the updates to the database would be handled by the registrars\n- nowadays, when you make updates to domain name management settings, the registrar will push out the updated information to other DNS servers. \n\nThe DNS can be thought of the administrative assistant of the internet. It carries all the administrative responsibility of making things work, albeit behind the scenes. \nThe DNS is made up of Internet nameservers and a communication protocol\n\n## Components of DNS\n- [[DNS Cache|dns.cache]]\n- [[DNS Lookup|dns.lookup]]\n- [[DNS Resolver|dns.resolver]]\n\n## Fully Qualified Domain Name (FQDN)\n- The full domain name, rather than its parts separated. The FQDM is notable because it is fully unambiguous. It therefore points to a very specific place, and cannot be interpreted in any other way.\n- It always ends in the TLD (therefore, paths don't count)\n- FQDN in DNS records should always be appended with a `.`\n\t- This is to differentiate them from paths.\n\n## Config\n`/etc/resolv.conf`\n- we can see what our system's DNS info is with this file\n\t- run `scutil --dns` on Mac to see DNS configuration\n\n# Resources\nhttps://shapeshed.com/unix-traceroute/\nhttps://activedirectorypro.com/dns-best-practices/#dns-aging-scavenging\n\n# UE Resources\n- [DNS sinkhole](https://en.wikipedia.org/wiki/DNS_sinkhole)","url":"https://tycholiz.github.io/Digital-Garden/notes/81e5a201-c843-49d1-b291-f99d818cee0c.html","relUrl":"notes/81e5a201-c843-49d1-b291-f99d818cee0c.html"},{"doc":"Zone","title":"Zone","hpath":"dns.zone","content":"\n## DNS Zone\n- The DNS is broken up into different zones. A DNS zone is a portion of the DNS namespace that is managed by a specific organization or administrator\n- A DNS zone is a subset, often a single domain, of the hierarchical domain name structure of the DNS\n\t- if DNS was a filesystem, a DNS zone would be each directory.\n\t- ex. if we are talking TLD DNS zone, then examples are `.com`, `.net`. If we are talking Domain-level DNS, then examples are `facebook.com`, `google.com`\n- A portion of the domain name space where administrative responsibility has been delegated to a single manager\n- The DNS Zone is described by the Zone File (aka. DNS record), which serves as the database for each nameserver.\n\t- The zone file contains mappings between domain names and IP addresses, along with other resources. This file is organized around resource records (A, CNAME etc.). In other words, resource records form the basis of the database. \n\t- We recognize the zone file when we go to a domain registrar, click onto one of our domains, and see all of the RRs that we have made.\n- a 'zone' is an area of control over namespace. A zone can include a single domain name, one domain and many subdomains, or many domain names. \n\t- In some cases, 'zone' is essentially equivalent with 'domain,' but this is not always true.\n\t- each zone has a *zone serial number*, which is a unique identifier\n\t\t- A DNS server can quickly look up a zone's records in its database via the serial number, which will bring up the SOA record.\n- A common mistake is to associate a DNS zone with a domain name or a single DNS server\n\t- In fact, a DNS zone can contain multiple subdomains and multiple zones can exist on the same server\n- We can decide which URLs should be their own zone, and which should be combined into a single zone.\n\t- ex. Below, as far as Cloudflare subdomains go, we have `blog`, `support`, and `community`. Support and community are small, so we put them in the same zone as the main `cloudflare.com`. However, the `blog` subdomain is a robust independent site that needs separate administration, so we give it its own zone.\n\nEach below is an example of a zone:\n![](/assets/images/2021-03-07-15-16-30.png)\n![](/assets/images/2021-03-07-15-16-44.png)\n\n### Zone Apex\n- Where `SOA` and `NS` records live. They are records whose names are the same as the zone itself.","url":"https://tycholiz.github.io/Digital-Garden/notes/95f70ce9-e8f5-46da-8377-dab55f656bda.html","relUrl":"notes/95f70ce9-e8f5-46da-8377-dab55f656bda.html"},{"doc":"Servers","title":"Servers","hpath":"dns.servers","content":"\n# DNS Request\nA DNS request is executed by the browser on a device. The first thing the OS checks is the hosts file. If the hosts file has an entry for the DNS, then this entry is always used, regardless of what comes next.\n- If the hosts file turns up no result, then the network card settings will be queried. This can come from one of 2 places:\n\t1. IP addresses of DNS servers configured in router\n\t2. IP addresses of DNS servers configured on device itself (in which the DNS settings on router would be set to *manual*)\nWhen the DNS resolves, the browser is enabled to connect to a web server or a CDN edge server \n\nDNS queries and responses are sent in plaintext (via UDP), which means they can be read by networks, ISPs, or anybody able to monitor transmissions (even when using HTTPS)\n\n# Nameservers\n- A DNS nameserver is a server that stores the DNS records for a domain\n\t- a DNS nameserver responds with answers to queries against its database.\n- A nameserver is a computer designated to translate domain names into IP addresses\n- Nameservers can be “authoritative”, meaning that they give answers to queries about domains under their control. Otherwise, they may point to other servers, or serve cached copies of other name servers’ data.\n\n# DNS Server\nThere are only 4 types of DNS Server:\n1. DNS Recursor\n2. Root nameserver\n3. TLD nameserver\n4. Authoritative nameserver\n\n- In instances where the query is for a subdomain such as `foo.example.com`, an additional nameserver will be added to the sequence after the authoritative nameserver, which is responsible for storing the subdomain’s `CNAME` record.\n- the ISP typically supplies the nameserver, but you can use public servers, like those offered by Google (which have IP `8.8.8.8` and `8.8.4.4`) or Cloudflare (`1.1.1.1` and `1.0.0.1`)\n\t- You could consider each IP address here to be a resolver.\n- The DNS server has expanded its role beyond only resolving domain names, and has other anciliary functionality\n\t- ex. a real-time blackhole list for spam\n- The DNS database is traditionally stored in a structured text file (the *Zone File*)\n\t- The Zone File describes a DNS Zone, and contains all RR for every domain in the zone.\n\nYour home network typically relies on a DNS Server supplied by your ISP\n- therefore your ISP's DNS servers see every domain you request.\n- some ISPs have found a way to monetize their DNS service. When you hit an erroneous domain, one that has no actual IP address, they divert your browser to a search and advertising page preloaded with a search phrase derived from the domain name\n\n### Library Analogy\n- *Resolver* - a librarian who, given a title, is asked to go fetch a book\n- *Root NS* - the blocks of bookshelves in the library\n- *TLD NS* - the specific rack within the bookshelf block\n- *Authoritative NS* - the specific book you asked for.\n\n## Misc\nthe same domain name may have multiple IP addresses associated with it.\n\nAnything that can be done with a DNS address can also be done with an IP address, since all a DNS does is translate from hostname (www.____.com) to IP.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1c97c6af-2ba6-4ad3-86d6-034efc7e2e01.html","relUrl":"notes/1c97c6af-2ba6-4ad3-86d6-034efc7e2e01.html"},{"doc":"Tld","title":"Tld","hpath":"dns.servers.tld","content":"\n### TLD nameserver\n- Holds information about all domains sharing the common TLD\n\t- ex. a `.com` TLD nameserver contains information for all the `.com` sites.\n- broken into 2 groups:\n\t1. Generic TLD, like .com, .org, .net, .edu, and .gov.\n\t2. Country code TLD, like .ca, .uk\n- overseen by IANA (a branch of ICANN)","url":"https://tycholiz.github.io/Digital-Garden/notes/c175f6b0-1747-42f3-ac26-70c8aaf89a78.html","relUrl":"notes/c175f6b0-1747-42f3-ac26-70c8aaf89a78.html"},{"doc":"Root","title":"Root","hpath":"dns.servers.root","content":"\n### Root nameserver\n- Accepts the domain name from the DNS resolver, and returns the appropriate TLD address (`.com`, etc.). The resolver then uses this to find the TLD nameserver.\n- The \"root\" here is a reference to the `.` in `.com`. If you think of the `.` as a `/` instead, it becomes clear why it's called root (it is a directory)\n- There are 13 types of root nameservers (therefore 13 different addresses)\n- ex. **F-Root Server**\n- overseen by ICANN","url":"https://tycholiz.github.io/Digital-Garden/notes/5f320b8a-944f-4968-9739-119537ffa490.html","relUrl":"notes/5f320b8a-944f-4968-9739-119537ffa490.html"},{"doc":"Authoritative","title":"Authoritative","hpath":"dns.servers.authoritative","content":"\n### Authoritative nameserver\n- the server that actually holds, and is responsible for, DNS resource records\n\t- This could be considered the \"domain's nameserver\"\n- Holds information specific to the actual domain name it serves (eg. google.com), and will send the IP address back to the recursive resolver.\n\t- The IP address it sends is found in the DNS A record.\n- Each domain has its own authoritative NS\n- spec: this nameserver holds the records that we see on our domain registrar dashboard for our domain. ","url":"https://tycholiz.github.io/Digital-Garden/notes/0264b7e5-2cf7-49d4-88cd-3373064ff77d.html","relUrl":"notes/0264b7e5-2cf7-49d4-88cd-3373064ff77d.html"},{"doc":"Resolver","title":"Resolver","hpath":"dns.resolver","content":"\n### DNS Resolver (aka. Recursive Resolver)\nthe recursive resolver is at the beginning of the DNS query (with the authoritative nameserver at the end).\n- the resolver receives requests from web browsers, and in turn makes additional requests to the rest of the servers in a final effort to resolve the query. \n- a resolver will tell us what the IP is, given a domain name. By default, if a resolver cannot tell us, then that is it. However, the fact that it is recursive means that not only can we query a resolver, but a resolver can make queries itself. Put another way, if it doesn't have the answer we are looking for, it has the capacity to make queries itself. \n- After receiving a request for a webpage, the resolver will either respond with cached data (given to it by the nameservers), or will send a request to the *root nameserver*, followed by a request to a *TLD nameserver*, and then one last request to an *authoritative nameserver* (This is called [[DNS Lookup|dns.lookup]])\n\t- At each step, there is a possibility that a cached version of the website will be found, and it can immediately be returned to the client, rather than continuing to recurse down the DNS tree.\n- After receiving the requested IP address (from the authoritative nameserver), the recursive resolver sends a response to the client\n\nDNS Client (aka *Resolver*) is a client machine configured to send name resolution queries to a DNS server\n- in other words, our computer is a DNS client when it sends requests to port 53 of a DNS server, with the intention of resolving an IP address  ","url":"https://tycholiz.github.io/Digital-Garden/notes/ae80b473-1749-4755-900e-e7350dbdea56.html","relUrl":"notes/ae80b473-1749-4755-900e-e7350dbdea56.html"},{"doc":"Registry","title":"Registry","hpath":"dns.registry","content":"\n### Domain name registry\n- a large database of all domain names and the associated registrant information in the TLDs of the DNS\n\t- With a database, third party entities (companies) can request administrative control of a domain name.\n- Most registries operate on the top-level and second-level of the DNS.\n- while registries manage domain names, they delegate the *reservation* of domain names to registrars","url":"https://tycholiz.github.io/Digital-Garden/notes/ea44888d-ce69-474b-8717-c257c1a7df02.html","relUrl":"notes/ea44888d-ce69-474b-8717-c257c1a7df02.html"},{"doc":"records","title":"records","hpath":"dns.records","content":"\n# Resource Records (RR)\nDNS records (aka zone files) are instructions that live in authoritative DNS servers and provide information about a domain\n- ex. You can think of a set of DNS records like a business listing on Yelp. That listing will give you a bunch of useful information about a business such as their location, hours, services offered, etc\n- a DNS record is a mapping between domain name and IP address. \n\nwhen we enter `facebook.com` in an address bar, we request an A record from the DNS. When we send an email, we request an MX record from the DNS. \n- A record is therefore the thing that we (the client) query for when engaging with the DNS\n\n## RR Field Values\nValues\n- `@` - indicates that this is a record for the root domain\n- `*` - indicates a wildcard, which will match all subdomains.\n\n### TTL (Time to Live)\n- How long the website will live in the caching nameserver before a new one is requested. If the TTL is 1 hour, then the server will only request a new version of the site every hour. All requests made to that URL will be made to the cached version, until the hour is up, and a new version is used.\n- As we know, there are many layers to DNS. It could be the case that we are able to resolve the domain name in the ISP nameserver, but we also may need to recurve further. The TTL will give us the max amount of time that particular resolution can exist on the ISP nameserver\n\t- In other words, how long can a given mapping of domain to IP address live for in the cache, before we need to make a more recursive query? \n\n### Record Data\n- Think of it like environment variables. We are passing data to the record. Naturally, whatever data we pass will vary in result depending on what type of record we have designated. \n\n### Record Class\n- this is the namespace of the record information.\n\t- The most commonly used namespace is that of the Internet (`IN`)\n\n# UE Resources\n- [DNS records - an introduction](https://www.linode.com/docs/networking/dns/dns-records-an-introduction/)","url":"https://tycholiz.github.io/Digital-Garden/notes/df7144fd-3605-421a-97ae-0d1b9f6bd2c0.html","relUrl":"notes/df7144fd-3605-421a-97ae-0d1b9f6bd2c0.html"},{"doc":"Txt","title":"Txt","hpath":"dns.records.txt","content":"\n### TXT\n- originally meant to hold human-readable notes, nowadays they are able to be used to pass in machine-readable code.\n- two of the most important uses for TXT records are email spam prevention and domain ownership verification","url":"https://tycholiz.github.io/Digital-Garden/notes/19aca11c-c4b2-47b1-93be-aa57a97b703d.html","relUrl":"notes/19aca11c-c4b2-47b1-93be-aa57a97b703d.html"},{"doc":"Srv","title":"Srv","hpath":"dns.records.srv","content":"\n### SRV\n- SRV records are how a port can be specified within the DNS\n- must point to an A record.","url":"https://tycholiz.github.io/Digital-Garden/notes/8d703452-4b6e-4681-a0b6-7ac3e32620eb.html","relUrl":"notes/8d703452-4b6e-4681-a0b6-7ac3e32620eb.html"},{"doc":"soa","title":"soa","hpath":"dns.records.soa","content":"\n### SOA (Start of Authority)\n- This is the most important address record, and must be specified.\n- Contains the authoritative master nameserver for the zone or domain, as well as an admin email that we specify.\n- Also contains administrative information about the zone\n- *MNAME* - the primary nameserver for the zone","url":"https://tycholiz.github.io/Digital-Garden/notes/8af2c434-28b4-4777-a175-2f6ced77c78b.html","relUrl":"notes/8af2c434-28b4-4777-a175-2f6ced77c78b.html"},{"doc":"Ns","title":"Ns","hpath":"dns.records.ns","content":"\n### NS \n- nameserver record - indicate which authoritative nameserver contains the actual DNS records\n\t- Basically, NS records tell the Internet where to go to find out a domain's IP address\n- A domain often has multiple NS records which can indicate primary and backup nameservers for that domain. \n\t- if one nameserver goes down or is unavailable, DNS queries can go to another one\n\t- Typically there is one primary nameserver and several secondary nameservers, which store exact copies of the DNS records in the primary server\n\t\t- Updating the primary nameserver will trigger an update of the secondary nameservers as well.\n- Without properly configured NS records, users will be unable to load a website or application.\n- When multiple nameservers are used (as in most cases), NS records should list more than one server\n- NS records must point to an A record\n- ex. the resolver may have the NS records, but no A record, and it will still be able to query those nameservers directly, rather than having to go through the TLD server\n\nUpdating NS records\n- Domain administrators should update their NS records when they need to change their domain's nameservers\n- update NS records if you want a subdomain to use different nameservers than the domain (ex. example.com and blog.example.com have 2 different nameservers)","url":"https://tycholiz.github.io/Digital-Garden/notes/3463aa7f-4f37-4e92-b0e6-1df389e17349.html","relUrl":"notes/3463aa7f-4f37-4e92-b0e6-1df389e17349.html"},{"doc":"Mx","title":"Mx","hpath":"dns.records.mx","content":"\n### MX \n- Mail exchange (SMTP)\n- the record indicates how email messages should be routed (in accordance with SMTP)\n\t- ex. When a user sends an email to john.smith@gmail.com, the *Message Transfer Agent* (MTA) sends a DNS query to identify the mail servers for that email address. The MTA establishes an SMTP connection with those mail servers, starting with the prioritized domains\n- must point to an A record\n- `priority` - lower number indicates preference. In the result of a send failure, the next priority domain will be attempted \n\t- `mailhost1.tycholiz.com` might have `priority` of 10, and `mailhost2.tycholiz.com` might have `priority` of 20, which would mean `mailhost2` only gets used when the first message fails to send. \n\t- if we use the same priority, then both servers will receive equal amount of mail (effectively a load balancer) ","url":"https://tycholiz.github.io/Digital-Garden/notes/6471d171-ea1e-4394-ab45-3994f5565565.html","relUrl":"notes/6471d171-ea1e-4394-ab45-3994f5565565.html"},{"doc":"Cname","title":"Cname","hpath":"dns.records.cname","content":"\n### CNAME \n- Canonical name record\n- points to a domain, not an IP address\n- map an alias name to a true (canonical) domain name\n- typically used to map a subdomain, like `www.stuff.com` to `stuff.com`, or `stuff.com` to `blog.stuff.com`)\n\t- This is good, because if the host IP address changes, then we only need to change the A record. The CNAME record depends on the domain, not the IP address it's associated with.\n- anal. Imagine a scavenger hunt where each clue points to another clue, and the final clue points to the treasure. A domain with a CNAME record is like a clue that can point you to another clue (another domain with a CNAME record) or to the treasure (a domain with an A record).\n- ex. imagine we give `blog.tycholiz.com` a CNAME with value `tycholiz.com`. This means that any time a DNS server hits the DNS records for `blog.tycholiz.com`, it actually triggers another DNS lookup to `tycholiz.com`, since we specified that as the CNAME\n\t- in this example, the canonical name (true name) is `tycholiz.com`\n\t- from this example, you can see how CNAMEs are kind of like relays, since they don't map to an IP at all, but point to a domain name, which maps to an IP. In other words, CNAME records cause A records to resolve domain names. \n- The CNAME record only points the client to the same IP address as the root domain\n\t- Therefore, the CNAME record does not have to resolve to the same website as the domain it points to.\n\t- ex. in the case where we hit `blog.example.com`, the DNS will return us the same IP as if we hit `example.com`.  \n\t\t- when the client actually connects to that IP address, the web server will look at the URL, see that it is blog.example.com, and deliver the blog page rather than the home page.\n- Pointing a CNAME to another CNAME is possible, but there is no point\n\n TEST","url":"https://tycholiz.github.io/Digital-Garden/notes/2620ec89-5627-4a5d-ab25-8d5c56c79ba0.html","relUrl":"notes/2620ec89-5627-4a5d-ab25-8d5c56c79ba0.html"},{"doc":"Aname","title":"Aname","hpath":"dns.records.aname","content":"\n### ANAME \n- Like a *CNAME record*, but at the root of the domain\n- Allows us to point the \"naked\" version of domain (eg. example.com) to another host\n- Common use case is CDN","url":"https://tycholiz.github.io/Digital-Garden/notes/494c6e8e-775b-4257-9a3a-2fba416b6553.html","relUrl":"notes/494c6e8e-775b-4257-9a3a-2fba416b6553.html"},{"doc":"A","title":"A","hpath":"dns.records.a","content":"\n### A \n- Address Record\n- maps the IP address to a given domain\n- it is normal to have just one A record. \n- `AAAA` for IPv6","url":"https://tycholiz.github.io/Digital-Garden/notes/7b847e2c-d1c9-4479-9fc2-079d9aab9d1a.html","relUrl":"notes/7b847e2c-d1c9-4479-9fc2-079d9aab9d1a.html"},{"doc":"Lookup","title":"Lookup","hpath":"dns.lookup","content":"\n## DNS Lookup\n- When `www.facebook.com` is searched, after the browser cache is checked, the resolver intercepts the request and checks its own cache. If it doesn't have anything, it will make a request to the root server to see if it has it cached. If it does not, then it proceeds down the chain until the authoritative nameserver. At any point, if the nameserver does indeed have the website cached, then it will return it to the resolver, who will proceed to return it to the client in domain form. \n- During a new DNS lookup, the lookup passes through the resolver, root server, and TLD server.\n\t- At each step, information is gathered and cached for later use\n\t- Therefore, in a DNS lookup, the resolution process runs until either it reaches the DNS server (?) and gets the IP, or one of the stages returns a cached version of the website. \n- DNS lookup:\n![b69b96041645046bf1c20539c81b07c1.png](:/8b0b65127b2248adad9d134c18ed3baa)\n\n## Reverse DNS Lookup\n- Forward DNS lookup is using an Internet domain name to find an IP address. Reverse DNS lookup is using an Internet IP address to find a domain name\n\t- when you put a URL in the address bar, the address is transmitted to a nearby router which does a forward DNS lookup in a routing table to locate the IP address\n\t- `PTR Record` is an RR for enabling reverse DNS lookups, which is the exact opposite of an A record.\n- Reverse lookups are commonly used by email servers, who check and see if an email message came from a valid server before bringing it onto their network.\n\t- Many email servers will reject messages from any server that does not support reverse lookups (the absense of a `PTR record` means reverse lookups aren't supported)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/fa38f0e2-5d5b-45e1-9f54-a95c44d8bbd5.html","relUrl":"notes/fa38f0e2-5d5b-45e1-9f54-a95c44d8bbd5.html"},{"doc":"DDNS","title":"DDNS","hpath":"dns.ddns","content":"\n## Dynamic DNS (DDNS)\n- a method of automatically updating a nameserver in the Domain Name System with the active DDNS configuration of its configured hostnames, addresses or other information.\n\t- each time your IP address changes, a program redirects the new address to the domain name and makes it permanently available on the internet.\n- DDNS solves the problem of having your public IP address change. Since getting a static public IP address from an ISP can be expensive, this is another solution to that problem.\n- The term is used to describe two different concepts\n\t1. **dynamic DNS updating** - systems that are used to update traditional DNS records without manual editing\n\t2. ?\n- DNS is only suitable for devices that don't change their IP often.\n\t- DDNS is a system that addresses the problem of rapid updates.","url":"https://tycholiz.github.io/Digital-Garden/notes/66ffb46f-ca05-469e-8300-ea54754bba5f.html","relUrl":"notes/66ffb46f-ca05-469e-8300-ea54754bba5f.html"},{"doc":"Cache","title":"Cache","hpath":"dns.cache","content":"\n### DNS Cache\n- There are multiple caches involved in the entire DNS. They are checked in sequential order. \n\t1. Browser\n\t2. OS - the process that handles this query is called a **stub resolver**, or **DNS Client** \n\t\t- the stub resolver will first check to see if it has the cached data, and if not, will call the DNS query that gets handled by the resolver (which is hosted by the ISP).\na residential router internally runs a DNS cache and DNS proxy server\n- it also advertises itself as the DNS server in all DHCP responses.\n- There are actually DNS caches at every hierarchy of the lookup process\n\t- The computer reaches your router, which contacts your ISP, which might hit another ISP before ending up at what's called the \"root DNS servers.\" Each of those points in the process has a DNS cache for the same reason, which is to speed up the name resolution process.\n- spec:the DNS cache caches records.","url":"https://tycholiz.github.io/Digital-Garden/notes/641013aa-5287-4909-a6ed-f0e9e215dd50.html","relUrl":"notes/641013aa-5287-4909-a6ed-f0e9e215dd50.html"},{"doc":"Deploy","title":"Deploy","hpath":"deploy","content":"\n# Trends\n[GitOps](https://www.atlassian.com/git/tutorials/gitops)","url":"https://tycholiz.github.io/Digital-Garden/notes/3b7ffbbf-762e-411f-8fad-5e2b1b95e384.html","relUrl":"notes/3b7ffbbf-762e-411f-8fad-5e2b1b95e384.html"},{"doc":"Serverless","title":"Serverless","hpath":"deploy.serverless","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/ba51d4c6-5196-4520-a5f1-a5c01e8a6366.html","relUrl":"notes/ba51d4c6-5196-4520-a5f1-a5c01e8a6366.html"},{"doc":"Functions","title":"Functions","hpath":"deploy.serverless.functions","content":"\nA cold start for serverless functions is directly related to the size of the function itself. The reason why cold starts exist is because the function is stored as a zip file, and it needs to be unzipped before it is mounted. Therefore, the solution to having shorter cold starts is to have smaller functions","url":"https://tycholiz.github.io/Digital-Garden/notes/93fe76d3-1e44-4a0c-a960-088abaa4879a.html","relUrl":"notes/93fe76d3-1e44-4a0c-a960-088abaa4879a.html"},{"doc":"Scaling","title":"Scaling","hpath":"deploy.scaling","content":"\n### Scaling Types\nVertical Scaling \n- getting bigger machines to handle the increasing workload.\n    - ie. make each node more powerful (adding CPUs etc)\nHorizontal Scaling \n- getting many more smaller machines to handle the increase.\n    - ie. add more nodes\n- ex. Kubernetes does this by spinning up new containers\n- ex. Azure functions does this by spinning up new function handlers","url":"https://tycholiz.github.io/Digital-Garden/notes/4bb13a69-69cc-4f42-8b81-3ad26aef4746.html","relUrl":"notes/4bb13a69-69cc-4f42-8b81-3ad26aef4746.html"},{"doc":"Pricing","title":"Pricing","hpath":"deploy.pricing","content":"\n### per Second\nThis is normally for something like Serverless Functions, where we pay for the time that the server is executing our functions. \n- ex. if we are charged $0.01 per second and the function takes 333ms to run, then the function can be executed 3 times for $0.01\n\n### per Uptime\nThis is likely for container-based deployments (ex. Heroku), where we can pay per hour of uptime. This usually comes with the idea of \"cold starts\", where if no one is using our website, then the server will go offline (and therefore we won't get charged)","url":"https://tycholiz.github.io/Digital-Garden/notes/47ac0e09-cb1f-4927-a371-4ec455bdb633.html","relUrl":"notes/47ac0e09-cb1f-4927-a371-4ec455bdb633.html"},{"doc":"Jamstack","title":"Jamstack","hpath":"deploy.jamstack","content":"\n# Jamstack\nJAM: JavaScript, APIs, and markup\n- JavaScript: any dynamic programming during the request/response cycle is handled by JS, running entirely on the client. This could be any frontend framework or library, or even vanilla JavaScript.\n- APIs: all server-side processes or database actions are abstracted into reusable APIs, accessed over HTTPS with JavaScript. These can be custom-built or leverage third-party services.\n- Markup: templated markup should be prebuilt at deploy time, usually using a site generator for content sites, or a build tool for web apps.\n\n![](/assets/images/2021-03-20-18-45-27.png)\n\n## Examples\n### Site Generators\nNext.js, Gatsby, Jekyll\n\n### Headless CMS\nNetflify\n\n## Architecture\nPurpose is to make the web faster, more secure, and easier to scale\n- core principles are pre-rendering, and decoupling\n    - pre-render - to generate the markup which represents a view (front-end) in advance of when it is required. This happens during a build rather than on-demand so that web servers do not need to perform this activity for each request recieved.\n- entire front end is prebuilt into highly optimized static pages and assets during a build process. This process of pre-rendering results in sites which can be served directly from a CDN. Since the CDN only has to serve already-rendered markup, it can be delivered very quickly and securely\n    - On this foundation, Jamstack sites can use JavaScript and APIs to talk to backend services, allowing experiences to be enhanced and personalized.\n- The Jamstack philosophy is to be modular and have a strong capacity to be able to hook into various 3rd party services/APIs\n\n## Benefits\n### Security\n- Jamstack naturally results in less moving parts, meaning there are naturally less surfaces to have to protect against.\n- Since CDNs only serve pre-generated files, meaning we can use read-only hosting, further reducing the damage that a third party can do.\n\n### Scaling\nsince everything is pre-generated, the CDN can cache the whole site.\n\n### Performance\nJamstack sites remove the need to generate page views on a server at request time by instead generating pages ahead of time during a build.\n\n### Maintenance\nJamstack apps are easier to maintain, as they only need to be served directly from a simple host (or CDN)\n- The work was done during the build, so now the generated site is stable and can be hosted without servers which might require patching, updating and maintain.\n\n## Approaches\n#### Thinking from CDN perspective\nBecause Jamstack projects don’t rely on server-side code, they can be distributed instead of living on a single server. Serving directly from a CDN unlocks speeds and performance that can’t be beat. The more of your app you can push to the edge, the better the user experience.\n\n#### Automating builds\nBecause Jamstack markup is prebuilt, content changes won’t go live until you run another build. Automating this process will save you lots of frustration. You can do this yourself with webhooks, or use a publishing platform that includes the service automatically.\n\n#### Atomic Deploys\nAs Jamstack projects grow really large, new changes might require re-deploying hundreds of files. Uploading these one at a time can cause inconsistent state while the process completes. You can avoid this with a system that lets you do “atomic deploys,” where no changes go live until all changed files have been uploaded.\n\n#### Instant Cache Invalidation\nWhen the build-to-deploy cycle becomes a regular occurrence, you need to know that when a deploy goes live, it really goes live. Eliminate any doubt by making sure your CDN can handle instant cache purges.\n\n#### Everything Lives in Git\nWith a Jamstack project, anyone should be able to do a git clone, install any needed dependencies with a standard procedure (like npm install), and be ready to run the full project locally. No databases to clone, no complex installs. This reduces contributor friction, and also simplifies staging and testing workflows.\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/48229dc9-bd77-4326-9b27-5babb58d85f4.html","relUrl":"notes/48229dc9-bd77-4326-9b27-5babb58d85f4.html"},{"doc":"Gitops","title":"Gitops","hpath":"deploy.gitops","content":"\nGitOps is implemented by using the Git distributed version control system (DVCS) as a single source of truth for declarative infrastructure and applications. Every developer within a team can issue pull requests against a Git repository, and when merged, a \"diff and sync\" tool detects a difference between the intended and actual state of the system. Tooling can then be triggered to update and synchronise the infrastructure to the intended state.\n\n# UE Resources\nhttps://www.weave.works/blog/gitops-operations-by-pull-request","url":"https://tycholiz.github.io/Digital-Garden/notes/743a8b40-098a-4781-8cfc-51569398b62f.html","relUrl":"notes/743a8b40-098a-4781-8cfc-51569398b62f.html"},{"doc":"Distributed","title":"Distributed","hpath":"deploy.distributed","content":"\n# UE Resources\n[Good Speaker (Kafka Guy)](https://www.youtube.com/watch?v=Y6Ev8GIlbxc)","url":"https://tycholiz.github.io/Digital-Garden/notes/d7e29704-bef4-4593-8c7a-f9ac3a29158c.html","relUrl":"notes/d7e29704-bef4-4593-8c7a-f9ac3a29158c.html"},{"doc":"RPC","title":"RPC","hpath":"deploy.distributed.RPC","content":"\nRPC is when we call a function that lives on a different machine, without the programmer explicitly coding the details for the remote interaction\n- That is, the programmer writes essentially the same code whether the subroutine is local to the executing program, or remote\n- caller is client, executor is server\n\nThe RPC model implies a level of location transparency, namely that calling procedures are largely the same whether they are local or remote, but usually they are not identical, so local calls can be distinguished from remote calls.\n\nRemote calls are usually orders of magnitude slower and less reliable than local calls, so distinguishing them is important.\n\nRPCs are a form of inter-process communication\n- ie. different processes have different address spaces","url":"https://tycholiz.github.io/Digital-Garden/notes/8e160a1d-5683-4b24-94dd-0b778cfd721f.html","relUrl":"notes/8e160a1d-5683-4b24-94dd-0b778cfd721f.html"},{"doc":"Db","title":"Db","hpath":"deploy.db","content":"\nReplicated tables are typically static data that does not change very often. Replicating them allows for read scalability.","url":"https://tycholiz.github.io/Digital-Garden/notes/eea76a00-2efd-47e1-95a0-4fb03c219267.html","relUrl":"notes/eea76a00-2efd-47e1-95a0-4fb03c219267.html"},{"doc":"CI","title":"CI","hpath":"deploy.ci","content":"\n\"continuous\" means that it happens with each \"checkin\" of the code (updating code on the host\n\na CI system like Travis CI will automatically build code and run tests, and provide immediate feedback on the result of those tests and build \n- a CI can also automate other parts of your development process by managing deployments and notifications.\n\n# Travis CI\n- when you instruct a build to be made, Travis clones the github repo into a brand-new virtual env, and runs the tests and creates the build.\n\t- if any of those tasks fail, the build is considered broken\n\t- if all tasks succeed, the build passes and Travis CI can deploy the code\n\nGithub Actions is github's CI system\n\nbuilding and testing happens in CI time\n\n## UE Resource\n[Good primer](https://hackernoon.com/continuous-integration-circleci-vs-travis-ci-vs-jenkins-41a1c2bd95f5)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f2a2819f-aa4b-46f7-bcfa-4326ac99f0e4.html","relUrl":"notes/f2a2819f-aa4b-46f7-bcfa-4326ac99f0e4.html"},{"doc":"Dendron","title":"Dendron","hpath":"dendron","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3683d886-1adb-4203-86d9-1f03c8bf008b.html","relUrl":"notes/3683d886-1adb-4203-86d9-1f03c8bf008b.html"},{"doc":"Vault","title":"Vault","hpath":"dendron.vault","content":"\n### Local Vault\nA local vault is a folder in your file system.\n\n### Remote Vault\nA remote vault is a git repository. If you choose a remote vault, you can choose from a vault from the registry or enter a custom git url.\n- Note that when you add a remote vault, the url can also point to a remote workspace. In that case, dendron will inspect the dendron.yml to get a list of all vaults within the workspace and add all vaults from inside the workspace.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/0790a9c8-0897-4c44-b9cb-2905792313f1.html","relUrl":"notes/0790a9c8-0897-4c44-b9cb-2905792313f1.html"},{"doc":"Searching","title":"Searching","hpath":"dendron.searching","content":"\n#### Scoping search to a sub-tree\n1. Open vscode advanced search with `<Cmd+shift+f>`\n2. Input search term\n2. In \"files to include\", include as many hierarchies (separated by `.`), followed by `**` at the end\n    - ex. `graphql.operators**`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/2f71e35c-246e-4693-b1aa-a159b1def7ea.html","relUrl":"notes/2f71e35c-246e-4693-b1aa-a159b1def7ea.html"},{"doc":"Db","title":"Db","hpath":"db","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f529cc34-aba0-45ca-ad7e-02ddda318941.html","relUrl":"notes/f529cc34-aba0-45ca-ad7e-02ddda318941.html"},{"doc":"Strategies","title":"Strategies","hpath":"db.strategies","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e99534fb-8334-4852-b948-1b7231c91600.html","relUrl":"notes/e99534fb-8334-4852-b948-1b7231c91600.html"},{"doc":"Sharding","title":"Sharding","hpath":"db.strategies.sharding","content":"\n# Sharding\nSharding is a method of splitting and storing a single logical dataset in multiple databases. By distributing the data among multiple machines, a cluster of database systems can store larger dataset and handle additional requests. Sharding is necessary if a dataset is too large to be stored in a single database\n- Sharding is also referred as horizontal partitioning","url":"https://tycholiz.github.io/Digital-Garden/notes/29d706e7-7bba-4151-9bbb-1502a8ff7f39.html","relUrl":"notes/29d706e7-7bba-4151-9bbb-1502a8ff7f39.html"},{"doc":"Pools","title":"Pools","hpath":"db.strategies.pools","content":"\n# Connection Pool\n- a cache of database connections maintained so that the connections can be reused when future requests to the database are required\n\t- after a connection is created, it is placed in the pool and it is used again so that a new connection does not have to be established\n- The client pool allows you to have a reusable pool of clients you can check out, use, and return\n\t- used to enhance the performance of executing commands on a database\n- If you're working on a web application or other software which makes frequent queries you'll want to use a connection pool.\n- Connecting a new client to the PostgreSQL server requires a handshake which can take 20-30 milliseconds. During this time passwords are negotiated, SSL may be established, and configuration information is shared with the client & server. Incurring this cost every time we want to execute a query would substantially slow down our application.\n- Typically, each application server instance would have its own pool\n\t- ex. If there are 4 Express servers running, then there will be 4 pools\n\t- between those 4 servers and the users sending requests to them sits a *load balancer*\n- The pool sits between the application server(s) and the database:\n```\nCLIENTs → load balancer → Express → pool → database\n```\n\nWith Postgraphile\n- The rootPgPool is not normally what is used with PostGraphile because its privileges are too elevatedt. typically you use rootPgPool for authentication tasks like Passport.js.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/decb1677-8ba2-40a5-9d7d-ed546a770943.html","relUrl":"notes/decb1677-8ba2-40a5-9d7d-ed546a770943.html"},{"doc":"Pagination","title":"Pagination","hpath":"db.strategies.pagination","content":"\n# Pagination\n## Limit/Offset Pagination\nA naive approach to pagination is to think like this: \"We want page 3, with a page size of 10, so we should load 10 items, starting after item 20\". This might look like this:\n```\nSELECT * FROM posts ORDER BY created_at LIMIT 10 OFFSET 20;\n```\nThis approach is naive because it has some major downfalls\n- If an item was added to the list while the user is switching pages, we will inevitably be skipping over items (ex. product site, chat app)\n- If an item was added at the top of the list while switching pages, then we might see the same item twice.\nThese previous examples show that the mental model of pages in a book is a poor analogy for pagination, since the data set is not static. This goes to show that for certain apps, the very concepts of page1 and page2 don't really make any sense, because the set of data and the boundaries between loaded sections is constantly changing.\n\n## Cursor-Based Pagination\nWhat if we could just specify the place in the list we want to begin, and then specify how many items we want to fetch? Then it doesn’t matter how many items were added to the top of the list in the meanwhile, since we have a constant pointer (cursor) to the specific spot where we left off.\n\nA cursor-based paginator needs 2 things to fetch more data: the current cursor position, and the number of items to fetch.\n\n*\"Cursor-based pagination is the most efficient method of paging and should always be used where possible.\"*\n- besides cursor-based pagination, the most\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c805a279-2f0e-4fff-b9bd-41acc00ba645.html","relUrl":"notes/c805a279-2f0e-4fff-b9bd-41acc00ba645.html"},{"doc":"Migrations","title":"Migrations","hpath":"db.strategies.migrations","content":"\n# Migrations\n- a migration is just a script that performs a set of synchronous database interactions\npreservation of data during migrations in general is not guaranteed because schema changes such as the deletion of a database column can destroy data\n\nmigrations should always be backwards compatible. This gives us the freedom to rollback the migration, and have a database that is still valid.\n- The proper approach to migrations is to expand the schema to work with the old version as well as the new version, then contract it to work only with the new version\n- Imagine 2 different users have different versions of an application. Upon signup, the older version asks for first and last name, while the newer asks for middle name as well.","url":"https://tycholiz.github.io/Digital-Garden/notes/438acc3f-8bf4-4984-bc8d-ca6843b66fee.html","relUrl":"notes/438acc3f-8bf4-4984-bc8d-ca6843b66fee.html"},{"doc":"Indexing","title":"Indexing","hpath":"db.strategies.indexing","content":"\n# Index\n- Indexing makes querying by a column faster, at the slight expense of create/update speed and database size.\n\t- For example, you will often want to query all comments belonging to a post (that is, query comments by its `post_id` column), and so you should mark the `post_id` column as indexed.\n\t- However, if you rarely query all comments by its author, indexing `author_id` is probably not worth it.\n\t- In general, most `_id` fields are indexed. Sometimes, boolean fields are worth indexing if you often use it for queries. However, you should almost never index date (`_at`) columns or string columns.\n- There are these hidden objects called indexes that can exist against a table. You can create an index against a set of columns and the SQL engine can jump to the specific record based on the index.\n- One way of optimizing queries is to create an index on a column or columns you might commonly filter/join against.\n- think of an indexing system as an umbrella. The metal tips on the edge of an umbrella represent the items that are available for retrieval in a database. If you were not using an index, you'd have to look at each tip to see if it's the item you want or not. With an index, you look from the top of the umbrella, and each arm leading to the tip would light up, indicating which items should be retrieved.\n\t- therefore, indexes basically tell us how to get a certain piece of data.\n- spec: so we should index the things that we are likely to query on. If we frequently want to retrieve books from the db filtered by a certain year, we'd want to add an index onto the \"year\" column, making that data easier to retrieve.\n\t- spec: otherwise we'd have to essentially search through every single book to see if it satisfies the filter requirements for the year we are seeking.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f606b993-1319-4804-b8f8-b10073f5ab5c.html","relUrl":"notes/f606b993-1319-4804-b8f8-b10073f5ab5c.html"},{"doc":"Etl","title":"Etl","hpath":"db.strategies.etl","content":"\n# ETL Pipeline\nStands for **Extract**, **Transform**, **Load**\n\nThe ETL Pipeline can be thought as a series of processes whose goal is to take data from some external source, transform it to fit our needs, then loading that transformed data into our own database. \n- With this capability, we are able to enhance reporting, analysis and data synchronization.\n\n## Extract\n- data might be extracted from business systems, APIs, data from physical sensors, marketing tools, transaction databases (eg. Stripe)\n\n## Transform\ndata is temporarily stored in at least one set of staging tables as part of the ETL process\n\n## Load\nthe load phase doesn't have to be the end of the pipeline. Once the data has been successfully inserted into our database, it can trigger webhooks in other systems to perform more actions.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/78a0242f-6878-4ec5-b13a-5a4207b447f1.html","relUrl":"notes/78a0242f-6878-4ec5-b13a-5a4207b447f1.html"},{"doc":"Cursors","title":"Cursors","hpath":"db.strategies.cursors","content":"\n# Cursor\n- a cursor is a piece of data (likely just an ID) that represents a location in a paginated list.\n- the cursor is the thing that allows us to traverse over records in a database\n- A cursor can be viewed as a pointer to one row within a set of rows\n- cursors also facilitate retrieval of the records, as well as creating and removing records\n- a cursor is conceptually and behaviourally very similar to an iterator\n- Cursors are used to process individual rows returned by database system queries\n- a cursor makes it possible to define a result set and perform complex logic on it, on a row by row basis","url":"https://tycholiz.github.io/Digital-Garden/notes/88ef9a87-29b9-4775-b3e7-4917f0632f9b.html","relUrl":"notes/88ef9a87-29b9-4775-b3e7-4917f0632f9b.html"},{"doc":"Local","title":"Local","hpath":"db.local","content":"\n# Local storage\n- anything inside local storage is accessible by any script inside your page. This is why storing JWTs in local storage is a bad idea, but storing state about the app would be fine.\n\n# Resources\n### real-time data\n[RethinkDB](https://github.com/rethinkdb/rethinkdb)\n[realtime](https://github.com/supabase/realtime)","url":"https://tycholiz.github.io/Digital-Garden/notes/a30883fd-5892-4ee5-80f7-d2c2ea2ea5ff.html","relUrl":"notes/a30883fd-5892-4ee5-80f7-d2c2ea2ea5ff.html"},{"doc":"Dbms","title":"Dbms","hpath":"db.dbms","content":"\n# DBMS\n*\"SQL tells the database what information you want, and the DBMS (Postgres, MySQL) determines the best way to provide it.\"*\n- its role in your software architecture is to handle concurrent access to live data that is manipulated by several applications, or several parts of an application.\n\n# UE Resources\n- [Evolutionary database design](https://martinfowler.com/articles/evodb.html)","url":"https://tycholiz.github.io/Digital-Garden/notes/050410ad-18fd-4fa7-8e04-e97831b4ef02.html","relUrl":"notes/050410ad-18fd-4fa7-8e04-e97831b4ef02.html"},{"doc":"Cluster","title":"Cluster","hpath":"db.cluster","content":"\n# Cluster\n- a collection of databases servers (ie. nodes, instances) that are connected to a single database file. It is made up of one master node, and 1+ slave nodes.\n- The benefits of clustering are: data redundancy, load balancing, high availability, and monitoring and automation.\n- a collection of databases managed by a single PostgreSQL server instance constitutes a database cluster.\n- a cluster is created with the `initdb` command\n- in file system terms, a database cluser is a single directory under which all data is stored.\n\t- normally `/usr/local/pgsql/data` or `/var/lib/pgsql/data` (determined by the package)\n- the unix user postgres should own this directory\n\n## Node\n- **node** - an instance of a database\n- the master node is typically the only one that gets written to. \n\t- The master can operate without slaves, and if there is an emergency, the slave node can be promoted to master, since it has the most up to date data.\n- the slave node typically exists as a backup, or as a read replica\n\t- a read replica means that the read and write traffic can be split between the two","url":"https://tycholiz.github.io/Digital-Garden/notes/b87039bc-2dec-48a0-b11f-7a82ec1d5df9.html","relUrl":"notes/b87039bc-2dec-48a0-b11f-7a82ec1d5df9.html"},{"doc":"Acid","title":"Acid","hpath":"db.acid","content":"\n# ACID\n\"As a developer, we should think of a database as something that is ACID-compliant.\" —Dmitri Fontaine\n\n* * *\n\nFrom the Postgres-XL docs, here is an implication of it being fully ACID:\n*When you start a transaction or query in Postgres-XL, you’ll see a consistent version of your data across the entire cluster. While you are reading your data on one connection, you could be updating the same table or even row in another connection without any locking. Both connections are working with their own versions of the rows, thanks to global transaction identifiers and snapshots.  Readers do not block writers and writers do not block readers.*","url":"https://tycholiz.github.io/Digital-Garden/notes/1390df9d-094c-4ed1-b98a-b34fc66a6b39.html","relUrl":"notes/1390df9d-094c-4ed1-b98a-b34fc66a6b39.html"},{"doc":"Isolated","title":"Isolated","hpath":"db.acid.isolated","content":"\n# Isolated\n- the opposite side of atomicity.\n- while we are doing our queries, are we allowed to see what is happening concurrently in the rest of the system?\n\t- ex. what if we want to make a backup with `pg_dump` that needs to run for several hours? That backup needs to be a consistent snapshot of the production database. If during the backup someone is doing inserts, we don't want these to be in the backup, since we want a snapshot that doesn't move.\t\t- to do this, postgres uses an isolation mode that prevents this from happening.\t\n","url":"https://tycholiz.github.io/Digital-Garden/notes/57e6c077-24da-43e2-b5b6-787b95e8d2a4.html","relUrl":"notes/57e6c077-24da-43e2-b5b6-787b95e8d2a4.html"},{"doc":"Durable","title":"Durable","hpath":"db.acid.durable","content":"\n# Durable\n- the idea that anything that has been known to have been committed by the client should still be there if we were to theoretically remove the power from our machine partway through the operation. \n- In Postgres, Durability is provided through write-ahead logs\n","url":"https://tycholiz.github.io/Digital-Garden/notes/9395c335-1826-438a-bf77-d38cda349a97.html","relUrl":"notes/9395c335-1826-438a-bf77-d38cda349a97.html"},{"doc":"Consistent","title":"Consistent","hpath":"db.acid.consistent","content":"\n# Consistent\n- The idea that we have business-logic that we can share with Postgres, and have it implement those guarantees for us. \n\t- ex. each employee must have a non-negative integer for their salary\n- Includes things like what data types we use, the shape of our schema, what constraints we have (not null, fkey etc.), relations...\n\t- ex. The schema and the data types. When we define these structures in our database, all data that is entered must comply with the rules set out. For instance, if we have an id column of type int, then the data entering *must* be an int.\n\t- ex. the fact that MongoDB is schemaless means that we do not have Consistency. Nor does it have transactions (the A and I)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3a12413e-c899-40d3-9199-b77f0b6d049e.html","relUrl":"notes/3a12413e-c899-40d3-9199-b77f0b6d049e.html"},{"doc":"Atomic","title":"Atomic","hpath":"db.acid.atomic","content":"\n# Atomic\n- The idea that if we have multiple things to do in a single transaction, we can roll it back as a chunk\n\t- Imagine a transaction consists of 2 inserts and 1 update. The fact that we have atomicity means that we can rollback that whole transaction. This distinction is even more important when you consider multi-table transactions, which MongoDB does not offer.\n\t\t- ex. In old versions of MongoDB if you needed to remove an item from inventory and add it to someone's order at the same time, you could not.\n\t- Imagine a situation we want to update the schema of our database (eg introduce a new table or modify an existing one). If our system crashed in the middle of the update and our database didn't have Atomicity, then the result would be a mix between v1 and v2.\n- In Postgres, Atomicity is provided through write-ahead logs. Shadow Paging is another technique to provide atomicity.\n\n### Transaction\n- logically clumping together multiple database interactions (CRUD) as if they were one action\n\t- ex. in a balance sheet, we want to treat the asset change that corresponds to the liability+shareholer's equity change as atomic. Without transactions, if we were to make one change and for some reason the second change fell over, we would have an imbalanced balance sheet. At least with Transactions, we can be guaranteed that this would never occur\n- When an execution of transaction is interrupted, the transaction is not executed at all.\n\n### Write-Ahead Logging (WAL)\n- Before the results of a transaction are written to the database, the changes are first recorded in a log. This log is written to stable storage to ensure that the whole complete file is saved. Only once this log is known to be securely stored is the change made to the database. \n\t- ex. imagine the machine hosting the database lost power midway through performing some operation. When the machine boots back up, we can use WAL to determine what atomic changes were *supposed* to have been made, and compare that to what actually changed. With this information, we can determine whether or not we should rollback. This guarantees both atomicity and durability.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/e9d3124f-2101-412a-ba40-9c4fb529ee52.html","relUrl":"notes/e9d3124f-2101-412a-ba40-9c4fb529ee52.html"},{"doc":"CSS","title":"CSS","hpath":"css","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1374e9e9-1cbc-4e1f-b1ca-66b8569533dd.html","relUrl":"notes/1374e9e9-1cbc-4e1f-b1ca-66b8569533dd.html"},{"doc":"Transform","title":"Transform","hpath":"css.transform","content":"\n3dtransform() will use GPU, transform(x) will use the CPU\n- 3dtransform will be a smoother experience, and is therefore the best practice","url":"https://tycholiz.github.io/Digital-Garden/notes/df69a998-2c57-4273-8329-3b9abc15c099.html","relUrl":"notes/df69a998-2c57-4273-8329-3b9abc15c099.html"},{"doc":"Layout","title":"Layout","hpath":"css.layout","content":"\n## Layout-isolated component\n- def - A component that is unaffected by the parent it is placed within, and does not itself affect the size and position of its siblings.\n    - this only applies to the root element of a reusable component\n![](/assets/images/2021-03-28-19-49-24.png)\n- avoid any properties on the root element of a component that affect, or are affected by elements outside of the bounds of that component.\n    - ex. margin, because it acts on elements outside of the component's scope\n    - ex. align-self, as it will stretch the width or height of the component depending on the flex-direction of its parent\n    - by contrast, padding is fine, as it is confined to the scope of the component\n    - Basically, if a property depends on, or impacts other components outside of its scope, discourage its use.\n```\n// Does NOT conform to layout isolation principals\nfunction MyComponent() {\n  return (\n    <div style={{alignSelf: 'center'}}>\n      <div />\n    </div>\n  )\n}\n\n// This component is layout isolated, since the potentially dangerous property is not on the root element\nfunction MyComponent() {\n  return (\n    <div>\n      <div style={{alignSelf: 'center'}}/>\n    </div>\n  )\n}\n```\n\n# E Resources\nhttps://visly.app/blogposts/layout-isolated-components\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f11b84eb-c069-42fb-8f6c-bd697a0766f2.html","relUrl":"notes/f11b84eb-c069-42fb-8f6c-bd697a0766f2.html"},{"doc":"Flexbox","title":"Flexbox","hpath":"css.flexbox","content":"\n# Flexbox\n- using explicit margins breaks flexbox's ability to position things nicely, since it doesn't play nicely with the math that it's relying on\n- when doign flexbox row dont use margin left/right, and when doing column, dont use margin top bottom, because it interferes with the math that flex is doing\n    - try and use justify-content: space-between instead, and set flex: 0 49% for example, which will almost serve as our margin\n \n note: all examples below assume `fd: row`. If `column`, then substitute width for height.\n## Flex-Grow\n- when there is available space after all boxes have taken up their room, `flex-grow` will allow the item to grow proportionally to the other item's `flex-grow` property.\n    - therefore, all items having flex-grow: 1 is the same as all items having flex-grow: 100. Think of it as a proportional growth rate. \n    - an item with flex-grow: 3 will grow 3 times faster than an item with flex-grow: 1 \n- a value of `0` means the item won't be resized during the size calculation to\n    accommodate the flex container's full main axis size\n- Imagine we use different flex-basis for each flex item. Even if we have `flex-grow: 1` on each item, they will still grow at a different rate, because of the basis at which they started.\n\n## UE Resources\n[flex grow article](https://css-tricks.com/flex-grow-is-weird/)\n## Flex-Shrink\n- The rate at which a flex item will shrink when the page is resized to a size smaller than the flex-containers most \"comfortable\" size.\n## Flex-Basis\n- initial main size of a flex item before any available space is distributed\n    among the flex items (which is determined according to flex-shrink and\n    flex-grow). \n- It will override any width property, unless left as `auto`\n    - when set to `auto`, it checks for a width property (if fd: row). If none found, it uses the content inside the flex-item to determine its width.\n    - `flex-basis` still obeys `min/max-width` settings\n- In a sense, it is similar to `min-width`\n    - (although `flex-shrink` determines how the flex item will behave when the\n    size of the whole flexbox shrinks below initial size)\n    - If we were to use `min-width` and shrink the browser size, the browser would make us scroll horizontally to see all of the content.\n## Flex Shorthand\n- by default, it is `flex: 0 1 auto` (?)\n- `flex: 1 1 0` - ==flex-grow== ==flex-shrink== ==flex-basis==\n    - also can be written `flex: `\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f54e7861-aef0-40f3-b57f-92c166bbc3f2.html","relUrl":"notes/f54e7861-aef0-40f3-b57f-92c166bbc3f2.html"},{"doc":"Crypt","title":"Crypt","hpath":"crypt","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5a9fb1df-478e-4687-9be0-5cb97e61ec57.html","relUrl":"notes/5a9fb1df-478e-4687-9be0-5cb97e61ec57.html"},{"doc":"Public Key","title":"Public Key","hpath":"crypt.public-key","content":"\n## Public Key Cryptography\n- In a simple cryptographic system, I have some message that I can encrypt using a key. I can then give you that key along with the information, and you can decrypt it and read the message.\n\t- This is known as Symmetric Encryption\n\t- The problem with this, is that I need to send you the key. Since we have not yet established a secure connection, we cannot be sure that no one is listening in.\n- in Public Key Cryptography, we generate a key pair: a private key and public key.\n\t- This is known as Asymmetric Encryption\n\t- It is Asymmetric because key A is used to encrypt the message, while key B is used to decrypt it.\n\t\t- Anything we encrypt with key A can only be decrypted with key B, and anything encrypted with key B can only be decrypted with key A\n- Now we can have a situation where both you and I have our own key-pairs, with each person's public key being widely distributable. \n\t- Now if I want to send something to you, I just need your public key. I can encrypt the message like this, and you are the only person that can decrypt it, since you have the private key. \n- This system also allows the receiver of a message know exactly who sent it. Consider that if I use my private key to encrypt a message and send it out, anyone with my public key can decrypt it. This has an important implication, which is that if you were able to successfully decrypt my message, then you know for a fact that I was the one who encrypted it. \n- In the public key system, the public key is for encrypting, while the private key is for decrypting.\n- Diffie-Helman\n\n### Mail Poker Example\n- *note:* is this an actual analogy of public key, or something else?\n- [source](https://www.youtube.com/watch?v=mthPiiCS24A&list=PLt5AfwLFPxWLXe-ZqZyu0kSsaWd4FjXbj)\n- Imagine we wanted to play a game over poker over the mail. Of course, each player needs to be assured that the same rules apply as if the game was in person (neither side knows the other's cards, each side only takes 5 cards, etc). \n- Player 1 puts each card in an envelope, and attaches a lock. This lock is only openable by player 1, but only one key is needed to open all the locks.\n- Player 1 mails all 52 cards to Player 2. Player 2 then puts his own lock on all envelopes. Again, his key will open all envelopes. Then Player 2 will send the double-locked envelopes back to Player 1. (the randomized nature of the mail process acts as a secure and trusted method of shuffling) \n\t- note: we have created a situation where neither player can reveal any card on their own. \n- Player 1 now has 52 double-locked envelopes, so now must choose 5 cards for Player 2. Once they are chosen, Player 1 takes his lock off those cards and mails just those 5 cards. Once Player 2 receives those cards, he can unlock them and now knows his hand. \n- Player 1 now has 47 double-locked envelopes, so now he must choose 5 for himself. Once he chooses, he sends them to Player 2. Player 2 removes his lock from those 5 cards, and sends them back to Player 1. Player 1 removes his lock, and now knows his own hand.\n\n- This system of double-locking creates what is called *commutativity*. A commutative crypographic algorithm is one that is order-independent (ie. it doesn't matter which order the locks are taken off)  \n\n### Bridging the Analogy Gap\n- If this game of poker were to take place over the internet, each of the 52 cards would have a numeric value (1-52). We need a way to \"lock\" each card. Player 1 creates his key (`k`) by choosing a random number. He then takes that number and raises each card to that value.\n\t- ex. if `k = 3`, then each card value is raised to the 3rd power. This is equivalent of locking the envelopes, as long as Player 2 doesn't know what `k` is.\n- Player 2 then can create his own key (`j`), and raise each (already encrypted) card value to that value. Now we have double-locked cards.\n\t- It's important to note that `(2^5)^3` is mathematiacally equivalent to `(2^3)^5`. In other words, the locks can be applied or removed in any order. \n- Now, to remove the locks, each player simply needs to multiple the exponent by the inverse of their key (if Player 1 wants to remove the locks of 5 cards, then he just needs to multiply the exponent by `-3`)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/84421186-d17f-43a1-95bf-074f697b437c.html","relUrl":"notes/84421186-d17f-43a1-95bf-074f697b437c.html"},{"doc":"Hashing","title":"Hashing","hpath":"crypt.hashing","content":"\n# Hashing \nlet's say we have a function that takes a number from zero through nine, adds three and, if the result is greater than ten, subtracts ten. So f(2) = 5, f(8) = 1, etc. Now, we can make another function, call it f', that goes backwards, by adding seven instead of three. f'(5) = 2, f'(1) = 8, etc.\n- Theoretically, any mathematical functions that maps one thing to another can be reversed. In practice, though, you can make a function that scrambles its input so well that it's incredibly difficult to reverse.\n- taking an input and applying a one-way function is called \"hashing\" the input\n- SHA1 is an example of this kind of \"one-way\" function","url":"https://tycholiz.github.io/Digital-Garden/notes/97eb066d-2fda-4fc8-af7c-c2d6a79dc1e1.html","relUrl":"notes/97eb066d-2fda-4fc8-af7c-c2d6a79dc1e1.html"},{"doc":"K Anon","title":"K Anon","hpath":"crypt.hashing.k-anon","content":"\n# K Anonymity\n- When entering a password, the client hashes it into a 130 bit hash, then sends only the first several characters to the server. The server checks its database of passwords hashes, and returns all of the passwords that begin in the same way. Once the client receives this, it selects the hash that matches with its password\n- The benefit of this is that a password hash is never actually sent from the client to the server. MITM attacks are less threatening. The server also doesn't gain any valuable information, since it won't know if any of the hashes it is sending actually matches the one owned by the client.\n- ex. run `curl https://api.pwnedpasswords.com/range/f42b7e\n`. We will get in return a list of hashes for passwords that have been cracked (ie. leaked in plain white text)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/25067733-134e-480f-b219-bd5414d0df19.html","relUrl":"notes/25067733-134e-480f-b219-bd5414d0df19.html"},{"doc":"Algo","title":"Algo","hpath":"crypt.hashing.algo","content":"\n# Hashing Algorithms\n## RSA\nRSA security is based on 2 matching keys. There is a public key for each user, and everybody can (should) know it. There is also a private key that only the user should know. A message encrypted by the public key can only be decrypted by the private key, and visa versa.\n- Thus, if I want to send you a message that only you can read, I get (from the network) your public key, encrypt the message with that key and you are the only person who can decrypt it.\n- Or, if I want to prove to you that I sent a message, I can encrypt the message with my private key, tell you (in open text or in another message) how it was encrypted. Then you could decrypt the message with my public key, and if it becomes readable, you know it came from me.\n- RSA-129 is a publicly available 129 digit number that is derived from multiplying 2 very large prime numbers (those numbers are unknowable)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/7c46fe72-7104-4de8-96a1-74d90f5799be.html","relUrl":"notes/7c46fe72-7104-4de8-96a1-74d90f5799be.html"},{"doc":"Cracking","title":"Cracking","hpath":"crypt.cracking","content":"\n# Password Cracking\n- with `hashcat`, dedicated servers for cracking passwords can hash a word and check for the presence of the hash in a big list of hashes 40 billion times per second (if the hashes were hased with md5 algo). If a more secure algo is used like SHA512 or Bcrypt, it would be in the thousands instead. \n- In a brute force attack, a 7 letter word all lower-case has 26^7 possible combinations, which is just over 8 billion combinations. This makes an md5 stored 7 letter word breakable in less than a second.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/b0e7ebf9-f777-4610-bf72-3c0649fdca67.html","relUrl":"notes/b0e7ebf9-f777-4610-bf72-3c0649fdca67.html"},{"doc":"Cloudflare","title":"Cloudflare","hpath":"cloudflare","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/f6feef57-b8f5-451f-a09a-7b63f6a07183.html","relUrl":"notes/f6feef57-b8f5-451f-a09a-7b63f6a07183.html"},{"doc":"Workers","title":"Workers","hpath":"cloudflare.workers","content":"\nCloudflare workers are serverless functions that allow us to intercept HTTP requests (run code in between request and response\n\nThere are notable differences with serverless functions:\n- Workers run on the edge. This means that they are available on a number of Cloudflare servers around the world so your user will get the lowest possible latency. This is not how cloud functions usually work. For example in AWS you have Lambda and Lambda at Edge.\n- Since workers are running in Cloudflare's infrastructure they have access to some unique API to interact with the CDN and caching which is the main Cloudflare product. So you can receive a request to your worker and then decide \"hey, put this image in the cache for next time\".\n- Workers have access to a Key Value (KV) store which is storing data on the edge.\n\nWorkers runs code on Cloudflare's edge. And each Worker is assigned to at least one HTTP endpoint. So these are scripts that live ~10ms away from most users and can be updated globally easily.\n\nex. Workers can be used to apply http headers if you use a serverless setup for hosting.\nex. You can host every static website / react SPA on cloudfare workers\nex. You can use workers to inject headers into responses without having to change Nginx configs","url":"https://tycholiz.github.io/Digital-Garden/notes/60db6e0b-08c5-4e77-acf4-6f6ad96102e0.html","relUrl":"notes/60db6e0b-08c5-4e77-acf4-6f6ad96102e0.html"},{"doc":"Kv","title":"Kv","hpath":"cloudflare.workers.kv","content":"\nCloudflare Workers KV provides access to a secure low latency key-value store at all of the data centers in Cloudflare's global network\n- usage ex. save the cart and checkout in Workers KV and when our webhook is notified by Stripe of a successful payment_intent, we create the order in our backend.\n\t- ex. this decouples your server having to be up and running from being able to process orders","url":"https://tycholiz.github.io/Digital-Garden/notes/68c1163e-d6e5-4c09-9be9-290925879904.html","relUrl":"notes/68c1163e-d6e5-4c09-9be9-290925879904.html"},{"doc":"Warp","title":"Warp","hpath":"cloudflare.warp","content":"\nSimilar to a VPN, in that it will encrypt your traffic, but it doesn't fake our IP like a VPN does.\n\nSince Workers are distributed across 190+ datacenters, you can get the website loading in milliseconds all around the world.","url":"https://tycholiz.github.io/Digital-Garden/notes/a2bcbbfb-c16a-47d2-ba2c-4deca6ea4686.html","relUrl":"notes/a2bcbbfb-c16a-47d2-ba2c-4deca6ea4686.html"},{"doc":"Chrome","title":"Chrome","hpath":"chrome","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/dc79799f-a55d-48ab-a8be-c48adb1b19c0.html","relUrl":"notes/dc79799f-a55d-48ab-a8be-c48adb1b19c0.html"},{"doc":"Debugger","title":"Debugger","hpath":"chrome.debugger","content":"\n## Debugging outbound network issues\n1. Sources tab, hit `Pause on exceptions` (stop sign with pause icon)\n2. Reload the page, and code will pause on the first exception that happens\n3. Hover over the failing line, and investigate\n\t- are args to the function call what you'd expect them to be? Does anything stand out? Why is an error being thrown?","url":"https://tycholiz.github.io/Digital-Garden/notes/3dd7af81-5842-4286-85a0-ad9ee07f0db3.html","relUrl":"notes/3dd7af81-5842-4286-85a0-ad9ee07f0db3.html"},{"doc":"Cmds","title":"Cmds","hpath":"chrome.cmds","content":"\n# Google Chrome\n## Devtools\n- swap between chrome tabs - `cmd+[`/`cmd+]`\n- move dock position - `cmd+shift+d`\n\n## Vimium\n- `<c-u>` move tab to new window (unpin)\n- `<c-n>` - pin the tab back to the main chrome window\n- `^` - go to most previous tab\n- `yt` - duplicate current tab\n- `gE` - edit the current url and open in new tab\n- `gi` - focus first (or nth) input box on page\n- `yy` - copy url that you are currently on\n- `gu` - go up a level in the url hierarchy","url":"https://tycholiz.github.io/Digital-Garden/notes/83f93e06-2b7a-44ba-99cc-3a46b2a77cb3.html","relUrl":"notes/83f93e06-2b7a-44ba-99cc-3a46b2a77cb3.html"},{"doc":"C","title":"C","hpath":"c","content":"\n- C is designed to interface with the output of the computer hardware. Therefore, it is a natural language to use when building operating systems. When you write code in C, you can better intuit what the resulting assembly language will look like. C effectively enables you to write code from a hardware-first perspective.\n\nIn C, if a function signature doesn’t specify any argument, it means that the function can be called with any number of parameters or without any parameters.\n\n- `main` is a function just like any other, and thus returns a value to the environment that executed the c program to begin with.\n\n# Header Files\n- purpose is to share functions and macros across source files.\n\n* * *\n\n## Aspects\n### Const\n- a *constant expression* involves only constants, and is therefore evaluated during compilation \n- an `emun` is a type of constant. By default, the first element has value 0, the second has value 1, and so on.\n\n## Operators\n### Token-pasting operator (##)\n- if 1+1=2, then 1##1=11\n\n## Concepts\n\n* * *\n\n### Symbolic Constants (`#define`)\n- `#define` creates a macro\n- Any constant defined in this manner will find all occurrences and replace it with the corresponding value *before* compilation \n\t- This contrasts with variables, in that data is actually stored inside of them (while macros are more like aliases) \n- Symbolic Constants are valauble for defining magic numbers\n\n* * *\n\n## Debugging\nErrors involving “token” almost always mean that you either missed a semicolon or your {}s, ()s, or []s aren’t matched.\n\t\t\n* * *\n\n### Indirection\n- The act of referencing something using a name, rather than using the value itself.\n- A common form of indirection is when we manipulate a value via its memory address\n\t- ex. accessing a variable by using a pointer.\n- aka Dereferencing\n\n### Handle vs Pointer vs Reference\n- While a pointer contains the address of the item it refers to, a handle is an opaque reference to an object. \n- The type of the handle is unrelated to the element referenced\n- using handles adds an extra layer of indirection, meaning that we can change the details at this level without breaking the program. The same couldn't be said for a pointer. \n- A pointer is the combination of an address in memory and the type of the object that resides in that memory location","url":"https://tycholiz.github.io/Digital-Garden/notes/4668ac41-f2d5-40c0-9144-efb456d8663b.html","relUrl":"notes/4668ac41-f2d5-40c0-9144-efb456d8663b.html"},{"doc":"Preprocessors","title":"Preprocessors","hpath":"c.preprocessors","content":"\n### Preprocessors (header of the file)\n- contains C function declarations and macro definitions to be shared between several source files.\n\t- In essence, they are like importing modules.\n- ex. `stdio.h` – Defines core input and output functions\n- In C, all lines that start with # are processed by preprocessor which is a program invoked by the compiler\n\t- a preprocessor takes a C program and produces another C program without any `#`\n- When we use `include` directive,  the contents of included header file (after preprocessing) are copied to the current file.\n\t- `<` and `>` instruct the preprocessor to look in the standard folder\n\t- `“` and `“` instruct the preprocessor to look into the current folder\n- When we use `define` for a constant, the preprocessor produces a C program where the defined constant is searched and matching tokens are replaced with the given expression\n\t- ex. `#define max 100` — `max` is searched for in the program, and is replaced with `100`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d048ee78-2ba4-4fe6-a4df-be32c9df3e45.html","relUrl":"notes/d048ee78-2ba4-4fe6-a4df-be32c9df3e45.html"},{"doc":"Makefile","title":"Makefile","hpath":"c.makefile","content":"\n## Purpose\n- to help decide which parts of a program need to be recompiled.\n- Makefiles allow us to give a series of instructions to run depending on what files have changed.\n- similar to the sript section of the `package.json`\n\n## Structure\n- a Makefile consists of a set of rules, which take the form:\n```\ntargets: prerequisites\n   command\n   command\n   command\n```\n- `targets` are filenames.\n- `commands` are a series of steps, normally used to make the target(s)\n- `prerequisites` are dependencies that are needed before the commands can be run.\n\n## Running \"make\" command\n- when we run `make` without arguments, the first target that doesn't begin with `.` is processed. This is known as the *default goal*.\n\t- To do this, it may have to process other targets, specifically ones that the first target depends on.\n- Often the *default goal* is called `all`, though this is just a convention.\n\n### Building from source with `make`\n1. run `make`\n2. run `sudo make install`\n\n## Syntax\n- `PG_CONFIG ?= pg_config` - set PG_CONFIG variable only if it isn't already set\n\n### UE Resources\n- [Makefile Tutorial](https://makefiletutorial.com/)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/5ba4161b-b33f-4f4b-8046-ca9bee060161.html","relUrl":"notes/5ba4161b-b33f-4f4b-8046-ca9bee060161.html"},{"doc":"Lang","title":"Lang","hpath":"c.lang","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/31185107-edea-4538-ab47-c18d9bdc52cd.html","relUrl":"notes/31185107-edea-4538-ab47-c18d9bdc52cd.html"},{"doc":"Variables","title":"Variables","hpath":"c.lang.variables","content":"\n### Variables\n- `external` and `static` variables are guaranteed to be initialized to zero if not explicitly declared.\n\t- The value of these variables is determined conceptually before the program begins execution \n- automatic and register variables have undefined (i.e., garbage) initial values, if not explicitly declared.\n\t- The value of these variables is determined when the function or block is entered.\n\nStatic\n- Decalaring a variable or fn *static* does 2 things:\n1. it becomes scoped to wherever it was defined (scoped to source file, fn etc)\n2. When static is declared inside a function, then that variable will live on between calls of the function, giving us permanent stoage. \n- Static variables are allocated memory in data segment, not stack segment\n- Unless initialized with a value, static variables default at ‘0’\n\nRegister\n- Declaring a variable with `register` advises the compiler that it will be heavily used, placing it in a machine register for quicker access\n- Only a few variables in each function may be kept in registers, and only certain types are allowed.\n\t- Excess register declarations are harmless, however, since the word register is ignored for excess or disallowed declarations\n\t\t- The actual limit varies from machine to machine.\n\t- It is impossible to get the memory address of a register variable. ","url":"https://tycholiz.github.io/Digital-Garden/notes/8d3fac8c-9f15-45f4-9b4e-a1d52dda74e2.html","relUrl":"notes/8d3fac8c-9f15-45f4-9b4e-a1d52dda74e2.html"},{"doc":"Types","title":"Types","hpath":"c.lang.types","content":"\n## Types\nIn the early days of C, pretty much everything was either an int or pointer (which is why int is the default type)\n\n### Numbers\nPrefixes\n- A leading `0` on an integer constant means octal\n- a leading `0x` means hexadecimal\n\nSuffixes\n- spec:an int is by default short, and has no suffix:\n`int num = 1234;`\n- but to make a long constant (double by default), we need to append the number with `L`:\n`long num = 123456789L`\n\t- note: even if `long` is not specified, the compiler will know it is long and declare it as such \n- to make an unsigned const, append `U`\n- to make a float const, append `F`","url":"https://tycholiz.github.io/Digital-Garden/notes/00857233-a2b5-4df1-95ac-5c6be609809f.html","relUrl":"notes/00857233-a2b5-4df1-95ac-5c6be609809f.html"},{"doc":"String","title":"String","hpath":"c.lang.types.string","content":"\n# String\n- The following are equivalent:\n```\nchar pattern[] = \"ould\";\n// and\nchar pattern[] = { ′o′, ′u′, ′l′, ′d′, ′\\0′ };\n```\n- C does not have a native string type. Instead, the language uses arrays of char terminated with a null char (`'\\0'`)\n\t- Therefore, the physical storage required is one more than the number of characters\n\t-  Therefore, the physical storage required is one more than the number of characters\n\t- The `\\0` gets automatically appended for us when array is initialized. Therefore if we access the last element, we get back `\\0`\n- there is a standard header `<string.h>` that gives us some string utilities\n- In C, arrays are of fixed length at the time they are declared. For this reason, it may make sense to declare an array with a length that is far more than you need, just so you know it can handle everything that gets thrown at it. \n\nSingle vs Double Quotes\n- In C, `'x'` is an integer representing the ASCII value, while `\"x\"` is a character array with 2 elements: `x` and `\\0` \n\n## Initialization\n- Character arrays can be initialized in different ways:\n\t- `char name[13] = \"StudyTonight\";`\n\t- `char name[] = \"hello\";` (spec:wide char array)\n\t- `char name[10] = {'L','e','s','s','o','n','s','\\0'};`\n- These character arrays can either be read-only or manipulatable\n\t- read-only: `char * str = \"Hello\";`\n\t- manipulatable: `char str[] = \"Hello\";`\n\n## Format specifiers \n- `%s` - Take the next argument and print it as a **string**\n- `%d` - Take the next argument and print it as an **int**\n\n## printf (print formatted)\n- The compliment to `printf` is `scanf`, in the sense that print is stdout and scanf is stdin. \n\t- `scanf` terminates its input on the first white space it encounters. Tip: use \"edit set conversion code `%[..]` \" to get around this.\n\t\t- alternatively, use `gets()`, which will terminate once hitting `\\n`\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3e60b366-91c5-4acb-85f0-d81f0045a395.html","relUrl":"notes/3e60b366-91c5-4acb-85f0-d81f0045a395.html"},{"doc":"Pointer","title":"Pointer","hpath":"c.lang.types.pointer","content":"\n# Pointer\n- `&` returns the memory address of the object\n\t- called \"address of\" operator\n\t\t- Therefore, this gets applied to the value to get the memory address\n\t- ie. `p = &c;`\n- `*` either: \n\t1. declares a pointer variable, or\n\t\t- ex. `int *p;`\n\t2. dereferences an existing pointer (indirection through a pointer)\n\t- called \"indirection\" or \"dereferencing\" operator\n\t\t- Therefore, this gets applied to the pointer to get the value in storage\n\t- `int a = 5` then `int *ptr = &a;`\n\t\t- this means \"locate\" the address where `a` is stored, and assign its value to `ptr`\n\t\t\t- In other words, the value of `ptr` will now be the address where `a` is stored. \n\t- `*ptr = 8`\n\t\t- this means take the address of `ptr`, \"locate\" that address in memory, and set its value to 8.\n\t- `int *ptr`\n\t\t- \"`ptr` is a pointer that points to an object of type `int`\", or simply: \"`ptr` is a pointer to `int`.\"\n- If p points to the integer x, then *p can occur in any context where x could, so:\n```\n*p = *p + 10;\n// is the same as\nx = x + 10;\n```\n\n![996035c6b28b5cf88b9156c920cc0d58.png](:/454c71a68a8845c99466685e0c038c4d)\n- This picture shows how a pointer references a storage location, which holds a variable `c`. When we use the `&` operator, we are talking about the place where `c` is stored. When we use the `*` operator, we are talking about the variable `c` itself. \n\n- each pointer points to a specific data type, which is why we declare a pointer variable `int *p;`\n\t- The exception is \"pointer to void\", which can hold any kind of pointer, but cannot be dereferenced itself.\n- C does not implicitly initialize storage duration of memory locations. Therefore, we should be careful that the address that the pointer points to is valid.\n\t- For this reason, some suggest initializing pointers to `NULL` (*null pointer*/*null reference*)\n\t- null pointer shown as `0x00000000`\n- Like other languages, manipulating a function argument will have no effect on the original variable that was passed to the function. This is because when we pass an argument, a copy is made and we are merely mutating the copy. \n\t- However, we are also able to call a function, passing in the variable's *address* as the argument `passByAddr(&x)`.\nfrom within the function, if we change the value like so `*m = 14`. this changes the value at the address, so outside the function we'll notice that the value changed\n","url":"https://tycholiz.github.io/Digital-Garden/notes/d6328b43-7d57-4ad4-9bf8-8c9c89f13851.html","relUrl":"notes/d6328b43-7d57-4ad4-9bf8-8c9c89f13851.html"},{"doc":"Opaque","title":"Opaque","hpath":"c.lang.types.opaque","content":"\n# Opaque\n- An opaque type is a type that \"wraps\" lower-level types, and is often used when either the underlying implementation is complex, or the user simply does not need to know about the inner workings\n\t- ex. there is a type in Swift called `CFString`. It is an opaque type that provides a series of methods that allow for string manipulation and string conversion. For example, we have a `.length` and `.indexOf` methods. The implementation details of these methods is unimportant to the user of this type, so they have been hidden. \n\t\t- This is the essence of an opaque type, in that a native type has had some extra functionality added to it by being \"wrapped\".\n- An \"opaque type\" is a type where you don't have a full definition for the struct (or class in the case of C++)\n\t- In C, you can tell the compiler that a type will be defined later by using a forward declaration:\n\t```\n\t// forward declaration of struct in C and C++ \n\tstruct Foo;\n\t```\n\t- here, the compiler only has enough info to be able to declare pointers to `Foo`, which is sometimes all we need to do\n\t\t- Allows library and framework creators to hide implementation details, allowing the users of that library to call helper functions to create, change or destroy instances of a forward declared `struct` (also `class` in C++) ","url":"https://tycholiz.github.io/Digital-Garden/notes/ff7d99d9-a260-44e6-a27d-adfde66d85fb.html","relUrl":"notes/ff7d99d9-a260-44e6-a27d-adfde66d85fb.html"},{"doc":"Int","title":"Int","hpath":"c.lang.types.int","content":"\n# int\ntypically, short is 16 bits (2 bytes), and long 32 bits (4 bytes)\n- therefore, an int is a 32-bit data type\n\t- this depends on the natural size of integers on the host machine\n- in reality, 31 bits are available for the number, since 1 has to be reserved for the sign (+/-)\n- Whenever a number is being assigned to an ‘int’ type variable, it is first converted to its binary representation then it is kept in memory at specific location.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/fc2157bb-6380-4b1a-81b5-431261bda803.html","relUrl":"notes/fc2157bb-6380-4b1a-81b5-431261bda803.html"},{"doc":"Float","title":"Float","hpath":"c.lang.types.float","content":"\n### float/double\n- floating-point types.\n\t- float - single-precision floating point\n\t- double - double-precision floating point\n","url":"https://tycholiz.github.io/Digital-Garden/notes/70675a91-9c06-44a6-94a6-a6b672986a80.html","relUrl":"notes/70675a91-9c06-44a6-94a6-a6b672986a80.html"},{"doc":"Char","title":"Char","hpath":"c.lang.types.char","content":"\n### Char\n- A char holds the ASCII value, rather than the character itself.\n\t- Therefore, by definition a `char` is just a small integer\n\t- We can leverage the fact that a char will evaluate to its ASCII value, by performing math on them\n\t\t- ex. we can test if something is a digit like with `if (val >= '0' && val < '9')`. Here, the `'0'` and `'9'` get converted to their ASCII values as the comparison is made. As it so happens, any ASCII value between 48 and 57 is a digit.\n- a char is a single byte\n","url":"https://tycholiz.github.io/Digital-Garden/notes/bc8bea84-7d19-432e-a736-f59501034459.html","relUrl":"notes/bc8bea84-7d19-432e-a736-f59501034459.html"},{"doc":"Array","title":"Array","hpath":"c.lang.types.array","content":"\n# Array\n- In C, arrays can be thought of as pointers to consecutive areas of memory\n\t- elements in an array are stored in physically adjacent blocks of memory \n\t\t- spec:This is why a pointer pointing to an array only points to the first element— because we can essentially keep checking \"is the next memory block part of that array?\"\n- the name of an array is a synonym for the memory location of the first element of that array. \n\t- therefore, the assignment `p = &x[0]` is identical to `p = x`\n\t- also, what follows from this is that a reference to `x[i]` can also be written as `*(x+i)`\n\t\t- We can understand this as \"`x[5]` is the address of the 5th element beyond `x`\".\n- When an array name is passed to a function, what is passed is the location of the initial element. Within the called function, the argument is a local variable, and so an array name parameter is a pointer (a variable containing an address)\n- the syntax for accessing arrays is identical for that which can be used to dereference pointers\n- `array[i]` is equivalent to `*(array + i)`. \n\t- this fact shows that arrays are pointers to consecutive blocks of memory\n- An array can be initialized with values like so: `int days[] = { 1, 2, 3 }` \n\t- If the size of the array is omitted in initialization, the compiler will count the number of elements \n","url":"https://tycholiz.github.io/Digital-Garden/notes/1231e2e5-1a23-476d-b7d8-db2978776903.html","relUrl":"notes/1231e2e5-1a23-476d-b7d8-db2978776903.html"},{"doc":"Structs","title":"Structs","hpath":"c.lang.structs","content":"\n### Structs\n- A struct is a composite type \n- Stands for “user defined data structure”\nIn C++, it is similar to a class (though by default the access level is public)\n- A struct is similar to an interface, in that we define a shape, which effectively becomes our new composite type. We can then initialize new variables that take this shape\n- The primary use of struct is for the construction of complex datatypes\n- Consider that a composite type such as this is analogous to a record in a database table. In a table, we implicitly create a type that has the same shape of the table  (in this sense the composite type is called a record)\n- All variables contained in a struct are physically located together. The consequence of this is that we can access the struct with a single pointer\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1b8aa29b-575d-4c63-afeb-180b22bf48dc.html","relUrl":"notes/1b8aa29b-575d-4c63-afeb-180b22bf48dc.html"},{"doc":"Statements","title":"Statements","hpath":"c.lang.statements","content":"\n### Statements\n- all statements in C end in semicolon `;`\n- braces `{`/`}` can be used to group declarations and statements together into a *compound statement* (or block)\n\t- this makes them syntactically equivalent to a single statement\n- the fact that `;` denotes the end of a statement rings true for *for loops* too. The first and third are assignments (or fn calls), while the second is a relational expression.\n\t- All are optional, so `for (;;)` is an infinite loop","url":"https://tycholiz.github.io/Digital-Garden/notes/7b5a2d71-1fd2-406c-9f43-6841cbb04322.html","relUrl":"notes/7b5a2d71-1fd2-406c-9f43-6841cbb04322.html"},{"doc":"Functions","title":"Functions","hpath":"c.lang.functions","content":"\n### Functions\n- In C, we must declare a function prototype before calling the function so that the signature is known to the compiler \n\t- this is no different from declaring an int with `int e`\n\t- The prototype looks identical to the function, expect the body is replaced by a semi-colon:\n\n```c\nchar *do_something(char *dest, const char *src);\n```\t\n\nThis is not strictly necessary, but unless we declare it, the compiler is left to guess the signature based on how the function is called, and it is often wrong.\n- if we are passing an argument that should remain constant within the function body, we can prepend `const` to the parameter: `int strlen(const char[]);`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/1c891c14-2910-41c5-85ea-590fed5dd8dc.html","relUrl":"notes/1c891c14-2910-41c5-85ea-590fed5dd8dc.html"},{"doc":"Compiler","title":"Compiler","hpath":"c.compiler","content":"\n### Compiler\n- unlike other languages, the C compiler does not look ahead to hoist definitions. It doesn't look backward or forward, nor does it scan the file multiple times to understand relationships. The compiler only scans forward in the file exactly once. Connecting function calls to function declarations is part of the linker's job, and is only done after the file is compiled down to raw assembly instructions.\n\t- This means that as the compiler scans forward through the file, the very first time the compiler encounters the name of a function, one of two things have to be the case: It either is seeing the function declaration itself, in which case the compiler knows exactly what the function is and what types it takes as arguments and what types it returns — or it's a call to the function, and the compiler has to guess how the function will eventually be declared.\n\n#### Compilation process\nwe can get all intermediate files with `clang -Wall -save-temps filename.c -o filename`\n\n1. Pre-processing\n\t- takes the source file and handles...\n\t\t- Removal of Comments\n\t\t- Expansion of Macros\n\t\t- Expansion of the included files\n\t\t- Conditional compilation\n\t- creates `filename.i`\n\t- At the end of the file, our source code is preserved\n2. Compilation\n\t- takes the `filename.i` file and compiles it, creating assembly level instructions that assembler can understand\n\t- creates `filename.s`\n\t- compilation phase is useful because it provides a common output language for different compilers for different high-level languages\n\t\t- ex. compilers of C and Fortran produce the same assembly language.\n3. Assembly\n\t- converts assembly language to machine level instructions\n\t- creates `filename.o`\n\t\t- produces a *relocatable object file*\n4. Linking\n\t- all the linking of function calls with their definitions are done\n\t- at this phase, we take code that has already been compiled (like `printf`, which comes from the `printf.o` file), and merge it with our own `.o` file.\n\t\t- We can see the effect of this phase by running `size filename.o` and comparing it to `size filename`. We see that `filename` is much larger\n\t\t- produces a *executable object file*, which can be loaded into memory and executed by the system.\n\n#### Different compilers\nIn C, it's possible that one compiler fails while another succeeds.\n- In the gcc compiler, `main()` cannot return `void`, but Turbo C compiler allows this.\n- To ensure we are writing proper C code, we have to look at the C Standard.\n- Therefore, just because C code compiles doesn't mean it is up to C standard.  \n","url":"https://tycholiz.github.io/Digital-Garden/notes/a5595a3c-217e-4f88-b1ca-7d0e6b7e322b.html","relUrl":"notes/a5595a3c-217e-4f88-b1ca-7d0e6b7e322b.html"},{"doc":"Browser","title":"Browser","hpath":"browser","content":"\nEvents are the native way to deal with user input in browser based web applications. Events are not part of the language of JavaScript itself, but they are part of the browser environment that JavaScript runs in","url":"https://tycholiz.github.io/Digital-Garden/notes/f3bcc3d9-7232-41f4-aaaf-86a99b56dc65.html","relUrl":"notes/f3bcc3d9-7232-41f4-aaaf-86a99b56dc65.html"},{"doc":"Bios","title":"Bios","hpath":"bios","content":"\n- Bios/UEFI serves as an interface between a system's OS and the software","url":"https://tycholiz.github.io/Digital-Garden/notes/105fd41a-910d-4b94-8686-98c3933931f2.html","relUrl":"notes/105fd41a-910d-4b94-8686-98c3933931f2.html"},{"doc":"Binary","title":"Binary","hpath":"binary","content":"\n# Bits and Bytes\n- Bit - a zero or 1 \n\t- portmanteau of \"binary digit\"\n- Byte - an atomic unit of digital information. From a memory standpoint, it is the smallest unit that can be stored and read.\n- Though 8 bits make up a byte, there is no reason for that number aside from the benefits gained by raising that number to the power of 2.\n\t- There have been many other instances of varying-bit bytes, such as 10-bit bytes, 4-bit bytes etc.\n- 1 octet equals 1 byte (as long as  the bytes exist in an 8-bit system, which is almost always)\n- by definition, binary numbers ending in 0 are even, and those ending in 1 are odd.\n\n## Bit\nthe number 255 represents 8 bits. \n- Therefore, it is the largest binary number that can be represented by 8 individual bits. \n\t- in binary, it is *11111111*\n\n### Conversion\n- the image below shows how the placement of the bit determines its numerical value on the whole. \n\t- just like in base 10, each placement to the left multiplies the number by 10, in base 2, each placement to the left multiplies the number by 2 \n\t- if we add up the value of each numerical position (128, 64...), we will get 255 \n![](/assets/images/2021-03-09-21-26-04.png)\n- This chart above can be used to convert to/from binary format\n\t- ex. if we want to represent 135 in binary, we just need to add each slot, with the biggest fitting number first. In this case, the 8th slot, the 3rd slot, the 2nd slot and the 1st slot (or, 10000111) \n\n### How bits are read\n- a bit can be stored by any device that can exist in one of 2 possible states\n\t- ex. lightswitch, presence/absense of a hole in a punchcard, thickness of barcode line, presence of a microscopic pit on a CD ROM\n\t\t- therefore, any of these can be represented by 0's and 1's\n\t- in the case of modern computers, that bit is stored by way of: electrical pulse, or no electrical pulse\n\n### Bit field\n- a type of data structure that directly stores bits\n- the bit field is made up of adjacently-located memory locations.\n- think of a bit field as if it were an array of bits","url":"https://tycholiz.github.io/Digital-Garden/notes/234faa4d-432f-43f4-99fb-08ac4c01bf9b.html","relUrl":"notes/234faa4d-432f-43f4-99fb-08ac4c01bf9b.html"},{"doc":"Bitwise","title":"Bitwise","hpath":"binary.bitwise","content":"\n# Bitwise Operators\n*Bitwise operation* - an operation done on a series of bits. Therefore, it is supported directly by the CPU and thus more efficient\n- the nature of bitwise is to treat a value as a series of bits, rather than as a numerical quantity\n- used to manipulate values for comparisons and calculations\n\t- the values must be an array of chars (string), array of ints, or a binary numeral\n- ex. NOT, AND, OR, XOR\n- ex. Say we perform the bitwise operation `4&1`. What happens is that a check is performed on each bit:\n```\n  00000011\n& 00000001\n__________\n  00000001\n```\n\n*Bitmask* (or simply mask) - in the example above (with `&`) we utilized the bitwise operator to make a mask. Basically, what comes after the `&` is the mask, and it specifies which bits from the leftside will \"shine through\".\n- the mask is determined by the bitwise operator (i.e. AND, OR XOR) and the value following it.\n\t- `&` (AND) will extract a subset of bits from the value\n\t- `|` (OR) will set a subset of the bits in the value\n\t```\n\t  00110110\n\t| 11010011\n\t__________\n\t  11110111\t\n\t```\n\t- `^` (XOR) will toggle a subset of the bits in the value\n\t```\n\t  00110110\n\t^ 11010011\n\t__________\n\t  1110001\t\n\t```\n- anal. imagine a mask in Inkscape who's job is to mask anything that isn't skin. The resulting mask would be black wherever there is not skin in the photo. When we apply this mask to the photo, the resulting image would be just skin. \n\n*Shifting* (`<<`/`>>`) - the action of shifting all bits to the left or right. Shifting to the left effectively doubles the value. Conversely, shifting to the right divides the value by 2.\n- when shifting, if there is no bit to take a place, it defaults to 0\n- Declaring the first argument to be unsigned ensures that when it is right-shifted, vacated bits will be filled with zeros, not sign bits\n","url":"https://tycholiz.github.io/Digital-Garden/notes/73e384b6-fbdb-4e62-bcbb-529dcc29b426.html","relUrl":"notes/73e384b6-fbdb-4e62-bcbb-529dcc29b426.html"},{"doc":"Bash","title":"Bash","hpath":"bash","content":"\n# High-level\n- A shell program is typically an executable binary that takes commands that you type and (once you hit return), translates those commands into (ultimately) system calls to the Operating System API.\n\t- a shell language is a language designed to ‘glue’ together other programs.\n- a shell is simply a macro processor that executes commands. \n\t- The term macro processor means functionality where text and symbols are expanded to create larger expressions.\n\nSince other shells are also programs, they can be run from within one another\n\n*\"Although most users think of the shell as an interactive command interpreter, it is really a programming language in which each statement runs a command. Because it must satisfy both the interactive and programming aspects of command execution, it is a strange language, shaped as much by history as by design.\"*\n- GNU specifies the shell as both a programming language AND a command interpreter. The command interpreter part provides the user interface to the GNU utilities, while the programming language part allows these utilities to be combined.\n\n- environment variables exsit in the same environment as the code that is executed\n\n\n### Precedence of Synonyms\n1. Alias\n2. Keyword (ie. syntax of the shell)\n3. Function\n4. Builtin\n\t- commands that are built into the shell. These are executed directly, as opposed to the shell having to load and execute the executable\n\t\t- ex. pwd, cd\n\n* * *\n\ncheck to see if alias exists in shell - `which g`\n\n*interactive* means that input is accepted from the command line, while *non-interactive* means that input is accepted from a file.\n\nif the first word of a given command does not correspond to a built-in shell command, then the shell assumed that it is the name of an executable file.\n\nwhen we run `bash -c`, a script, or other executable in the shell, it becomes a child of the current environment.\n- ex. `bash -c 'echo \"child [$var]\";` will only have access to `$var` if it was exported beforehand from the shell that the command is run in.\n\nIn bash utilities, the `--` signifies the end of command options, after which only positional parameters are accepted.\n- often, `--` can signify the separation from the command options, and the following Regex \n\nuse `<C-r>` to search command history\n\n### Status code 127 \nThink of it as \"Error: the program you tried to use was not found\"\n- it's returned by your shell `/bin/bash` when any given command within your bash script or on bash command line is not found in any of the paths defined by PATH system environment variable.\n- to fix, make sure the command we are using is discoverable through `$PATH`\n\n## UE Resources\n[GNU High-quality documentation](https://www.gnu.org/software/bash/manual/bash.html)","url":"https://tycholiz.github.io/Digital-Garden/notes/a95e8592-84c0-4698-a0d9-07e8e130b322.html","relUrl":"notes/a95e8592-84c0-4698-a0d9-07e8e130b322.html"},{"doc":"Variables","title":"Variables","hpath":"bash.variables","content":"\nvariables should be **snake_case**\n- kebab-case does not work","url":"https://tycholiz.github.io/Digital-Garden/notes/01646f4f-2c65-4f92-8e1a-b974cc1df7d1.html","relUrl":"notes/01646f4f-2c65-4f92-8e1a-b974cc1df7d1.html"},{"doc":"Internal","title":"Internal","hpath":"bash.variables.internal","content":"\n`$?`\n- Exit status of a command, function, or the script itself\n\n`$$`\n- Process ID (PID) of the script itself\n    - The `$$` variable often finds use in scripts to construct \"unique\" temp file names\n\n`$_`\n- Special variable set to final argument of previous command executed.\n\n`$!`\n- PID (process ID) of last job run in background","url":"https://tycholiz.github.io/Digital-Garden/notes/1dab38ed-4e81-43f7-a235-b1246f4a284c.html","relUrl":"notes/1dab38ed-4e81-43f7-a235-b1246f4a284c.html"},{"doc":"Test","title":"Test","hpath":"bash.test","content":"\n[ -e FILE ] - True if FILE exists\n[ -d FILE ] - True if FILE exists and is a directory.\n[ -f FILE ] - True if FILE exists and is a regular file.\n[ -z STRING ] - True if the length if \"STRING\" is zero.\n[ -n STRING ] or [ STRING ] - True if the length of \"STRING\" is non-zero. \n\n```sh\nhttp_status=$(curl ___)\nif [ $http_status != \"200\" ]\n    then\n        echo \"command failed\"\nfi\n```\n\n### Test based on exit status of previously executed command\n- The `?` variable holds the exit status of the previously executed command (the most recently completed foreground process).\n```sh\nif [ $? -eq 0 ]\nthen echo 'That was a good job!'\nfi\n```","url":"https://tycholiz.github.io/Digital-Garden/notes/881923a6-d135-44ea-b273-e392701d1ccd.html","relUrl":"notes/881923a6-d135-44ea-b273-e392701d1ccd.html"},{"doc":"Redirection","title":"Redirection","hpath":"bash.redirection","content":"\n# Redirection\n- the redirect operator `<`/`>` takes the stdout from the preceding command and redirects it to a file\n- implicity, `>` === `1>`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/c8e16fb4-209d-4fd0-84da-d829a8955ac3.html","relUrl":"notes/c8e16fb4-209d-4fd0-84da-d829a8955ac3.html"},{"doc":"Quotes","title":"Quotes","hpath":"bash.quotes","content":"\n# Quotes\n- Single quotes `''` - preserves the literal value of each character within the quotes (don't allow anything to be interpolated inside). Take it exactly as it appears.\n\t- ex. if `$MYVARIABLE` is within the `''`, the string `$MYVARIABLE` will be interpreted.\n- Double quotes `\"\"` - evalutate the functions within, interpolating into the string\n\t- ex. if `$MYVARIABLE` is within the `\"\"`, the value that `$MYVARIABLE` stands for will be interpreted.\n\nnm: double quotes are bigger, so they have more capabilities (the ability to interpret and expand variables)\n\n```sh\n# NO STRING INTERPOLATION\n$ echo '$(echo \"upg\")'\n# $(echo \"upg\")\n\n# js equivalent: \n# console.log(\"$(echo \"upg\")\")\n\n# STRING INTERPOLATION\n$ echo \"$(echo \"upg\")\"\n# upg\n\n# js equivalent\n# `${}`\n```\n- The implication of this is that when we use double quotes, since the inerpolated functions are called when defined, the variable that gets set to the string will only be executed once.\n- ex:\n```sh\nPROMPT=\"$(git_status) $\"\n# defining this variable will cause git_status to run, interpolating the return \n# value of that function. The only way to update this variable, is by sourcing .zshrc\n\nPROMPT='$(git_status) $'\n# here, we literally pass the string '$(git status)' to the shell (?), \n# and let it interpolate (?) on its own, each time it is displayed in the terminal\n```\n\n## Globbing\n- quotes used in a globbing context do not behave as described above. Globs are not expanded when in either single or double quotes","url":"https://tycholiz.github.io/Digital-Garden/notes/b8e3f8c6-f76b-41da-982c-317af838cda2.html","relUrl":"notes/b8e3f8c6-f76b-41da-982c-317af838cda2.html"},{"doc":"Pipeline","title":"Pipeline","hpath":"bash.pipeline","content":"\n# Pipeline\n- a pipe takes stdout from one process and passes it as stdin to another\n\t- spec: if the first program fails, then a `0` will be passed as stdout. (maybe not true)\n- Each command in a pipeline is executed in its own subshell, which is a separate process\n","url":"https://tycholiz.github.io/Digital-Garden/notes/a69786b8-757c-463d-9f1b-1654f6f9bad3.html","relUrl":"notes/a69786b8-757c-463d-9f1b-1654f6f9bad3.html"},{"doc":"Parameters","title":"Parameters","hpath":"bash.parameters","content":"\n# Special Parameters\n- `$1, $2, $3` are the positional parameters.\n- `\"$@\"` is an array-like construct of all positional parameters\n\t- ex. `{$1, $2, $3 ...}`\n- `\"$*\"` is the IFS expansion of all positional parameters\n\t- ex. `$1 $2 $3`\n- `$#` is the number of positional parameters.\n- `$-` current options set for the shell.\n- `$$` pid of the current shell (not subshell).\n- `$_` most recent parameter (or the abs path of the command to start the current shell immediately after startup).\n- `$IFS` is the (input) field separator.\n- `$?` is the most recent foreground pipeline exit status.\n- `$!` is the PID of the most recent background command.\n- `$0` is the name of the shell or shell script.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/8f156901-986e-4ed2-8aa3-725db951eac0.html","relUrl":"notes/8f156901-986e-4ed2-8aa3-725db951eac0.html"},{"doc":"Glob","title":"Glob","hpath":"bash.glob","content":"\n# Glob Patterns\n`*` is not the only globbing primitive. Other globbing primitives are:\n- `?` - matches any single character\n- `[abd]` - matches any character from a, b or d\n- `[a-d]` - matches any character from a, b, c or d\n\n- `.` has no meaning as a glob.\n\n## Globbing vs Regex\n- spec: Globbing is interpreted by the shell, while regex is interpreted by a program (like `rename`)\n- commands surrounded by single-quotes are not interpreted by the shell.","url":"https://tycholiz.github.io/Digital-Garden/notes/497fc196-4ace-4e16-a80b-be8be3adfb0a.html","relUrl":"notes/497fc196-4ace-4e16-a80b-be8be3adfb0a.html"},{"doc":"Expansion","title":"Expansion","hpath":"bash.expansion","content":"\n# History Expansion\n[source](https://www.thegeekstuff.com/2011/08/bash-history-expansion/)\n- History expansion is performed immediately after a complete line is read, before the shell breaks it into words, and is performed on each line individually\n\nDesignators:\n- Event Designators - refer to a particular command in history\n\t- start with `!`\n- Word Designators - refer to a particular word of a history entry\n\t- often gets combined with Event Designators, which are demarcated by `:`\n\t- ex. `!cp:^` finds the most recent `cp` command in history and grabs the 1st argument\n- Modifiers - modify result of the Event/Word Designator\n\nCommands:\n- `!ls` - execute most recent `ls` command\n\t- ex. if I ran `git log`, then `!git`, bash will say \"oh ok, you want the last executed bash command\", and will run `git log`\n- `!?apache` - execute most recent command that contains the keyword `apache`.\n\t- `!%` will refer to the word that was matched by the previous `!?<pattern>` search\n- `!-3` - execute 3rd most recent command\n\t- `!!` === `!-1` (most recent command)\n- `!$` - reuse last part of the most recent command\n\t- ex. `less ~/myfile` then `vim !$` will run `vim ~/myfile`\n- `!*` - reuse all parts of most recent command\n- `^ls^cat^` - modify the pattern `ls` with `cat` in the previous command\n- `!!:s/ls -l/cat/` - replace `ls -l` of previous command with `cat`\n\t- `!!:gs/...` for global substitution\n- `!cp:^` - get 1st arg of the most recent `cp` command\n- `!cp:$` - get last arg of the most recent `cp` command\n- `!cp:2` - get 2nd arg of the most recent `cp` command\n- `!cp:*` - get all args of the most recent `cp` command\n* * *\n`<C-R>` to search through history. We can start to type a command, then by hitting this shortcut, the history will be searched for that string.\n- `d` - show previous 10 directories visited\n    - press the number beside it to go to that directory\n   ","url":"https://tycholiz.github.io/Digital-Garden/notes/44647156-6b58-4154-9e25-d0ba65169a47.html","relUrl":"notes/44647156-6b58-4154-9e25-d0ba65169a47.html"},{"doc":"Environment Variables","title":"Environment Variables","hpath":"bash.environments","content":"\n# Environments\nan environment is just a map of key-value pairs\n- Each command is executed in its own environment, which includes (but not limited to):\n\t1. files that have been sourced\n\t2. current working directory\n\t3. functions defined during execution, or inherited from shell's parent in the environment\n- When a non-builtin command is executed, it is invoked in a separate execution environment.\n\n## Environment Variables\nSince every instance of a shell is a separate process, we have a different set of environment variables in each shell\n- they can be seen by running `env`\n\t- That isn’t all the variables that are set in your shell, though. It’s just the environment variables that are exported to processes that you start in the shell.\n- `compgen -v` allows us to see *all* variables available in shell\n- `export` allows us to add parameters and functions to the environment\n- When a non-builtin command is executed, it is invoked in a separate execution environment.\n\n### Export\n- Exported variables get passed on to child processes, not-exported variables do not.\n\t- When we use `export`, we are adding the variable onto the shell's list of all env variables. This list is exclusive to the shell. When this shell creates a child process, all of these env variables are made available to it.\n\t- This means that if we only need the variable in the current environment, then we don't need to use `export`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/3102838d-feae-4cc4-a185-c7bea1c205b4.html","relUrl":"notes/3102838d-feae-4cc4-a185-c7bea1c205b4.html"},{"doc":"Cmds","title":"Cmds","hpath":"bash.cmds","content":"\n### Line Navigation\n- go to start of line - `<C-a>`\n- go to end of line - `<C-e>`","url":"https://tycholiz.github.io/Digital-Garden/notes/e682e684-a366-4724-bfb9-2f2114dc4565.html","relUrl":"notes/e682e684-a366-4724-bfb9-2f2114dc4565.html"},{"doc":"Azure","title":"Azure","hpath":"azure","content":"\n## Deployment\n- To deploy Functions to the cloud, we need to create 3 different resources: Resource group, storage account, and function app\n\n### Resource Group\n- a logical container for related resources. We would group together resources that we want to manage together as a group.\n- all resources in a RG should be a part of the same lifecycle, meaning we deploy, update, and delete them together.\n- resource groups can be stored in a different location than the resource that is comprises. This is possible because the location of the RG is merely the location of the RGs metadata about its resources.\n- created with command `az group create`\n- ex. `AzureFunctionsQuickstart-rg`\n\n### Storage Account\n- maintains state and other information about your projects\n- created with command `az storage account create`\n\n### Function app\n- provides the environment for executing your function code\n- A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources\n- created with command `az functionapp create`\n\t- When running this command, we also determine the `<APP_NAME>`. This will serve as the default DNS domain for the function app (ie. we invoke the function with an HTTP request to `<APP_NAME>.azurewebsites.net`)\n\n* * *\n\n## Artifacts\n- Artifacts allows us to create/share/manage dependencies (ex. npm packages) in our project (client-read, client-publisher)\n\t- usually we explicitly define each feed that we pull our package from. Artifacts allows us to use a single point of entry for multiple existing feeds.\n- Github Package Manager is analogous to Artifacts\n\nArtifact Feed\n- \"feed\" is an Azure-specific term\n- a feed is a construct that allows us to store, manage and group packages (like npm), and control who we share it with.\n- a feed is like an npm registry, similar to an endpoint that specifies where these packages can be found.\n- a feed gives us access to a collection of packages.\n- a feed is made up of artifacts\n\nBuild Artifacts\n- build artifact is the output of running the azure-pipeline.yml CI file.\n- Build Artifacts are different than Artifacts\n- ex. build artifact is the result of taking some input data, processing it in some way, then stamping it with a commit SHA as well as a build #, letting us track it.\n\nAzure has 2 types of pipelines: build pipeline and release pipeline.\n- the build pipeline is CI (build, test and create artifacts), while release pipeline is CD\n* * *\n\n### Resource Manager\nWhen a user sends a request from any of the Azure tools, APIs, or SDKs, Resource Manager receives the request. Then it:\n1. authenticates and authorizes the request\n2. sends the request to the Azure service\n- since it is a central API that all requests pass through, we can see logs and everything related to these services and the requests they receive.\n- the Resource Manager allows us to declaratively manage resources (as opposed to having to use scripts)\n\n#### Scope\nAzure provides four levels of scope: management groups, subscriptions, resource groups, and resources.\n![](/assets/images/2021-03-08-21-28-23.png)\n- management settings can be applied to any of these levels of scope, and the level we choose will determine how widely the setting is applied (lower levels inherit from higher).\n\n## Terminology\n**Resource**\n- Any manageable item that is available through Azure.\n- ex. VMs, storage accounts, web apps, databases, blob storage, virtual networks, resource groups, management groups etc.\n\n## Questions\n- What would the connection string be a connection to?\n\t- would it be the host of the function?","url":"https://tycholiz.github.io/Digital-Garden/notes/4d5aab64-f4a4-40a0-9155-f28aaf49d38a.html","relUrl":"notes/4d5aab64-f4a4-40a0-9155-f28aaf49d38a.html"},{"doc":"Aws","title":"Aws","hpath":"aws","content":"\n# Lambda\nlike an anonymous function (or a callback) that runs code in response to events.\n- Think of them like event handlers, but for web services, not components within a webpage\n\n# EC2\n- rent a vm on an amazon server\n- elastic refers to the fact that it can scale up/down as needed automatically\n\n# S3\n- blob storage solution\n\n# Fargate\n- serverless compute engine for containers\n- adds about 20% in price, but removes a lot of the admin overhead\n- removes the need to provision and manaage servers\n\t- don't have to worry about scaling, patching, securing, and managing servers\n- Fargate automates how much computing power you need and will scale up/down automatically\n- with Fargate, you only interact with your containers\n![](/assets/images/2021-03-08-21-25-27.png)\n\n# RDS (Relational Database Service)\n- databases shouldn't be run in a container\n- by default will spin up a single instance in a single availability zone\n\t- if we want more redundency, we can add an active backup instance\n- doesn't support downloading postgres extensions\n \n# Amplify\n- toolbox for front-end/mobile development, with some overlap with Firebase\n\t- ex. tools for creating onboarding flow, tools for implementing AI/ML features, tools for auth \n- also includes tools to help implement cloud-based features in the app\n- includes tools to make real-time apps (ex. news feed, chat)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/b965d9fa-c200-4a8d-9323-73b9c2565812.html","relUrl":"notes/b965d9fa-c200-4a8d-9323-73b9c2565812.html"},{"doc":"Ecs","title":"Ecs","hpath":"aws.ecs","content":"\n# ECS (Elastic Container Service)\n- running containers via AWS proprietary orchestrator\n- There are 2 main models for running containers: ec2 and fargate\n- ECS clusters can be run a few different ways:\n\t- run on vms\n\t- managed docker container runtime\n\t- EC2\n\t- Fargate\n- **ECS Control Plane** - The tools to manage ECS\n\t- handles autohealing\n- by default, containers behave like other Linux processes with respect to access to resources like CPU and memory. this means they get access to all of the host's CPU and memory capacity\n  - ecs provides mechanisms through which we can limit this (in the task)\n\n### Task\n**Task definition** - To prepare your application to run on Amazon ECS, you create a task definition, which is a JSON file that describes between 1 and 10 containers\n- Think of it as a blueprint for your application\n- Task definition parameters might be:\n\t- Which containers should be used?\n\t- Which ports should be opened?\n\t- Which data volumes should be used within the containers?\n\t- How are container linked together\n\n- a `task` is an instantiation of a `task definition`\n\t- the basic unit of deployment is a `task`\n- a `task` is a logical construct that models 1+ containers\n\t- therefore, the ECS API operates on tasks rather than individual containers\n- in ECS you run a task, which in turn runs a container\n- each fargate task has its own isolation boundary and doesn't share kernel, cpu resources, memory resources etc\n\n- **ECS Cluster** - a regional grouping of container instances, upon which we can run task requests\n\t- When tasks are run on Fargate, cluster resources are managed for us\n\t- ECS cluster is a regional grouping of one or more container instances on which you can run task requests\n\nECS objects and how they relate\n![](/assets/images/2021-03-08-21-29-20.png)","url":"https://tycholiz.github.io/Digital-Garden/notes/9de1b0f8-ba4e-428c-8b35-f05c5788214b.html","relUrl":"notes/9de1b0f8-ba4e-428c-8b35-f05c5788214b.html"},{"doc":"Auth","title":"Auth","hpath":"auth","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/facc2b01-755a-409f-99f6-57bef2d1501f.html","relUrl":"notes/facc2b01-755a-409f-99f6-57bef2d1501f.html"},{"doc":"Session","title":"Session","hpath":"auth.session","content":"\n# Session\n- When user logs in, the server creates a session for the user. The sessionId is stored on a cookie in the user's browser. That cookie gets sent with each request to the server, where it gets compared against what is in the db.\n- sessions are not an option for mobile apps.\n\n- sessions are stateful (unlike JWTs, which are stateless)\n\t- This means that one of the communicating parties must keep track of (store) information about the current state, and save information about session history.\n- Sessions piggyback off of stateless HTTP requests to make a stateful connection. \n\t- anal: the way TCP on top of IP\n- When there is an active session, the server responds to the request with an ETag (entity tag), which is an id corresponding to a particular \"version\" of data. If the data arrives and it is identical to the ETag, then we know it is the same as the cached version.\n\n## Persistent Session Store\na place to store all session-related data (but persistent so it doesn't get erased when the user closes the tab or app)\n- session store is like the next evolution after cookies. Cookies were being used to remember user's preferences and some searching data about them. When those client-side cookies reached their capacity, session storage starting taking over for that sort of data storage about a user. ","url":"https://tycholiz.github.io/Digital-Garden/notes/21805007-1539-4696-9bf9-bef239bd931e.html","relUrl":"notes/21805007-1539-4696-9bf9-bef239bd931e.html"},{"doc":"Passport","title":"Passport","hpath":"auth.passport","content":"\n## Serializing/Deserializing\n```\npassport.serializeUser(function(user, done) {\n    done(null, user.id); \n   // where is this user.id going? Are we supposed to access this anywhere?\n});\n\n// used to deserialize the user\npassport.deserializeUser(function(id, done) {\n    User.findById(id, function(err, user) {\n        done(err, user);\n    });\n});\n```\n\n- the `user.id` we pass as an arg to `done` (inside `serializeUser`) is saved in the session. We can later use it to retrieve the whole user object (`deseralizeUser`)\n- `serializeUser` determined which data in the `User` object shoould be saved in the session \n\t- the result of `serializeUser` is attached to the session as `req.session.passport.user`\n- the `id` that is passed to `deseralizeUser` is used to find the session object that has already been stored. The key it looks for is the same key provided to the `done` funtion in `serializeUser`\n\t- the fetched object is attached to the request object as `req.user`","url":"https://tycholiz.github.io/Digital-Garden/notes/f7753c66-8844-49c4-92c0-bb8fb9bc4143.html","relUrl":"notes/f7753c66-8844-49c4-92c0-bb8fb9bc4143.html"},{"doc":"Jwt","title":"Jwt","hpath":"auth.jwt","content":"\n### Token (ex. JWT)\n- When user logs in, the server creates a JWT with the secret, and sends it to the client. The client stores the JWT in local storage, and includes that JWT in every request.\n- The biggest difference here is that the user’s state is not stored on the server, as the state is stored inside the token on the client side instead\n\n- the whole point of JWTs is to not require centralised coordination\n\t- this is why there is no `/logout` endpoint to hit. \n- A JWT is just a regular javascript object that is stringified, hashed, and cryptographically signed.\n- Your token is signed with the secret, known only by the server. If someone changes the token on client side, it would fail validation and the server side framework would reject it. Therefore you can trust your token. Of course, the jwtSecret should be a secret only known by your authentication server and resource server.\n- JWTs are agnostic to what form of authentication you are using (ex. email, OAuth etc). Regardless, the response will contain the JWT.\n- You generate the token only if you trust the user who requested it.\n- You trust the token as long as it has not expired and can be verified with the secret.\n- The information in a JWT can be read by anyone, so do not put private information in a JWT. What makes JWTs secure is that unless they were signed by our secret, we can not accept the information inside the JWT as truth.\n- JWTs guarantee that the bearer of the token also owns the data that he is requesting.\n- However JWTs don't guarantee encryption, which is why HTTPS is required. Otherwise, a man in the middle could take that server response (with the jwt) and use it to authenticate itself on your behalf, gaining access to all data.\n- JWTs come with a death sentence— that is, by nature they have an expiry date.\n\t- this value is stored in the `iat` property. This can be thought of as the `date_of_death` property.\n\t- The server determines the lifespan of the JWT, since it controls the expiry date. Therefore, JWTs give a uniform lifespan to all JWTs, but like God, it has control over ending your life prematurely in order to deny access.\n- JWTs are stateless (while sessions are stateful). This fact enables them to be verified on the server, without having to make a database call. This effectively makes them faster than using sessions (since sessions need to be stored)\n\t- In reality, it's likely that the times we need to authenticate ourseleves with the JWT are also times that we need to interact with the database. This makes \"saving trips to the database\" more of a pipe-dream than a reality. At the end of the day, if we need to make a database interaction *and* authenticate ourselves, we are quicker just using a session rather than authenticating with a JWT (due to their size)\n- JWTs are huge. Storing a userid in a cookie is 6 bytes, while storing in the JWT (along with headers+secret) makes it 304 bytes.\n\n![](/assets/images/2021-03-08-16-41-50.png)\n\n- unlike a cookie, a JWT can contain an unlimited amount of data\n\n### Logout\nThere is no `/logout` endpoint to hit, as all we need to do is delete the jwt kept on the client.\n- This means the token is still valid even after you logout. This is why keeping a short expiry date is important\n\n### Analogy\n*\"Pretend I’m blind and hard of hearing. Let’s also pretend that last week you bought me lunch, and now I need your Venmo address to pay you back. If I ask you for your Venmo address in person, and someone else shouts their Venmo address, I might accidentally send them the money I owe you.\nThat’s because I heard someone shout a Venmo address, and I trusted that it was you, even though in this case, it wasn’t.\nJWTs were designed to prevent this sort of thing from happening. JWTs give people an easy way to pass data between each other, while at the same time verifying who created the data in the first place.\nIf I received 1,000,000 different JWTs that contained a Venmo address, I’d easily be able to tell which one actually came from you.*\"\n\n# E Resources\n[Good high level overview. Includes links to other good content](https://blog.logrocket.com/jwt-authentication-best-practices/#:~:text=A%20JWT%20needs%20to%20be,storage%20(or%20session%20storage).)","url":"https://tycholiz.github.io/Digital-Garden/notes/051303a2-2e53-4bae-b7d5-e4e779a59f5a.html","relUrl":"notes/051303a2-2e53-4bae-b7d5-e4e779a59f5a.html"},{"doc":"Signature","title":"Signature","hpath":"auth.jwt.signature","content":"\n### Signature\n- created using the header, the claims AND the secret. Therefore, this unique combination creates a hash, and if something in the claim were to change, then the signature would be different and would no longer match up. \n\t- What this means is that if the 1st and 2nd set of the JWT don't change, than neither will the 3rd (of course assuming the secret remains unchanging)\n\t\t- in practice jwt will always be different between sign ins of the same user, since the `exp` variable will always be different\n![8ded9f2b6239d7464fa7e85badce77c6.png](:/fa5c3e7e2d95477e85f51bed49c6b4d9)","url":"https://tycholiz.github.io/Digital-Garden/notes/4a111bd2-64c4-4fd7-873c-cb841a5ecb15.html","relUrl":"notes/4a111bd2-64c4-4fd7-873c-cb841a5ecb15.html"},{"doc":"Refresh Token","title":"Refresh Token","hpath":"auth.jwt.refresh-token","content":"\n### JWTs together with Refresh tokens\n- A refresh token is a special token that is used to generate additional jwt tokens. This allows you to have short-lived access tokens (JWTs) without having to collect credentials every time one expires. The server sends the client this token alongside the access (JWT) and/or ID tokens as part of a user's initial authentication flow\n\t- The refresh token should be saved in the database in the relevant row of the user's table.\n\t\t- Therefore, we can handle the renewing login with Postgres.\n\t- the refresh token can be sent from the server to the client as an `HTTPOnly` cookie\n- a Refresh token is what allows us to login to a website, close the browser, and still be logged in upon reopening it.\n\t- When a new session starts (reopening the browser tab), the app is able to see that there is no JWT in memory, so it triggers a *silent refresh*\n- Imagine we set a jwt to have a lifetime of 15 minutes. Without the refresh token, this means that the server would send back an http 401: unauthorized every 15 minutes (probably at which point your app will log the user out and display the login screen)\n- A refresh token has 2 properties:\n\t1. It can be used to make an API call (say, `/refresh_token`) to fetch a new JWT token before the previous JWT expires.\n\t2. It can be safely persisted across sessions on the client!","url":"https://tycholiz.github.io/Digital-Garden/notes/683ecdcb-9db2-493f-a939-0d43a17e88ed.html","relUrl":"notes/683ecdcb-9db2-493f-a939-0d43a17e88ed.html"},{"doc":"Jwk","title":"Jwk","hpath":"auth.jwt.jwk","content":"\n### JWK (JSON Web Key)\n- The JWK is a JSON object that contains a well-known public key which can be be used to validate the signature of a signed JWT\n- If the issuer of your JWT used an asymmetric key to sign the JWT, it will likely host a file called a JSON Web Key Set (JWKS). The JWKS is a JSON object that contains the property keys, which in turn holds an array of JWK objects.\n- The service may only use one JWK for validating web tokens, however the JWKS may contain multiple keys if the service rotates signing certificates.\n\t- we can use a service like Auth0 or Okta to store our JWKs, which will be located at an endpoint hosted at their domain. \n\t- Any time your application validates a JWT, it will attempt to retrieve the JWK(S) from the issuer in order to ensure the JWT signature matches the content.\n\nLogin\n1. user enters email/pw combination\n2. email/pw sent to server. The server takes in that password, appends the salt to it, then hashes it and compares it to the hashed pw stored in the database.\n3. if the passwords match, then the server will send back a signed JWT \n\t- the jwt is composed of 3 parts:\n\t\t- header - usually contains type of token and signing algo used:\n\t\t\t```\n\t\t\t{\n\t\t\t  \"alg\": \"HS256\",\n\t\t\t  \"typ\": \"JWT\"\n\t\t\t}\n\t\t\t```\n\t\t- claims (aka payload)\n\t\t\t- this is the data that we can encode into the token\n\t\t- signature\n\nHTTP requests\n1. server receives the request, along with the JWT.\n2. JWT header and claims are combined with the secret (stored on the server) to generate a \"test signature\", to compare against the original signature (also received in the JWT) ","url":"https://tycholiz.github.io/Digital-Garden/notes/b28d04eb-f785-43a8-8f87-a58f250e0ecb.html","relUrl":"notes/b28d04eb-f785-43a8-8f87-a58f250e0ecb.html"},{"doc":"Claim","title":"Claim","hpath":"auth.jwt.claim","content":"\n### Claim\n- the core of a JWT, since they are the data contained in the JWT\n- claims are pieces of information that are \"claimed\" about a subject (most often a user). In other words, they are just properties of an object.\n\t- ex. name, sub, admin\n- the claim refers to the key, not the value\n- usually not encrypted, meaning if we don't use https, this information is potentially compromised.\n- anyone will be able to decode them and to read them, we cannot store any sensitive data in here\n\t- not an issues because of the secret\n- ex:\n```\n{\n\t\"sub\": \"1234567890\",\n\t\"name\": \"John Doe\",\n\t\"admin\": true\n}\n```\n- when the server receives a JWT from an HTTP request's authorization header, like so:\n```\nAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhIjoxLCJiIjoyLCJjIjozfQ.hxhGCCCmGV9nT1slief1WgEsOsfdnlVizNrODxfh1M8\n```\nit will verify the token using the secret, and then will serialize the claims in that token to the database. Therefore, we will be able to access the data in `current_settings` (if using Postgres)\n\n- claims can be used as a means to differentiate users. Imagine we are building a postgres database and decide that all signed-in users will have the group role `user_login`. We can use the claims in the jwt to distinguish users\n- in postgres, a claim can be retrieved with `current_setting('request.jwt.claim.email', true)`\n\n*Registered claim* - recommended pre-defined claims\n\t- ex. **iss** (issuer), **exp** (expiration time), **sub** (subject), **aud** (audience)\n\t\n","url":"https://tycholiz.github.io/Digital-Garden/notes/31d9dd7a-6da1-4312-a8ea-a7817d6955cd.html","relUrl":"notes/31d9dd7a-6da1-4312-a8ea-a7817d6955cd.html"},{"doc":"Cookies","title":"Cookies","hpath":"auth.cookies","content":"\n### httpOnly Cookie\n- a cookie that is only sent in the http requests to the server.\n\t- Therefore, never accessible to the client-side javascript (making it immune to XSS attacks.)","url":"https://tycholiz.github.io/Digital-Garden/notes/f2e725af-be5e-4bde-a753-75aa31274938.html","relUrl":"notes/f2e725af-be5e-4bde-a753-75aa31274938.html"},{"doc":"Arduino","title":"Arduino","hpath":"arduino","content":"\nArduino abstracts the circuit board part of electronics away from us, by giving us a pre-made integrated circuit\n- Arduino allows us to perform logical computations\n    - ex. When I press *this*, *that* happens\n\n### Parts of Arduino\nHardware\n1. a Printed Circuit Board (PCB), with contains the Integrated Circuit\n2. Pin headers, so we can easily connect to other devices\n3. USB port to connect to a computer\n\nSoftware\n1. IDE","url":"https://tycholiz.github.io/Digital-Garden/notes/3445f14b-9f67-412d-9762-dc14ee7c90e4.html","relUrl":"notes/3445f14b-9f67-412d-9762-dc14ee7c90e4.html"},{"doc":"Apollo","title":"Apollo","hpath":"apollo","content":"\n# Overview\nIdea is that all the lower-level networking tasks (HTTP requests etc) as well as storing the data (caching) should be abstracted away and the declaration of data dependencies should be the dominant part.\n- This is precisely what GraphQL client libraries like Relay or Apollo will enable you to do. They provide the abstraction that you need to be able to focus on the important parts of your application rather than having to deal with the repetitive implementation of infrastructure. \n\nApollo is completely separate from the view-layer, making it framework agnostic.\n\nApollo allows us to send queries from the view-layer\n\n### As state management\n[high-level overview](https://www.apollographql.com/blog/dispatch-this-using-apollo-client-3-as-a-state-management-solution/)\n\n### vs Relay\nRelay is more opinionated, Apollo is unopinionated","url":"https://tycholiz.github.io/Digital-Garden/notes/f29f31f3-7235-41ad-83a1-b64f3f466916.html","relUrl":"notes/f29f31f3-7235-41ad-83a1-b64f3f466916.html"},{"doc":"Links","title":"Links","hpath":"apollo.links","content":"\n# Apollo Link\n- a system of modular components for GraphQL networking\n\t- Each link represents a subset of functionality that can be composed with other links to create complex control flows of data.\n\t- In a few words, Apollo Links are chainable \"units\" that you can snap together to define how each GraphQL request is handled by your GraphQL client.\n- When you fire a GraphQL request, each Link's functionality is applied one after another, allowing you to control the request lifecycle in a way that makes sense for your application\n\t- ex. Links can provide retrying, polling, and batching\n- At a basic level, a link is a function that takes an operation and returns an observable\n\t- an operation is an object with information like: query, variables, context\n- links are required when creating an Apollo client instance.\n- the core of a link is the `request` method, which accepts the `operation` (ex. query, mutation) as an argument\n\t- this method is called every time execute is run on that link chain (typically each time an operation is passed through the link chain)\n\n## HTTP Link\n- most common Apollo Link\n- The http link is a terminating link that fetches GraphQL results from a GraphQL endpoint using http\n\t- This can be used for authentication, persisted queries, dynamic uris, and other granular updates.\n\n## State link\n- allows us to use graphql query operations on client state. \n\n## Auth link\n- allows us to add an authorization header to each request","url":"https://tycholiz.github.io/Digital-Garden/notes/ca9a2340-806b-463d-8cb0-5d2417d44db7.html","relUrl":"notes/ca9a2340-806b-463d-8cb0-5d2417d44db7.html"},{"doc":"Hooks","title":"Hooks","hpath":"apollo.hooks","content":"\n# Query & Mutation\n- `useQuery` will execute on component mount, giving us back the loading, error and data state. Therefore, it is declarative \n- `useLazyQuery` will execute the query on command. This is perfect to use in events other than component mount, like on button click \n\t- To be clear, you could put this in `useEffect` and it would operate similarly to `useQuery`, since `useEffect` gets run on component mount\n- `useMutation` will return a function that we can execute to perform the mutation \n\n### Errors\nwe need to stringify errors:\n`console.log('error', JSON.stringify(err, null, 2))`\nspec: alternatively, we can use `apollo-link-error`\n\n","url":"https://tycholiz.github.io/Digital-Garden/notes/04c94ebd-55e5-4170-9bd2-7660d1d831f7.html","relUrl":"notes/04c94ebd-55e5-4170-9bd2-7660d1d831f7.html"},{"doc":"Client","title":"Client","hpath":"apollo.client","content":"\n","url":"https://tycholiz.github.io/Digital-Garden/notes/ea472414-ba4f-4e48-b784-810034a1ab8f.html","relUrl":"notes/ea472414-ba4f-4e48-b784-810034a1ab8f.html"},{"doc":"Core","title":"Core","hpath":"apollo.client.core","content":"\nthe core part of apollo that is framework-agnostic. In other owrds it doesn't care about react and has no concept of hooks or anytihng like that","url":"https://tycholiz.github.io/Digital-Garden/notes/66279c59-b631-4365-a3d8-42f2625cd6cc.html","relUrl":"notes/66279c59-b631-4365-a3d8-42f2625cd6cc.html"},{"doc":"Cache","title":"Cache","hpath":"apollo.cache","content":"\n# Cache\n- Apollo caches the result of every graphql query in a normalized cache\n- when a single resource is updated with a mutation, Apollo handles caching for us (as long as we return the `id` and fields that were updated). When we create or delete, or if we update many at once, we will have to manually take care of caching.\n\t- `useMutation` has an `update` function, whose purpose is to modify your cached data to match the modifications that a mutation makes to your back-end data\n- we can interact directly with the cache with `readQuery, writeQuery, readFragment, writeFragment`, accessible from `ApolloClient` class\n- we can get the cached version of an entity with `cache.identify`\n\t- an input `{ id: 1, title: '', mediaItems: [{...}] }` gives us `Nugget:1`\n\nreadQuery\n- like a regular graphql query, only it is performed on the cache, rather than the GraphQL API\n\nwriteQuery\n- we can use this function to update the apollo cache \n\nWe can search only data that is in the cache (ie no API call made) by using the `@client` decorator\n\n## Normalization\n- The `InMemoryCache` normalizes query response objects before it saves them to its internal data store.\n- normalization happens in the following steps:\n\t1. The cache generates a unique ID for every identifiable object included in the response. \n\t\t- ID will be in format: `<__typename>:<id>` (ex. `Bucket:232`)\n\t2. All the objects are stored by that generated ID in a flat lookup table.\n\t3. Whenever an incoming object is stored with the same ID as an existing object, the fields of those objects are merged.\n\t\t- this means that the only time anything is overwritten is when the field names are the same. If the incoming object has different fieldnames than the existing one, they will be preserved \n- The apollo cache is an object where each key is the ID (`typename:id`), and the value is an object of the corresponding record from the database:\n```\ndata: {\n\t'Bucket:1': {\n\t\tid: 1,\n\t\ttitle: 'Psychology',\n\t\t__typename: 'Bucket'\n\t}\n}\n```\n\n### Updating the cache\n- we can pass an `update` method to `useMutation`\n\t- this method allows us to interact with the cache as if we were interacting with a graphql API. \n\t\t- ex. we can make general queries, as well as use fragments to help\n- `cache.modify` is a method we can execute on our cache that lets us modify individual fields directly \n\t- differs from `writeQuery` and `writeFragment` in that it will circumvent any `merge` function we have defined, meaning that fields are always overwritten with exactly the values you specify.\n\n### Type Policies\n- By defining type policies, we can determine how the cache interacts with specific types in the schema\n\t- done by mapping a `__typename` to the whole `TypePolicy` object.\n\t- in other words, the `typePolicies` object has `key`-`values` of `__typename`-`TypePolicy Object`\n- each field in a `typePolicies` object is a type's `__typename`\n- we can customize how a particular field within our Apollo cache is written to and read. For this, we have 2 methods: `merge` and `read`.\n\t- with `read`, the cache calls that function whenever your client queries for the field. In the query response, the field is populated with the read function’s return value, instead of the field’s cached value.\n\t\t- Read is useful for manipulating values when they’re read from the cache, for example, things like formatting strings, dates, etc.\n\t- with `merge`, the cache calls that function whenever the field is about to be written with an incoming value. When the write occurs, the field’s new value is set to the merge function’s return value, instead of the original incoming value.\n\t\t- Merge can take incoming data and manipulate it before merging it with existing data. Suppose you want to merge arrays or non-normalized objects.\n\t- to define the policy for a single field, we need to first know which TypePolicy object the field corresponds to.\n- `FieldPolicy` lets us customize how individual fields in the Apollo Client cache are read and written.\n","url":"https://tycholiz.github.io/Digital-Garden/notes/aba603c3-d3b8-4d25-86c7-167eb488c57a.html","relUrl":"notes/aba603c3-d3b8-4d25-86c7-167eb488c57a.html"},{"doc":"API","title":"API","hpath":"api","content":"\nAn api is really an interesting thing, because it allows you to do the work once, and have its effect (result) be replicated ad infinitum. Consider the way the cursor works in WoW. The cursor points over something, and somehow it knows what it is hovering over. Well, For a mouse to able to locate something in that sense, it has to have the coordinates of the item (ex. a chest). If the coordinates that you are hovering over match the coordinates of the chest, then you know you are hovering over the chest. The function to determine this is complex, but it is at the end of the day reducible to a function. We can still walk away with an atomic piece that can be transported anywhere. So imagine we take the functionality of determining where the coorinate of the cursor is and wrap it up into a function. Now all we need to do is run that function every time the mouse moves. We have just created an API that easily replicable across the entire application. \n\n# UE Resources\n[difference between API and SDK](https://nordicapis.com/what-is-the-difference-between-an-api-and-an-sdk/#:~:text=By%20definition%2C%20an%20SDK%20is,to%20allow%20communication%20between%20applications.)\n","url":"https://tycholiz.github.io/Digital-Garden/notes/610b951b-7363-471f-acd5-210710b9d104.html","relUrl":"notes/610b951b-7363-471f-acd5-210710b9d104.html"},{"doc":"Apache","title":"Apache","hpath":"apache","content":"\nApache is an HTTP server (just like NGINX)\n\n# How it works\n- Apache listens to the IP addresses identified in its config file (HTTPd.conf). Whenever it receives a request, it analyzes the headers, and takes action; considering the rules specified for it in the Config file,\n\n## Four main directories on apache server\n- `htdocs` contains the files to be served to the client upon receiving HTTP requests\n\t- files and sub-directories under htdocs are available to the public \n- `conf` contains all server configuration files \n\t- similar to Dockerfile, it is a manifesto of instructions for the apache server\n- `logs` contains server logs\n- `cgi-bin` contains CGI scripts\n\n# E Resources\n[primer on apache](https://code.tutsplus.com/tutorials/an-introduction-to-apache--net-25786)","url":"https://tycholiz.github.io/Digital-Garden/notes/4586ac41-3af7-4176-aaf0-440ad1773bc9.html","relUrl":"notes/4586ac41-3af7-4176-aaf0-440ad1773bc9.html"},{"doc":"Android","title":"Android","hpath":"android","content":"\n### Hermes\n- a engine for optimizing RN apps on Android","url":"https://tycholiz.github.io/Digital-Garden/notes/a19c5ab0-fd8c-4bd0-9aed-da5abefabc45.html","relUrl":"notes/a19c5ab0-fd8c-4bd0-9aed-da5abefabc45.html"},{"doc":"Cmds","title":"Cmds","hpath":"android.cmds","content":"\n# Android\n- open developer menu - `cmd+m`\n","url":"https://tycholiz.github.io/Digital-Garden/notes/6bdd733e-5a00-4373-b8ff-f0f0a5492ba6.html","relUrl":"notes/6bdd733e-5a00-4373-b8ff-f0f0a5492ba6.html"},{"doc":"Adb","title":"Adb","hpath":"android.adb","content":"\n# Adb reverse and port forwarding\n`adb -s 192.168.76.101:5555 reverse tcp:8081 tcp:8081`\nIn either case, it's basically just port forwarding. It's probably called \"reverse\" because it's actually setting up a \"reverse proxy\" in which a http server running on your phone accepts connections on a port and wires them to your computer... Or vice versa. The port is specified twice because you have the ability to control the listening port on both sides of the proxy.\n\nWhen the RN packager is running, there is an active web server accessible in your browser at 127.0.0.1:8081. It's from this server that the JS bundle for your application is served and refreshed as you make changes. Without the reverse proxy, your phone wouldn't be able to connect to that address.\n\nNow when your phone tries to access http://localhost:3000/ your request will be routed to localhost:3000 of your laptop. Just recompile your app to use localhost:3000 as the API endpoint.\n\nif we had written `adb reverse tcp:80 tcp:3000`, it would redirect your phone’s port 80 to your computer’s port 3000","url":"https://tycholiz.github.io/Digital-Garden/notes/5f92dc11-32af-4625-8d8f-d682555cb2ee.html","relUrl":"notes/5f92dc11-32af-4625-8d8f-d682555cb2ee.html"},{"doc":"CGI","title":"CGI","hpath":"CGI","content":"\n# Common Gateway Interface (CGI)\n- a specification for web servers (like Nginx, Apache) to execute command line programs that run on a server and generate web pages dynamically.\n\t- Such programs are known as CGI scripts or simply as CGIs\n\t- Normally, a CGI script executes at the time a request is made and generates HTML.\n- ex. an HTTP GET or POST request from the client may send HTML form data to the CGI program (via STDIN). Simultaneously, the CGI program will receive other data like URL path and HTTP headers as a list of environment variables. \n\nPurpose\n- The HTTP server (ex. Express) will have a directory that is designated as a document collection (ie. each file in the directory is a document).\n\t- These files can be sent to users accessing the website. \n- ex. if our website was www.example.com and we had a document collection stored at `/usr/local/apache/htdocs` of the local filesystem, the Web server will respond to a request for `http://example.com/index.html` by sending to the browser the (pre-written) file `/usr/local/apache/htdocs/index.html`\n\n## FastCGI\n- this is an alternative approach and variation of CGI.\n- it is a protocol for interfacing interactive programs with a web server\n- purpose is to reduce overhead by allowing a single, long-running process to handle more than one user request\n\t- result is that the server can handle more web page requests per unit of time.\n- FastCGI applications remain independent of the web server.\n- FastCGI can be implemented in any language that supports network sockets\n\t- ex. Node, PHP, Python\n- both Nginx and Apache implement FastCGI","url":"https://tycholiz.github.io/Digital-Garden/notes/723b0cf7-a5f3-4c71-a811-2c782faa4cc4.html","relUrl":"notes/723b0cf7-a5f3-4c71-a811-2c782faa4cc4.html"},{"doc":"Changelog","title":"Changelog","hpath":"root.changelog","content":"","url":"https://tycholiz.github.io/Digital-Garden/notes/changelog.html","relUrl":"notes/changelog.html"}]
