<h1 id="pytorch">Pytorch<a aria-hidden="true" class="anchor-heading icon-link" href="#pytorch"></a></h1>
<h2 id="data-primitives">Data Primitives<a aria-hidden="true" class="anchor-heading icon-link" href="#data-primitives"></a></h2>
<p>PyTorch has two primitives to work with data: <code>torch.utils.data.DataLoader</code> and <code>torch.utils.data.Dataset</code></p>
<h3 id="dataset">Dataset<a aria-hidden="true" class="anchor-heading icon-link" href="#dataset"></a></h3>
<p><code>Dataset</code> represents a map between key (label) and sample (features) pairs of your data.</p>
<ul>
<li>ex. images and their associated labels</li>
</ul>
<h3 id="dataloader">DataLoader<a aria-hidden="true" class="anchor-heading icon-link" href="#dataloader"></a></h3>
<p><code>DataLoader</code> wraps an iterable around the <code>Dataset</code></p>
<ul>
<li>this is accomplished by passing <code>Dataset</code> as an arg to <code>DataLoader</code>, giving us automatic batching, sampling, shuffling and multiprocess data loading.</li>
</ul>
<h2 id="domain-specific-libraries">Domain-specific Libraries<a aria-hidden="true" class="anchor-heading icon-link" href="#domain-specific-libraries"></a></h2>
<ul>
<li>TorchText </li>
<li>TorchVision</li>
<li>TorchAudio</li>
<li>TorchRec - Recommendation Systems</li>
</ul>
<h3 id="torchvision">TorchVision<a aria-hidden="true" class="anchor-heading icon-link" href="#torchvision"></a></h3>
<p>Every TorchVision Dataset includes two arguments: <code>transform</code> and <code>target_transform</code> to modify the samples and labels respectively.</p>
<hr>
<h2 id="reproducibility">Reproducibility<a aria-hidden="true" class="anchor-heading icon-link" href="#reproducibility"></a></h2>
<p>Generating random numbers is an <a href="/notes/k4ib1hhlkmzcrfx8vljz7sg#how-a-neural-network-learns">essential aspect</a> to a deep learning model. However, this presents a problem: if we are generating random numbers, how can we reproduce the same results across different executions of the code and across different machines, given the same data and parameters?</p>
<p>There are a couple of things we can do to limit the amount of nondeterministic behaviour from Pytorch:</p>
<ol>
<li>control sources of randomness that can cause multiple executions of your application to behave differently</li>
<li>configure PyTorch to avoid using nondeterministic algorithms for some operations. As a result, multiple calls to those operations, given the same inputs, will produce the same result.
<ul>
<li>this involves specifying the seed so that the <a href="/notes/5jswj4f1an4wt4970pq66tv">PRNG</a> will produce the same sequence of "random" numbers, even run across different machines.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="torch-hub--torchvisionmodels">Torch Hub / <code>torchvision.models</code><a aria-hidden="true" class="anchor-heading icon-link" href="#torch-hub--torchvisionmodels"></a></h3>
<p>Allows us to access many pre-built deep learning models (allowing us to leverage <a href="/notes/03o3n0hz9v9jtb7j889zpd4#transfer-learning">transfer learning</a>)</p>
<h2 id="resources">Resources<a aria-hidden="true" class="anchor-heading icon-link" href="#resources"></a></h2>
<ul>
<li><a href="https://pytorch.org/tutorials/beginner/ptcheat.html">Pytorch Cheatsheet</a></li>
</ul>
<h2 id="learning-resources">Learning Resources<a aria-hidden="true" class="anchor-heading icon-link" href="#learning-resources"></a></h2>
<ul>
<li><a href="https://www.dataquest.io/blog/pytorch-for-beginners/">https://www.dataquest.io/blog/pytorch-for-beginners/</a></li>
<li><a href="https://youtu.be/Z_ikDlimN6A?si=FX6o8eF3Xh6fbqnA&#x26;t=23078">Learn PyTorch for deep learning in a day. Literally</a>
<ul>
<li><a href="https://www.learnpytorch.io/">Accompanying notes</a></li>
</ul>
</li>
</ul>
<hr>
<strong>Children</strong>
<ol>
<li><a href="/notes/elq5ymv90d7r0hzxv8kwmbr">Ap</a></li>
<li><a href="/notes/8qm42jxtuar5qn15w7nphwy">Tensors</a></li>
</ol>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/v3rh8rqc5aorupfno1w4otw">Functions</a></li>
</ul>