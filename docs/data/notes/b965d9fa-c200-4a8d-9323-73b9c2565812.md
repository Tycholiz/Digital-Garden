
# Lambda
like an anonymous function (or a callback) that runs code in response to events.
- Think of them like event handlers, but for web services, not components within a webpage

Effectively, everything is abstracted away aside from a function interface.

If EC2 is a complete computer in the cloud, Lambda is a code runner in the cloud.
- With EC2 you get an OS, a file system, access to the server’s hardware, etc. 
- With Lambda, you just upload some code and Amazon runs it for you.

### When to use
Lambda is most suitable for small snippets of code that rarely change.
- think of Lambda functions as part of the infrastructure rather than part of the application

Lambda can be used as a plugin system for other AWS services, for example:
- S3 doesn’t come with an API to resize an image after uploading it to a bucket, but with Lambda, you can add that capability to S3.

### Limitations
- Your functions will suffer a cold start when a function is invoked after a period of inactivity
- limit of 250 MB for your code bundle, including all your dependencies

# EC2 (Elastic Compute Cloud)
An instance of EC2 is a VM on an AWS server
The nice thing about EC2 is that the computer you get will be very similar to the computer you use to develop your software. If you can run your software on your computer, you can almost certainly run it on EC2 without any changes.
- This is one of EC2’s main advantages compared to other types of compute platforms (such as Lambda): you don’t have to adapt your application to your host.

Elastic refers to the fact that it can scale up/down as needed automatically

EC2 has dozens of options you will probably never need, and thus it comes with sensible defaults.
-  This is the result of the highly varied workloads and use cases serviced by EC2

Selecting an instance type will be the most consequential decision when provisioning an EC2 instance.
- There are 256+, and can be narrowed to a few categories, defined by what they're optimized for:
	- CPU
	- Memory
	- Network
	- Storage

If you were building your own server, there would be an infinite number of ways to configure it, but with EC2 you get to pick an instance type from its catalog.
- This is the tradeoff you make as opposed to getting you build your own server, but most likely it shouldn't even be a concern.

### Security
the defaults are sensible, but we may have to modify:
- The security group
	- You can think of security groups as individual firewalls for your instances. With security groups, you can control what goes in and out of your instances.
- The VPC ACL.
	- You can think of VPC ACL as a network firewall. With the VPC ACL, you can control what goes in and out of your network.

### Scaling
The auto part of Auto Scaling allows us to automatically add/remove EC2 instances (therefore horizontal scaling). In theory it works, but is often impractical.
The main premise of Auto Scaling is that once you decide how much headroom you want, you’ll be able to make that headroom a constant size

Unless the following 2 are true, then it's probably not worth messing around with auto-scaling:
- Are your EC2 costs high enough that any reduction in usage will be materially significant?
- If your EC2 bill were to go down by 30%—would that be a big deal for your business?

#### How many EC2 instances to run?
Capacity Headroom - we need to have a buffer between the expected peak demand and the maximum capacity that our system can handle.

Pricing
- only pay for the number of seconds your instance is running.

# ELB (Elastic Load Balancing)
Comes in 3 variants:
- Classic 
	- Legacy software.
- Application (ALB)
	- a proper reverse proxy that sits between the internet and your application
	- Every request to your application gets handled by the load balancer first. The load balancer then makes another request to your application and finally forwards the response from your application to the caller.
- Network (NLB)
	- behave like load balancers, but they work by routing network packets rather than by proxying HTTP requests

# S3 (Simple Storage Service)
Fundamentally, you can think of S3 as a highly-durable hash table in the cloud. 
- The key can be any string, and 
- the value any blob/object of data up to 5 TB.

Object storage is used for purposes such as storing photos on Facebook, songs on Spotify, or files in online collaboration services, such as Dropbox.

Data gets streamed at a rate of around 90 MB/s.

You can have as many parallel uploads and downloads as you want, thus, the infinite bandwidth.

S3 offers built-in redundancy.

At first you can start with the default storage class and ignore all the other classes. Don't bother with the implications of the different storage classes until you really need to start saving money from S3 storage costs.

### Limitations
- cannot append to objects.
- It doesn’t support HTTPS when used as a static website host, so you cannot host static websites

- Alternatives: Azure Blob

Pricing
$23.55/TB/month
$5/million uploads and $0.40/million downloads

# Fargate
- serverless compute engine for containers
- adds about 20% in price, but removes a lot of the admin overhead
- removes the need to provision and manage servers
	- don't have to worry about scaling, patching, securing, and managing servers
- Fargate automates how much computing power you need and will scale up/down automatically
- with Fargate, you only interact with your containers
![](/assets/images/2021-03-08-21-25-27.png)

# RDS (Relational Database Service)
- databases shouldn't be run in a container
- by default will spin up a single instance in a single availability zone
	- if we want more redundancy, we can add an active backup instance
- doesn't support downloading postgres extensions
 
# Amplify
- toolbox for front-end/mobile development, with some overlap with Firebase
	- ex. tools for creating onboarding flow, tools for implementing AI/ML features, tools for auth 
- also includes tools to help implement cloud-based features in the app
- includes tools to make real-time apps (ex. news feed, chat)

# Elastic Beanstalk
Purpose is to deploy + scale web apps/services built with common web languages (Javascript, PHP etc) onto webservers like Nginx/Apache
- Elastic beanstalk hosts all the services needed for deployment/scaling

Handles things like:
- Provisioning services
- loadbalancing
- scaling infra
- updating platform with latest patches
- app health monitoring

Also allows us to reimplement portions of the code as containerized services, allowing us to achieve a more [[distributed|deploy.distributed]] architectural design

Allows to retain control over the resources powering your application, so you can decide on how much you want to manage.

Alternatives: Azure App Service, Google Cloud App Engine

### Process
1. upload source code bundle to Elastic Beanstalk console
2. Elastic Beanstalk return a new URL for the webapp

# DynamoDB
Data is stored as a partitioned B-Tree.
Unlike Redis, in that it is immediately consistent and highly-durable, centered around that single data structure.
- If you put something into DynamoDB, you’ll be able to read it back immediately and, for all practical purposes, you can assume that what you have put will never get lost.

DynamoDB data is schemaless. The DB engine can manage structured or semi-structured data, including JSON documents.
Fully managed.
supports key–value and document data structures
The general rule of thumb is to choose Dynamo for low throughput apps as writes are expensive and consistent reads are twice the cost of eventually consistent reads
High vendor lock-in.
provides seamless integration with services such as Redshift (large scale data analysis), Cognito (identity pools), Elastic Map Reduce (EMR), Data Pipeline, Kinesis, and S3. Also, has tight integration with AWS lambda via Streams and aligns with the server-less philosophy; automatic scaling according to your application load, pay-per-what-you-use pricing, easy to get started with, and no servers to manage. 

When to use DynamoDB?
- In case you are looking for a database that can handle simple key-value queries but those queries are very large in number
- In case you are working with OLTP workload like online ticket booking or banking where the data needs to be highly consistent

When not to use DynamoDB?
- In cases where you have to do computations on the data. 
	- Relational databases run their queries close to the data, so if you’re trying to calculate the sum total value of orders per customer, then that rollup gets done while reading the data, and only the final summary (one row per customer) gets sent over the network. However, if you were to do this with DynamoDB, you’d have to get all the customer orders (one row per order), which involves a lot more data over the network, and then you have to do the rollup in your application, which is far away from the data.

Pricing
$256/TB/month

By default, you should start with DynamoDB’s on-demand pricing and only consider the provisioned capacity as cost optimization. On-demand costs $1.25 per million writes, and $0.25 per million reads.
- Then, if your usage grows significantly, you will almost always want to consider moving to provisioned capacity (significant cost savings).
- if you believe that on-demand pricing is too expensive, then DynamoDB will very likely be too expensive, even with provisioned capacity. In that case, you might want to consider a relational database.

# CloudFormation
When working in AWS, you almost always want to use some CloudFormation (or a similar tool).
lets you create and update the things you have in AWS without having to click around on the console or write fragile scripts.
- for instance, gives us the ability to tear down everything cleanly and recreate your AWS set up in one click

rule of thumb is to let CloudFormation deal with all the AWS things that are either static or change very rarely, like load balancers, deployment pipelines, VPC configs, security groups

### Overview
- define your AWS resources as a YAML script
- point CloudFormation to your AWS account, and it creates all the resources you defined idempotently.
- Updates can be rolled back.

# SQS (Simple Queue Service)
highly-durable queue in the cloud
- put messages on one end, and a consumer takes them out from the other side.
Messages are consumed *almost* in FIFO, but there is no strictness to adhere to this.
- Strictness *can* be guaranteed, but there is a performance cost.

SQS requires zero capacity management.
- no limit on the rate of messages enqueued or consumed
- don’t have to worry about any throttling limits.
- number of messages stored in SQS (the backlog size) is also unlimited.

great default choice for dispatching asynchronous work.

# Kinesis
A highly-durable linked list in the cloud

Use-cases are similar to SQS—you would typically use either Kinesis or SQS when you want to enqueue records for asynchronous processing.

Difference with SQS:
- SQS can only have one consumer, while Kinesis can have many.
- Once an SQS message gets consumed, it gets deleted from the queue. But Kinesis records get added to a list in a stable order, and any number of consumers can read a copy of the stream by keeping a cursor over this never-ending list.
- Multiple consumers don’t affect each other, and if one falls behind, it doesn’t slow down the other consumers.
- Whenever consumers read data out of Kinesis, they will always get their records in the same order.
- Often cheaper than SQS
- Kinesis can carry a significant operational burden with the need to provision capacity (shards).

# CloudFront
CloudFront is a CDN in AWS which operates at Edge Locations around the world closer to the users. It can cache the static assets and deliver them to the end-users quite fast.

Lambda Edge functions run here