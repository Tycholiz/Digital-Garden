{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title"},{"path":["body"],"id":"body","weight":1,"src":"body"}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Root","n":1},"1":{"v":"\n# Digital Garden\n\nThis Wiki of Personal Information is organized according to domains and their\nsub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git, would be `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner` etc.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the \"aha\" moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nThe ability I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course, this is all\nvague sounding, and really depends on the domain, along with the ends I am\ntrying to reach.\n\nTo achieve this, the system should be steadfast to:\n- be able to put information in relatively easily, without too much thought\n\trequired to make that determination.\n- be able to extract the information that we need, meaning there is a\n\thigh-degree in confidence in the location of the information. The idea is\n\tthat information loses a large amount of its value when it is unfindable.\n\tTherefore, a relatively strict ideology should be used when determining\n\twhere a piece of information belongs.\n","n":0.059}}},{"i":2,"$":{"0":{"v":"Yarn","n":1},"1":{"v":"\n### yarn.lock\n- The purpose of a lock file is to lock down the versions of the dependencies specified in a package.json file\n\t- This means that in a `yarn.lock` file, there is an identifier (ie. exact version specified) for every dependency and sub-dependency that is used for a project\n\t\t- sub-dependencies are the dependencies of a dependency\n- The equivalent of `yarn.lock` for npm is `package-lock.json`. If using both npm and yarn, we need both of them, and they need to remain in sync (use yarn's import directive to accomplish this)\n- if we didn't have a `yarn.lock`, then if a co-worker cloned our repo and ran `yarn install`, they may get different versions of a dependency, since `package.json` can specify version ranges. \n\t- Instead, since `yarn.lock` is checked into version control, when the co-worker clones the repo and runs `yarn install`, `yarn.lock` will be checked and the version specified will be installed.\n- critical to have if working on a team or if working alone with a CI server.\n- `yarn.lock` gets updated any time a dependency is added, removed or modified\n\t- If we want to ensure `yarn.lock` is not updated, use `--frozen-lockfile`\n\t\t- The difference between `--frozen-lockfile` and `--pure-lockfile` is that the former will fail if an update is needed \n- In a perfect world, yarn.lock is unnecessary, because the point of semver is that unless the major version changes, the upgraded package will still work. In other words, if the version in package.json is listed as ^16.0.1, then `yarn install` is free to go to the latest minor version, which doesn't matter since semver defines that as fully backwards compatible.\n\t- however, in the real world not everyone follows semver best practices, and sometimes it is just mistakes which ruin backward compatibility \n\n#### Upgrading packages\n- if we have a dependency version in `package.json` specified at `^3.9.1`, this means that any version between 3.9.1 and 4.0.0 will be acceptable. Of course, since we have a lockfile, upgrades will not automatically happen.\n- `yarn upgrade` allows us to upgrade all dependencies in `package.json`. If we use the `^` specifier, then the latest version within the range will be added. This will be reflected in `yarn.lock`\n- we can ignore the version range by passing the `--latest` flag.\n\t- This modifies both `yarn.lock` and `package.json`\n- We can see all packages that can be upgraded with `yarn upgrade-interactive --latest`\n\n### Link\n- `yarn link` allows us to create symlinks to local projects, from within the project (with package.json) we are currently in.\n- ex. if we have a `rn-client` project and a `components` project, and we want to use `components` within `rn-client`, we can do the following:\n\t1. go to `components` project and run `yarn link`\n\t2. go to `rn-client` project and run `yarn link components` (name field of package.json)\n\t\t- this creates a symlink at `rn-client/node_modules/components`\n\t4. from `rn-client` project, `import components from 'components'` \n- It is meant for development-only purposes\n- spec: think of `yarn link` as exporting the package, and `yarn link <package>` as importing it.","n":0.045}}},{"i":3,"$":{"0":{"v":"Workspaces","n":1},"1":{"v":"\nWorkspaces allow us to install dependencies from multiple package.json files at once.\n- Yarn can also create symlinks between workspaces that depend on each other, ensuring directories are correct and consistent\n- When there are repeated dependencies in the `node_modules` of sub-modules, yarn workspaces will pull up (hoist) the common dependencies to live in the root-level `node_modules`.\n\t- This is why we don't need to run `lerna bootstrap --hoist` when using yarn + lerna — yarn already hoists.\n- if package1 depends on package2, then a symlink to package2 will also be created in the root `node_modules`. This allows package2 to `require` in the package, and take advantage of node's recursive resolver to find the package.\n- workspaces uses a single yarn.lock file at the root.\n- any module (either your own code or the code of a dependency) that contains native code (ie. Swift, Java etc) must not be hoisted.\n\n## How to implement\n1. Add a `workspaces` key to the root `package.json`:\n- `@app` holds all the modules\n```json\n\"workspaces\": {\n\t\"packages\": [\n\t\t\"@app/*\"\n\t]\n}\n```\n2. Change the name field of each module's `package.json` to follow a pattern so that they can be referenced easily\n3. Add module script shortcuts to root `package.json`, using the module names as changed in the previous step:\n```json\n\"scripts\": {\n\t\"db\": \"yarn workspace @tycholiz/db\",\n}\n```\n4. Add `private: true` property to root `package.json`.\n5. Add script in root `package.json` that allows us to execute multiple commands at once (note: consider using `concurrently` package to run servers in parallel)\n```json\n\"scripts\": {\n    \"dev\": \"concurrently --kill-others-on-fail \\\"yarn sanity dev\\\"  \\\"yarn web dev\\\"\",\n}\n```","n":0.064}}},{"i":4,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n### List all recognized workspaces\n`yarn workspaces info`\n\n### Run the `start` command in each workspace\n`yarn workspaces run start`\n","n":0.243}}},{"i":5,"$":{"0":{"v":"Webserver","n":1},"1":{"v":"\nWeb servers like Apache, Nginx and Caddy can perform various enhancements to our requests\n- Reverse proxy with caching\n- IPv6\n- Load balancing\n- FastCGI support with caching\n- WebSockets\n- Handling of static files, index files, and auto-indexing\n- TLS/SSL with SNI\n\nWeb servers are built to handle a lot of incoming requests simultaneously. Nginx and Apache each take a different approach to how they take a request from the incoming request queue and hand it off to a worker process.\n","n":0.115}}},{"i":6,"$":{"0":{"v":"Proxy","n":1},"1":{"v":"\n# Proxy server\n- A proxy server is a gateway from one network to another \n\t- Ex. the Cloudflare proxy server gives the local network access to the network that holds that cached information.\n- a proxy acts on behalf of the client(s), while a reverse proxy acts on behalf of the server(s)\n\t- When you see \"proxy\" think: \"something that stands-in for something else.\"\n\n## Caching\nAnother advantage of a proxy server is that its cache can serve a lot of requests. If multiple clients access a particular resource, the proxy server can cache it and serve it to all the clients without going to the remote server.\n\nWhen you think about it, HTTP requests have the same benefits that pure functions do: for a given request, there is a given response. This means that if we remember all of the details of the request, we may be able to store that corresponding response, and serve that data to all subsequent identical requests. \n- The proxy server figures out whether or not it should cache data based on the headings that are found in the request. \n- a cache is used by a proxy server. Think of it as storage that the proxy server returns to the client making the request. \n\nWhen we send a request to a server for a particular resource, the proxy server intercepts the message (establishing a TCP connection between it and the browser) and checks if it has a copy of the resource cached. If it does, then it returns it. If it doesn't then it opens a new TCP connection between it and the origin server, gets the resource, then returns it to the client. The client is none the wiser that any of this has happened.\n\nWeb caches are not computation heavy, and therefore can be low-cost machines.\n","n":0.058}}},{"i":7,"$":{"0":{"v":"Reverse","n":1},"1":{"v":"\nReverse proxies are servers that sit between the request-response process that retrieve resources from a server. These resources are then returned to the client, appearing as if they originated from the proxy server itself.\n- ex. This might include services such as caching, security (inc. SSL), load balancing, and content optimization\n\nThe backends that are receiving the proxied requests are called *upstreams*\n- ie. these would be your application servers/database etc.\n\n- Proxying is typically for load balancing, to seamlessly show content from different websites, or to pass requests for processing to application servers over protocols other than HTTP.\n- It's called a reverse proxy because the client makes a request, and before that request can hit the server, it's intercepted by a server (the proxy server), which gives the request what it wants, and sends (reverses) it back to the client. \n- A common scenario is that we run our REST API behind a reverse proxy. Among other reasons, we might want to do this so our API server is on a different network/IP than our front-end application. Therefore, we can secure this network and only allow traffic from the reverse proxy server.\n- A reverse proxy like one from Nginx can be used to implement SSL\n\t- [guide](https://nginx.org/en/docs/http/configuring_https_servers.html)\n\n\n\n### Cook\n#### Proxy requests to different local backends (ie. upstreams)\nImagine we want to have a RPi that exposes 3 websites to the internet, without having to specify port at the end of the URL. As it is, we can't expose them all on port 80/443. To get around this, we can configure a webserver like Apache/Nginx/Caddy as a reverse proxy to forward the requests to each specific application hosted on the RPi. With a reverse proxy configured, the webserver is able to know which requests go to which server based on the domain (or even the URL path). We can set this up by exposing the webserver to the internet via ports 80 & 443, and when the webserver receives requests, it will proxy them on to each individual app.\n","n":0.055}}},{"i":8,"$":{"0":{"v":"Forward","n":1},"1":{"v":"\n# Forward Proxy (or simply Proxy)\n- From the server's perspective, requests came from the proxy server (in this case, the proxy server is just passing along the request made by the client)\n- It's called a forward proxy because when a request is detected, the proxy server intercepts it, and forwards it on to the next destination\n\n## Web Proxy Server\n- It's common to run web apps behind a proxy such as NGINX\n\t- Nginx listens on port 80, then forwards traffic to the app on another port, like 4000\n- An early limiting factor of Nginx that will be encountered will be related to the number of open sockets that the Nginx server can handle. The result is delays on the client side. \n\t- the next limiting factor would be the number of ports available (there are only 64k). These 64,000 ports can handle 500 requests/second \n","n":0.083}}},{"i":9,"$":{"0":{"v":"Webpack","n":1},"1":{"v":"\nNode.js is built with a modular-first mentality. However, the web has been slow to adopt this philosophy of having clearly defined and swappable modules. Webpack aims to solve this by allowing us to implement modules in our code\n- unlike Node, webpack can also treat css/sass files, and image URLs as modules.\n\nWebpack is eager, in that javascript modules are figured out ahead of time, and are delivered in one big file. This negates the need to make network requests to get more of the javascript. The consequence of this fact is:\n- we cannot import dynamically in a data-driven way (since the importing will already have been done by the time the data-driven code is run.)\n- we cannot import in a function, since a function occurs after webpack does its job.\n- Module objects are frozen. There is no way to hack a new feature into a module object, polyfill style.\n- import errors are detected at compile-time\n- There is no hook allowing a module to run some code before its dependencies load. This means that modules have no control over how their dependencies are loaded.\n\n# Concepts\n- each webpack config has an entrypoint from which the dependency graph starts to be made. Unless specified, this defaults to `./src/index.js`\n- each webpack config also has an output, which specifies where the bundle will be emitted. Unless specified, this defaults to `./dist/main.js` for the main file, and the rest of the files also end up in `dist/`\n\n## Loaders\n- in Webpack 4 and below, webpack only understands js and json files. For this reason, we use loaders to allow different files to form a part of the dependency graph.\n- ex. `{ test: /\\.txt$/, use: 'raw-loader' }`\n\t- basically, loaders say \"hey Webpack, when you come across `.txt` files whenever you are trying to import, use the `raw-loader` to transform it before adding it to the bundle.\n- loaders are declared in `module.rules`\n\n## Types of Modules\n- There are more, but the 2 main types of modules we are concerned with are ECMAScript Modules, and Asset Modules\n\n### Asset Modules\n- In webpack 5, Asset Modules allow us to use asset files (ie. fonts, icons), without configuring additional loaders.\n\t- before Webpack 5, we had to use `raw-loader`, `url-loader`, `file-loader` etc., since out of the box, those versions of webpack only understood js and json files.\n\n### Module Resolution\nA resolver is a library which helps in locating a module by its absolute path\n- Effectively, webpack can be in charge of locating the files, and creating an alias for that path, so that that alias can be used throughout the codebase, giving us the benefit of only having to change that path once, in the event that it needs to change.\n\n* * *\n### Resolve\n#### resolve.modules\n- in webpack.config, we can tell webpack where it should go to look for certain modules.\n\t- ex. `node_modules` will be a default location, but we can specify paths that have higher priority so they are checked first before `node_modules`\n- if relative paths are used, then webpack will scan for that file recursively, similar to how node resolves `node_modules`\n\n#### resolve.alias\n- allows us to import more easily in our project.\n- we basically tell webpack, \"hey, any time you see me `import _ from 'ui-components`, just look for that module at whatever path I specify.\"\n\nwebpack continuously builds our app as we go.\n\n* * *\n\n### Contexts (require.context)\n- allows us to create our own context by passing a pattern that will match a filename.\n- One of the main functions of webpack's compiler is to recursively parse all modules to form a dependency graph.\n- A context is simply a base directory that resolves paths to modules (ie. it is a directory and its dependencies)\n\t- ex. in a React app, we can choose to declare the `components/` directory a context. This will allow us to search for files within that directory \"tree\"\n- Realistically, the root directory (current working directory) is a context for webpack to actually resolve the path to `index.js`\n- The intention is to tell webpack at compile time to transform that expression into a dynamic list of all the possible matching module requests that it can resolve, in turn adding them as build dependencies and allowing you to require them at runtime.\n- In short, you would use require.context in the exact same situation when in Node.js at runtime you would use globs to dynamically build a list of module paths to require. The return value is a callable object that behaves like require, whose keys contain the necessary module request data that can be passed to it as an argument to require the module.\n\n# Alternatives\n- Snowpack: very fast\n- Rollup\n- Parcel: simple config that works out of the box\n- Vite: Built by Evan You (of Vue.js). Out of the box with Sveltekit\n\n# UE Resources\n[Chunking in Webpack](https://medium.com/hackernoon/the-100-correct-way-to-split-your-chunks-with-webpack-f8a9df5b7758)","n":0.036}}},{"i":10,"$":{"0":{"v":"Watermelon","n":1},"1":{"v":"\n# Philosophy\ndeclaratively define the connection between the component, and the data you want it to display. When the data changes, every component that is connected will automatically update.\nwatermelon is fast in part because it uses a declarative API. The declarative API means that all of the expensive computation is being done natively (Java or Swift). Since Javascript is quite slow compared to these 2 languages, this allows our computations to be done more efficiently.\n\n# Tables\n### Columns\nColumns have one of three types: string, number, or boolean\nFields of those types will default to '', 0, or false respectively\n\nTo allow fields to be null, mark the column as isOptional: true\n\n### withObservables\n- This is the principal way that we connect WatermelonDB to our component\n- let's us enhance a component by turning a non-reactive component to become reactive, meaning that UI will update in accordance with localdb changes\n- we make our component reactive by feeding it an observable for the data we want to display\n\n```js\nwithObservables(['post'], ({ post }) => ({\n  post: post.observe(), // inject enhanced props into the component\n  author: post.author.observeCount()\n}))\n```\n- above:\n\t- `({ post })` are the input props for the component\n\t- The first argument: `['post']` is a list of props that trigger observation restart. So if a different post is passed, that new post will be observed\n\t- Rule of thumb: If you want to use a prop in the second arg function, pass its name in the first arg array\n\t- This is also the place that we should make relations\n\t\t- the relation is enabled by the `@children` decorator on the parent model\n\n# Actions\nMutation (Create, Update) queries can be made from anywhere in the app, but the preferred way is to execute them through actions\n- An action is a function that can modify the database\n\n# Migrations\nEach migration must migrate to a version one above the previous migration\n- of course, each migration simply builds on the previous ones, meaning that when we want to make changes, we need to add the new changes as an item in the `migrations` array (to the front) and mark it with the next integer `toVersion`.\n\nSteps to making schema changes:\n1. make the change in migrations.js\n2. wait for the error in the simulator:\n\t- `Migrations can't be newer than schema. Schema is version 1 and migrations cover range from 1 to 2`\n3. if the error is there, make the change in schema.js, updating the `schemaVersion` to the latest migration\n\n### Testing migrations work properly\n1. *Migrations test*: Install the previous version of your app, then update to the version you're about to ship, and make sure it still works\n2. *Fresh schema install test*: Remove the app, and then install the new version of the app, and make sure it works\n\n# Q Module\n- This module provides us to make SQL-like clauses to help construct our query\n\t- This is where we can use WHERE, JOIN (on), AND, OR, LIKE etc.\n\t\t- note: remember to escape `Q.like`\n- JOINs are done through `Q.on`\n\n# Observable\n- `.observe()` will return an observable\n- we can hook up observables to components\n- Because WatermelonDB is fully observable, we can create a @lazy function that will observe a database value and give us updated results in real-time (ie. without having to query the database)\n\t- ex. imagine we have a blog site, and blog posts can have a \"popular\" banner if they have at least 10 comments. We can make a function on the model layer that will observe the number of comments and will reactively give us the correct flag for the boolean:\n\t```js\n\tclass Post extends Model {\n\t\t@lazy isPopular = this.comments.observeCount().pipe(\n\t\t\tmap$(comments => comments > 10),\n\t\t    distinctUntilChanged()\n\t\t)\n\t}\n\t```\n\tand then directly connect it to the component:\n\t```js\n\tconst enhance = withObservables(['post'], ({ post }) => ({\n\t\tisPopular: post.isPopular,\n\t}))\n\t```\n\t- since this is reactive, a rise above/fall below the 10 comment threshold will cause the component to re-render.\n\t- Dissecting:\n\t\t- `this.comments.observeCount()` - take the Observable number of comments\n\t\t- `map$(comments => comments > 10)` - transform this into an Observable of boolean (popular or not)\n\t\t- `distinctUntilChanged()` - this is so that if the comment count changes, but the popularity doesn't (it's still below/above 10), components won't be unnecessarily re-rendered\n\t\t- `@lazy` - also for performance (we only define this Observable once, so we can re-use it for free)\n\n# Sync\nAny backend will work, as long as it complies with the following spec:\n![6d0a0837a90af34681ce452b015d4b19.png](:/860953c1a363424aac94bdff9d490b90)\n- `changes` is an object with a field for each model (table) that has changes. For each model, there are 3 fields: `created`, `updated`, `deleted`.\n\t- When the `changes` object is received from a Pull Request, it is the selection of changes that were made on the server since our last sync, that we need to now update locally.\n\t- When the `changes` object is sent with a Push Request, it is the selection of changes that we've made locally, that have not yet been sent to the remote database.\n\n### Pulling\nWhen Watermelon runs `synchronize()`, `pullChanges` will get run, which will pass along with it information about the last time a pull was made (`lastPulledAt`). `pullChanges` will call an endpoint to the backend, passing along that `lastPulledAt` timestamp, and the server will confer with the backend DB, and send back all of the changes made since the last pull, along with the current timestamp. When the mobile app receives the response, it will then proceed to apply those changes to the local db.\n\n### Pushing\nWe send to the server a `change` object, containing everything that needs to be updated remotely, as well as a timestamp of the last time a pull was made (`lastPulledAt`). When the server receives the request, it will use `lastPulledAt` to check conflicts with the remote db. If there is no conflict, the server will then update the db with the provided changes.\n\n### Sync limitations\nThere are currently limitations of Sync, as outlined in this blog: [How to Build WatermelonDB Sync Backend in Elixir | Fahri NH](:/f615cdc32a5c4be4a768caee30774aa9)\n\n### How does it know when to re-render?\nfor individual records, just listen to changes, and if the record changes, re-render\n\nfor queries, like \"tasks where a=b\", listen to the collection of tasks, and when a record in that collection changes, check if the record matches the query. If it does: if record was on the rendered list, and was deleted — remove from rendered list. if it wasn't on the rendered list, and now matches — add to rendered list.\n\nfor multi-table queries like \"tasks that belong to projects where a =b\", listen to all relevant collections, and if there's a change in any of them, re-query the database. There's ways to make it more efficient, but need to measure if the perf benefit is worth it\n\n# Solutions to:\n- [prop drilling](https://nozbe.github.io/WatermelonDB/Components.html#database-provider)\n\n# UE Resources\n[Logrocket Tutorial](https://blog.logrocket.com/offline-app-react-native-watermelondb/)\n[how sync works](https://fahri.id/posts/how-to-build-watermelondb-sync-backend-in-elixir/)\n[conf](https://www.youtube.com/watch?v=uFvHURTRLxQ)","n":0.03}}},{"i":11,"$":{"0":{"v":"Vscode","n":1}}},{"i":12,"$":{"0":{"v":"Workspaces","n":1},"1":{"v":"\n# Workspace\nA collection of one or more folders that are opened in a VS Code window (instance)\n- Think of a workspace as the root of a project that has extra VS Code knowledge and capabilities.\n\nThe concept of a workspace enables VS Code to:\n- Configure settings that only apply to a specific folder or folders but not others.\n- Persist task and debugger launch configurations that are only valid in the context of that workspace.\n- Store and restore UI state associated with that workspace (for example, the files that are opened).\n- Selectively enable or disable extensions only for that workspace.\n\n## Multi-root workspaces\nallow us to pick and include other folder trees to exist within the current workspace, allowing us the benefits that having all files under a workspace gives us.\n\n# UI\n### Workbench\n\"Workbench\" refers to the overall Visual Studio Code UI that encompasses the following UI components:\n- Title Bar\n- Activity Bar\n- Side Bar\n- Panel\n- Editor Group\n- Status Bar","n":0.081}}},{"i":13,"$":{"0":{"v":"Keybindings","n":1},"1":{"v":"\nEach rule consists of:\n- a key that describes the pressed keys.\n- a command containing the identifier of the command to execute.\n- an optional when clause containing a boolean expression that will be evaluated depending on the current context.\n\n### Parts of UI\n\"Workbench\" refers to the overall Visual Studio Code UI that encompasses the following UI components:\n- Title Bar\n- Activity Bar\n- Side Bar\n- Panel\n- Editor Group\n- Status Bar","n":0.123}}},{"i":14,"$":{"0":{"v":"Debugger","n":1},"1":{"v":"\n## Setup\n1. Create a Javascript Debug Terminal (from command palette)\n2. Run command to start servers (ie. `yarn start`)\n    - command will take longer than normal to run, because vscode is attaching debuggers to all the processes that are about to run.\n3. Set some breakpoints of where we want the code to stop\n4. Manually cause the code to be executed\n    - ex. by visiting a url, clicking a button in UI etc.\n\n## Analysis\n- We can see what each variable evaluates to in the main window\n\n### Debug panel\n- In the debugger left panel, we can see the variables\n\n### Debug Console\n- We can open the Debug Console to get a Debug Console REPL (cmd+shift+y)\n\n\n* * *\nthe debugger will not work for client-side code, since this code is executed in a browser. We would need to use the browser debugging tool to accomplish this.\n- alternatively, we can have a `launch.json` file to allow us to debug\n\n# Chrome Debugger\n- The vscode debugger can connect to Chrome via its Chrome Debugger protocol. Doing this allows us to map files loaded in the browser to the files open in Visual Studio Code.\n    - This enables developers to debug in vscode, by...\n        - setting breakpoints directly in their source code.\n        - setting up variables to watch and see the full call stack when debugging","n":0.068}}},{"i":15,"$":{"0":{"v":"Actions","n":1},"1":{"v":"\n* Debugger Actions\n### Continue\nMove along to the next debugger point\n\n### Step Over\nCall the function without causing our debugging context to be inside the function call\n- Therefore, if we are at a function and \"step over\", we will not enter the function. However, the function itself will of course still be executed.\n\n### Step Into\nGo into the current function and execute each line within 1 by 1\n\n### Step Out\nExit the current function debugger context.\n","n":0.118}}},{"i":16,"$":{"0":{"v":"Cmds","n":1}}},{"i":17,"$":{"0":{"v":"Nav","n":1},"1":{"v":"\nChange vscode workspace:\n`<C-r>`\n\nToggle Intellisense popup\n`Cmd-k Cmd-i`\n\nswitch tab\n`<C-j/k>`\n\ntoggle between terminal\n`<Cmd-j>`","n":0.333}}},{"i":18,"$":{"0":{"v":"Vm","n":1},"1":{"v":"\n# Virtual Maachines (VM)\nA VM is made up of a Host VM and a Guest VM\n\n### Host VM\n- the server component of a VM, which provides resources to a Guest VM, such as processing power, memory, disk, network I/O etc.\n\n### Guest VM\n- can exist on a single phsyical machine, but is usually distributed across multiple hosts for load balancing.\n- The guest VM is not even aware that it is a guest VM, and therefore is not aware of any other physical resources that are allocated to other guest VMs.\n\n### Hypervisor\n- a piece of software in a computer that will create and run virtual machines\n- the hypervisor intermediates between the host and guest VM, which isolates individual guest VMs. This allows a host VM to support multiple guests running on different operating systems.\n\n* * *\n\n### Swap Space\n- Swap space in Linux is used when the amount of physical memory (RAM) is full\n\n### Memory Page (a.k.a. Virtual Page)\n- a fixed length contiguous block of virtual memory.\n- each page is described by an record in a page table","n":0.076}}},{"i":19,"$":{"0":{"v":"Vim","n":1},"1":{"v":"\n# Operators and Motions\n- Operator --> Number --> Motion\n\n## operator (Verb)\n- what we are doing\n- ***ex.*** - `d`, `c`, `y`, `f`, `t`\n    - `f#`/`F#` - (*mn.* find)\n        - inclusively go to first occurrence of `#` in the line\n        - `;` to cycle through, `,` to go backwards\n    - `t#`/`T#` - (*mn.* til)\n        - exclusively go to first occurrence of `#` in the line\n\n## Modifier\n- more specific information about the modifier\n- ***ex.*** - `i` (inside), `a` (around)\n\nCombine with numbers to further specify text objects\n- ***ex.*** - `d2f:` - delete until the second `:` character\n- ***ex.*** - `d2/:<cr>` - delete until the second `:` character\n\n## motion (noun) - what we are operating on\n- ***ex.*** - `$`, `0`, `w`, `e`, `b`\n\n* * *\n\n# strat submodules\nStructure the vim module from the perspective of a user who is trying to accomplish a certain task in vim.\n- Are they trying to substitute all occurrences in a file?\n- are they trying to modify text in some way (surround, uppercase, )\n\n- nav (moving the cursor around)\n    - line\n        - search\n        - find\n    - buffer\n        - search\n    - windows\n    - tree\n        - netrw\n        - MRU\n        - <C-6>\n- modify\n    - surround\n    - substitute\n        - greplace\n    - uppercase\n- feat\n    - marks\n    - macro\n    - shell (execute shell commands)\n- plug\n    - fzf\n    - fugitive\n    - guter (git-gutter)\n- repeating\n","n":0.068}}},{"i":20,"$":{"0":{"v":"Substitution","n":1},"1":{"v":"\n# Substitution\nrepeat last substitute\n- `:&`\n    - `:&&` command repeats the last substitution with the same flags.\n        - You can supply the additional range(s) to it (and concatenate as many as you like):\n\nreplace multiple search terms at once\n- `:%s/Kang\\|Kodos/alien/gc`\n\n`:6,10s/<search_string>/<replace_string>/g | 14,18&&`\n\n## within Visually Selected Area\n1. visually select the area you want substitutions to take place in, then `<ESC>`\n2. `:%s/\\%Vfoo/bar/g` to replace all `foo` with `bar`\n\nAlternatively, the vis.vim plugin can be used\n\n### Upper/Lower Case\n- `guw`/`gUw` - make word lowercase/uppercase\n    - `guu`/`gUU` - make line uppercase/lowercase\n- `g~` - swap case\n\n## Flags\n### c\ny/n/a/q substitute (within single file)\n- `:%s/search/replace/gc`\n\n#### Values\n`l` - last (substitute this match, then quit)\n`a` - all (this, and all remaining)\n    - note: `l` and `a` are opposites, since `l` will treat the current match as the final one in our substitution process, whereas `a`\n`<C-e>`/`<C-d>` - scroll","n":0.086}}},{"i":21,"$":{"0":{"v":"Settings","n":1},"1":{"v":"\n### Syntax highlighting\nVim uses syntax definitions to highlight source code.\n- Syntax definitions simply declare where a function name starts, which pieces are commented out and what are keywords.\n- To color them, Vim uses colorschemes. You can load custom color schemes by placing them in .vim/colors, then load them using the colorscheme command\n\n### Pasting\nwhen you paste text into your terminal-based Vim with a right mouse click, Vim cannot know it is coming from a paste. To Vim, it looks like text entered by someone who can type incredibly fast :) Since Vim thinks this is regular key strokes, it applies all auto-indenting and auto-expansion of defined abbreviations to the input, resulting in often cascading indents of paragraphs.\n\nThere is an easy option to prevent this, however. You can temporarily switch to “paste mode”, simply by setting the following option:\n`set pastetoggle=<F2>`\n\nThen, when in insert mode, ready to paste, if you press `<F2>`, Vim will switch to paste mode, disabling all kinds of smartness and just pasting a whole buffer of text. Then, you can disable paste mode again with another press of `<F2>`. Nice and simple.","n":0.074}}},{"i":22,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get the value of a setting\n`:set expandtab?`\n\n\n#### Get the value of setting that is a variable\nto find out what the current syntax highlighting mode is (encoded in a variable, not an option):\n`echo b:current_syntax`\n\n#### See where a variable was set\n`verbose set expandtab?`","n":0.154}}},{"i":23,"$":{"0":{"v":"Search","n":1},"1":{"v":"\n#### Partial Word Search\n- When you're typing and you enter a partial word, you can cause Vim to search for a completion by using the `<C-p>` (search for previous marching word) and `<C-n>` (search for next match).\n\n### Where you land\nSearches in Vim put the cursor on the first character of the matched string by default\n- if you search for Debian, it would put the cursor on the D.\n\nWe can offset the cursor placement in a few ways:\n- To land on the last character in the matched string, rather than the beginning, add an /e to your search:\n    - ex. `/Debian/e`\n- To land 2 lines above/below, add `-2`/`+2` to the end\n    - ex. `/Debian/-2`\n- To offset 2 chars from the beginning of the string, add `b+3`\n- To offset 2 chars from the end of the string, add `e-3`\n\n\n## Tips\n- use search with an operator to find the location easier\n    - ex. Yank til `})` \n        - `y/})<Enter>`","n":0.08}}},{"i":24,"$":{"0":{"v":"Special Chars","n":0.707},"1":{"v":"\n### `.`\nmatches every single character.\n\n","n":0.447}}},{"i":25,"$":{"0":{"v":"Options","n":1},"1":{"v":"\n# Search options\nTo remove these options, prepend the command with `no`(`:set nohls`)\n- `:set ic` - ignore case\n- `:set hls` - highlight search\n- `:set is` - incsearch\n","n":0.196}}},{"i":26,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### Re-execute the last search pattern (`/`)\n`gn`\n- This puts us in v-mode. If we type `gn` again, it will highlight until the next occurrence of that pattern\n- `gn` is a text-object representing the last searched word\n\t- leverage this with `cgn` to replace the next occurrence of the search pattern","n":0.143}}},{"i":27,"$":{"0":{"v":"Regex","n":1},"1":{"v":"\nRegex in Vim largely follows Basic Regex (BRE) (POSIX)\n- see `:help perl-patterns` for differences between Perl Regex and Vim's\n\nTo get Regex to behave more like traditional Regex, use Vim \"very magic\" mode by prepending with `\\v`\n- See `:help /\\v`\n\n","n":0.16}}},{"i":28,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\nfind empty lines (only whitespace)\n- `^\\s.*$`\n\nfind lowercase/non-lowercase\n- `\\l`/`\\L`\n\nfind uppercase/non-uppercase\n- `\\l`/`\\L`\n\nfind word boundary\n- `\\<term\\>`\n\nfind both \"dog\" and \"Dog\"\n- `/[Dd]og`\n\n### Greedy vs Non-greedy Quantifiers\nVim allows you to specify how many of something you wish to match by adding quantifiers to the term.\n- \"greedy quantifiers\" are those that match as many of the specified characters as possible.\n    - `*` matches 0 or more\n        - ex. `/abc*` will match abc, abby, and absolutely\n    - `\\+` matches 1 or more\n        - ex. `/abc\\+` will match abc, but not abby or absolutely\n    - `\\{5,}` matches 5 or more characters\n- \"non-greedy quantifiers\" are those that match only the specified number, or as few as possible.\n    - `\\=` matches 0 or 1\n        - ex. `/abc\\=` will match abc, abby, and absolutely.\n    - `\\{0,10}` matches between 0 and 10 of a character\n    - `\\{5}` matches 5 characters\n    - `\\{,5}` matches 5 characters or less\n    - `\\{-}` matches as little as possible","n":0.081}}},{"i":29,"$":{"0":{"v":"Plug","n":1},"1":{"v":"\n`localrc`: lets you load specific Vim settings for any file in the same directory (or a subdirectory thereof). Comes in super handy for project-wide settings.\n","n":0.2}}},{"i":30,"$":{"0":{"v":"Gutter","n":1},"1":{"v":"\n# Vim-gitgutter\n- `[c`/`]c` - jump between hunks\n- `<leader>hs`/`<leader>hu` - stage/unstage individual hunk (when cursor is\n    within it)\n  - can get more granular with visual mode selection\n- `<leader>hp` - preview hunk","n":0.183}}},{"i":31,"$":{"0":{"v":"Greplace","n":1},"1":{"v":"\n### Usage\n1. run `:Gsearch <term>`\n2. specify directories to search\n3. make the substitution in the resulting buffer\n4. run `:Greplace`, and respond: yes/no/all/quit","n":0.218}}},{"i":32,"$":{"0":{"v":"Nav","n":1}}},{"i":33,"$":{"0":{"v":"Split","n":1},"1":{"v":"\n###### Split vertically\n`<C-w>v`\n\n###### Split horizontally\n`<C-w>-s`","n":0.447}}},{"i":34,"$":{"0":{"v":"Line","n":1},"1":{"v":"\n- go to column 15 of current line - `15|`\n- `3f:` - find the 3rd occurrence of `:` in the line","n":0.218}}},{"i":35,"$":{"0":{"v":"Dir","n":1},"1":{"v":"\n# Directory Navigation\nopen file under cursor\n- `gf` \n  - If doesn't work, try visually selecting first.\nopen file under cursor in new h-split (`<C-w>L` for v-split)\n- `<C-w>f`\n","n":0.196}}},{"i":36,"$":{"0":{"v":"Buffer","n":1},"1":{"v":"\n# Buffer Navigation\n## \nmove cursor up/down full screen\n- `ctrl + b`/`ctrl + f` \n\nscroll page up/down\n- `ctrl + e`/`ctrl + y`\n\njump between empty lines \n- `{`/`}`\n\ngo back 3 sentences \n- `3(`\n  - tip: trigger in *i n mode*\n\nmove to top/bottom\n- `zt`/`zb` \n\nmove current line to top of screen \n- `z<CR>`\n\nreplace a word with a yanked one \n- `vep`\n\ngo to top/mid/end line of current screen (home) \n- `H`/`M`/`L`\n    - go to 50% of page - `50%`\n\n## Returning to previous locations\ngo to last place you inserted text \n- `gi`\n\ngo to last place that you were \n- ***``***\n\ngo back and forth through list of positions you were in \n- `g;`/`g`,\n\ngo to start/end of previously changed or yanked text \n- `'[`/`']`\n","n":0.092}}},{"i":37,"$":{"0":{"v":"Search","n":1},"1":{"v":"\n# Search within Buffer\n### Search\n- `/`/`?` to find forward/backward\n    - `n` to show next, `N` to show previous\n    - `gn` will visually select the match\n- `/<CR>` - search for last searched pattern \n- You can search with word boundary by doing `/\\<word\\>`.\n\n### Symbol find\n- `*` to find forward for the word that the cursor is under\n    - prepend with `g` so find without word delimiters\n- `#` to find backward for the word that the cursor is under\n\n- `<C-o>`/`<C-i>` to go back/forward","n":0.111}}},{"i":38,"$":{"0":{"v":"Marks","n":1},"1":{"v":"\n# Marks\n- `m[a-zA-Z]` marks the current cursor location with the designated letter\n    - lowercase marks are local to the buffer, while uppercase are globally accessible\n        - therefore, if we have 5 buffers open, each buffer can have mark \"a\", but only one can have mark \"A\"\n- **`a** will jump to the exact spot of *mark \"a\"*\n    - *mn.* - more precise, just like js template literals (which use back ticks)\n- **'a** will jump to the **line** of *mark \"a\"*\n    - more useful in the context of an *ex command*\n- **`** and **'** will both jump to marks. **'** will take you to the line, and **`** will shoot you to the extact spot.\n\n## Automatic marks\n- vim automatically sets up some marks for us:\n\n| Keystroke | Buffer Contents                               |\n| --------- | --------------------------------------------- |\n| \"         | Position before last jump within current file |\n| 0         | Position of cursor when the file was last closed|\n| [         | Start of last change/yank                     |\n| ]         | End of last change/yank                       |\n| <         | Start of last visual selection                |\n| >         | End of last visual selection                  |\n| 1, 2..    | latest position of cursor in last file opened |\n","n":0.072}}},{"i":39,"$":{"0":{"v":"Modes","n":1}}},{"i":40,"$":{"0":{"v":"Visual","n":1},"1":{"v":"\n# Visual Mode\nreselect last v-mode selection\n- `gv`\n\ngo to the *other* end of the v-mode selection\n- `o`\n\nwhen using *dot command* to repeat a v-mode command, it acts on the same amount of text as was marked by the recent visual selection\n\n## Tips\n- use visual block mode to create multiple cursors to edit multiple places at once\n    - ***ex.*** - to append `;` to end of three lines (hence the `jj`), `<C-v>jj$A;`\n- when visually selecting by motion (ex. `vi(`), you can increase the level of enclosing parens that you navigate\n    - for example, if we have nested parens, we can start off with the above command, then say `i(` to go an additional layer\n- When using visual block to edit multiple places, you must use a command that puts you in i-mode(eg. `c`, `I`, `A`)\n    - note the capitalization of `I` and `A`. This shows that in visual block mode, the commands to \"insert at start\" and \"insert at end\" consider the demarcations to be what is within the visual block (instead of the start/end of the entire line, as is normally the case)","n":0.074}}},{"i":41,"$":{"0":{"v":"Insert","n":1},"1":{"v":"\n# Insert Mode\n- vim records keystrokes until we leave *i mode*\n- moving around in *i mode* resets the change, meaning once we exit *i mode* and undo with `u`, it will undo all changes made in that *i mode*.\n\n## Insert-Normal Mode\n- use *insert normal mode* from within insert mode for a one shot command within normal mode (then return to i mode)","n":0.127}}},{"i":42,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n## Modifying text from Insert-mode\n- CTRL-h (delete one character left)\n- CTRL-w (delete word left)\n- CTRL-u (clear line)","n":0.243}}},{"i":43,"$":{"0":{"v":"Cmdline","n":1},"1":{"v":"\n### Command line Completion\nWhen executing a command line command, use `<C-d>` to see suggestions based on the text you've already typed\n- doing this will give us a list of all the commands containing the string we've already typed\n- ex. `:help CTRL-W`, then type `<C-d>` to see all commands with `CTRL-W`\nAlternatively we can `<TAB>` to get the completion window\n\n* * *\n\n## Executing Shell commands\n### Running commands without using output\nPrecede any shell command with `!` in the command prompt to have it interpreted as a shell command.\n\n### Backtick expansion\nIf we wrap text in backticks, vim will run the enclosed command using our configured shell, and use its stdout as the argument for the given vim command\n    `` :args `cat files.txt` ``\n- if files.txt is just a list of files, then vim will add all those files to the argument list\n\n## Getting pwd/filename\n- `%` refers to filepath of current buffer\n    - if our current buffer was 3 levels deeper than our pwd, we could open a sibling (same level) file with `:e path/to/file`, or we could just say `:e %:h<Tab>`, which would auto complete the folder of the current buffer\n        - `:h` here removes the filename from the path\n        - `%:h` has been aliased to `%%`\n\n## Functions\nwe can call functions like so:\n- `:call MyFunction()`\n","n":0.069}}},{"i":44,"$":{"0":{"v":"Global","n":1},"1":{"v":"\nGlobal allows us to perform some Ex command on a range (default whole buffer)\n- ex. delete all lines that have the word \"require\" in it\n`:g/require/d`\n\nnote: an Ex command is one starting with a colon such as `:d` for delete\n\n# UE Resources\n[Some use-cases for global command](https://dev.to/iggredible/discovering-vim-global-command-49ad)","n":0.149}}},{"i":45,"$":{"0":{"v":"Innner","n":1}}},{"i":46,"$":{"0":{"v":"Text Object","n":0.707},"1":{"v":"\nOne of the keys to using Vim effectively is trying to edit by text object\n\n# Text object \n- a defined region of text that is defined by the motions, and modifier of that motion (eg. `vaw`, or `v2aw`)\n    - The definition of the text's scope depends on what vim commands we are using\n        - ex. When we say `vi\"` we are defining a text object, whose definition is \"everything inside `\"\"`\". Text objects define regions of text by some sort of structure.\n- text objects are good because we don't have to worry about where in the word the cursor is located. as a result, they work well with `.` \n\n## Types\n- Word (`w`)\n- Sentence (`s`)\n\n## Extended Text Objects (Plugins)\n### CamelCaseMotion\nDefines a text object that is a single word in a larger camel-case word\n- Delete inside the camel-case word\n    - `di,w`\n\n### VimTextObject\nDefines a text object that is a function argument/parameter\n- Delete inside the argument\n    - `dia`\n\n# UE Resources\n[info](https://blog.carbonfive.com/2011/10/17/vim-text-objects-the-definitive-guide/)","n":0.08}}},{"i":47,"$":{"0":{"v":"Inner","n":1},"1":{"v":"\n# viminfo file\nThe viminfo is like a cache, to store cut buffers persistently\n- If you exit Vim and later start it again, you would normally lose a lot of\ninformation.  The viminfo file can be used to remember that information, which\nenables you to continue where you left off.\n- Vim writes this file for us.\n\nThe viminfo file is used to store:\n- The command line history.\n- The search string history.\n- The input-line history.\n- Contents of non-empty registers.\n- Marks for several files.\n- File marks, pointing to locations in files.\n- Last search/substitute pattern (for 'n' and '&').\n- The buffer list.\n- Global variables.","n":0.102}}},{"i":48,"$":{"0":{"v":"Register","n":1},"1":{"v":"\n# Register \n([info](https://www.brianstorti.com/vim-registers/))\n- a space in memory with an identifier that vim uses to store text\n    - analogous to a clipboard\n- access register `a` - `\"a`\n    - access register in *i mode* - `<C-r>` (instead of `\"`)\n- vim has a default register, which is where any deleted or yanked text will be\n    - denoted `\"\"` - so `p` is shorthand for `\"\"p`\n    - this is more like copy/paste\n- when writing to a register, lowercase letters will overwrite the register, while uppercase will append to the register\n- Sequential register\n    - `\"1` is most recent (or `\"\"`), `\"2` is second most recent, and so on\n    - this is only for `dd` and `yy` operation\n- Yank register\n    - `\"0`\n- Blackhole register\n     - `\"_` - prepending a command with this will prevent anything from entering a register.\n- Clipboard register\n    - `\"+`/`\"*` - gives access to the system clipboard\n- Expression register\n    - `\"=`\n    - when fetching contents from the register, we enter command-line moe\n    - an exception to the idea that \"registers are containers that hold a block of text\"\n- use `:reg` to see all registers\n    - `:reg a b c` will show registers `a b c`\n- copy text into register `r`, then paste it - `\"ry`, `\"rp`\n- *Yank Register* - when yanking, the copied text is placed into the yank register (`\"0`), as well as the default register.\n- *Delete/yank Register* - `\"-`\n    - only small delete (ie. no `dd`)\n- when using visual mode to make a selection of text that we will replace with `p`, the highlighted text will go into the register as the *pasted* text exits\n","n":0.062}}},{"i":49,"$":{"0":{"v":"Buffer","n":1},"1":{"v":"\n# Buffer\n- a part of vim's memory that can hold text\n- This is normally in the form of:\n    - an actual file\n    - text stored temporarily (yank, paste etc)\n- when opening an existing file, the text of that file is copied and put into a new buffer. When we save that text, the original file is replaced by writing the buffer to disk\n- buffer list vs argument list\n    - argument list is subset of buffer list\n    - buffer list can be seen as more random access, while argument list could have more organization to it\n        - if you are working with only a few files in your current micro session, you'd have just those ones in the arg list, while having many more in the buffer list\n    - idea is that we might have lots of buffers open, and want to execute a macro across many files, but not all buffers.\n        - one solution is to prune the buffer list, but the most practical solution is to populate the argument list with just the files we are interested in. \n","n":0.075}}},{"i":50,"$":{"0":{"v":"Help","n":1},"1":{"v":"\n# Overview\nThe Vim documentation consists of two parts:\n1. The User manual\n   Task oriented explanations, from simple to complex.  Reads from start to\n   end like a book.\n2. The Reference manual\n   Precise description of how everything in Vim works.\n\nIf you only have a generic keyword to search for, use `:helpgrep` and open the quickfix window:\n```\n:helpgrep quickfix\n:copen\n```\n\n# How to...\n\n## Navigate\nJump to a subject\n- put cursor on a subject tag (blue text), and hit `<C-]>`\n    - In fact, this works on any word\n- return with `<C-o>`, or `<C-t>`\n\n## Get specific help\nwe can prepend a binding with the mode to see what it does, and get help on it\n- ex. `:help v_u` to get help on what `u` command does in visual mode.\n- ex. `:help CTRL-W_CTRL-I`\n- ex. `:help i_CTRL-G_<Down>`\n- ex. `:help i^W`\n\nprefixes:\nvisual mode - `v_`\ninsert mode - `i_`,\nEx mode - `:`\n- normal mode has no prefix\n\n\n# UE Resources\n[Guide to using vim help](https://vi.stackexchange.com/questions/2136/how-do-i-navigate-to-topics-in-vims-documentation)\n","n":0.082}}},{"i":51,"$":{"0":{"v":"Fugitive","n":1},"1":{"v":"\n# Vim-Fugitive\n## Gstatus\nor simply `:G`\n`<C-n>`/`<C-p>` - navigate files\n`-` stage/unstage the file that you are hovering over\n- can use visual mode\n\n`p` - run `git add --patch` for the file under cursor\n- splits file into hunks of changes and allows you to pick which parts to stage for commit\n- not as useful, since we can just use `diffget` as described below.\n\n`cc` - commit\n`D` - open file under cursor as a diff\n`U` - unstage a file (\"I've staged a file and want to remove it from the index\")\n`X` - untrack a file (\"I've changed a file in the working tree and want to discard all changes\")\n  - `git checkout -- <filename>`\n\n`]c`/`[c` - navigate between hunks when in vimdiff window\n\n## Gblame\n`A` resizes to end of author column\n`C` resizes to end of commit column\n`D` resizes to end of date/time column\n`<CR>` opens the patch that was applied by that commit\n\n## Reconciling differences between working copy and index version\n- `:Gw[rite]` - write (save) the current file and stage it\n- `:Gr[ead]` - read the index version of the file into the current buffer\n\t- good for seeing what changes were made (that haven't been staged yet)\n\n- `Gwrite` and `Gread` are opposites. Running `Gread` in the working file will do the exact same thing as running `Gwrite` in the index file +vv (both cause the working copy to revert to the index version)\n  - Both of these commands reconcile differences between the working copy and\n      the index version of a file by overwriting the contents of one buffer.\n- We can be more granular with our reconciliation by using vim built-in `diffget`/`diffput` (alias `do`/`dp`, respectively. These aliases also run :diffupdate afterwards, to fix coloring issues)\n  - While running `:Gdiff`, if we are in the index window and position the\n     cursor over a hunk of text and run `diffget`, it will pull that hunk into\n     the working copy of the file.\n  - If there are changes in the working copy (right side) that you'd like to move to the index, you can visually select those lines and run `diffput`\n    - this opens a new buffer with just the hunks you've added. we need to save this buffer before we `\n  - `diffget` makes changes in the currently active buffer, while `diffput` makes changes in the inactive buffer (by drawing in differences from the active buffer)\n![](/assets/images/2021-03-08-21-36-16.png)\n  - run `:diffupdate` if colors get messed up as a result of doing this\n- `:Gedit :0` - open index version of current file (remember: staging a file\n    updates the index version of the file (?))\n- `:Gremove` - wipe out the buffer and run `git remove` on the current file\n- `:Gmove` - rename the current file\n  - does a whole bunch more than `git move`, so it's not a direct replacement\n  - ex. `:Gmove new-file-name` - (relative to the path of the current file)\n- jump between conflicting regions with `[c`/`]c`\n\n## Merge conflicts\n- open a file with merge conflicts and run `:Gdiff`\n\t- might need to run `Gdiffsplit!` to get 3 windows\n- The windows shown are:\n\t1. Target Branch - The branch we are merging into (normally master)\n\t2. Working Tree - The file as it currently is (with changes visible from both target branch and merge branch)\n\t3. Merge Branch - The branch we are introducing\n- if using `diffget` and want to accept changes from the target branch (likely master), run `:diffget //2`. If we wan to accept changes from merge branch (likely feature-branch), run `:diffget //3`\n\n# Diff\nLeft window is the commit we are comparing against, and right window is the current working copy\n\n### Colors\nred - lines were deleted\ngreen - lines were added\nteal - lines were modified\n\n- knowing colors, we can easily notice if something has actually been removed, or if it was just moved/has a new structure\n  - In this case, you'd find the green code in the left hand side, and try and match it up with a green chunk on the righthand side. If we find a match, then we know the code format just changed, and that no actual code was deleted.\n\n## Seeing diff between 2 versions of a file\n`:Gdiff <sha>`\n\n`:Gdiff :0`\n- diff between current file and staged version\n\n`:Gdiff !~3`\n- diff between current file and 3 commits ago\n\n\n### UE Resources\n- http://vimcasts.org/episodes/fugitive-vim-resolving-merge-conflicts-with-vimdiff/\n- [cheatsheet](https://gist.github.com/mikaelz/38600d22b716b39b031165cd6d201a67)\n","n":0.038}}},{"i":52,"$":{"0":{"v":"Feat","n":1}}},{"i":53,"$":{"0":{"v":"Macros","n":1},"1":{"v":"\n# Macros\n- allow us to record and number of keystrokes into a register\n- ideal for repeating changes over a set of similar lines, paragraphs and files.\n- macros allow us to easily append commands to the end of an existing one\n    - for complex ones, we can paste the macro into a document, edit it, then yank it back into the register\n- golden rule - ensure every command in a macro is repeatable\n- before starting to record a macro, ask: where am I, where did I come from, where am I going?\n    -  before doing anything make sure the cursor is positioned to the next command does as we'd expect, and where we'd expect\n- any motions that fail cause the macro to fail.\n    - ex. - pressing `k` on line 1 does nothing, so it would fail if we tried.\n- `@@` - repeat last macro\n\nWhen you are making a macro, imagine you are looping a piece of paper back onto itself. The edge you start at must touch the edge you finish at. This is the same concept as making macros: After identifying a repetitive action, define the start point of a single action, start recording and do the stuff, then finish the macro when you are at the start of the next action.\n\n## Tips\n\nWhen using macros to perform a repetitive action on multiple lines, it's a good idea to make the first action to move our cursor to the starting position (ie. the edge in the paper loop analogy)\n- ex. we can start with `0` to go to the start of the line","n":0.062}}},{"i":54,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### Run a macro on all lines that match a pattern\n","n":0.302}}},{"i":55,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n# Macros\n- `qa` - start recording into register `a`\n- `q` - stop recording\n- `@a` - execute macro stored in register `a`\n- `@@` - execute the most recently executed macro\n- execute macro on each line in selection - `:'<,'>normal @q`\n","n":0.16}}},{"i":56,"$":{"0":{"v":"Jump List","n":0.707},"1":{"v":"\nVim remembers the locations you have recently visited (where you jumped from). Each position (file name, column number, line number) is recorded in a jump list, and each window has a separate jump list that records the last 100 positions where a jump occurred.\n- The commands that are regarded as \"jumps\" include searching, substitute and marks. Scrolling through a file is not regarded as jumping.\n\nLike a web browser, you can go back, then forward:\n- Press Ctrl-O to jump back to the previous (older) location.\n- Press Ctrl-I (same as Tab) to jump forward to the next (newer) location.\n\nuse `:jump` to see the jumplist","n":0.099}}},{"i":57,"$":{"0":{"v":"Change List","n":0.707},"1":{"v":"\nVim remembers the locations where changes occurred. \n- Each position (column number, line number) is recorded in a change list, and each buffer has a separate change list that records the last 100 positions where an undo-able change occurred.\n- if you make a change at a certain position, then make another change nearby, only the location of the later change is recorded \n    - \"nearby\" means on the same line, within a certain number of bytes.\n\n## Usage\n- Type g; to jump back to the position of the previous (older) change.\n- Type g, to jump to the position of the next (newer) change.\n\nType `:changes` to view the list","n":0.097}}},{"i":58,"$":{"0":{"v":"Config","n":1}}},{"i":59,"$":{"0":{"v":"Viminfo","n":1},"1":{"v":"\n```\nset viminfo=%,<800,'10,/50,:100,h,f0,n~/.vim/cache/.viminfo\n\"           | |    |   |   |    | |  + viminfo file path\n\"           | |    |   |   |    | + file marks 0-9,A-Z 0=NOT stored\n\"           | |    |   |   |    + disable 'hlsearch' loading viminfo\n\"           | |    |   |   + command-line history saved\n\"           | |    |   + search history saved\n\"           | |    + files marks saved\n\"           | + lines saved each register (old name for <, vi6.2)\n\"           + save/restore buffer list\n```\n`set viminfo=xxx` should come after `set nocompatible`\n\nif we set `'` parameter, then marks and other things set during the buffer editing will persist even after we close vim.","n":0.101}}},{"i":60,"$":{"0":{"v":"Runtime Path","n":0.707},"1":{"v":"\n# Runtime path\n- vim looks for scripts in various directories. The directories that vim will look in is known as the `runtime path`\n- to see it, type `:set runtimepath?`\n\n# Runtime directory\nAssuming that you're using some flavor of Unix, your personal runtime directory is ~/.vim. This is where you should put any plugin used only by you.\nYou should not install any plugins into the $VIMRUNTIME directory. That directory is intended for plugins distributed with Vim.","n":0.116}}},{"i":61,"$":{"0":{"v":"Mapping","n":1},"1":{"v":"\n# Mapping\n- `:map` - recursive version of mapping command\n- `:noremap` - non-recursive version of mapping command\n    - \"Recursive\" means that the mapping is expanded to a result, then the result is expanded to another result, and so on.\n- below, `j` will be mapped to `gg`. `Q` will also be mapped to `gg`, because `j` will be expanded for the recursive mapping. `W` will be  mapped to `j` (and not to `gg`) because `j` will not be expanded for the non-recursive mapping.\n    - The expansion stops when one of these is true:\n        1. the result is no longer mapped to anything else.\n        2. a non-recursive mapping has been applied (i.e. the \"noremap\" [or one of its ilk] is the final expansion).\n```\n:map j gg\n:map Q j\n:noremap W j\n```\n- since VIM is a **modal editor**, there are corresponding mapping commands for each mode\n    - ***ex.*** - `:imap`, `:nmap`, `:map!` (*i mode* and command line)\n\n- Recursive map - the rhs says to the lhs \"if you get triggered, when I get triggered. It stops with me, and nothing past me gets triggered\"\n- Non-recursive map (nore)- the rhs says \"When you get triggered, I may server as the lhs of another mapping that will trigger something else.\"\n- map is the \"root\" of all recursive mapping commands. \n    - The root form applies to \"normal\", \"visual+select\", and \"operator-pending\" modes. \n- \"Recursive\" means that the mapping is expanded to a result, then the result is expanded to another result, and so on.\n- The expansion stops when one of these is true:\n    1. the result is no longer mapped to anything else.\n    2. a non-recursive mapping has been applied (i.e. the \"noremap\" [or one of its ilk] is the final expansion).\n- At that point, Vim's default \"meaning\" of the final result is applied/executed.\n- \"Non-recursive\" means the mapping is only expanded once, and that result is applied/executed.\n    - Example:\n```\nnmap K H\nnnoremap H G\nnnoremap G gg\n```\n- The above causes K to expand to H, then H to expand to G and stop. It stops because of the nnoremap, which expands and stops immediately. The meaning of G will be executed (i.e. \"jump to last line\"). At most one non-recursive mapping will ever be applied in an expansion chain (it would be the last expansion to happen).\n- The mapping of G to gg only applies if you press G, but not if you press K. This mapping doesn't affect pressing K regardless of whether G was mapped recursively or not, since it's line 2 that causes the expansion of K to stop, so line 3 wouldn't be used.\n","n":0.048}}},{"i":62,"$":{"0":{"v":"Autocmd","n":1},"1":{"v":"\n## Autocommands\n- Autocommands are a way to tell Vim to run certain commands whenever certain events happen.\n\n[info](https://learnvimscriptthehardway.stevelosh.com/chapters/12.html)\n","n":0.243}}},{"i":63,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n- when an operator command is invoked twice, it means it is operating on the current line\n    - `dd`, `yy`, `>>`","n":0.218}}},{"i":64,"$":{"0":{"v":"Surround","n":1},"1":{"v":"\n# Surround\nsurround in visual mode with `()` \n- `Sb`\nchange surrounding `''` with `\"\"` \n- `cs'\"`\nchange `<p>` with `<b>` \n- `cst<b>`\ndelete `\"\"` surroundings \n- `ds\"`\ndelete tags \n- `dst`\nsurround parentheses content with `{}` \n- `ysib{`\n    - *mn.* - \"you surround!\"\nsurround line with `\"\"` \n- `yss`\nsurround 3 words with `\"\"` \n- `v3eS\"`\ndelete the second outer pair of brackets \n- `2ds}` ","n":0.132}}},{"i":65,"$":{"0":{"v":"Unix","n":1},"1":{"v":"\n# Unix \n\n## Philosophy\nIn Unix philosophy, \"everything is a file\"\n- ex. sockets (both have functions to read, write, open, close etc)\n\n## Unix Domain Socket\n- unix domain sockets allow us to exchange data between processes on the same host machine.\n- a.k.a IPC socket","n":0.154}}},{"i":66,"$":{"0":{"v":"Threads","n":1},"1":{"v":"\n# Threads\n**Scheduling** is the action of assigning work to different resources in a system. \n- this work is carried out on threads.\nTherefore a thread is the smallest amount of instructions that can be handled by a scheduler.\n- think of it as the atomic unit from the scheuler's perspective.\n\nA **process** is an instance of a program that is being executed on 1 or more threads.\n- multiple threads can exist on process simultaneously.\n\t- the number of threads depends on the programming language and source code that the process is running on.\n- threads within a process share resources like memory, but threads in different processes to not share resources.\n- a process can be forked (split) into many child processes\n\n### Multi-process vs Multi-threaded\nIf a single process in a multi-process application crashes, that process alone dies.  The buffer is flushed, and all the other child processes continue happily along.  In a multi-threaded environment, when one thread dies, they all die.\n\n- On some OSs, processes tend to be more expensive than threads (though on Linux, the difference is not that great)","n":0.076}}},{"i":67,"$":{"0":{"v":"Symlink","n":1},"1":{"v":"\nSoft symlinking a directory will not link the contents as well;\n\nA symbolic or soft link is an actual link to the original file, whereas a hard link is a mirror copy of the original file. If you delete the original file, the soft link has no value, because it points to a non-existent file.\n\nBut in the case of hard link, it is entirely opposite. Even if you delete the original file, the hard link will still has the data of the original file. Because hard link acts as a mirror copy of the original file.\n\nIn a nutshell, a soft link...\n- can cross the file system,\n- allows you to link between directories,\n- has different inode number and file permissions than original file,\n- permissions will not be updated,\n- has only the path of the original file, not the contents.\n\nA hard Link...\n- can't cross the file system boundaries (i.e. A hardlink can only work on the same filesystem),\n- can't link directories,\n- has the same inode number and permissions of original file,\n- permissions will be updated if we change the permissions of source file,\n- has the actual contents of original file, so that you still can view the contents, even if the original file moved or removed.","n":0.07}}},{"i":68,"$":{"0":{"v":"Streams","n":1},"1":{"v":"\n# What is it?\n- Streams are used to read or write input into output sequentially\n- What makes streams unique, is that instead of a program reading a file into memory all at once like in the traditional way, streams read chunks of data piece by piece, processing its content without keeping it all in memory.\n\t- This makes streams really powerful when working with large amounts of data, for example, a file size can be larger than your free memory space, making it impossible to read the whole file into the memory in order to process it. That’s where streams come to the rescue!\n- Streams are a way to handle reading/writing files, network communications, or any kind of end-to-end information exchange in an efficient way\n- streams are not only about working with media (ie. streaming movies/music) or big data. They also give us the power of ‘composability’ in our code\n\t- it’s possible to compose powerful pieces of code by piping data to and from other smaller pieces of code, using streams.\n\nit is best to think of a Stream as an established connection to some input, some output, or both. A lot of things can muddy this definition of stream, but that is the fundamental idea... A connection to some input or some output or some I/O endpoint that can handle both input and output.\n- ex. one might have a readable stream to a file that one wants to get the contents of. In this case the stream is an input connection to the file. The input stream doesn't hold the contents of the file, but rather holds the connection to the file. It is up to the program to then read and store the contents (assuming that is the desired operation).\n\t- One place a stream might store the content is in a [[Buffer|memory.buffer]] object (if we were using Node).\n\n## Why use streams over other data handling methods?\n1. Memory efficiency: you don’t need to load large amounts of data in memory before you are able to process it\n2. Time efficiency: it takes significantly less time to start processing data as soon as you have it, rather than having to wait with processing until the entire payload has been transmitted\n\n## Standard Streams\n- communication channels between a program and its environment (ie. the host that executes it).\n- stdin and stdout are inherited from the parent process, though it can be redirected.\n- anal: The Services menu, as implemented on Mac OSX, is analogous to standard streams. On this operating systems, graphical applications can provide functionality through a systemwide menu that operates on the current selection in the GUI, no matter in what application.\n- The correct way to think about redirection is that one redirects output to a file (`>`), and one redirects a file's contents to stdout (`<`).\n\n### File Descriptor (FD)\n- A FD is a number that uniquely identifies an open file in a computer's operating system\n\t- A file can be thought of simply as a resource. In other words, something that provides data. A file as we know it fulfills this definiion, but also consider that streams do as well. Therefore, a file in this sense is more general, and simply stands for \"a resource that provides data\".\n- A FD gives us the means to access a file, or perhaps more interestingly, a standard stream\n\t- therefore an FD is a reference to the standard stream\n- ex. The file descriptor for standard input has id = 0\n\nA file descriptor is associated with an input/output resource, which would include regular files, network sockets, and pipes\n<!-- - except for redirections with `<&`, any time we do a redirection, -->\n\n### STDIN\nStandard input is a stream from which a program reads its input data.\n- the program requests this data transfer by using the *read* operation\n- ex. the keyboard is the stdin to a text editor, and also to an interactive shell.\n\nWhen we redirect with `<`, implicitly we are running `1<`\n\n### STDOUT\nStandard output is a stream to which a program writes its output data.\n- the program requests this data transfer by using the *write* operation\n\n### STDERR\n- having 2 different output streams is analogous to functions that return a pair of values\n- The terminal that executes the program is the default stderr destination.\n\t- sometimes stdout is redirected, so the fact that stdout and stderr are different streams allows stderr to continue outputting its streams elsewhere than stdout.\n\t\t- ex. when we pipe the output of one program into another. Imagine we were piping data between different hosts. If there was a single stream, it would get passed along the chain. Now with stderr, we are able to see stderr in the executing terminal and trace down the location of the logs to find the offending program.\n\n#### 2>&1\n- we can append `2>&1` on any unix command to redirect stderr to the same destination as stdout\n\t- often, we see `2>/dev/null`, which means \"redirect stderr to `/dev/null`, a blackhole\"\n\t- ex. the **find** utility displays both stdout and stderr to the terminal. If we append the command with blackhole redirection, then only the stdout will be shown.\n- placement of `2>&1` is critical to determining its meaning. Consider the following variants of the same command:\n\n- `|&` is shorthand for `2>&1 |`\n- `echo hello > stuff.txt` === `echo hello 1> stuff.txt`.\n\t- ie. the `1` (stdout) is implicit. we are saying \"redirect the stdout of the echo command to `stuff.txt`\"\n- `0` means stdin, `1` means stdout, `2` means stderr\n- in the context of redirections, `&` means \"the following is a file descriptor (ie. the type of standard stream) and not a filename\". Without `&`, we would be writing to a file named `1`\n```\n// stderr goes to terminal, and nothing gets written to outfile\ncommand_does_not_exist 2>&1 > outfile\n\n// stderr goes to stdout, and gets written into outfile\ncommand_does_not_exist > outfile 2>&1\n```\nIn the first example, when we encountered `2>&1`, we were saying \"pipe all stderr into the same stream as stdout\". At this point in the command, stdout was simply the terminal (spec: since that is the default stdout of a command). Since stdout was the terminal at this point, we also declared that stderr should point to the terminal. In this way, `2>&1` doesn't cause stderr to equal stdout. It merely points it the same way stdout is facing. Nothing is stopping stdout from changing direction later on, which is exactly what happens. When we write `> outfile`, we are declaring that stdout should be `outfile`.\n\nIn the second example, we see that by the time we evaluate `2>&1`, stdout is `outfile`. This causes stderr to also point at `outfile`, hence is why in this example, the file is populated with `command not found: command_does_not_exist`\n- the `2>&1` operator points file descriptor 2 to the same target file descriptor 1 is pointing at.\n\n## Special File\n- spec: In Unix, the most comfortable means for programs to communicate is via files. It is easy to set up standard streams with files, so it is an appropriate level of abstraction to communicate with the Unix system. For this reason, Unix has this concept of a *special file*, which is a file that represents something that is not a file at all.\n- ex. consider a partition of a hard drive that \"exists\" in the FS at `/dev/sda3`. In reality, this is just how the filesystem \"knows of\" the partition. The `sda3` file enables the partition to communicate with the Unix system via standard streams.\n","n":0.029}}},{"i":69,"$":{"0":{"v":"Process","n":1},"1":{"v":"\nsince a process is a type of environment, environment variables can be associated with a process.\n- ex. when we say `process.env` we are talking about getting environment variables that exist during a specific process\n\nIf a program is a set of instructions to carry out a specific task, then a process is just **a program in execution**\n\na **Signal** is the way that a process can communicate with the OS\n- Signal and interrupt are basically same, but with a small distinction:\n    - interrupts are generated by the processor and handled by the kernel\n    - signals are generated by the kernel and handled by the process.\n\n\n# Debugging\n- strace/truss, ltrace and gdb are generally good ideas for looking at why a process is stuck. (truss -u on Solaris is particularly helpful; I find ltrace too often presents arguments to library calls in an unusable format.) Solaris also has useful /proc-based tools, some of which have been ported to Linux. (pstack is often helpful).","n":0.079}}},{"i":70,"$":{"0":{"v":"Netrc","n":1},"1":{"v":"\nThe `.netrc` file contains configuration and autologin information for the ftp client \n- It resides in the user's home directory\n\n`.netrc` file takes the form:\n> `remote-machine` `name`\n\nThe auto-login process searches the `.netrc` file for a machine token that matches the remote machine specified on the ftp command line\n- Once a match is made, the subsequent `.netrc` tokens are processed, stopping when the end of file is reached or another machine or a default token is encountered\n\nThe following three lines must be included in the file. The lines must be separated by either white space (spaces, tabs, or newlines) or commas:\n\n```\nmachine <remote-instance-of-labkey-server>\nlogin <user-email>\npassword <user-password>\n```\nAn example:\n```\nmachine mymachine.labkey.org\nlogin user@labkey.org\npassword mypassword\n```\nor:\n```\nmachine mymachine.labkey.org login user@labkey.org password mypassword\n```\n\n### Using API Keys\nWhen API Keys are enabled on your server, you can generate a specific token representing your login credentials on that server and use it in the netrc file. The \"login\" name used is \"apikey\" (instead of your email address) and the unique API key generated is used as the password\n\n* * *\n\nNote: the `.netrc` file only deals with connections at the machine level and should not include a port or protocol designation, meaning both \"mymachine.labkey.org:8888\" and \"https://mymachine.labkey.org\" are incorrect.","n":0.072}}},{"i":71,"$":{"0":{"v":"Man Pages","n":0.707},"1":{"v":"\n# Man Pages\n- anything in `[]` brackets indicates optional\n- `--` - signify end of command options. Only positional params are accepted after this point.\n    - ex. say we want to grep for the string `-v`. If we just executed `grep -v file.txt`, the `-v` would be interpreted as an argument on grep. If we execute `grep -- -v file.txt`, `--` tells us \"ok, that's it. No more arguments accepted\". Since the section after the args section is the pattern section, `-v` gets interpreted as a pattern.\n\n### Valid command form\n- angle brackets for required parameters: ping <hostname>\n- square brackets for optional parameters: mkdir [-p] <dirname>\n- ellipses for repeated items: cp <source1> [source2…] <dest>\n- vertical bars for choice of items: netstat {-t|-u}","n":0.091}}},{"i":72,"$":{"0":{"v":"Fs","n":1},"1":{"v":"\n# Unix filesystem\nThe UNIX filesystem unifies all physical hard drives and partitions into a single directory structure. It all starts at the top–the root (/) directory. All other directories and their subdirectories are located under the single Linux root directory. This means that there is only one single directory tree in which to search for files and programs.\n\n## Types of files\n- ordinary file\n- directory\n- special (device) file - represents a physical device (ex. printer).\n\t- special files are used for sending outputs to the device, and receiving inputs\n- pipe - a temporary file that gets created when we pipe commands into each other. Pipes are the mechanism through with programs chain together.\n- socket - a stream of data very similar to network stream, but all the transactions are local to the filesystem.\n- symbolic link\n\n## Filesystem metadata\n- The general file system model, to which any implemented file system needs to be reduced, consists of several well-defined metadata entities: `superblock`, `inode`, `file`, and `dentry`\n\t- they are metadata about the filesystem\n- each entity here interacts using the VFS, and each entity is treated as an object.\n- each entity has its own data structure, and a pointer to a table of methods \n\n## Mount Point\n- a mount point is a special directory within the unix filesystem. It is special because when an external filesystem is mounted (ex. HDD, SSD, USB, SD), its contents are dumped into the mount point, which is accessible from the root folder (`/`)\n- ex. `/media`\n- so when we run `mount /dev/hda2 /media/photos`, we are saying \"hey, I want you take all of the contents of `/dev/hda2` (partition 2 of HDD), and make it all accessible through `/media/photos`\".\n\t- spec:think of it like a map, where it will take the external file system on the USB (in FAT or NTFS format) and it will make all of the contents visible within the UNIX filesystem\n\t- to carry out this \"mapping\", unix has filesystem drivers \n\n## Directory structure\n`/bin` - binaries generally needed by all users of the system.\n`/lib` - contains system libraries\n`/usr` - contains executables, libraries, and shared resources that are not system critical (ex. X Window)\n`/usr/bin` - stores binaries distributed with the OS, that aren't in `/bin` \n`/usr/lib` - stores libraries for programs in `/usr`\n`/dev` - contains file representations of peripheral devices. Therefore, to view files, the external device needs to be mounted (see Mount Point below) \n`/media` - default mount point for removable devices\n`/mnt` - default mount point for temporary filesystems\n`/etc` - contains system-wide config files/system databases.\n`/proc` - contains all running processes displayed as their own files.\n`/tmp` - files that get cleaned frequently. Often, this directory gets cleared on reboot. \n`/var` - contains files that get changed often. usually where files go that are not managed by the system-wide package manager\n`/var/mail` - contains all incoming mail\n`/var/www/html` - default root folder of the web server (e.g. Apache)\n\n### Subdirectories\n`sys/` - operating system kernel, handling memory management, process scheduling, system calls etc.","n":0.045}}},{"i":73,"$":{"0":{"v":"Permissions","n":1},"1":{"v":"\n# File Permissions\n- There are 3 permission groups: Owner, Group, Other\n- each permission group has 3 permissions, called a permission set\n\t- ex. `rw-` is a single set\n- each file/directory has 3 permission sets— one for each permission group.\n- rwx mean different things if we are referring to a directory or a file\n\t- `r` \n\t\t- on file means we can read the contents \n\t\t- on directory means we can run `ls`\n\t- `w` \n\t\t- on file means we can modify file contents\n\t\t- on directory means we can add/delte files \n\t- `x`\n\t\t- on a file means we can run it\n\t\t- on a directory means we can `cd` into it \n\n\n### File ownership\n- every file is owned by a specific user (`UID`) and a specific group (`GID`)\n\t- `chown` is used to change both\n\t\t- ex. `chown <user>:<group> test.txt`\n- each member can belong to many groups (`/etc/group`), though a user can only have one primary group (`/etc/passwd`).\n\t- run `$ id` to see the groups the current user belongs to\n\t- When a user creates a file, the file will be owned by the primary group\n- similar to how we need to source the `.zshrc` before changes are live, we need to log out and log back in before group membership is \"activated\"\n\n### Settings Permissions\nThe numerical method is quite easy. For example, we can just replace each `rwx` set by it's binary positional value (from RTL: 1, 2, 4, 8, 16, 32...) and add the the numbers in each set.\n```\n-(rw-)(rw-)(r--)\n-(42-)(42-)(4--)\n664\n```\n- By this, we can define a `7` as `rwx`, a `5` as `r-x`, and so on\n\n* * *\n\nSet new files/directories in a subdirectory to follow the group ownership of the specified directory\n- `chmod g+s /var/www/my-project`\n\n[Unix permissions guide](https://support.plex.tv/articles/200288596-linux-permissions-guide/)","n":0.06}}},{"i":74,"$":{"0":{"v":"Inode","n":1},"1":{"v":"\n# inode\n- a data structure that describes a FS object (like a file/directory)\n- an inode is a virtual filesystem entity (in other words, it exists in the VFS world)\n- the inode stores metadata about the object, such as its disk block location, file size, file type \n- it also stores owner/permission data about the object\n- In reality, a directory is just a list of names that are each assigned to an inode\n\t- A directory contains an entry for itself, its parent, and each of its children.\n- on a UNIX system, files are user facing (ie. we work with them directly). the structure of a file exists only as a virtual FS entity in memory (there is no phsyical correspondent of it)\n- From the point of view of the underlying filesystem, the inode abstracts away the files that the user would directly interact with. From the point of view of the user, the file abstracts away the inode.\n- stands for *index node*\n\n## Dentry\nThe dentry (directory entry) associates an inode with a file name","n":0.076}}},{"i":75,"$":{"0":{"v":"Environment Variables","n":0.707},"1":{"v":"\n# Environments\nAn environment is created every time a shell is initialized\nan environment is just a map of key-value pairs\n- Each command is executed in its own environment, which includes (but not limited to):\n\t1. files that have been sourced\n\t2. current working directory\n\t3. functions defined during execution, or inherited from shell's parent in the environment\n- When a non-builtin command is executed, it is invoked in a separate execution environment.\n\n## Environment Variables\nSince every instance of a shell is a separate process, we have a different set of environment variables in each shell\n- they can be seen by running `env`\n\t- That isn’t all the variables that are set in your shell, though. It’s just the environment variables that are exported to processes that you start in the shell.\n- `compgen -v` allows us to see *all* variables available in shell\n- `export` allows us to add parameters and functions to the environment\n- When a non-builtin command is executed, it is invoked in a separate execution environment.\n\n### Export\n- Exported variables get passed on to child processes, not-exported variables do not.\n\t- When we use `export` in bash, we are adding the variable onto the shell's list of all env variables. This list is exclusive to the shell. When this shell creates a child process, all of these env variables are made available to it.\n\t- This means that if we only need the variable in the current environment, then we don't need to use `export`\n\nthe environment variables that an application can see are based on how the application was launched (from the dock, from the commandline, etc)","n":0.062}}},{"i":76,"$":{"0":{"v":"Daemons","n":1}}},{"i":77,"$":{"0":{"v":"Inetd","n":1},"1":{"v":"\n### Internet Daemon (inetd)\n- To preserve system resources, UNIX handles many of its services through the internet daemon (inetd), as opposed to a constantly running daemon.\n- inetd is a super server that listens to the various ports and handles connection requests as it receives them by initiating a new copy of the appropriate daemon (program). The new copy of the program then takes it from there and works with the client, and instead goes back to listening to the server ports waiting for new client requests to handle. Once the request is processed and the communication is over, the daemon exits.\n","n":0.1}}},{"i":78,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n# Misc\nIn Unix `<C-d>` on command line specifies an EOF character to end the input.","n":0.258}}},{"i":79,"$":{"0":{"v":"Zip","n":1},"1":{"v":"\n### File Compression\n- The core idea of file compression is: the content within a file has redundancies. Why not list data once, then simply back to it when it occurs again?\n\t- ex. the JFK quote: *\"Ask not what your country can do for you -- ask what you can do for your country.\"* has 17 words, made up of 61 letters, 16 spaces, 1 `-` and 1 `.`. If each character was 1 unit of memory, that would be 79 units. If instead of storing each instance of a word in memory, we just refered the repeated words back to the first occurrence, we would cut the phrase in half.\n- In reality, the compression program doesn't look for words, but looks for patterns. Therefore, the larger the file we are compression, the more compression there will be.\n\t- This is why binary files like mp3 don't compress well — there is hardly any repetition of patterns.","n":0.08}}},{"i":80,"$":{"0":{"v":"Xargs","n":1},"1":{"v":"\n`xargs` reads items from stdin, and runs a command (default `echo`) on each one\n- each item may either be delimited by:\n    1. blanks (which can be protected with double or single quotes or a backslash))\n    2. newlines\n\n#### Handling blanks and newlines\nBecause Unix filenames can contain blanks and newlines, this default behaviour is often problematic; filenames containing blanks and/or newlines are incorrectly processed by `xargs`.\n- in these situations pass `-0` option, which would prevent these problems, though we need to ensure that the program which produces the input for `xargs` also uses a null character as a separator","n":0.102}}},{"i":81,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Pass in grep results to a command\nImagine we wanted to run a command each file in the stdout of `eslint`:\n```\n/Users/kyletycholiz/programming/projects/project-zero/modules/client-ui-components/src/atoms/Radio/FontFaceRadio.js\n  62:50  warning  'onClick' is missing in props validation  react/prop-types\n\n/Users/kyletycholiz/programming/projects/project-zero/modules/client-ui-components/src/atoms/Radio/TextAlignRadio.js\n  38:36  warning  'children' is missing in props validation  react/prop-types\n```\nWe could use grep to get just the filenames, then run a command like `yarn lint | grep /Users/kyletycholiz | xargs npx react-proptypes-generate fix` to pipe those filenames into a command that takes an arbitrary number of args\n- this would be like running: `npx react-proptypes-generate fix FontFaceRadio.js TextAlignRadio.js`","n":0.108}}},{"i":82,"$":{"0":{"v":"Tar","n":1},"1":{"v":"\n# Tar\n\n### Compress a folder into a tarball\n`tar -czvf output.tar.gz input optional_input2`\n\n### Uncompress a tarball using a gzip compressor\n`tar xvzf file.tar.gz`\n\nSend to different directory than current\n`tar xvzf file.tar.gz -C ~/Downloads`\n\nspec: it seems that we have to cd into the directory that we will create the tar from. If we don't, then the resulting tar will take the whole absolute directory along with it, and when we uncompress it, we will get something like `/Users/kyletycholiz/tarred-file`\n- The same can definitely be said for `zip` utility","n":0.11}}},{"i":83,"$":{"0":{"v":"Ssh","n":1},"1":{"v":"\n\n\n\n# SSH Key\n## Components\n- Each SSH key pair has 2 components: public key and private key\n- They are both created with `ssh-keygen` \n### Public key\n- Public key is copied to the SSH server so it knows it can trust the user with the corresponding private key when it tries to connect via SSH\n- Anyone that has the public key is able to encrypt the data in such a way that only the bearer of the private key can decrypt it. \n- Once an SSH server receives the public key from a user, it marks the key as trustworthy and is moved to the `authorized_keys` file.\n- `ssh-copy-id` allows us to copy an SSH key from our local machine to a server to be used as an authorized key, allowing s the SSH client to login without password\n\t- What it does is it assembles a list of one or more fingerprints, and tries to log in with each key. It then makes a list of all the keys that failed login, and enables logins with those keys on the remote server (by adding them to the remote's `authorized_keys` file).\n- Public keys are stored on SSH clients \n\n### Private key\n- Considered proof of the user's identity, and only if the private key corresponds to the public key will the user be authenticated\n- Because they are used for authentication, they are called identity keys\n- The private key is stored on the SSH server\n- When we open the private key, what we see is its encrypted form. To decrypt it, we need to enter the password that we created with `ssh-keygen`.\n\t- Therefore, without the password, the private key is useless.\n\n* * *\n\n## SSH Agent\nssh-agent stores unencrypted private keys in memory, allowing us to login without entering our password each time.\n- Therefore, ssh-agent is a form of Single-Sign on (SSO)\n- Without ssh-agent, upon entering the password, the decrypted key is stored in program memory (which is associated with a process (PID)). However, the SSH client process cannot be used to store the decrypted key, since the process is terminated once the remote login session has ended. This is why we use `ssh-agent`\n\n`ssh-agent` works by creating a socket then checks for connections from ssh.\n- anyone who is able to connect to this socket can also connect to the ssh-agent.\n- upon starting, the agent creates a new directory in `/tmp` that determines permissions.\n\nOn most Linux systems, ssh-agent is automatically configured and run at login. \n\nOn the server side, `PubkeyAuthentication` must be enabled in the `ssh_config` file to allow these key-based logins. \n\nwe can use `ssh-add` tool to add identities to the agent.\n- running `ssh-add` without args will add the default private keys to the agent (`~/.ssh/id_rsa`, `~/.ssh/id_dsa` etc.)\n- `ssh-add -l` will list out the private keys accessible to the agent\n\n### Agent Forwarding\nAgent forwarding is a mechanism that gives an SSH server access to the SSH client's `ssh-agent`, allowing it to use it as if it were local. \nTo use agent forwarding, the `ForwardAgent` option must be set to yes on the client \n- This allows us to execute commands on the server we SSHed into and have access to keys stored in `ssh-agent`. \n\t- ex. if we are using keys to interact with Github (instead of entering password each time), we would be able to access them from the server. \n\t- However, there is an implication of this, which is that we would not be able to run these commands as cron jobs on the server. Once our SSH connection ends, the the server no longer has access to `ssh-agent`, so it cannot use the private key.\n\n## Debugging\n- check client logs with `ssh -vvv <user>@<host>`\n\n- check server logs at `/var/log/auth.log`\n\n- on server, try editing `/etc/ssh/sshd_config`, changing: \n\t1. `PasswordAuthentication yes`\n\t2. `PermitRootLogin yes`\n\t- restart sshd `service sshd restart`\n\t- on client, run `ssh-copy-id -i <remote-user>@<remote-ip>`\n\t- on host, revert the `sshd_config` options to their prior state\n\n* * *\n\n## Tunneling\n- In order to tunnel, we need to set up a protocol. SSH is an example of such a protocol, where the receiver of the SSH command must have an SSH server running so that it may intercept the SSH message, and interpret it and enable the SSH connection. \n\n* * *\n\nan OS can be preconfigured to enable SSH, so that it can be SHHed into the first time it is turned on.\n\n* * *\n\n## Config \nClient config (~/.ssh/config)\n[good resource with lots of info](https://gravitational.com/blog/ssh-config/)\n","n":0.037}}},{"i":84,"$":{"0":{"v":"Sed","n":1},"1":{"v":"\nSed is a steam editor\n- here a steam can be thought of as a body of text\n- sed works by reading text line-by-line into a buffer. For each line, it performs pre-defined instructions, if applicable.\n- Sed helps us automate the same sort of tasks that we'd accomplish manually by opening a textfile and making manual, predictable edits.\n- like Vim, if we substitute without the `g` flag, then only the first occurrence on the line will substituted.\n- though `/` is the most common delimiter, we can use (almost) anything, like `|` or `:`. This is helpful if the `/` is part of the pattern we want to substitute.\n\nWord-Boundary Expression\n- use `\\b` to disallow partial-word matches\n\t- ex. we want to replace `foo` with `kyle`, but want to leave `foobar` alone: `sed -i '' 's|\\bfoo\\b|kyle|g file.txt'`\n\n![[dendron://code/unix.cli.sed.formulas]]\n\n### UE Resources\nhttps://www.brianstorti.com/enough-sed-to-be-useful/\nhttps://linuxize.com/post/how-to-use-sed-to-find-and-replace-string-in-files/","n":0.086}}},{"i":85,"$":{"0":{"v":"Formulas","n":1},"1":{"v":"\n### Formulas\nFind and Replace a pattern in all files of a tree\n- find and replace all occurrences of `foo` with `bar` within a directory tree\n\t- `grep -rl 'foo' . | xargs sed -i .bk 's|foo|bar|g'`\n\t- the first part gets a list of all files that have the pattern `foo` in it, then it pipes that list into the second part, which runs the sed substitution on each file\n\t- if we don't want to create a backup, then replace .bk with ''\n\nFind and Replace a pattern in certain files\n- find and replace all occurrences of `foo` with `bar` in all .js files\n\t- `find . -name \"*.js\" -exec sed -i '' s/foo/bar/g {} +`\n","n":0.094}}},{"i":86,"$":{"0":{"v":"Less","n":1},"1":{"v":"\n- `-S<CR>` - enable horizontal scrolling","n":0.408}}},{"i":87,"$":{"0":{"v":"Kill","n":1},"1":{"v":"\n- Generally, you should use kill (short for kill -s TERM, or on most systems kill -15) before kill -9 (kill -s KILL) to give the target process a chance to clean up fter itself. (Processes can't catch or ignore SIGKILL, but they can and often do catch SIGTERM.) If you don't give the process a chance to finish what it's doing and clean up, it may leave corrupted files (or other state) around that it won't be able to understand once restarted.\n\n- Generally, you should use kill (short for kill -s TERM, or on most systems kill -15) before kill -9 (kill -s KILL) to give the target process a chance to clean up fter itself. (Processes can't catch or ignore SIGKILL, but they can and often do catch SIGTERM.) If you don't give the process a chance to finish what it's doing and clean up, it may leave corrupted files (or other state) around that it won't be able to understand once restarted.\n\t- strace/truss, ltrace and gdb are generally good ideas for looking at why a stuck process is stuck. (truss -u on Solaris is particularly helpful; I find ltrace too often presents arguments to library calls in an unusable format.) Solaris also has useful /proc-based tools, some of which have been ported to Linux. (pstack is often helpful).\n\n# SIGTERM vs SIGKILL\n- SIGKILL is a higher process killing a lower one, such as a kernel shutting down one of the applications that is currently running\n- SIGTERM is a request to the runner of the application to shut itself down.\n- analogy: we humans are the running of the application, and there is boss above us, giving us commands. SIGTERM would be the boss telling us to shut it down, which we then carry out. This is better, because I have information about what I'm currently working on that will allow me to shut down my process gracefully (ie. without losing data). Sometimes the runner of the application is not in a state where they can shut themselves down. SIGKILL on the other hand is like the boss coming by and forcefully shutting down my computer. The SIGTERM never knew about it, so it's possible something got screwed up while the runner (us) was in the middle of working on something.\n","n":0.051}}},{"i":88,"$":{"0":{"v":"Grep","n":1},"1":{"v":"\n`-l` - only print out the filenames that have the pattern\n`-r` - recurse, allowing us to run grep on directories\n\n- grep for lines with *pattern1* while filtering out lines with *pattern2* from *file.txt*\n    - `grep pattern1 file.txt | grep -v pattern2`\n","n":0.156}}},{"i":89,"$":{"0":{"v":"Fzf","n":1},"1":{"v":"\n# FZF\nFZF afts like an interactive filter. The responsibility of the program is not to get us the list of occurrences of our search pattern; that is the job of the search tool we use (eg. grep, ack, ag, find). FZF's only job is to take the list of occurrences as input, and perform the fuzzy searching logic on that list.\n- FZF's default search program is `find`, but it can be changed with the `FZF_DEFAULT_COMMAND` env variable.\n- Silver Searcher is good, because it respects the `.gitignore` and `.ignore` files.","n":0.106}}},{"i":90,"$":{"0":{"v":"Curl","n":1},"1":{"v":"\n## HTTP Requests\n\n#### Post\n```\ncurl -X POST [options] [URL]\n```\n\nexample\n```sh\ncurl -X POST -H \"Content-Type: application/json\" \\\n    -d '{\"email\": \"linuxize@example.com\", \"password\": \"123\"}' \\\n    http://localhost:5678/login\n```\n\n#### Put\n```\ncurl -X PUT -d arg=val -d arg2=val2 localhost:8080\n```\n\n#### Setting return value of curl to variable\n```\nhttp_code=$(curl -s -o /dev/null -w \"%{http_code}\" https://www.google.com)\n```","n":0.154}}},{"i":91,"$":{"0":{"v":"Cron","n":1},"1":{"v":"\nCron assumes that the machine is running continuously.\n- Therefore, it can't reliably be used on machines that aren't running 24 hours a day.\n- If this is an issue, then try [anacron](https://linux.die.net/man/8/anacron) (linux only), which removes this limitation\n    - Might be able to get a similar effect with [[launchd|mac.OS.launchd]] on OS X\n\nSee all active Crontabs:\n`crontab -e`\n","n":0.135}}},{"i":92,"$":{"0":{"v":"Ag","n":1},"1":{"v":"\n# Patterns\n- `ack <search-string> ../src`\n\n# Flags\n- `-i` - ignore case\n- `-w` - only match whole words\n- `-1` - stop searching after 1 match found\n- `-Q` - treat all characters as literal\n    - so if we use `/w` in the pattern, it is taken literally and will not mean \"match word only\"\n\n# Types\n- `ack --react <PATTERN>` - search all files with type react (js, jsx)\n- `--help-types` - see all types defined\n- `ack 'my pattern' ./src` - search for *my pattern* in *src* directory\n\n# Regex (Ag)\n- Ag is magic by default, meaning if we want to match literal characters, we need to escape them. all letters will be literal, and will need to be escaped to get their special meaning (ex. `\\d`, `/s`)\n    - ex. ag '.*\\ddog' - will match `sdfhj5dog` (anything any number of time,\n\tfollowed by a number, followed by 'dog')\n- see `man pcre2pattern` for regex flavors\n\n\nAll devDependencies should be at the root\neverything we need to build the proejct is at the root","n":0.079}}},{"i":93,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### search for pattern in a specific filename\n- ex. search for pattern 'react' within all package.json\n```\nfind . -name 'package.json' | ack -x 'react'\n```\n\nThis has a flaw though, where it will only ack the first result from find.","n":0.164}}},{"i":94,"$":{"0":{"v":"Binaries","n":1},"1":{"v":"\nnormally installed in `/sbin`, `/usr/sbin`, `/usr/local/bin` or `~` as a hidden directory\n","n":0.289}}},{"i":95,"$":{"0":{"v":"Unity","n":1},"1":{"v":"\nto allow enemies to walk into a new room, you must add to the walkable area by *baking* it\n\nmisallocation of GameObjects is a major factor in poor performance.\n\n## UI\n![](/assets/images/2021-08-15-16-09-42.png)\n(B) The Hierarchy window is a hierarchical text representation of every GameObject in the Scene\n(C) The Game view simulates what your final rendered game will look like through your Scene Cameras\n(D) The Scene view allows you to visually navigate and edit your Scene.\n(E) The Inspector Window allows you to view and edit all the properties of the currently selected GameObject.\n(F) The Project window displays your library of Assets that are available to use in your Project. When you import Assets into your Project, they appear here.\n\n# Organizing files\nWe can use a folder structure similar to MVC architectures:\n\n### Model\nRaw data classes\n- World class, Tile class, Map class etc.\n\n### View\nGameObjects\n\n### Controller\nScripts attached to the GameObjects\n\n# Resources\n[High-quality tutorials on fundamentals of graphics in Unity (fractals, shaders, abstract shapes etc)](https://catlikecoding.com/unity/tutorials/basics/)\n- this has a basis in programming and C# scripts\n\n[Socketweaver multiplayer sdk for unity](https://www.socketweaver.com/)\n- also look into Photon or Mirror","n":0.076}}},{"i":96,"$":{"0":{"v":"Terminology","n":1},"1":{"v":"\n*Project* - whole game you are building. Contains all scenes and all assets\n- you never \"save your project\", the only thing you save is the currently open scene. Scenes are containers which can contain one level or many levels, it's up to you to decide how you want to break things up.\n*Scene* - a level in the project. A scene holds related assets. A scene may also be used for the welcome screen, a config screen, intro and other movies, and a credits.\n \nGame view - the view that a player sees as they are playing the game. This is determined by the Main Camera GameObject\nScene view - the perspective of the developer building levels\n\n### Renderer \n- A renderer is what makes an object appear on the screen. \nThis class can be used to access the renderer of any object, mesh or Particle System.\n\n\n### Layers\nLayers allow us to define some functionality across different GameObjects\n\nA layer has no inherent functionality, and we must give it by specifying it on the affected GameObject (e.g. a camera)\nex. we can specify an object to be ignored by the camera with the `Culling Mask` attribute on the Camera.\n\nLayer is set at the top of the Inspector of a GameObject","n":0.07}}},{"i":97,"$":{"0":{"v":"Game Object","n":0.707},"1":{"v":"\nthe inspector window displays the properties of a single GameObject\n\nthe inspector is divided into *components*\n","n":0.258}}},{"i":98,"$":{"0":{"v":"Tag","n":1},"1":{"v":"\nTags are a way of identifying GameObjects.\n- ex. Imagine we have orcs, trolls and Urokhai in the enemy army. We can tag all 3 as `Enemy`, and in our code be able to easily check for all GameObjects with that tag.\n\nTags can be added by selecting a GameObject and using the dropdown at the top of the *inspector*\n","n":0.131}}},{"i":99,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get reference to GameObjects that have a Tag\n```cs\nenemy = GameObject.FindWithTag(\"Enemy\");\n// or find multiple\nenemy = GameObject.FindGameObjectsWithTag(\"Enemy\");\n```\n\n#### Check to see if the current gameObject has a particular tag\n```cs\nif (transform.position.x < tippingPoint && gameObject.CompareTag(\"Obstacle\")) {\n    Destroy(gameObject);\n}\n```","n":0.171}}},{"i":100,"$":{"0":{"v":"Particle","n":1},"1":{"v":"\nA particle is a GameObject, like smoke appearing behind tires screeching on the pavement.\n\n","n":0.267}}},{"i":101,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Attach particle to another GameObject's action (e.g. Projectile GameObject)\n1. set the particle GameObject to be a child GameObject of the Projectile\n2. get the particle from within the Projectile's script: \n    - `public ParticleSystem explosionParticle;`\n3. call `Play()` on the object: `explosionParticle.Play();` from within the condition that would trigger it. (e.g. on collision of projectile)\n4. attach the particle GameObject to the Projectile for the `Explosion Particle` param.\n```cs\n// ...\npublic ParticleSystem explosionParticle;\n// ...\nprivate void OnCollisionEnter(Collision collision) {\n    if (collision.gameObject.CompareTag(\"Ground\")) {\n        explosionParticle.Play();\n    }\n}\n```\n","n":0.113}}},{"i":102,"$":{"0":{"v":"Light","n":1},"1":{"v":"\n### Directional Light\nex. A simulation of the sun\n- Changing the position of the light will not have an influence on the scene but changing the rotation will change the angle of the light in the Scene.\n\n### Spot light\nex. A simulation of a torch\n\n# Resources\n[Unity: Guide on light](https://docs.unity3d.com/Manual/Lighting.html)\n","n":0.146}}},{"i":103,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Access public variable from different GameObject class\n```cs\n// assuming we have a PlayerController.cs script that is attached to a Player GameObject\nprivate PlayerController playerControllerScript;\n\nvoid Start() {\n    playerControllerScript = GameObject.Find(\"Player\").GetComponent<PlayerController>(); \n}\n\nvoid Update() {\n    // access gameOver variable from the PlayerController instance\n    if (!playerControllerScript.gameOver) {\n        transform.Translate(Vector3.left * Time.deltaTime * speed);\n    }\n}\n```\n","n":0.144}}},{"i":104,"$":{"0":{"v":"Component","n":1},"1":{"v":"\nComponents add behaviour and functionality to an object.\n\"component\" in this sense is like \"component of a GameObject\". Another good word would be \"aspect\", as in \"these are aspects of the object that give it *these* characteristics\n\nspec: Components would be implemented as methods on an object. A component is a kind of capability of the object. For instance a spatial object has a component of `transform`. That is, it can be positioned, rotated, and scaled.\n\n* * *\n\n### C# Script\n`create empty GameObject > Add component > new script`\n","n":0.108}}},{"i":105,"$":{"0":{"v":"Physical","n":1},"1":{"v":"\n### RigidBody\n`RigidBody` gives a GameObject physical properties so that it can interact with gravity, air density, and other GameObjects. \nRigidBody can be used to determine if an object should be frozen in 2D/3D space or not\n\n### Materials\n`Materials` are components that define the surface characteristics of objects and how those surfaces interact with light. In each new 3D Scene, directional light is included to simulate the sun.\n\nYou can drag a PNG into the assets folder of a Unity project, and by doing so, it will become a material (subfolder of `assets/`) that can be dragged onto an object of the scene\n\n#### Physic Materials\nWith Physic Materials, you can make an object bounce and change its friction and drag properties.These properties take effect when the object is under the effects of gravity.\n- in project window, right-click for dropdown and `create > physic material`\n- Collider components\n\n### Colliders\nThe sense of collision, in that you cannot pass through it.\nA collider has a material property, giving us the ability to drag one of our materials on top to give characteristics to the collision of an object\n- ex. if we have a physic material that gives the quality of bounciness, then we can apply that to the collider, which makes the underlying GameObject bouncy. \n\nObjects like spheres and cubes by default have collision\n\nColliders require a RigidBody to detect collision in our physics\n\n","n":0.067}}},{"i":106,"$":{"0":{"v":"Graphical","n":1},"1":{"v":"\n### Mesh\nA mesh is the main graphics primitive in Unity, and make up a large part of the 3D world\n- meshes turn into pixels that look like real objects\n- meshes can be made from either triangular or quadrangular polygons\nA mesh consists of triangles arranged in 3D space to create the impression of a solid object.\n- A triangle is defined by its three corner points or vertices (stored in a single array of integers; understood as each group of 3 belonging to a single vertex) <- so elements 0, 1 and 2 define the first triangle, 3, 4 and 5 define the second, and so on.\n\n### Skybox\nSkyboxes are rendered around the whole scene in order to give the impression of complex scenery at the horizon.\nInternally skyboxes are rendered after all opaque objects\n\n[How to make a Skybox](https://docs.unity3d.com/2019.2/Documentation/Manual/HOWTO-UseSkybox.html)\n","n":0.086}}},{"i":107,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get component of a GameObj within a script\n```cs\nprivate Rigidbody playerRb;\n// ...\n// can also pass any component, like `Collider`\nvoid Start() {\n    playerRb = GetComponent<Rigidbody>();\n}\nvoid Update() {\n    // cause player to jump (obvs we need a Spacebar click-handler here)\n    playerRb.AddForce(Vector3.up * jumpForce, ForceMode.Impulse);\n}\n```\n\n","n":0.154}}},{"i":108,"$":{"0":{"v":"Audio","n":1},"1":{"v":"\n### Audio source\n#### spatial blend \nThis property allows us to increase the volume of objects as a character gets nearer to them\n\n#### rolloff \nthis property defines its range in 3D space, and the rate at which it fades at greater distances and becomes inaudible. You can set the rolloff of your sounds to simulate the ways that different sounds carry.\n\n#### 3D sound settings \nthis property is the quality of audio that it changes orientation based on where the player is.\n- accomplished with an Audio Listener component (attached to MainCamera)\n\n- the the parameters of the 3D Sound Settings property control how volume and pitch can change based on the positions of the Audio Source and the Audio Listener.  \n![](/assets/images/2021-08-16-17-07-11.png)\n\n#### min/max distance\nIf a character is closer to a source of sound than the `minimum distance` (ie. character distance < minimum distance), then the audio plays at max volume\nThe `max distance` will define the most quiet that this object can possibly be\n- min and max denoted by the thin blue spheres around the red pot in the centre\n![](/assets/images/2021-08-16-17-14-38.png)\n\n","n":0.076}}},{"i":109,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Add sound effects to a Player/Unit\n1. add a variable of type `AudioClip` in a script attached to the GameObject to emit sounds (like `PlayerController`)\n2. attach the asset to the script (ie. pass it in as parameter in the Inspector)\n3. add an `Audio Source` component to the Player/Unit\n4. in the script file, get ahold of the component and run `audio.PlayOneShot` during the event.\n```cs\n// ...\n    private AudioSource playerAudio;\n    public AudioClip jumpSound;\n    public AudioClip crashSound;\n\n    void Start() {\n        playerAudio = GetComponent<AudioSource>();\n    }\n\n    void Update() {\n        // Jump\n        if (Input.GetKeyDown(KeyCode.Space)) {\n            // ...\n            playerAudio.PlayOneShot(jumpSound, 1.0f);\n        }\n    }\n```\n","n":0.103}}},{"i":110,"$":{"0":{"v":"Animator","n":1},"1":{"v":"\nThe animator is a [[state machine|general.patterns.state-machine]]\n\n### Animation Layers\nLayers serve to help us organize the complex state machine that is our Animator Component\nWe can use *Layers* to manage the state machine for a specific body part, or specific action\n- ex. Head, Body\n- ex. walking, jumping, running, dying, crouching\n\nWe can also use Layers in such a way where we have a layer for lower-body, and one for upper-body\n\nOn each layer, you can specify the mask (the part of the animated model on which the animation would be applied), and the Blending type. Override means information from other layers will be ignored, while Additive means that the animation will be added on top of previous layers.\n\n#### Mask\nThe Mask property is there to specify the mask used on this layer. For example if you wanted to play a throwing animation on just the upper body of your model, while having your character also able to walk, run or stand still at the same time, you would use a mask on the layer which plays the throwing animation where the upper body sections are defined, like this:\n![](/assets/images/2021-08-26-11-07-48.png)\nNote: the `M` symbol denotes that the layer has a mask applied\n","n":0.072}}},{"i":111,"$":{"0":{"v":"Script","n":1},"1":{"v":"\n### Set animator param value\n```cs\npublic class PlayerController : MonoBehaviour\n{\n    private Rigidbody playerRb;\n    private Animator playerAnim;\n\n    void Update() {\n        if (Input.GetKeyDown(KeyCode.Space) && isOnGround) {\n            playerRb.AddForce(Vector3.up * jumpForce, ForceMode.Impulse);\n            isOnGround = false;\n            playerAnim.SetTrigger(\"Jump_trig\");\n        }\n    }\n}\n```\n\nAlso: `SetFloat`, `SetBool` etc.\n","n":0.167}}},{"i":112,"$":{"0":{"v":"Camera","n":1},"1":{"v":"\n### Follow player\nIn a C# script (attached to the Main Camera), we can specify a new public variable of type `GameObject`. Then we can modify that variable's position attribute in the `update()` method. Then, as a final step we can drag an actual GameObject that we have defined (such as our Player) onto the script, as a way to pass the GameObject to the script.\n- this would be getting a reference to the Player GameObject\n```cs\n    public GameObject player;\n    // Update is called once per frame\n    void Update()\n    {\n        transform.position = player.transform.position;\n    }\n```\n","n":0.104}}},{"i":113,"$":{"0":{"v":"Editor","n":1},"1":{"v":"\n## Local vs Global\n(setting found at the top, next to the QWERTY tools)\nThis refers to `local` or `global` coordinates. If we are set to use `local`, then the directional arrows (x,y,z) of the GameObject will change with the moving object. If we are set to use `global`, then they will stay the same, even if the GameObject moves/rotates.\n- Imagine we have a ball rolling down a slope. If set to `local`, then the y-axis will move with the ball, since that is consistent with the coordinate system of the GameObject itself (ie. the local coordinates). If however we set it to `global`, then the y-axis would always point upward.\n\n* * *\n\n#### Importing `.unitypackage` files\nFrom the top menu in Unity, \nselect `Assets > Import Package > Custom Package`, then find the `.unitypackage` file in the file explorer.\n","n":0.086}}},{"i":114,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n# Tools (QWERTY)\nQ: Hand tool (move camera)\nW: Move tool (to select and change position)\nE: Rotate tool (select + rotate)\nR: Scale tool\nT:Rect Transform tool, to scale in 2D\n- combines move, rect and scale tools; only really useful for 2D\nY: Transform tool, to move, scale, and rotate with one Gizmo\n\n# Actions\nFocus (zoom into object) - `f`\nMove camera view to be same position as scene view - `<cmd+shift+f>`\n\n# Windows\nOpen console - `<cmd+shift+c>`\n\n# Camera movement\n- Hold right-click + WASD to fly to the vehicle, then try to rotate around it\n- With the vehicle selected and your mouse in the Scene view, Press F to focus on it\n- then use the scroll wheel to zoom in and out and hold the scroll wheel to pan\n- Hold alt+left-click to rotate around the focal point or hold alt+right-click to zoom in and out\n\n### Align view to selected\nselect a game object, select `GameObject` in menubar, Align view to selected\n\n# Resources\n[Asset store](https://assetstore.unity.com/)","n":0.081}}},{"i":115,"$":{"0":{"v":"Assets","n":1}}},{"i":116,"$":{"0":{"v":"Sprites","n":1},"1":{"v":"\n### Sorting\nspec: SortingLayer should be used in favor of z-index.\n\nthe render order of Renderers through their `Render Queue`.\n\nThere are 2 types of Render Queue:\n1. Opaque queue\n2. Transparent queue (the main one); this contains the `Sprite Renderer` , `Tilemap Renderer`, and `Sprite Shape Renderer` types.\n\nThe Camera component sorts Renderers based on its Projection setting, of which there are 2 options:\n1. Perspective - the sorting distance of a Renderer = the direct distance of the Renderer from the Camera's position.\n2. Orthographic - The sorting distance of a Renderer = the distance between the position of the Renderer and the Camera along the Camera’s view direction. \n\nsprites that belong together can be put into a `Sorting Group`, which share the same `Sorting Layer`, `Order in Layer` and `Distance to Camera`","n":0.089}}},{"i":117,"$":{"0":{"v":"Script","n":1},"1":{"v":"\nDevelopment for Unity is for a large part procedural in nature, not object-oriented. Unity is component-based. You're working with collections of data with behavior attached. This sounds like objects, but encapsulation and identity protection are not at all part of the architecture. Trying to force Unity to always adhere to a strict OOP standard is not practical. In fact, Unity Technologies is shifting focus to data-driven-design with ECS (entity component system) and DOTS (data oriented technology stack).\n- Data-oriented design puts the emphasis of coding on solving problems by prioritizing and organizing your data to make access to it, in memory, as efficient as possible. This contradicts the object-oriented principle that the design of code should be led by the model of the world you are creating.\n\nThe typical object-oriented workflow is to:\n1. Create a GameObject\n2. Add components to it\n3. Write `MonoBehaviour` scripts that change the properties of these components\n\n## Update\n`update()` runs on every frame. One device might get 20 fps, and another might get 60 fps, so the function will be called a disproportionate and unpredictable amount of times. Therefore we need to factor a time-frame into our calculations for code inside `update()`\n- For instance, if we want to move a vehicle forward (the higher the device's fps, the lower the `Time.deltaTime` value will be)\n    - If you multiply a value by `Time.deltaTime`, it will change it from 1x/frame to 1x/second\n\n```cs\n// move ~20 meters/second as long as the user is pressing up arrow\ntransform.Translate(Vector3.forward * Time.deltaTime * 20 * horizontalInput)\n```\nVector3.forward is shorthand for `(0,0,1)`, so when we multiply a vector, each value of the vector is multiplied. Therefore, a vector multiplied by an integer results in a vector\n- ie. `Vector3.forward * 20` results in `(0,0,20)`\n\n## Working with GameObjects\nYou can find a GameObject in the scene like this:\n```cs\nprivate GameObject playerObj = null;\n\nprivate void start() {\n    if (playerObj == null) {\n        playerObj = GameObject.Find(\"Player\");\n    }\n}\n\n```\n\nThen you can get access to things like position:\n```cs\nif (playerObj.transform.position.x > leftBoundary);\n```\n\n`MonoBehavior` is a class that is meant to be added to a component on a GameObject\n- Objects descended from `MonoBehavior` get instantiated through background serialization taking place in Unity\n\n## Misc\nScript template path:\n`/Applications/Unity/Hub/Editor/2020.3.16f1/Unity.app/Contents/Resources/ScriptTemplates`\n\n[gitignore template](https://github.com/github/gitignore/blob/master/Unity.gitignore)\n[.gitattributes file (for git-lfs)](https://gist.github.com/Tycholiz/d9d11c2e8cc8c0c898addc80631c5294)","n":0.053}}},{"i":118,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get user keyboard/controller input\n```cs\npublic float verticalInput;\npublic float horizontalInput;\n\nvoid Update()\n{\n    verticalInput = Input.GetAxis(\"Vertical\");\n    horizontalInput  = Input.GetAxis(\"Horizontal\");\n\n    // move the plane forward at a constant rate\n    transform.Translate(Vector3.forward * speed);\n\n    // tilt the plane up/down based on up/down arrow keys\n    transform.Rotate(Vector3.right * rotationSpeed * Time.deltaTime * verticalInput);\n\n    // steer the plane left/right based on arrow keys\n    transform.Rotate(Vector3.up * rotationSpeed * Time.deltaTime * horizontalInput);\n}\n\n```\n\n#### Get input based on key\n```cs\nif (Input.GetKeyUp(KeyCode.Space)) {\n    // instantiate a GameObject\n    Instantiate(projectilePrefab, transform.position, projectilePrefab.transform.rotation);\n}\n```\n\n#### Run a function every 2 seconds, starting after 1 second\n`InvokeRepeating(\"methodName\", 1, 2)`\n\n#### Get/Set(?) property of a component\n```cs\nprivate float repeatWidth;\nvoid Start() {\n    // get the x position value of the box collider component of the GameObject that this script it attached to\n    repeatWidth = GetComponent<BoxCollider>().size.x / 2;\n}\n```","n":0.091}}},{"i":119,"$":{"0":{"v":"Components","n":1}}},{"i":120,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\nNaturally, components can be controlled from scripts.\n\n#### Apply force to a GameObject\nFor instance, cause a player to jump, cause an enemy to fly backwards when hit etc.\n```cs\nprivate RigidBody playerRigidBody\nvoid Update() {\n    if (Input.GetKeyDown(KeyCode.Space)) {\n        playerRb = GetComponent<Rigidbody>();\n        playerRb.AddForce(Vector3.up * 1000);\n    }\n}\n```\n\n#### Detect a collision with the GameObject\nDefine a new method on your script's main class\n```cs\nprivate void OnCollisionEnter(Collision collision) {\n    // do the thing when there is a collision\n    if (collision.gameObject.CompareTag(\"Ground\")) {\n        isOnGround = true;\n    } else if (collision.gameObject.CompareTag(\"Obstacle\")) {\n        Debug.Log(\"Game over, bro\");\n        gameOver = true;\n    }\n}\n```","n":0.108}}},{"i":121,"$":{"0":{"v":"Prefab","n":1},"1":{"v":"\nPrefabs are equivalent to the concept of components in React.\n\nPrefabs are a special type of component that allows fully configured GameObjects to be saved in the Project for reuse among scenes.\n- A major benefit to Prefabs is that they are essentially linked copies of the asset that exist in the Project window, so changes made to the original prefab will propagate to the rest.\n    - furthermore, you can edit a single instance of a prefab, but it will not necessarily propagate to others \n        - the `revert` button allows us to go back to the master prefab\n        - the `apply` button allows us to apply the changes of the current prefab to the master, and therefore to all instances of it as well.\n\nPrefabs are created automatically when an object is dragged from the Hierarchy into the Project window. \n- prefabs look similar to other objects that appear in the Project window. When selected, you'll see that they are of filetype `.prefab`\n- When prefabs are present in the Hierarchy, they’re represented with blue text and a blue cube.\n\nWhen you convert an object to a Prefab, a new set of buttons is added above the Transforms values:\n![](/assets/images/2021-08-19-21-21-55.png)\n\n#### Creating from existing GameObject\nDrag GameObject from Hierarchy window into `prefab/` directory and choose to create `Original Prefab`\n\n#### Making changes to an instance and propagate changes to Prefab\nAfter making the changes, select a GameObject and in the top right of the Inspector window select `Overrides` and `Apply All`","n":0.064}}},{"i":122,"$":{"0":{"v":"TS","n":1},"1":{"v":"\nThe whole value of typescript in being able to define code that is type-safe, and have any non-type safe code to be uncovered *at* compile time. This inevitably saves us a large amount of effort, since we don't have to wait until runtime to be able to catch these bugs. It makes our code tighter, easier to reason about and more robust.\n- Generics is one way to implement this when writing functions\n\nTypeScript’s type system is very powerful because it allows expressing types in terms of other types. We can accomplish this via:\n- [[generics|ts.lang.generic]]\n- [[typeof operator|ts.lang.op.typeof]]\n- [[keyof operator|ts.lang.op.keyof]]\n- indexed access types\n- conditional types\n- mapped types\n- template literal types\n\nNaturally, any code that does not exist as far as Javascript is concerned (e.g. Generics, Interfaces) does not add to the runtime cost of the code.\n\n\n\n#### non-null assertion\nNon-null assertion operator shouldn't be used unless you have a very good reason to do so\n- Non-null assertion operators are simply a way of telling the compiler, \"trust me, I’m absolutely sure that this optional value is in fact never empty.\"\n- While you might think that a value is always non-empty, you could be wrong. Or worse, you could be correct at the moment, but it will not be the case in the future, because some other developer will call your function with a null or undefined.\n```ts\nfunction sayHello(person: Person | undefined) {\n    person!.hello(); // no errors!\n}\n```\n\nThe operator could come in handy, but generally our feeling for it should be that if we are reaching for it, there is probably a more correct way to do it.\n- sometimes the overhead of the type system is not justified, and we just want to create a quick escape hatch that makes sense. We have to address these case by case.\n\nAnother situation in which it might make sense to use this operator is when you want to enable strict null checks on a large, existing codebase. Enabling the flag in such a case might result in hundreds or thousands of errors. It’s a huge effort to go through them all at once. On the other hand, you cannot merge your changes to the master if the whole project doesn’t compile successfully. In this case, the non-null assertion operator might come in handy, as it will allow you to quickly suppress the errors that you don’t have time to fix properly.\n\n### Compiler\nWe can use ESBuild to compile typescript on server side","n":0.05}}},{"i":123,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n# Concepts\n### Type assignability\nWe say that type A is assignable to type B if you can use a value of type A in all places where a value of type B is required. For example:\n- assigning a value to a variable\n- passing a function argument\n- returning a value from a function that has an explicit return type\n\n### Optionality\nconsider that by default, `null` and `undefined` are part of the [[domain|dendron://thoughts-on/math.domain]] of virtually every type. That is, a string can be `null`, an object property can be `null`, and so on. When we enable strict null checking, we are removing `null` from the domains of these types\n![](/assets/images/2021-10-21-15-22-40.png)\n- `strictNullChecks` forces you to explicitly distinguish between values that can be `null` or `undefined` and those that cannot be.\n\nwe could explicitly extend the type to once again include `null` as part of the type's domain:\n```ts\nfunction bar(person: Person | null) {\n```\n\nBut this may not be preferable. Instead we can use optional params (in functions and interfaces) to determine if something can be optional or not:\n- also beneficial to use [[optional chaining|js.lang.op.optional-chaining]] to achieve the effect of optionality, (spec:)as opposed to type guards\n```ts\ninterface Person {}\n\nfunction hello(person?: Person) {}\n\ninterface Employee {\n  id: number;\n  name: string;\n  supervisorId?: number;\n}\n\nhello();\nconst employee: Employee = {\n  id: 1,\n  name: 'John',\n};\n/* * * * */\ninterface Person { hello(): void }\n\nfunction sayHello(person: Person | undefined) {\n  person.hello(); // 🔴 Error!\n\n    // this is a type guard, and it allows the Typescript Type checker to \n    // deduce that the type of person inside the if statement is narrowed to just `Person`\n    if (person) {\n        person.hello(); // OK\n    }\n}\n```\n\nConsider that a `getUser` function might not return a user for a given ID. Therefore, we should set the return value of the function as a union between User and undefined.\n```ts\ngetUser(id: number): User | undefined {\n  return this.users[id];\n}\n```\n\n* * *\n\nspec: If we don't want to specify a new type, we can just do this:\n```ts\ntype TodoPrevious = Pick<Todo, \"title\" | \"completed\">\n```\n\nHere, `\"title\" | \"completed\"` resolves to a type. We could have written it like this:\n```ts\ntype ValueKeys = \"tite\" | \"completed\"\ntype TodoPrevious = Pick<Todo, ValueKeys>\n```","n":0.054}}},{"i":124,"$":{"0":{"v":"Type","n":1}}},{"i":125,"$":{"0":{"v":"Util","n":1},"1":{"v":"\nA utility type is also known as a Type Constructor\n\nType constructor is a function (in the mathematical sense) that takes a type and returns a new type (based on the original type).\n- Notice that a generic interface with only one type argument is exactly that\n\nUtility types allow us to construct a new type, taking an initial type, and modifying it some way, determined by which utility type we use.\n\nThink of a utility type as a function that takes in 1+ types, and returns a new type\n\n## Examples\n- `Partial<Type>` - construct a new type with all properties of `Type`, but those properties are all optional.\n- `Required<Type>` - construct a new type with all properties of `Type`, but those properties are required. \n    - therefore opposite of Partial\n- `Record<Keys, Value>` - contruct a new object type, where the keys are of type `Keys` and the values are of type `Value`\n","n":0.082}}},{"i":126,"$":{"0":{"v":"Record","n":1},"1":{"v":"\n`Record<string, number>`\n\nconstructs an object type, whose keys are of type `string`, and properties are of type `number`\n\n```ts\nconst peopleAge: Record<string, number> = {\n    'James': 22,\n    'Frank': 45\n}\n```\n\nOf course, we can pass custom types as well:\n```ts\ninterface CatInfo {\n  age: number;\n  breed: string;\n}\n \ntype CatName = \"miffy\" | \"boris\" | \"mordred\";\n \nconst cats: Record<CatName, CatInfo> = {\n  miffy: { age: 10, breed: \"Persian\" },\n  boris: { age: 5, breed: \"Maine Coon\" },\n  mordred: { age: 16, breed: \"British Shorthair\" },\n};\n```\n\nWhat if we wanted to write a function that takes in an array of objects, and those objects could have any shape *as long as they have an id field*?\n```ts\nfunction getIds<TElement extends Record<'id', string>>(elements: TElement[]) {\n    return elements.map(element => element.id);\n}\n```","n":0.093}}},{"i":127,"$":{"0":{"v":"Pick","n":1},"1":{"v":"\n`Pick<Type, Keys>`\n- Constructs a type by picking the set of properties `Keys` (string literal or union of string literals) from `Type`.\n\nIn the below example, we are creating a new type `TypePreview` based on a subset of fields available on the `Todo` type.\n```ts\ninterface Todo {\n  title: string;\n  description: string;\n  completed: boolean;\n}\n\ntype TodoPreview = Pick<Todo, \"title\" | \"completed\">;\n\nconst todo: TodoPreview = {\n  title: \"Clean room\",\n  completed: false,\n};\n```\n","n":0.124}}},{"i":128,"$":{"0":{"v":"Partial","n":1},"1":{"v":"\nA Partial type is derived from an existing type. The only difference between the Partial type and the original type is that the Partial type has all properties set to optional.\n\nExample:\n```ts\ninterface Todo {\n\timportance: number;\n\ttext: string;\n}\n\nfunction updateTodo(todo: Todo, fieldsToUpdate: Partial<Todo>) {\n  return {\n\t\t...todo,\n\t\t...fieldsToUpdate,\n\t};\n}\n```\nAbove, the resulting Partial type would be like this:\n```ts\ninterface PartialTodo {\n\timportance?: number,\n\ttext?: string,\n}\n","n":0.136}}},{"i":129,"$":{"0":{"v":"Unknown","n":1},"1":{"v":"\nThis is similar to the `any` type, but is safer because it’s not legal to do anything with an `unknown` value\n\nIf a type is of type `unknown`, then it means the variable can be assigned to any type. At the moment of initializing an `unknown` variable, we are saying \"we don't know the type of this one. If you know it, feel tree to coerce it into its proper type. You can change it however you want.\"\n\nOn the other hand, it also means that we can't set the `unknown` value to a different type.\n```ts\n// This is ok\nlet x: unknown;\n\nx = 123; // no error\n/* * * * * * * * * * */\n// This is not\nfunction foo2(bar: unknown) {\n  const a: string = bar // error: Type 'unknown' is not assignable to type 'string'\n  //...\n}\n```\n\nBoth `any` and `unknown` represent an unknown type, but there is a key difference:\n- as demonstrated in the code above, all types are assignable to the `unknown` type, but `unknown` is not assignable to any type\n    - therefore `unknown` is a type-safe alternative to `any`\n\n![](/assets/images/2021-10-21-08-39-26.png)\n\nThis distinction between `any` and `unknown` is important, because it makes our code stricter. \n- If you have a value of `unknown` type, you need to cast it to some other type before you can do anything useful with it. The consequence of this is that `unknown` doesn’t propagate like any does, which is much safer.\n\n## Use-cases\nWhen you are working with external values (e.g. APIs), there is a good chance you won't know what the return type is. In such cases, it’s a good idea to type such value as `unknown` instead of `any`.\n```ts\ninterface Article {\n    title: string;\n    body: string;\n}\n\nfetch(\"http://example.com\")\n    .then(response => response.json())\n    .then((body: unknown) => {\n        const article = body as Article;\n        // we need to cast, otherwise we'd get an error\n        console.log(article.title);\n    });\n```\n\nIn this example, we use type assertion to tell TypeScript that we know the type of body. It’s still not type-safe because we could be wrong, but at least it’s explicit. The proper solution here would be to perform a runtime check to make sure that body is indeed an Article. We’ll look into such a solution in the lesson dedicated to user-defined type guards.\n","n":0.052}}},{"i":130,"$":{"0":{"v":"Union","n":1},"1":{"v":"\nThe pipe (`|`) in a union can be properly thought of as \"or\", since it means that \"this variable can be of one type or the other\"\n\nA Union is a way to achieve type composition (as are [[intersections|ts.type.intersection]])\n- composition being a way to create new types from existing types\n\nThe rule for union types is that we only allow an operation if it would be legal to do on each member of the union\n\n```ts\nlet ambiguouslyEmptyAlice: Person | null | undefined;\n```\n\nTake the following union:\n```ts\ninterface Foo {\n    foo: string;\n    xyz: string;\n}\n\ninterface Bar {\n    bar: string;\n    xyz: string;\n}\n\nconst sayHello = (obj: Foo | Bar) => { /* ... */ };\n```\nHere, we are saying \"`Foo | Bar` is a type that has either all required properties of `Foo` OR all required properties of `Bar`.\"\n- In the absence of a guard, we'd only be able to access `obj.xyz`, since that is the only property that exists on either type.\n\n\nUnion type is very often used with either null or undefined.\n```ts\nconst sayHello = (name: string | undefined) => { /* ... */ };\n```\n\n### Relation to Unions in statistics\nWhen we consider [[unions|math.set-theory.union]] in a statistical sense, it may seem counter-intuitive how Unions and Intersections are implemented in Typescript. \n- If have have a union type `string | number`, it means the type has to be either string or number.\n- If we have an intersection type, it means the type must have both.\n\nThe lack of clarity lies in the perspective we are taking. We are meant to take the perspective of what is accepted as the union type. \n- we say that the set of things that can be set to `string | number` is the union of the string set and the number set.\n    - ex. imagine there was a big list of all strings and all numbers that were loaded into memory. The union of `string | number` would be the set of all of them\n- we say that the set of things that can be set to `string & number` is zero, because there is zero overlap between `string & number`.\n\nPut another way, the incorrect way to see it is: \"a union is a set operator on the field\". The correct way is \"a union is a set operator of the sets of what each type represents.\"\n","n":0.051}}},{"i":131,"$":{"0":{"v":"Discriminated Union","n":0.707},"1":{"v":"\nDiscriminated unions are a concept closely related to [[state machines|general.patterns.state-machine]].\n\nDiscriminated unions allow us to compose types and enforce some business logic rules *at compile-time*.\nFor example, we can implement the following logic:\n> Customer needs to provide either email or phone number.\nAn obvious (and naive) solution to this is to just make both the `email` and `phone` properties optional; but then we end up with the possibility that neither is provided.\n\nInstead, we can create a new type `Contact` which is the union of 2 interfaces: 1 has a `phone` property, and the other has a `email` property:\n```ts\ninterface EmailContact {\n    kind: 'email';\n    email: string;\n}\n\ninterface PhoneContact {\n    kind: 'phone';\n    phone: number;\n}\n  \ntype Contact = EmailContact | PhoneContact;\n\ninterface Customer {\n    name: string;\n    contact: Contact;\n}\n\nfunction printCustomerContact({ contact }: Customer) {\n    // Here comes the power of discriminated unions. TypeScript analyses the code and sees an if statement that checks whether contact.kind is equal to \"email\". If that’s true, it can be certain that the type of contact is EmailContact, so it narrows the type inside the first if branch. The only other option for contact is to be a PhoneContact, so the type is narrowed to PhoneContact in the second if branch.\n    if (contact.kind === 'email') {\n        // Type of `contact` is `EmailContact`!\n        console.log(contact.email);\n    } else {\n        // Type of `contact` is `PhoneContact`!\n        console.log(contact.phone);\n    }\n}\n```\n\nEach union member follows a special convention of having a `kind` property. This property is called a *discriminator*, and it encodes type information into the object so that it is available at runtime. Now, TypeScript can figure out which union member you’re dealing with by looking at the `kind` property.\n- `kind` is just a convention, and it can be any word.\n\n\n### Example\nUsers place orders for products. Users have contact information, email or postal addresses, and at least one is required. Orders should include price, product name, quantity, payment date, paid amount, sending date, and delivery date.\n```ts\ntype Customer = {\n  name: string;\n  contactInfo: ContactInfo;\n};\n\ntype ContactInfo =\n  | { kind: \"emailOnly\"; email: string }\n  | { kind: \"postalOnly\"; address: string }\n  | { kind: \"emailAndPostal\"; email: string; address: string };\n\ntype PaidOrderData = { paymentDate: Date; amount: number };\ntype SentOrderData = { sendingDate: Date };\ntype DeliveredOrderData = { deliveryDate: Date };\n\ntype OrderState =\n  | { kind: \"new\" }\n  | { kind: \"paid\"; paidData: PaidOrderData }\n  | { kind: \"sent\"; paidData: PaidOrderData; sentData: SentOrderData }\n  | {\n      kind: \"delivered\";\n      data: PaidOrderData;\n      sentData: SentOrderData;\n      deliveredData: DeliveredOrderData;\n    };\n\ntype Order = {\n  customer: Customer;\n  state: OrderState;\n  productName: string;\n  price: number;\n  quantity: number;\n};\n```\n\n### Multi-value Discriminator\na discriminator does not have to be a single property. A group of literal properties can also act as a discriminator! In such a case, every combination of values marks a different member of the union type.\n```ts\ntype Foo =\n  | { kind: 'A', type: 'X', abc: string }\n  | { kind: 'A', type: 'Y', xyz: string }\n  | { kind: 'B', type: 'X', rty: string }\n\ndeclare const foo: Foo;\n\nif (foo.kind === 'A' && foo.type === 'X') {\n    // here, the intellisense on `foo` would show how it's type is narrowed to the only type it could be, namely the first one shown in `Foo`\n  console.log(foo.abc);\n}\n```\n\n","n":0.044}}},{"i":132,"$":{"0":{"v":"Intersection","n":1},"1":{"v":"\nAn Intersection is a way to achieve type composition (as are [[unions|ts.type.union]])\n\nConsider the following intersection:\n```ts\nconst sayHello = (obj: Foo & Bar) => { /* ... */ };\n```\n\nHere, we are saying that `obj` must have *all* properties that are included on both `Foo` and `Bar`. \n","n":0.149}}},{"i":133,"$":{"0":{"v":"Interface","n":1},"1":{"v":"\n```ts\ninterface SquareConfig {\n  color?: string;\n  width?: number;\n}\n```\n","n":0.378}}},{"i":134,"$":{"0":{"v":"Classes","n":1},"1":{"v":"\n```ts\nclass HeaderComponent {\n    // with `strictPropertyInitialization` enabled, we must set properties in the constructor\n    constructor(private header: string) {}\n\n    render() {\n        return `<h1>${this.header.toUpperCase()}</h1>`;\n    \n    constructor(private header: string) {}\n\n    render() {\n        return `<h1>${this.header.toUpperCase()}</h1>`;\n    }\n}\n```\n","n":0.177}}},{"i":135,"$":{"0":{"v":"Assertion","n":1},"1":{"v":"\nType assertions helps you to force types when you are not in control of them. For e.g. -\n1. you are processing user data (with unreliable types)\n2. working with data that has changed shape over years (employee code was numeric, now it is alphanumeric :))\n3. receiving data from an external program\n\nType assertions let the Typescript compiler know that a given variable should be treated as belonging to a certain type. There are no “exceptions” or data restructuring associated with assertions, except minimal validations (we refer this behaviour as “validations that are applied statically”).\n\nThere are two ways to do type assertions.\n1. Bracket syntax. e.g. `let length: number = (<string>lengthField);`\n2. Use as. e.g. `let length: number = lengthField as string;`\nThere is no difference between the two ways and the end result is always the same. Note that if you are using JSX you have to use `as` syntax.\n","n":0.083}}},{"i":136,"$":{"0":{"v":"TS Config","n":0.707},"1":{"v":"\n\n## tsconfig.json\n```json\n{\n  // some other settings\n  \"compilerOptions\": {\n    // some other options\n    \"strictNullChecks\": true,\n    \"strictPropertyInitialization\": true,\n    \"noImplicitAny\": true,\n  }\n}\n```\n\n`strictNullChecks` - see [[ts.types#optionality]]\n\n# Resources\n[tsconfig options](https://www.typescriptlang.org/docs/handbook/compiler-options.html)\n","n":0.204}}},{"i":137,"$":{"0":{"v":"Overloading","n":1},"1":{"v":"\nTypescript supports [[overloading|paradigm.oop.overloading]]\n\nIn TS, we can overload a function by having multiple call signatures, though it is still the same physical function at runtime (of course, since TS doesn't exist at runtime)\n- From the point of view of the caller, this is virtually the same as languages that properly have overloading.\n\nThe point of overload signatures is that it allows you to express certain function contracts that can't be safely implemented in TS.\n- So almost any overloaded function is going to have some casting, or `any`, or some such 'type unsafeness', otherwise you wouldn't need the overloads\n\nWhen implementing overloads, you're making things stricter for the caller at the expense of some safety inside the function\n\nIn the presence of overloads, the implementation signature is invisible to callers\n\nExample:\nStripe has 2 different versions of `stripe.paymentIntents.list`. The first version looks like this:\n```\n(params?: PaymentIntentListParams, options?: RequestOptions): ApiListPromise<PaymentIntent>\n```\n\nThe second looks like this:\n```\n(options?: RequestOptions): ApiListPromise<PaymentIntent>\n```\n\nThe benefit to doing this is that we can call the same function, even if we don't want to pass in a `params` object. When we call `stripe.paymentIntents.list`, the TS compiler will figure out which version of the function it should call, based on the arguments that are passed to the function call. This process is what is known as Method overloading.\n- If we call `stripe.paymentIntents.list` and our arguments don't line up with the first function signature (ex. because one of the args we pass doesn't line up with `PaymentIntentListParams` interface), then TS will attempt to use the second version of the function. This will result in Intellisense spitting both errors at us, even though only the first one is what we should pay attention to\n","n":0.061}}},{"i":138,"$":{"0":{"v":"Lang","n":1}}},{"i":139,"$":{"0":{"v":"Op","n":1}}},{"i":140,"$":{"0":{"v":"Typeof","n":1},"1":{"v":"\nWhile Javascript already has `typeof` that can be used in an expression context, Typescript adds a `typeof` that can be used in a type context.\n- that is, we can use it to refer to the type of a property/variable.\n- it’s only legal to use `typeof` on identifiers (i.e. variable names) or their properties\n\nLike `keyof`, `typeof` is used to create new types\n\n```ts\nlet s = \"hello\";\nlet n: typeof s;\n```\n\ntypeof can be used to conveniently express many patterns.\n- imagine we want to grab the return type of function `f()` for usage elsewhere:\n```ts\nfunction f() {\n  return { x: 10, y: 3 };\n}\n// note: ReturnType is a built-in, and gives us the return type of a given function\ntype P = ReturnType<typeof f>;\n```\n","n":0.092}}},{"i":141,"$":{"0":{"v":"Keyof","n":1},"1":{"v":"\nThe `keyof` operator takes an object type and produces a union of its keys\n\nLike `typeof`, `keyof` is used to create new types\n\nFor any type `T`, `keyof T` is the union of public property names of `T`\n```ts\ninterface Person {\n  age: number;\n  name: string;\n}\n\ntype PersonKeys = keyof Person; // \"age\" | \"name\"\n```\n\nThe type `PersonKeys` here is the same as:\n```ts\ntype PersonKeys = \"age\" | \"name\"\n```\n\nIf the type uses a string or number as the index, then `keyof` will return those types instead:\n```ts\ninterface Arrayish = {\n    [n: number]: unknown\n}\ntype A = keyof Arrayish\n// A = number\n```\n","n":0.104}}},{"i":142,"$":{"0":{"v":"Generic","n":1},"1":{"v":"\nTS takes a more parametric approach to generics \n- ie. the type is a variable being passed in just like the values, and the type is unknown while writing the function, which necessarily limits what you can do with it\n\nWe must pass in the type parameters (e.g. `<T>`), because otherwise Typescript wouldn't know if `T` is a type argument or an actual type.\n\nIt's critical to remember that everything in TS happens at compile-time. Therefore, against what your intuition might suggest, generic types are not checked when the function is called (ie. run-time). This is the beauty of Typescript, because every type must be known in advance, even though the code has not even been run yet. Therefore, we need to be strict in our typing, which means allowing for a given variable to be any type that it may be *depending* on the conditions happening during runtime.\nex. consider that while an API is *expected* to return some data of a certain structure, that may not always be the case. Perhaps the network request fails, and the variable you are trying to set remains `undefined`. Therefore your type system must be smart enough to recognize this possibility. You must therefore build that possibility into the system.\n\n## Generic constraints (ie. `extend`ing generics)\n### Constraining type parameters by interfaces\nImagine we have a function that returns the length of the argument\n```ts\nfunction getLength<T>(arg: T): number {\n    return arg.length\n}\n```\n\nWritten like this, only string and array would be able to work, and everything else would throw an error.\nTo solve this, we can use an interface and extend the generic:\n```ts\ninterface Lengthwise {\n  length: number;\n}\nfunction getLength<T extends Lengthwise>(arg: T): number {\n// ...\n```\n### Constraining type parameters by other type parameters\nThis generic constraint basically says \"`Key` can only be a public property in `Type`\".\n- It has nothing to do with extending a type or inheritance, contrary to extending interfaces.\n```ts\nfunction getProperty<Type, Key extends keyof Type>(obj: Type, key: Key) {\n  return obj[key];\n}\n\nconst person: Person = {\n  age: 22,\n  name: \"Tobias\",\n};\n\n// name is a property of person\n// --> no error\nconst name = getProperty(person, \"name\");\n\n// gender is not a property of person\n// --> error\nconst gender = getProperty(person, \"gender\");\n```\n\n* * *\n\n### Default generic parameter\nIn Javascript, we can set a default parameter if one isn't passed:\n```js\nfunction registerName(fullName = 'John Doe') {\n```\n\nIn Typescript, we can set a default generic type, if the type isn't passed:\n- this means that if `Data` type isn't passed (which must extend to `DataType`), then the generic will be `DataType`\n```ts\nexport interface FormProps<Data extends DataType = DataType> { }\n```\n\n#### Example: Implementing HTML\n- type `T` of the following signature defaults to a `div` element. If `create` is called with its first argument, then it will extend an `HTMLElement` type.\n- type `U` defaults to an array of type `T`. If `create` is called with its second argument, `U` becomes the type of whatever was passed in\n    - ex. if a `<p>` was passed as a child of the main element being created, then `U` = `HTMLElement`. If an array of `<p>` was passed, then `U` = `HTMLElement[]`\n```ts\ndeclare function create<\n    T extends HTMLElement = HTMLDivElement, \n    U = T[]\n>(element?: T, children?: U): Container<T, U>;\n```\n\nA generic parameter default follows the following rules:\n- A type parameter is deemed optional if it has a default.\n- Required type parameters must not follow optional type parameters.\n- Default types for a type parameter must satisfy the constraint for the type parameter, if it exists.\n- When specifying type arguments, you are only required to specify type arguments for the required type parameters. Unspecified type parameters will resolve to their default types.\n- If a default type is specified and inference cannot chose a candidate, the default type is inferred.\n- A class or interface declaration that merges with an existing class or interface declaration may introduce a default for an existing type parameter.\n- A class or interface declaration that merges with an existing class or interface declaration may introduce a new type parameter as long as it specifies a default.\n\n* * *\n\n### Unbound Type Variable\nTake the following example:\n```ts\nfunction foo<T>(): T {}\nlet x = foo();\n// what type is x? the world may never know...\n```\n\n`foo` can use `T` inside the implementation, but it has to treat it as basically `unknown`, because it could be anything at all\n- put another way, `T` is `unknown` during compile-time, and only known during run-time.\nfrom the caller's perspective, if you haven't given it something to infer `T` from, it will just default to its constraint. And if there's no constraint, that means `unknown`\n\nHere's some examples that help illustrate the issue:\n```ts\ndeclare function reduce <T, Acc>(inputArr: T[], reducerMethod: (accumulator: Acc, currVal: T) => Acc, initialValue?: Acc): Acc;\n\nreduce([\"string\"], (acc, val) => acc.concat(val), [] as string[])\n// reduce<string, string[]>, 😀\nreduce([\"string\"], (acc, val) => acc.concat(val))\n//                               ^^^\n// Object is of type 'unknown'.\n// reduce<string, unknown>, errors with a somewhat mysterious error 🤔\nreduce<string, string[]>([\"string\"], (acc, val) => acc.concat(val));\n// reduce<string, string[]>, compiles 😭\n```","n":0.035}}},{"i":143,"$":{"0":{"v":"Interface","n":1},"1":{"v":"\nGeneric interfaces are very much like functions that transform types.\n\nA generic interface lets us define a function signature, and enforce that signature on a future value that we define. Put another way, we are describing a generic function\n```ts\ninterface GenericIdentityFn {\n  <Type>(arg: Type): Type;\n}\n \nfunction identity<Type>(arg: Type): Type {\n  return arg;\n}\n \nlet myIdentity: GenericIdentityFn = identity;\n```\n\nIn this case, when we use `GenericIdentityFn` we now will also need to specify the corresponding type argument (here: `number`), effectively locking in what the underlying call signature will use. This changes its use markedly from the previous implementation.\n```ts\ninterface GenericIdentityFn<Type> {\n  (arg: Type): Type;\n}\n \nfunction identity<Type>(arg: Type): Type {\n  return arg;\n}\n \nlet myIdentity: GenericIdentityFn<number> = identity;\n```\n\nThis changes it slightly. Now we have a non-generic function signature that is a part of the generic type\n- Understanding when to put the type parameter directly on the call signature and when to put it on the interface itself will be helpful in describing what aspects of a type are generic.\n\nIt’s not possible to refer to a generic interface without providing its type argument.\n\nFor example, if you want to write a function that accepts an instance of a generic interface, you have two options:\n1. Write a specialized function that accepts a very specific instance of the interface (e.g. `FormField<string>`)\n2. Write a generic function\n```ts\ninterface FormField<T> {\n    value?: T;\n    defaultValue: T;\n    isValid: boolean;\n}\n\n// Very specialized. Only works with `FormField<string>`.\nfunction getStringFieldValue(field: FormField<string>): string {\n    if (!field.isValid || field.value === undefined) {\n        // Thanks to the specialization, the compiler knows the exact type of `field.defaultValue`.\n        return field.defaultValue.toLowerCase();\n    }\n    return field.value;\n}\n\n// Generic. Can be called with any `FormField`.\n// It’s important to understand that here, T is the type argument of the function, not of the FormField interface. It gets passed only to FormField like any regular type does.\nfunction getFieldValue<T>(field: FormField<T>): T {\n    if (!field.isValid || field.value === undefined) {\n        // On the other hand, we don't know anything about the type of `field.defaultValue`.\n        return field.defaultValue;\n    }\n    return field.value;\n}\n```\n\n### Generic constraints on interfaces (Type argument constraint)\nyou might decide to enforce the fact that `FormField` should only contain string, number, or boolean values.\n```ts\ninterface FormField<T extends string | number | boolean> {\n    value?: T;\n    defaultValue: T;\n    isValid: boolean;\n}\n\n// Type 'T' does not satisfy the constraint 'string | number | boolean'\nfunction getFieldValue<T>(field: FormField<T>): T { /* ... */ }\n```\n\nThe reason for this error is that the `T` type argument of the function has no restrictions. You’re trying to pass it to `FormField` which only accepts types that extend string, number or boolean. Therefore, you get a compile error. To get rid of the error, you need to put the same or stricter restrictions on the type argument `T`.\n\n```ts\ninterface FormField<T extends string | number | boolean> {\n    value?: T;\n    defaultValue: T;\n    isValid: boolean;\n}\n\n// Type 'T' does not satisfy the constraint 'string | number | boolean'\nfunction getFieldValue<T extends string | number>(field: FormField<T>): T {\n    return field.value ?? field.defaultValue;\n}\n```","n":0.046}}},{"i":144,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Object where keys are integers as string\n```ts\ninterface IDictionary<TValue> {\n    [id: string]: TValue;\n}\n\n// later\nconst myObject: IDictionary<string> = { \"0\": \"teleport\" }\n```\n\n### Object where keys are not known ahead of time\n[source](https://stackoverflow.com/questions/12710905/how-do-i-dynamically-assign-properties-to-an-object-in-typescript#answer-44441178)\n```ts\nconst constants: Record<string, any> = {}\n\nconstants.dendronTechUrl = 'https://tycholiz.github.io/Digital-Garden/'\n\nexport default constants\n```","n":0.16}}},{"i":145,"$":{"0":{"v":"Function","n":1},"1":{"v":"\n\nThe following function uses closure to remember the pair\n```ts\nfunction makePair<F, S>() {\n  let pair: { first: F; second: S }\n  function getPair() {\n    return pair\n  }\n  function setPair(x: F, y: S) {\n    pair = {\n      first: x,\n      second: y\n    }\n  }\n  return { getPair, setPair }\n}\n\nconst { getPair, setPair } = makePair<string, number>()\nsetPair(5, 3)\n// error: Argument of type 'number' is not assignable to parameter of type 'string'.\n\nsetPair(\"age\", 3)\n// all good!\n```\n","n":0.12}}},{"i":146,"$":{"0":{"v":"Class","n":1},"1":{"v":"\n```ts\nclass GenericNumber<NumType> {\n  zeroValue: NumType;\n  add: (x: NumType, y: NumType) => NumType;\n}\n \nlet myGenericNumber = new GenericNumber<number>();\nmyGenericNumber.zeroValue = 0;\nmyGenericNumber.add = function (x, y) {\n  return x + y;\n};\n```\n\n","n":0.189}}},{"i":147,"$":{"0":{"v":"Generic","n":1}}},{"i":148,"$":{"0":{"v":"Examples","n":1},"1":{"v":"\n### Reduce Function with double type coercion and overloading\n```ts\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn, currVal: TElement) => TReturn\n): TReturn\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn, currVal: TElement) => TReturn, \n    initialValue: TReturn\n): TReturn\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn, currVal: TElement) => TReturn, \n    initialValue?: TReturn\n): TReturn {\n    let acc: TReturn\n    let currentIndex: number\n\n    if (initialValue === undefined) {\n        acc = inputArr[0] as any as TReturn\n        currentIndex = 1\n    } else {\n        acc = initialValue\n        currentIndex = 0\n    }\n\n    while (currentIndex < inputArr.length) {\n        const currentVal = inputArr[currentIndex]\n        acc = reducerMethod(acc, currentVal)\n        currentIndex++\n    }\n    return acc\n}\n```\n\n### Reduce function with unions\n```ts\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn | TElement, currVal: TElement) => TReturn\n): TReturn\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn | TElement, currVal: TElement) => TReturn, \n    initialValue: TReturn\n): TReturn\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn | TElement, currVal: TElement) => TReturn, \n    initialValue?: TReturn\n): TReturn | TElement {\n    let acc: TReturn | TElement\n    let currentIndex: number\n\n    if (initialValue === undefined) {\n        acc = inputArr[0]\n        currentIndex = 1\n    } else {\n        acc = initialValue\n        currentIndex = 0\n    }\n\n    while (currentIndex < inputArr.length) {\n        const currentVal = inputArr[currentIndex]\n        acc = reducerMethod(acc, currentVal)\n        currentIndex++\n    }\n    return acc\n}\n```","n":0.069}}},{"i":149,"$":{"0":{"v":"Fetch","n":1},"1":{"v":"\nThe fetch API can't know in advance what the shape of the returned objects are going to be, so you have to define it and assert that he results conform to it. We can define an interface for the object returned by the fetch request.\n","n":0.149}}},{"i":150,"$":{"0":{"v":"Examples","n":1},"1":{"v":"\n### Making optional code more strict\n\nBefore\n```ts\ninterface Address {\n  street: string;\n  city?: string;\n}\n\ninterface User {\n  name: string;\n  address: Address;\n  meta: Record<string, string>;\n}\n\ninterface SuperUser extends User {\n  permissions: string[];\n}\n\nclass UserRepository {\n  private users: User[];\n\n  constructor() {\n    this.users = [\n      // Do not change the data. Let's assume it comes from the backend.\n      {\n        name: \"John\",\n        address: undefined,\n        meta: { created: \"2019/01/03\" }\n      },\n      {\n        name: \"Anne\",\n        address: { street: \"Warsaw\" },\n        meta: {\n          created: \"2019/01/05\",\n          modified: \"2019/04/02\"\n        }\n      }\n    ];\n  }\n\n  getUser(id: number): User | undefined {\n    return this.users[id];\n  }\n\n  getCities() {\n    return this.users\n      .filter(user => user.address?.city)\n      .map(user => user.address.city);\n  }\n\n  forEachUser(action: (user: User) => void) {\n    this.users.forEach(user => action(user));\n  }\n}\n\nconst userRepository = new UserRepository();\n\nconsole.log(userRepository.getUser(1).address.city.toLowerCase());\n\nconsole.log(\n  userRepository\n    .getCities()\n    .map(city => city.toUpperCase())\n    .join(\", \")\n);\n\nconsole.log(new Date(userRepository.getUser(0).meta.modfified).getFullYear());\n```\n\nAfter\n```ts\ninterface Address {\n  street: string;\n  city?: string;\n}\n\ninterface User {\n  name: string;\n  address: Address | undefined; /* 1 */\n  meta: Record<string, string>;\n}\n\ninterface SuperUser extends User {\n  permissions: string[];\n}\n\nclass SafeUserRepository {\n  private users: User[];\n\n    /* 2 */\n  constructor() {\n    this.users = [\n      // Do not change the data. Let's assume it comes from the backend.\n      {\n        name: \"John\",\n        address: undefined,\n        meta: { created: \"2019/01/03\" }\n      },\n      {\n        name: \"Anne\",\n        address: { street: \"Warsaw\" },\n        meta: {\n          created: \"2019/01/05\",\n          modified: \"2019/04/02\"\n        }\n      }\n    ];\n  }\n\n  // `user` with given `id` might not exist, so marking the return type as possibly undefined\n  getUser(id: number /* 3 */): User | undefined /* 4 */ {\n    return this.users[id];\n  }\n\n  getCities() {\n    return this.users\n        /* 5 */\n      .map(user => user.address?.city)\n      .filter(city => city !== undefined);\n  }\n\n  forEachUser(action: (user: User) => void) {\n    this.users.forEach(user => action(user));\n  }\n}\n\nconst safeUserRepository = new SafeUserRepository();\n\n/* 6 */\nconsole.log(safeUserRepository.getUser(1)?.address?.city?.toLowerCase());\n\nconsole.log(\n  safeUserRepository\n    .getCities()\n    /* 7 */\n    .map(city => city?.toUpperCase())\n    .join(\", \")\n);\n\n/* 8 */\nconsole.log(new Date(safeUserRepository?.getUser(0)?.meta.modfified ?? 0).getFullYear());\n```\n","n":0.06}}},{"i":151,"$":{"0":{"v":"Declaration Merging","n":0.707},"1":{"v":"\n## Overview\nA fundamental concept in TS is the language's ability to describe the shape of javascript objects at the type level.\n- One example of the implementation of this concept is the idea of **Declaration Merging**\n\nIn TS, a declaration creates entities in at least one of three groups: namespace, type, or value.\n- Namespace-creating declarations create a namespace, which contains names that are accessed using a dotted notation.\n- Type-creating declarations do just that: they create a type that is visible with the declared shape and bound to the given name.\n- Value-creating declarations create values that are visible in the output JavaScript.\n![](/assets/images/2021-04-04-13-05-32.png)\n\n## Interface merging\nthe merge mechanically joins the members of both declarations into a single interface with the same name.\n```ts\ninterface Box {\n  height: number;\n  width: number;\n}\ninterface Box {\n  scale: number;\n}\nlet box: Box = { height: 5, width: 6, scale: 10 };\n```\n\nIf 2 objects have a function member with the same name but different signatures, then they will be [[overloaded|paradigm.oop.overloading]], and both functions will appear on the merged object:\n```ts\ninterface Cloner {\n  clone(animal: Animal): Animal;\n}\ninterface Cloner {\n  clone(animal: Sheep): Sheep;\n}\n\n// The two interfaces merged will create a single declaration:\ninterface Cloner {\n  clone(animal: Animal): Animal;\n  clone(animal: Sheep): Sheep;\n}\n","n":0.072}}},{"i":152,"$":{"0":{"v":"Declaration File","n":0.707},"1":{"v":"\nTypeScript is a superset of JavaScript, meaning you can write and use JavaScript libraries from within TypeScript. In such situations how should TypeScript handle the lack of Type information in JavaScript? We can either:\n- Accept the lack of types from the JS file\n- use Declaration files, which give us an ad-hoc way to specify the shape of things in Javascript.\n\n`.d.ts` files are analogous to C++ header files.\n\n### Incorporating type declarations from third parties\nWhen we want to use Javascript 3rd party libraries in our TS project, we can often find declaration modules as packages\n- for example, `@types/lodash` is a module that gives us types out of the box with Lodash.\n- these declaration files get saved in `node_modules/@types/`\n\nWhen a type declaration is included in the `@types` directory, the types are automatically available for us to use in our project, as the TS compiler finds these types and makes them available during compilation time.\n\n[DefinitelyTyped](http://definitelytyped.org/) is a project that hosts declaration files for popular JS libraries.\n\n### Config\n\"typeRoots\" and \"types\" are the two properties of the tsconfig.json file that can be used to configure the behavior of the type declaration resolution.\n\n### Example\nImagine we had a Javascript file in a Typescript project:\n```js\n// main.ts\nlet ajala\n\najala = {\n name: \"Ajala the traveller\",\n age: 12,\n getName: function() {\n     return this.name;\n   }\n};\n\najala.lol()\n```\nWhen we compile this file with `tsc main.ts`, the compilation would be successful, and `main.js` is outputted.\n\nIf we then run `node main.js`, we will get a runtime error that `ajala.lol is not a function`. Since no type information was available for it, Typescript could not have warned us about this at compile time.\n\nTo fix this, we can create a file `main.d.ts`:\n```ts\ndeclare module \"MyTypes\" {\n\texport interface Person {\n\t\tname: string;\n\t\tage: number;\n\t\tgetName(): string;\n\t}\n}\n```\n\nAnd then add a triple-slash directive and type annotation in `main.ts`\n- note: this triple-slash directive is possible, but not recommended. Instead, just install npm type declaration modules (below)\n```ts\n/// <reference path=\"Main.d.ts\" />\n\nimport * as MyTypes from \"MyTypes\"\n\nlet ajala: MyTypes.Person\n\najala = {\n\tname: \"Ajala the traveller\",\n\tage: 12,\n\tgetName: function() {\n\t\treturn this.name;\n\t}\n};\n\najala.lol();\n```\nWith the declaration file provided and included via the triple-slash directive, the TS compiler now has information about the shape of `Person`, and will throw us an error at compile time.\n\n### How to type a third party module\n1. create a `@types` folder at same level as `tsconfig.json`\n2. add a `.d.ts` file using the format `[nameOfTheThirdPartyModule].d.ts`\n3. add the following to top-level of `tsconfig.json`:\n```json\n\"files\": [\n  \"@types/[nameOfTheThirdPartyModule].d.ts\"\n],\n```","n":0.051}}},{"i":153,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Destructure obj/array with types\nIntuitively, we might try and do:\n```ts\nconst { name: string, age: number } = body.value\n```\n\nBut in reality we are renaming the destructed values. The correct way is like this:\n```ts\nconst { name, age }: { name: string; age: number } = body.value\n```\n","n":0.151}}},{"i":154,"$":{"0":{"v":"Torrent","n":1},"1":{"v":"\n### Magnet Link\nA magnet link is a type of hyperlink that enables the downloading of files and data from P2P sharing networks, particularly torrent networks. It works in a server-less environment and contains all the information a torrent client requires to download a specific file. Once the user clicks a magnet link, its data is sent to the desktop torrent client software, which automatically starts the download.\n- It is preferable to a .torrent file because it eliminates the need to download a tracker file and search for uploading peers.\n- Therefore, a magnet link replaces a .torrent file extension/mechanism with only a hyperlink, which consists of the magnet identifier, file name and cryptographic content hash.\n\nThe idea of magnets is instead of downloading the `.torrent` file from a webserver, you download it directly from a seed/leecher. The biggest advantage is that you might be able to download the content of the torrent, even if the tracker is down or closed for registration.\n\nThe main advantage for Bittorrent indexers is that they do not have to store the torrents on their servers anymore which could be beneficial for them in several ways. It could reduce the pressure from the media creation industry and reduce hardware infrastructure expenses thanks to less tracking and downloading.\n\n","n":0.069}}},{"i":155,"$":{"0":{"v":"Tmux","n":1}}},{"i":156,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n# Command line commands\nnote: all prefixed with prefix `<C-a>`, except where noted\n- `?` see list of available commands\n\n# Plugins\n###### Fetch new plugins (like PlugInstall)\n`I`\n\n###### Update plugins\n`U`\n\n###### Uninstall plugins not on list\n`opt+u`\n\n\n# Integration shortcuts\n###### Paste from clipboard\n`p`\n- `P` - choose which buffer to paste from\n\n* * *\n\n## Modes\n### Copy Mode (no prefix)\nenter copy mode\n- prefix + `[`\n\nmove up/down a paragraph\n- `{`/`}`\n\ngo to middle\n- `M`\n\ncopy until end of line\n- `D`\n\n\n### Normal\n- paste from buffer - `]`\n- send the contents of the current buffer to a temp file - `P`\n\t- custom bind\n- open current pane in vim - `v`\n- If you want to switch to a window based on something displayed in it (this also includes window names and titles but not history), (starting with more than one window open) press Ctrl-b f then type the string to search for and press Enter. You will be switched to a window containing that text if it's found. If more than one window matches, you'll see a list to select from.\n\n* * *\n\n### Other\n- edit tmux.conf - `e`\n\n# Resources\nhttp://tmuxp.git-pull.com/en/latest/examples.html\nhttps://www.barbarianmeetscoding.com/blog/2019/12/25/jaimes-guide-to-tmux-the-most-awesome-tool-you-didnt-know-you-needed","n":0.076}}},{"i":157,"$":{"0":{"v":"Window Commands","n":0.707},"1":{"v":"\n###### Create new window\n`c`\n\n###### Go to window 1\n`1`\n\n###### Go to last window\n`<C-i>`\n\n###### swap between windows\n`<>`/`<C-l>`\n\n##### Swap window location\n`<C-o>`\n\n###### Close window\n`&`\n\n###### List all windows\n`w`\n\n###### Move window to another session\n`.`","n":0.189}}},{"i":158,"$":{"0":{"v":"Session Commands","n":0.707},"1":{"v":"\n###### Detach\n`d`\n\n###### Rename session\n`$`\n\n###### Swap between sessions\n`(`/`)`\n\n###### Attach\n`a`\n- `-t 0` - attach to \"session 0\"\n\n###### New session\n`:new`\n- `-s` - add a name to the session\n`tmux new -s <session-name>`\n\n###### Rename session\n`$`\n\n###### List overview of sessions\n`s`\n\n###### Kill session\n`:kill-ses -t <session-name>`\n\n###### Change default directory of new windows\n`:attach -c /path/to/default/directory`\n","n":0.147}}},{"i":159,"$":{"0":{"v":"Pane Commands","n":0.707},"1":{"v":"\n###### Split vertically\n`shift + |`\n\n###### Split horizontally\n`shift + _`\n\n###### Close pane\n`x`\n\n###### Last active pane\n`;`\n\n###### Move current pane left/right\n`{`/`}`\n\n###### Pane zoom (cmd+shift+return in iTerm2)\n`z`\n\n###### Swap/rotate pane locations\n`<`/`>`\n\n###### Toggle between horitontal and vertical split\n`<space>`\n\n###### Break current pane out of window\n`!`\n\n###### Resize pane\n`H`/`J`/`K`/`L`\n","n":0.158}}},{"i":160,"$":{"0":{"v":"Third Party","n":0.707}}},{"i":161,"$":{"0":{"v":"Services","n":1},"1":{"v":"\n### Hosting\n[Render](https://render.com)\n    - *\"If Heroku and Netlify had a baby, it would be Render.com\" -Scott Tolinski*\n[Begin](https://begin.com)\n[Netlify: for static apps](https://netlify.com)\n- plain html or generated with create-react-app or gatsby. It provides a good, fast CDN that is pretty easy to use.\n- Netlify typically gets used for pure-frontend (it now supports serverless functions too)\n\n[Vercel](https://vercel.com/)\n- for dynamic apps like the ones generated with Next.js\n- with Vercel you can deploy your API next to your frontend (assuming you're using Next.js).\n\n### CMS\n[Keystone](https://keystonejs.com/)\n[Strapi](https://strapi.io/)\n[Prismic](https://prismic.io/)\n[Sanity](https://www.sanity.io/)\n\n### Diagram sketching\n- tags: wireframing, wireframe\n[Excalidraw](https://excalidraw.com/)\n\n### infrastructure-as-code\n[Terraform](https://www.terraform.io/)\n[Pulumi](https://www.pulumi.io/)","n":0.11}}},{"i":162,"$":{"0":{"v":"Testing","n":1}}},{"i":163,"$":{"0":{"v":"Unit","n":1},"1":{"v":"\n\n### Setup\nspec: this is a [[harness|testing.harness]]\nBefore running tests, it is often useful to prepare the grounds so that the tests can do their work.\n- We can draw parallels this with the world of manual testing, where a tester would need to seed their database before being able to effectively perform their tests.\n\nex. if we are running tests against the database, then we might want to get some preliminary data in place. We might also want to establish a `pgPool` for us to query with.\n\n### Teardown\nAfter we are done running the tests, we might want to end the `pgPool` connection, or maybe we want to delete all test users from the db\n\n## Snapshot testing\nsnapshots can capture any serializable value and should be used anytime the goal is testing whether the output is correct.\n\nYour tests should be deterministic. Running the same tests multiple times on a component that has not changed should produce the same results every time.\n- if you have a `Clock` component that uses `Date.now()`, the snapshot generated each time the test is run will be different. To fix this, mock the `Date.now()` method.\n\nIn some scenarios, snapshot testing can potentially remove the need for unit testing for a particular set of functionalities (e.g. React components), but they can work together as well.\n\n* * *\n\n#### Common pitfall: not separating interface from implementation#\nBecause some classes may have references to other classes, testing a class can frequently spill over into testing another class.\n\nA common example of this is with classes that depend on a database: in order to test the class, the tester often writes code that interacts with the database. This is a mistake, because a unit test should not usually go outside of its own class boundary, and should especially not cross process/network boundaries because doing so can introduce unacceptable performance problems to the unit test-suite. Crossing such unit boundaries turns unit tests into integration tests, so when test cases fail, it is much less clear which component is causing the failure.\n\nInstead, the software developer should create an abstract interface around the database queries, and then implement that interface with their own mock object. By abstracting this necessary attachment from the code (temporarily reducing the effective net coupling), the independent unit can be more thoroughly tested than may have been previously possible. This results in a higher quality unit that is also more maintainable.","n":0.05}}},{"i":164,"$":{"0":{"v":"Mock","n":1},"1":{"v":"\nMocking is a technique to isolate test subjects by replacing dependencies with objects that you can control and inspect.\n- The goal for mocking is to replace something we don’t control with something we do, so it’s important that what we replace it with has all the features we need.\n\nMocking means to create a fake version of an external dependency. The purpose is to reduce the surface area of our tests to ensure we are properly scoping our test to the unit, and to not allow external things to bleed into the test.\n\nMocking allows us to test the links between code by erasing the actual implementation, and focusing on:\n- how many times a function/method was called (along with the args passed)\n\nImagine we have a function that does some computation, then sticks the result in a new file. Since we are unit testing, we aren't testing whether or not the file gets created— that can be safely assumed (or tested in another unit test). Instead, we can safely mock the creation of the file.\n- If we didn't mock this, we'd have to clean up after ourselves each time we run a test. It would result in a slower testing suite.\n\n### Examples\n#### forEach\nIf we were writing a unit test for `forEach`, we would just mock the callback.\n```js\nfunction forEach(items, callback) {\n  for (let index = 0; index < items.length; index++) {\n    callback(items[index]);\n  }\n}\n```\n\nImagine we had a mock function:\n```js\nconst mockCallback = jest.fn(x => 42 + x);\nforEach([0, 1], mockCallback);\n```\n\nThere are a few things we care about when it comes to unit testing this function:\n- was the callback called twice?\n- was the first argument of the first call `0`?\n- was the return value of the first call `42` (42 + 0)?\n\n#### Difference between mock and stub\nA mock is like a stub but the test will also verify that the object under test calls the mock as expected\nex. You can stub a database by implementing a simple in-memory structure for storing records. The object under test can then read and write records to the database stub to allow it to execute the test. This could test some behavior of the object not related to the database and the database stub would be included just to let the test run. If you instead want to verify that the object under test writes some specific data to the database you will have to mock the database. Your test would then incorporate assertions about what was written to the database mock.","n":0.049}}},{"i":165,"$":{"0":{"v":"Integration","n":1},"1":{"v":"\nWith integration tests, we are testing relationships between services\n- A naive approach might be to get all of the dependent services up and running for the testing environment. But this is unnecessary, and creates a lot of potential failure points from services that our outside of our control.\n- Instead, we could narrow it down by writing a few service integration tests using mocks and stubs.\n\nIn integration testing, the rules are different from unit tests. Here, you should only test the implementation and functionality that you have the control to edit. Mocks and stubs could be used for this purpose.","n":0.101}}},{"i":166,"$":{"0":{"v":"Test Harness","n":0.707},"1":{"v":"\nA Test Harness is the collection of all the items needed to test software at the unit, module, application or system level, and provides the mechanism to execute the tests\n- Every item such as input data, test parameters, test case, test script, expected output data, test tool, and test result report is part of the test harness.\n\nA test harness (a.k.a *automated test framework*) is a combination of:\n1. The code to be tested\n2. The test data\n    - because we need to test different circumstances, there will be multiple sets of test data\n\nTest harnesses allow for the automation of tests\n- They can call functions with supplied parameters and print out and compare the results to the desired value.\n\nA test harness should allow specific tests to run (this helps in optimizing), orchestrate a runtime environment, and provide a capability to analyse results.\n\nThe typical objectives of a test harness are to:\n- Automate the testing process.\n- Execute test suites of test cases.\n- Generate associated test reports.\n\nThink of a Test Harness as an 'enabler' that actually does all the work of (1)executing tests using a (2)test library and (3)generating reports. It would require that your test scripts are designed to handle different (4)test data and (5)test scenarios. Essentially, when the test harness is in place and prerequisite data is prepared (aka data prep) someone should be able to click a button or run one command to execute all your tests and generate reports.","n":0.065}}},{"i":167,"$":{"0":{"v":"E2e","n":1}}},{"i":168,"$":{"0":{"v":"Tools","n":1},"1":{"v":"\n## Frameworks\n[Strong recommendation from Juan](https://playwright.dev/)","n":0.447}}},{"i":169,"$":{"0":{"v":"Test Anything Protocol","n":0.577},"1":{"v":"\nTAP is a protocol to allow communication between unit tests and a [[test harness|testing.harness]]\n\nWith TAP, each test (ie. a TAP producer) can communicate its results to the testing harness.\n- This communication is done in a language-agnostic way","n":0.164}}},{"i":170,"$":{"0":{"v":"System Design","n":0.707},"1":{"v":"\n## Capacity Estimation and Constraints\nConsider how read-heavy the data in your application is. For instance, maybe 5x more people are uploading data than are downloading it (a 5:1 read:write ratio). Let's assume 1M writes per day, giving us 5M reads:\n\nCan users upload as much as they want? For instance, if we were building Instagram, we would want to make efficient storage of photos a top priority.\n\n### Leveraging User Expectations\nHave a good understanding of user expectations while designing the application. For instance, in an Instagram-clone, it's not necessary that every user sees the latest content at any given time— we can tolerate some delay. This enables us to retract our focus from strict caching and replication implementations, and instead focus on things that are more important for our system.\n\nHow fast do writes need to be? How about reads?\n- For Instagram, fast uploads would not be expected by users. Reads, on the other hand would be expected, since there is a high degree of swiping going on. For this, we'd want to have a more sophisticated caching system in place.\n- We should keep in mind that web servers have a connection limit before designing our system. If we assume that a web server can have a maximum of 500 connections at any time, then it can’t have more than 500 concurrent uploads or reads. To handle this bottleneck, we can split reads and writes into separate services. We can have dedicated servers for reads and different servers for writes to ensure that uploads don’t hog the system. This has the added benefit of being able to scale each set of services independently.\n- If users can upload a large amount of data, then efficient management of storage should also be a focus\n\nHow reliable is your app expected to be?\n- if you are Instagram, your users expect 100% reliability. Therefore, you should store multiple copies of each file so that if one storage server dies, the photo can be retrieved from the other copy present on a different storage server.\n    - If we want to have high availability of the system, we need to have multiple replicas of services running in the system so that even if a few services die down, the system remains available and running. Redundancy removes the single point of failure in the system.\n\n### Redundancy\nAll potential bottlenecks and points of failure should be scrutinized. Think of how the system gets more complex as we implement [sharding|db.strategies.sharding]. Now we need a sharding rule to determine in which shard a row is stored. Suppose we implement round robin with `recordId % 10` to spread out the records evenly. Well, now we have a new problem: we cannot have the shards increment the IDs themselves, since we need the ID first before we can even determine which shard it belongs in. So to solve that, we need some sort of external key generating service. This service can generate IDs, store them in a table (to indicate that it is used) when a new item is created. Each shard can use this service, and we can have perfectly incremented IDs across all shards. However, one last issue arises. This key generation system is now a single point of failure, so we have to iterate further. We could spin up another such key generating service, and split the load between them by having one service give out even-numbered IDs and the other odd-numbered IDs. Finally, we can stick a load balancer in front of them to round-robin the load between them. In case of failure of the even-numbered service, the only consequence is that there will be more odd-numbered IDs; something we probably don't even care about.\n\n### Estimation\nFrom there, assume that each item is 10KB, so we need to store 10GB per day. If we want to store this data for an average of 10 years then we need storage capacity of 36TB.\n- To keep some margin, we will assume a 70% capacity model (meaning we don’t want to use more than 70% of our total storage capacity at any point), which raises our storage needs to 51.4TB.\n\nNew items per second:\n- 1M / (24 hours * 3600 seconds) ~= 12 pastes/sec\nresults in \n120KB of ingress per second.\n- 12 * 10KB => 120 KB/s\n\nReads per second:\n- 5M / (24 hours * 3600 seconds) ~= 58 reads/sec\nresults in\ntotal data egress (sent to users) will be 0.6 MB/s.\n- 58 * 10KB => 0.6 MB/s\n\nThis number of records totals 3.6 Billion in 10 years. Since 6 random base64 encoding ([A-Z, a-z, 0-9, ., -]) can be used to generate 68.7 billion unique strings, this should be sufficient for uniqueness.\n\nWe can cache some of the hot data points that are frequently accessed. Following the 80-20 rule, meaning 20% of hot data points generate 80% of traffic, we would like to cache these 20% data points.\n- ex. if designing a paste-bin clone, cache the top 20% of most accessed pastes.\n\nSince we have 5M read requests per day, to cache 20% of these requests, we would need:\n- 0.2 * 5M * 10KB ~= 10 GB\n","n":0.034}}},{"i":171,"$":{"0":{"v":"Svg","n":1},"1":{"v":"\nTo make an `<img>` accessible, we must add alt text. However, to make svgs accessible, we must:\n1. Add a `<title>` as the first child of it's parent element\n2. Give the `<title>` a unique ID\n3. Add role=\"img\" and aria-labelledby={uniqueId}to the `<svg>`\n\n### strokeOffset property\nSvg property to animate svg. This is used any time you see an svg being drawn on. You can also apply css animations to it\n\n# Tools\nhttps://www.svgator.com/","n":0.122}}},{"i":172,"$":{"0":{"v":"Svelte","n":1},"1":{"v":"\nUnlike the traditional frameworks (React and Vue) which carry out the bulk of their work in the browser, Svelte shifts that work into a compile step that happens when an app is built.\n\n# Features of the framework\n## Updates to the DOM\nWhen you update component state in Svelte, it doesn't update the DOM immediately. Instead, it waits until the next microtask to see if there are any other changes that need to be applied, including in other components. Doing so avoids unnecessary work and allows the browser to batch things more effectively.\n- sometimes this behavior is undesirable, and for that we can turn to `tick()` lifecycle method\n\n## Reactivity in Svelte\nSvelte's reactivity is triggered by assignments (ie. setting a new variable, or changing the assignment of an existing variable)\n- therefore `map`, `filter` and `reduce` all trigger re-renders, because those methods return a new array. `push` would not trigger a re-render, unless we reassign the same variable to equal itself:\n```js\nfunction addNumber() {\n\tnumbers.push(numbers.length + 1);\n\tnumbers = numbers;\n}\n```\nThe idiomatic way of course is just to use non-mutative methods.\n\nUpdating properties of an object will also cause the re-render, since assigning new values to keys of an object is considered to be assignment.\n\n### Reactive declarations\nSome parts of a component's state need to be computed from other state , and therefore need to be computed when its dependent values change \n- eg. fullname computed from firstname+lastname. If firstname changes, then fullname needs to be recomputed.\n\nFor these circumstances, we can use reactive declarations, which look like:\n```js\nlet firstname = 'joe';\nlet lastname = 'schmidt';\n$: fullname = firstname + lastname;\n```\n\n`$: ` basically says, \"do this thing (e.g. set fullname equal to firstname + lastname) whenever the dependent variables involved are updated (ie. whenever firstname or lastname change)\". \n- In other words, this symbol marks a statement as reactive.\n\nThis symbol is known as the \"destiny operator\" in reactive programming.\n- A destiny operator ensures a variable is updated whenever values that it's computed from are changed)\n\nReactive values become particularly valuable when you need to reference them multiple times, or you have values that depend on other reactive values.\n\nWe can also just run arbitrary code blocks that are executed any time a dependent variable changes:\n- ex. here, any time firstname or lastname changes, the codeblock is run\n```js\n$: {\n    console.log(`Nice to meet you!`)\n    console.log(`my fullname is ${firstname} ${lastname}`)\n}\n```\n\nWe can even use conditionals:\n```js\n$: if (count >= 10) {\n\talert(`count is dangerously high!`);\n\tcount = 9;\n}\n```\n\n## Props\nDeclare props in the `<script>` tag:\n```js\n<script>\n\texport let answer;\n    export let name = 'Kyle'; // defaultValue\n</script>\n```\n\nProps are passed (almost) identically to React\n```js\n<Nested answer={42}/>\n```\n\n## DOM Logic\n### Conditional rendering\n```js\n{#if user.loggedIn}\n\t<button on:click={toggle}>\n\t\tLog out\n\t</button>\n{:else}\n\t<button on:click={toggle}>\n\t\tLog in\n\t</button>\n{/if}\n```\n\n### Loop\n```js\n<ul>\n\t{#each cats as { id, name }} // destructuring here\n\t\t<li><a target=\"_blank\" href=\"https://www.youtube.com/watch?v={cat.id}\">\n            ID: {id}\n\t\t\tName: {name}\n\t\t</a></li>\n\t{/each}\n</ul>\n```\n\n### Data fetching\nSvelte makes it easy to await the value of  directly in your markup:\n- Only the most recent promise is considered, meaning you don't need to worry about race conditions.\n\n```js\n{#await promise}\n\t<p>...waiting</p>\n{:then number}\n\t<p>The number is {number}</p>\n{:catch error}\n\t<p style=\"color: red\">{error.message}</p>\n{/await}\n```\n\n## Events\nMore or less in the same manner as React, any event can be listened to with the `on` directive:\n```html\n<div on:mousemove={handleMousemove}>\n\tThe mouse position is {m.x} x {m.y}\n</div>\n```\n\n### Event modifiers\nDOM event handlers can have modifiers that alter their behaviour. \n- ex. a handler with a once modifier will only run a single time:\n```html\n<button on:click|once={handleClick}>\n\tClick me\n</button>\n```\n\nNotable modifiers:\n- `preventDefault`\n- `once` - remove the handler after first time it runs\n- `self` - only trigger handler if `event.target` is the element itself\n\n### Event dispatching\nA component can be set to dispatch events by creating an event dispatcher and calling them via a handler. The parent component (here `App`) can listen to messages dispatched from a child component via the `on:message` directive (where `message` is the event name we are dispatching).\n- Without this `on:message` attribute, messages would still be dispatched, but the App would not react to it.\n\ninner.svelte:\n```html\n<script>\n\timport { createEventDispatcher } from 'svelte';\n\n\tconst dispatch = createEventDispatcher();\n\n\tfunction sayHello() {\n\t\tdispatch('message', {\n\t\t\ttext: 'Hello!'\n\t\t});\n\t}\n</script>\n\n<button on:click={sayHello}>\n\tClick to say hello\n</button>\n```\n\nApp.svelte:\n```html\n<script>\n\timport Inner from './Inner.svelte';\n\n\tfunction handleMessage(event) {\n\t\talert(event.detail.text);\n\t}\n</script>\n\n<Inner on:message={handleMessage}/>\n```\n\n### Event forwarding\nUnlike DOM events, component events don't [[bubble|js.event-loop.event-bubbling]]. If you want to listen to an event on a deeply nested component, the intermediate components must *forward* the event.\n\nThis would be the mid-layer, with a dispatched event called in the `Inner` component. Here, `on:message` is what bubbles the event up to the parent\n```html\n<script>\n\timport Inner from './Inner.svelte';\n</script>\n\n<Inner on:message/>\n```\n\n### Binding\nThough Svelte takes a top-down data flow approach like React, it can be useful to break that paradigm, and can be done so with bindings.\n\nwe can use the bind:value directive:\n```html\n<input bind:value={name}>\n```\n\nThis means that not only will changes to the value of `name` update the input value, but changes to the input value will update `name`.\n\nCheckbox:\n```html\n<input type=checkbox bind:checked={yes}>\n```\n\nRadio:\n```html\n// `scoops` is an array of choices\n<input type=radio bind:group={scoops} name=\"scoops\" value={1}>\n```\n\n## Lifecycles\nLike React, Svelte has a familiar (albeit simpler) list of lifecycle methods.\n`onMount`, `onDestroy`, `onUpdate`, `beforeUpdate`, `afterUpdate`\n\n### tick\n`tick` is a lifecycle method distinct from the familiar ones. It's different in that it can be called any time. It returns a promise that resolves as soon as pending state changes have been applied to the DOM.\n\n## Stores\nGlobal state solution of Svelte. \nA store is simply an object with `subscribe` method, allowing interested parties to be notified whenever the value changes.\n```js\n// stores.js\nimport { writable } from 'svelte/store';\n\n// this is a writable store\nexport const count = writable(0);\n\n/* * * * * * * * * * * * * * * * * * * */\n// Incrementor.svelte\nimport { count } from './stores.js';\n\nfunction decrement() {\n    count.update(n => n - 1);\n}\n```\n\n## Motions, Animations and Transitions\nSvelte provides lots of motion, animation and transition support out-of-the-box:\n\nExamples: \n- https://svelte.dev/tutorial/tweened\n- https://svelte.dev/tutorial/spring\n- https://svelte.dev/tutorial/transition\n\n## Actions\nActions are essentially element-level lifecycle functions. They're useful for things like:\n\n- interfacing with third-party libraries\n- lazy-loaded images\n- tooltips\n- adding custom event handlers\n\nSvelte actions allows you to build code in response to the lifecycle of DOM elements\n\n[Make an on-screen object pannable](https://svelte.dev/tutorial/actions)\n\n## Context API\nThe context API provides a mechanism for components to 'talk' to each other without passing around data and functions as props, or dispatching lots of events.\n\n### Parts to it\n[source](https://svelte.dev/tutorial/context-api)\nThere are 2 halves to the API: `setContext` and `getContext`.\n- if a component calls `setContext(key, context)`, then any child component can retrieve that context with `getContext(key)`\n\nUnlike React Context, you do not need to import and wrap your subtree with the Provider in order to get access to the context. Simply, the parent component calls `setContext` and that context is available to all children to want access to it.\n\n* * *\n\n## CSS Class shorthand\n```html\n<button\n\tclass=\"{current === 'foo' ? 'selected' : ''}\"\n\ton:click=\"{() => current = 'foo'}\"\n>foo</button>\n```\n\nCan be shortened to:\n```html\n<button\n\tclass:selected=\"{current === 'foo'}\"\n\ton:click=\"{() => current = 'foo'}\"\n>foo</button>\n```\n\n## Component children\nWe can render children of a component like React by passing content between a component's tags.\n\nIn React, it looks like this:\n```js\nconst childComponent = ({ children }) => {\n    return (\n        <div>\n            {children}\n        </div>\n    )\n}\n```\n\nIn Svelte, it looks like this:\n```html\n<div>\n\t<slot></slot>\n</div>\n```\n\nFallbacks can be specified by placing data between the `<slot>` tags. Think of this like a default value to the children \"prop\". This is called a *default slot*d\n- [named slots](https://svelte.dev/tutorial/named-slots) can also be used for more control over placement.\n","n":0.03}}},{"i":173,"$":{"0":{"v":"Sveltekit","n":1},"1":{"v":"\n\nThere are two basic concepts to Sveltekit:\n- Each page of your app is a Svelte component\n- You create pages by adding files to the `src/routes` directory of your project. These will be server-rendered so that a user's first visit to your app is as fast as possible, then a client-side app takes over\n\n## Router\nAt the heart of SvelteKit is a filesystem-based router. \n\nThere are two types of route — pages and endpoints.\n\n* * *\n\n## Misc\nThe value of Sveltekit is similar to the value of Next, but even extending beyond that. Sveltekit takes care of the modern best-practices surrounding web development and does it for you, including build optimizations, so that you load only the minimal required code; offline support; prefetching pages before the user initiates navigation; and configurable rendering that allows you to generate HTML on the server or in the browser at runtime or at build-time.\n\nA filename that has a segment with a leading underscore, such as src/routes/foo/_Private.svelte or src/routes/bar/_utils/cool-util.js, is hidden from the router, but can be imported by files that are not.\n\n# Resources\n[Realworld Sveltekit app (Medium clone)](https://github.com/sveltejs/realworld)","n":0.075}}},{"i":174,"$":{"0":{"v":"Routes","n":1}}},{"i":175,"$":{"0":{"v":"Pages","n":1},"1":{"v":"\nPages typically generate HTML to display to the user (as well as any CSS and JavaScript needed for the page).\nBy default, pages are rendered on both the client and server, though this behaviour is configurable.\n\nSince pages are Svelte components, they are of filetype `.svelte`\n\nBy default, when a user first visits the application, they will be served a server-rendered version of the page in question, plus some JavaScript that 'hydrates' the page and initialises a client-side router. From that point forward, navigating to other pages is handled entirely on the client for a fast, app-like feel where the common portions in the layout do not need to be rerendered.\n\nThe filename determines the route. \n\nDynamic parameters are encoded using `[brackets]`. For example, a blog post might be defined by `src/routes/blog/[slug].svelte`\n- that parameter can then be accessed in 2 ways: a load function, or the page store\n\nA good structure is to have minimal information in your page `.svelte` file, and import the entirety of the visual html from a hidden file (`_Editor.svelte`), like this:\n```js\n<script context=\"module\">\n\texport function load({ session }) {\n\t\tif (!session.user) {\n\t\t\treturn {\n\t\t\t\tstatus: 302,\n\t\t\t\tredirect: `/login`\n\t\t\t};\n\t\t}\n\n\t\treturn {};\n\t}\n</script>\n\n<script>\n\timport Editor from './_Editor.svelte';\n\tlet article = { title: '', description: '', body: '', tagList: [] };\n</script>\n\n<Editor {article}/>\n```","n":0.071}}},{"i":176,"$":{"0":{"v":"load Function","n":0.707},"1":{"v":"\npages and layouts can export a `load()` function that runs before the component is created.\n- This function runs both during server-side rendering and in the client\n- An example of its usage would be to allow us to get page data without showing a loading spinner and fetching data in `onMount`.\n- `load` is similar to `getStaticProps` or `getServerSideProps` in Next.js, except that it runs on both the server and the client.\n\nIf load returns nothing, SvelteKit will fall through to other routes until something responds, or will respond with a generic 404.\n\nSvelteKit's load receives an implementation of fetch, which has the following special properties:\n- it has access to cookies on the server\n- it can make requests against the app's own endpoints without issuing an HTTP call\n- it makes a copy of the response when you use it, and then sends it embedded in the initial page load for hydration\n\ncode inside `load` blocks should never reference `window`, `document`, or any browser-specific objects\n\ncode inside `load` blocks should not directly reference any API keys or secrets, which will be exposed to the client, but instead call an endpoint that uses any required secrets\n\nspec: If we want to make HTTP calls in our `load` function, we will have to import a created API endpoint (`export async function get()...`) and use it in `load`. \n\nThe `load` function is reactive, and will re-run when its parameters change, but only if they are used in the function.\n- Specifically, if page.query, page.path, session, or stuff are used in the function, they will be re-run whenever their value changes. \n- Note that destructuring parameters in the function declaration is enough to count as using them. \n\nIf you return a `Promise` from load, SvelteKit will delay rendering until the promise resolves.\n\n* * *\n\n### Signature of `load()`\nThe `load` function receives an object containing four fields — `page`, `fetch`, `session` and `stuff`\n\n#### page\n```ts\ninterface page: {\n    host: string;\n    path: string;\n    params: PageParams;\n    query: URLSearchParams;\n};\n```\nSo if the example above was `src/routes/blog/[slug].svelte` and the URL was `https://example.com/blog/some-post?foo=bar&baz&bizz=a&bizz=b`, the following would be true:\n- `page.host === 'example.com'`\n- `page.path === '/blog/some-post'`\n- `page.params.slug === 'some-post'`\n- `page.query.get('foo') === 'bar'`\n- `page.query.has('baz')`\n- `page.query.getAll('bizz') === ['a', 'b'`]\n\n#### stuff\n`stuff` is passed from layout components to child layouts and page components and can be filled with anything else you need to make available. For the root `__layout`.svelte component, it is equal to `{}`, but if that component's load function returns an object with a `stuff` property, it will be available to subsequent load functions.\n\n```ts\nexport interface LoadInput<\n\tPageParams extends Record<string, string> = Record<string, string>,\n\tStuff extends Record<string, any> = Record<string, any>,\n\tSession = any\n> {\n\tpage: {\n\t\thost: string;\n\t\tpath: string;\n\t\tparams: PageParams;\n\t\tquery: URLSearchParams;\n\t};\n\tfetch(info: RequestInfo, init?: RequestInit): Promise<Response>;\n\tsession: Session;\n\tstuff: Stuff;\n}\n```\n\n```ts\nexport interface LoadOutput<\n\tProps extends Record<string, any> = Record<string, any>,\n\tStuff extends Record<string, any> = Record<string, any>\n> {\n\tstatus?: number;\n\terror?: string | Error;\n\tredirect?: string;\n\tprops?: Props;\n\tstuff?: Stuff;\n\tmaxage?: number;\n}\n```","n":0.047}}},{"i":177,"$":{"0":{"v":"Layouts","n":1},"1":{"v":"\nLayouts are useful when there are elements that should be visible on every page, such as top-level navigation or a footer.\n- The only requirement is that the component includes a `<slot>` for the page content.\n\n### Nested Layouts\nSuppose we don't just have a single /settings page, but instead have nested pages like /settings/profile and /settings/notifications with a shared submenu (for a real-life example, see [github.com/settings](github.com/settings)).\n\nWe can create a layout that only applies to pages below /settings (while inheriting the root layout with the top-level nav):\n\n```html\n<!-- src/routes/settings/__layout.svelte -->\n<h1>Settings</h1>\n\n<div class=\"submenu\">\n\t<a href=\"/settings/profile\">Profile</a>\n\t<a href=\"/settings/notifications\">Notifications</a>\n</div>\n\n<slot></slot>\n```\n\n#### Resetting the layout stack\nTo reset the layout stack, create a __layout.reset.svelte file instead of a __layout.svelte file. For example, if you want your `/admin/*` pages to not inherit the root layout, create a file called `src/routes/admin/__layout.reset.svelte`.\n\n### Error pages\nSveltekit renders a default error page, but if we supply a `__error.svelte` alongside the component, then that will be shown to the user.","n":0.082}}},{"i":178,"$":{"0":{"v":"Endpoints","n":1},"1":{"v":"\nEndpoints run only on the server\n- This means it's the place to do things like access databases or APIs that require private credentials or return data that lives on a machine in your production network. Pages can request data from endpoints. Endpoints return JSON by default\n\nEndpoints are written in `.js` (or `.ts`) files.\n\nEndpoints export functions that correspond to HTTP methods.\n- `get`, `post`, `del`, `put`, `patch`\n\n> We don't interact with the req/res objects you might be familiar with from Node's http module or frameworks like Express, because they're only available on certain platforms. Instead, SvelteKit translates the returned object into whatever's required by the platform you're deploying your app to.\n\nheaders (including cookies) can be set in the return value of the HTTP method.\n\nif we had a `routes/blog/post1.svelte` page, we could request data from `routes/blog/post1.json`\n- both could be represented in the file structure as `blog/[slug].svelte` and `blog/[slug].json`, respectively.\n","n":0.083}}},{"i":179,"$":{"0":{"v":"Styled Components","n":0.707}}},{"i":180,"$":{"0":{"v":"TypeScript","n":1},"1":{"v":"\n\n### Use Prop type to pass to StyledComponent\n```ts\ninterface Props {\n  onPress: any;\n  src: any;\n  width: string;\n  height: string;\n}\n\nconst Icon = styled.Image<Props>`\n  width: ${p => p.width};\n  height: ${p => p.height};\n`;\n```\n\nand if you want to be more precise and ignore the `onPress`, effectively giving us a subset of `Props`:\n```ts\nconst Icon = styled.Image<Pick<Props, 'src' | 'width' | 'height'>>`\n  width: ${p => p.width};\n  height: ${p => p.height};\n`;\n```\n","n":0.126}}},{"i":181,"$":{"0":{"v":"API","n":1},"1":{"v":"\nA styled component can take any prop. It passes it on to the HTML node if it's a valid attribute, otherwise it only passes it into interpolated functions.\n\n#### `attrs`\nA chainable method called on a styled component, with the purpose of attaching some props to the styled component.\n- The fact that it's chainable shows that calling `.attrs` on a styled component returns another styled component.\n\nThe first and only argument is an object that will be merged into the rest of the component's props\n\nThe *attributes* refer to those found on the component/HTML node in question.\n\n##### Purpose\n`attrs` is useful for defining default attributes, and defining dynamic props\n- ex. you want a `<Button />` component with default attribute of `type=\"button\"` (instead of the normal `type=\"submit\"`)\n- ex. also, if you want a default size of small, but to be able to override that.\n\n```js\nconst Button = styled.button.attrs(props => ({\n  // Every <Button /> will now have type=\"button\" as default\n  type: props.type || 'button'\n  // small is the default size, but this can be overridden\n  size: props.size || 'small'\n}))`\n  display: block;\n  ...\n`\n```\n\n#### `as`\nkeep all the styling you've applied to a component but just switch out what's being ultimately rendered\n- ex. Keep all the stylings of your `Button` styled component, but actually render it as a `<input />` instead of a `<button />`\n\nThis is done at runtime.\n\nThis is known as a \"polymorphic prop\"\n","n":0.067}}},{"i":182,"$":{"0":{"v":"Stripe","n":1},"1":{"v":"\n## Misc\nwith (spec) stripe elements, we can use the `stripe_code` on the client as a way to get the credit card information (presaved) into the UI.\n\nBy default, anyone can POST data to our webhook. This is why we have the webhook secret to protect us, and only react to the POST request once we have verified that it is coming from Stripe\n- each webhook secret is associated with a single webhook endpoint\n\n### Credit Cards Internationally\nRegions like Europe and India require our application to handle requests from banks to authenticate a purchase (3DS or OTP)\n- If we are using webhooks, then the authentication is already handled and we don't have to worry about this.\n","n":0.094}}},{"i":183,"$":{"0":{"v":"Integration","n":1},"1":{"v":"\n## Integrating Stripe\nThere are different ways to integrate Stripe into your application.\n\n### Only Client-side handling\nStripe allows you to integrate a Checkout mechanism, where you only need to implement client code to perform payments.\n\nHere is a reference for one time payments with only client-side code: https://stripe.com/docs/payments/checkout/client\n\n### Client-side and Server-side\nBut there are sometimes usecase where client and server need to communicate with each other before/while a payment process (https://stripe.com/docs/payments/integration-builder). For example if you don't manage your inventory with Stripe you will need some own logic on the server. One reason would be that you just want to offer Stripe just as a payment gateway for credit cards besides other payment methods like PayPal.\n\n### Webhooks\nWell webhooks can be used in both cases. Webhooks allow you to let Stripe communicate with your backend server to inform you about succeeded payment, failed payments, customer updates, orders, billings and so on. By defining a webhook URL you can specify which events you want to receive from Stripe. You can then use events to update specified data in your database.","n":0.076}}},{"i":184,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n# CLI\n## listen\n- from the CLI, we can listen for events that happen on the stripe API, and have those events forwarded to us.\n- by default, `listen` will listen to the events happening on our live configuration (endpoint), found in Stripe dashboard. If we want to test, then we can listen for events happening at a different endpoint, with `stripe listen --forward-to localhost:5000/webhook`\n\n## trigger\n- from the CLI, we can fake an event happening (ie. fake a call to Stripe's API)\n- if we are listening in another terminal, then we should see the fakes command being listened to.\n\n## payment intents\n### create\n```\nstripe payment_intents create \\\n  --amount=2000 \\\n  --currency=cad \\\n  -d \"payment_method_types[]\"=card\n```\n[ref](https://stripe.com/docs/api/payment_intents/create)","n":0.096}}},{"i":185,"$":{"0":{"v":"API","n":1},"1":{"v":"\n# API Keys\n## Keys\nSecret key is used on server side code and publishable key is to be used on the client side to associate with the Stripe account.","n":0.189}}},{"i":186,"$":{"0":{"v":"Setup Intent","n":0.707},"1":{"v":"\n## Setup Intent\n- A SetupIntent guides you through the process of setting up and saving a customer's payment credentials for future payments.\n- If the SetupIntent is used with a Customer, upon success, it will automatically attach the resulting payment method to that Customer.\n- if we wanted to save a credit card at the time of purchase, then we can add the `setup_future_intent` at the time of creating a PaymentIntent. This negates our need to use the SetupIntent. spec: Therefore, the SetupIntent is needed only when saving a payment for later, but not making a purchase at the time (like when someone click \"add payment method\".\n- Creating a SetupIntent will generate a PaymentMethod to be attached to a customer, which can then be used to create a PaymentIntent when you are ready to charge them.","n":0.086}}},{"i":187,"$":{"0":{"v":"Price","n":1},"1":{"v":"\nEach Stripe Product can have multiple prices. In other words, Products/Prices have a 1:many relationship\n- ex., your premium monthly subscription (the product) may have 3 different prices attached to it:\n\t1. $9/month\n\t2. $100/year\n\t3. $500 one-time-purchase\n","n":0.171}}},{"i":188,"$":{"0":{"v":"Payment Method","n":0.707},"1":{"v":"\nrepresent the customer's payment instruments\n- they can be either:\n\t1. included as part of the payment intent when we are creating (so we know which credit card to use)\n\t2. saved to a stripe customer object for future payments\n\n### Types\n- *CARDS*\t- Cards are linked to a debit or credit account at a bank. To complete a payment online, customers enter their card information at checkout.\n- *WALLETS*\t- Wallets are linked to a card or bank account, but can also store monetary value. Wallets typically require customer verification (e.g., biometrics, SMS, passcode) to complete a payment.\n\t- ex. Apple pay, Google pay\n- *BANK DEBITS* -\tBank debits pull funds directly from your customer’s bank account. Customers provide their bank account information and typically agree to a mandate for you to debit their account.\n- *BANK REDIRECTS* - Bank redirects add a layer of verification to complete a bank debit payment. Instead of entering their bank account information, customers are redirected to provide their online banking credentials to authorize the payment.\tNo, but Stripe supports recurring for some methods by converting to direct debit\n- *BANK CREDIT TRANSFERS* -\tCredit transfers allow customers to push funds from their bank account to yours. You provide customers with the bank account information they should send funds to.\n- *BUY NOW, PAY LATER* - Buy now, pay later is a growing category of payment methods that offers customers immediate financing for online payments, typically repaid in fixed installments over time.\n- *CASH-BASED VOUCHERS* -\tWith cash-based vouchers, customers receive a scannable voucher with a transaction reference number that they can then bring to an ATM, bank, convenience store, or supermarket to complete the payment in cash.\n","n":0.061}}},{"i":189,"$":{"0":{"v":"Payment Intent","n":0.707},"1":{"v":"\nThere should be exactly one payment_intent for each order or customer session so we can reference the PaymentIntent later to see the history of payment attempts for a particular session.\n- The PaymentIntent API tracks a payment, from initial creation through the entire checkout process, and triggers additional authentication steps when required. Therefore, a payment intent has different states during its lifetime.\n\nWhen you use a PaymentIntent to collect payment from a customer, Stripe creates a charge behind the scenes\n\n- The best practice is to create a PaymentIntent as soon as the purchase amount is known\n\t- ex. this might be when the customer begins the checkout process. This would be fine, since we can always update the amount if the user backs out, adds another item to their cart, then begins checkout again\n- We should store the `PaymentIntentID` in our database, to be associated with the customer's shopping cart (order). If this isn't feasible, then we can store it on their session.\n\t- This would allow us to track and failed payment attempts for a given cart (or session)\n- We should provide an `idempotency key` to the PaymentIntent\n\t- Doing this will help us avoid creating duplicate PaymentIntents for the same purchase\n\t- this key would be an ID associated with the cart/session, which we store in the database\n- The PaymentIntent contains a client secret key which is unique to the individual PaymentIntent. In other words, a client secret uniquely identifies a payment intent.\n\t- This client secret is used as a parameter when calling `stripe.confirmCardPayment` on the client.\n\t- We retrieve the client secret from the PaymentIntent on our server, which we then can pass to the client. There are different approaches to getting the client secret to the client side.\n- The client secret of a paymentintent is dependent on the paymentintentid.\n\t- An example of the secret is `pi_1IR0JiKQXfStDK4vk3nikCNH_secret_GAWuc4MSiWoezicYMqMe5qgWj`, showing that the client secret would change if the payment intent was different.\n- If the customer leaves the CheckoutSession incomplete, it will expire and cancel the PaymentIntent automatically after 24 hours.\n\n## Refunds\n- We can provide an `amount` parameter to the Refunds API. Otherwise, the default amount will be the exact amount paid in the initial payment intent","n":0.053}}},{"i":190,"$":{"0":{"v":"Storage","n":1},"1":{"v":"\n# Storage\nalmost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away\n- Generally the fast volatile technologies (which lose data when off power) are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".\n\n## Levels\n### Registers (L0)\n- registers hold words from cache memory\n### On-chip cache (L1)\n- L1 cache holds cache lines retrieved from the L2 cache\n- ex. SRAM\n### Off-chip cache (L2)\n- L2 cache holds cache lines retrieved from memory\n- ex. SRAM\n### Main memory (L3)\n- Main memory holds disk blocks retrieved from local disks\n- ex. DRAM\n### Local secondary storage (L4)\n- local disks hold files retrieved from disks on remote network servers\n- ex. local disk\n### Remote secondary storage (L5)\n- ex. distributed filesystems, web servers\n\n* * *\n\n### Storage on a CD\n- the data on a CD is stored in an outward spiral pattern. The CD port on the computer has motor which rotates the CD. It also has a laser arm which follows the spiral, as the disk spins.\n- the spiral track has little bumps along it. The bumps are used to encode the binary value. The laser arm in the CD port has a sensor attached to it, which \"listens for\" the reflection of the laser. If the sensor receives the light, an electrical signal is generated and sent to the microprocessor, which interprets it as a `1`\n![](/assets/images/2021-03-09-21-22-35.png)\n","n":0.065}}},{"i":191,"$":{"0":{"v":"Methods","n":1},"1":{"v":"\n# Methods\n\n## RAID (Redundant Array of Independent Disks) \n- A RAID is simply multiple physical hard drives acting as a single virtual hard drive\n- The two main reasons are to safeguard your data with some sort of backup/failsafe, or to speed up reading/writing.\n\t- When data is read, it is read from both hard drives and checked to make sure they agree.\n- There are 3 main types (with example of storing a song):\n\t1. *RAID-0* - the song is stored across multiple hard drives. Therefore, if one fails then the file is corrupted \n\t\t- Least safe\n\t2. *RAID-1* - all hard drives have the same data. Therefore, if one fails, the redundancy provided by the second will allow us to retain all of the data.\n\t\t- RAID-1 is where disk mirroring is most commonly used\n\t4. *RAID-5* - the song is stored across multiple hard drives (like RAID-0), but each hard drive will have its own backup. If one hard drive fails, the backup is restored so we don't get a corrupted file. \n\t\t- This is the safest, but also more expensive and a bit slower.\n","n":0.074}}},{"i":192,"$":{"0":{"v":"Formats","n":1},"1":{"v":"\n# Storage Formats \nThere are 3 main ways (formats) that data can be stored digitally: \n1. file storage \n2. block storage \n3. object storage\n\n## File storage\n- organizes and represents data as a hierarchy of files in folders\n- ex. Unix, NAS\n- Since a system must be the thing that implements a filesystem, data stored in this way is not easily portable \n\t- ex. using Windows FS and trying to open it on Mac will prove difficult\n\n## Block storage\n- chunks data into arbitrarily organized, evenly sized blocks. Each block is given an ID, making it easily retrievable from storage.\n\t- it takes several blocks to make up one file.\n- Data can be retrieved fast because it doesn't rely on a single path to find the data.\n- the more data you need to store, the better off you’ll be with block storage.\n- ex. Storage Area Network (SAN)\n\n## Object storage\n- manages data and links it to associated metadata.\n","n":0.081}}},{"i":193,"$":{"0":{"v":"Backups","n":1},"1":{"v":"\n# Backups\n- 3-2-1 rule of backups: 3 copies, 2+ formats, at least one copy offsite.\n- If something is built into a software workflow, then it is not a backup\n\t- ex. Joplin sync does not count as backup, nor does Mackup. The reason is that we interact with these tools in a dynmaic way. We don't treat them as immutable snapshots. A backup is only a backup if it's immutable (or treated as such).\n- If you haven't yet tried to restore all of your data via your backup file, then you don't have a backup system in place.\n","n":0.102}}},{"i":194,"$":{"0":{"v":"Sqlite","n":1},"1":{"v":"\n- show tables - `.tables`\n- show databases - `.databases`\n- open database - `.open ~/.config/joplin/database.sqlite`\n- show signature of table - `.shema <tablename>`\n\t- more readable form - `pragma table_info('<tablename>')`\n\n### Migrations\nThe sqlite package includes a database migrations feature","n":0.169}}},{"i":195,"$":{"0":{"v":"Sql","n":1},"1":{"v":"\nSQL is a declarative language— we declare the result we want to obtain in terms of a data processing pipeline that is executed against a known database model and a dataset.\n\nEvery single sql query contains some level of business logic\n\nThe SQL language is statically typed: every query defines a new relation that must be fully understood by the system before executing it. This is the reason why sometimes cast expressions are needed in your Queries\n\nIn SQL, you need to explain your problem, unlike most programming languages where you need to focus on a solution you think is going to solve your problem. When struggling to write an sql query, First write down a single sentence in English that perfectly describes what you’re trying to achieve.\n\n## Parts of SQL\nAll SQL can be broken down into either DDL or DML\n- both DDL and DML have their own CRUD operations\n\n### DDL (data definition language)\n- used to define data structures (working with the structure of the data)\n    - associated with creating tables, defining relationships (primary, foreign and composite keys, for example) and data types, constraints, etc, and also for modifying them.\n- ex. `CREATE`, `ALTER`, `DROP`\n\n### DML (data manipulation language)\n- used to manipulate data (working with the data itself)\n- ex. `INSERT`, `UPDATE`, `DELETE`, `SELECT`\n\n# E Resource\n[quality tutorial (current progress)](https://pgexercises.com/questions/joins/self2.html)\n[quality tips](https://blog.jooq.org/2016/03/17/10-easy-steps-to-a-complete-understanding-of-sql/)\n","n":0.068}}},{"i":196,"$":{"0":{"v":"Terms","n":1},"1":{"v":"\n# SQL Term Map\n- relation -> table\n- attribute -> column\n- tuple -> row\n![](/assets/images/2021-03-09-17-13-13.png)","n":0.277}}},{"i":197,"$":{"0":{"v":"Relation","n":1},"1":{"v":"\n# What is relational?\nA relation is a set of tuples (fixed length array)\n- in other words, a single row in a single database is a relation between all the values that represent that row\n- in SQL, a relation is represented by a table, with each row being a tuple\n- a tuple is described as a function, since it maps names to values.\n\t- ex. for row 1, name -> \"Kyle\"\n- each element (cell) of the tuple is termed an `attribute value`\n\t- an attribute is the combination of the column name and its type\n\t\t- ex. age int\n\t- an attribute value is the cross-section of an attribute and a tuple (ie. it is a single piece of data)\n- each SQL clause will transform one or several relations in order to produce new relations.\n- Data is not relational. Rather, data has relationships.\n- *relational* is the concept that we have a set of elements that all share the same properties (aka attribute domains)\n\t- ex. if we have a table `foo` with 2 columns: id: int and name: text, then every single record in this database will look exactly like that.\n","n":0.074}}},{"i":198,"$":{"0":{"v":"Heading","n":1},"1":{"v":"\n# Heading\n- A heading is a set of attributes in which column names are unique amongst each other\n- In SQL, the Heading is simply the set of all column names in a table\n- This logically follows from the earlier definition, that each tuple has a corresponding (and unique) Heading","n":0.143}}},{"i":199,"$":{"0":{"v":"Body","n":1},"1":{"v":"\nBody - the set of tuples that correspond to a single Heading is called a *Body* \n- In SQL, the Body is simple all rows in the table\n- A relation is thus a heading paired with a body\n\t- in other words, a table","n":0.152}}},{"i":200,"$":{"0":{"v":"Tables","n":1}}},{"i":201,"$":{"0":{"v":"Partition","n":1},"1":{"v":"\n# Partitioning\nrefers to splitting what is logically one large table into smaller physical pieces\nbenefits:\n- faster querying","n":0.25}}},{"i":202,"$":{"0":{"v":"Junction","n":1},"1":{"v":"\n# Junction tables\nMany:Many relationships are achieved by associative tables\n- An associative (or junction) table maps two or more tables together by referencing the primary keys of each data table. In effect, it contains a number of foreign keys, each in a many-to-one relationship from the junction table to the individual data tables. The PK of the associative table is typically composed of the FK columns themselves.\n","n":0.123}}},{"i":203,"$":{"0":{"v":"Design","n":1},"1":{"v":"\n## Should this be a table or not?\n- can you imagine the current data being *expanded* in the future?\n\t- ex. imagine we had a Book table, which included 3 properties: genre1, genre2, genre3. Now if we decided that each book should have 4 potential genres, we would have to change the structure. The solution would be to have a Books table and a Genres table\n- Is the data being held in a table \"genericable\"? If you are tempted to combine different concepts into something that has a common thread, it might be a sign that they should be different tables. Framed another way, if you find yourself in a position where it would be difficult to extend a table, it should give us pause.\n\t- ex. Imagine we have to keep track of `InvoiceStatus`, `BackOrderStatus`, `ShipViaCarrier`, `CreditStatus`, and `CustomerStatusType`. Each of these concepts has a common thread, which is that aside from their respective PKs, they only have a single text field. We might therefore think it appropriate to combine all 5 concepts into a single generic table. However, this becomes problematic when we start to query information. What results is a necessity to JOIN ON many different fields, and to use aliases to distinguish multiple \"versions\" of the same table:\n- if we were to look at 5 rows of a single table and noticed that 3/5 of the rows have an identical value for one of the columns, that would be a sign that the column should be its own table\n\t- ex. we have a table called `shoes` and there is a field to describe what type of shoe it is (business, casual etc). If we were to query some records, we would find that 'business' would show up a lot in the 'type' column. This is a sign that we should create a \"shoe type\" table.\n\n```sql\nSELECT *\n\tFROM Customer\n\tJOIN GenericDomain as CustomerType\n\tON Customer.CustomerTypeId = CustomerType.GenericDomainId\n\t  and CustomerType.RelatedToTable = \"Customer\"\n\t  and  CustomerType.RelatedToColumn = \"CustomerTypeId\"\n\tJOIN GenericDomain as CreditStatus\n\tON Customer.CreditStatusId = CreditStatus.GenericDomainId\n\t  and CreditStatus.RelatedToTable = \"Customer\"\n\t  and CreditStatus.RelatedToColumn = \"CreditStatusId\"\n```\n\t- on the other hand, if we separate them all out as seperate tables, then it greatly simplifies our query:\n```sql\nSELECT * FROM Customer  \n\tJOIN CustomerType \n\t\tON Customer.CustomerTypeId = CustomerType.CustomerTypeId  \n\tJOIN CreditStatus    \n\t\tON Customer.CreditStatusId = CreditStatus.CreditStatusId\n```\n","n":0.052}}},{"i":204,"$":{"0":{"v":"Derived","n":1},"1":{"v":"\n# Derived Tables\n- Imagine we were in a position where we needed to make complex queries that involved some aggregation functions. What we could do to fulfill this, is create a temporary table, fill it with data (using INSERT), and use a *cursor* to loop through the temporary data, making updates as we go. Finally, we could execute a query against the temporary table, allowing us to obtain the final results. \n\t- An alternative approach for some data problems is to use *derived tables* (also known as inline views)\n- A derived table is a virtual table that is created within the scope of a query\n\t- The table is created with a SELECT statement of its own and is given an alias using the AS clause\n\t- the contents of this table can then be used within the query.\n\t- the result is similar to the above naive solution, and doesn't carry the overhead of having to actually CREATE/DELETE tables and run INSERTs on them\n\t\t- Therefore, derived tables eliminate the requirement to use cursors and temporary tables\n- you can think of a derived table as a temporary variable that points to a table that we just defined \n\n## Using Derived Tables\n- Because derived tables are used in place of an actual table, the query to build the derived table goes in the same spot as we normally have our FROM clause. All we need to do is replace the table name with parentheses, and define an alias with AS afterward\n- In the below example, all our derived table is really doing is making a new table that we can query against, where instead of having 3 columns: `first_name`, `last_name` and `value`, we have 2 columns: `salesperson` and `value`. This is a contrived example to show that a derived table allows us to transform how the original table appears, which we can then take advantage of to support our more complex queries of the data. \n```sql\nSELECT * FROM\n(\n\tSELECT first_name || ' ' || last_name AS salesperson, value_of_sales FROM sales_table\n) as derived_sales_table\nWHERE value_of_sales > 100 \n```\n- The data from this inner query can be used in the column list, in joins, and any other area where table data would be acceptable.\n\nIn Postgres, we can use the WITH clause to clean up the logic of derived tables:\n```sql\nSELECT first_name, last_name, age\nFROM (\n  SELECT first_name, last_name, current_date - date_of_birth age\n  FROM author\n)\n```\nbecomes\n```sql\nWITH a AS (\n  SELECT first_name, last_name, current_date - date_of_birth age\n  FROM author\n)\nSELECT *\nFROM a\nWHERE age > 10000\n```\n- with the above example, if we were anticipating the need to query the data set resulting from that subquery, we could just make it into a view so we wouldn't have to repeat that logic with each query we make\n\n[tutorial](http://www.blackwasp.co.uk/SQLDerivedTables.aspx)\n","n":0.047}}},{"i":205,"$":{"0":{"v":"Query","n":1}}},{"i":206,"$":{"0":{"v":"Subquery","n":1},"1":{"v":"\n# Subquery\n- think of subqueries as fluent syntax that's chained by nesting rather than step by step after each other.\n\t- similar to functional programming and how you read from the inside-out, using the output of the inner function as input for the next outer function \n- subqueries return a subset of data from a larger set of data (which arose from a different query)\n\t- ie. perform a query to get some data, then immediately perform another query on that data.\n- subqueries can be used almost anywhere in an SQL query.\n- if a subquery is slow, try and convert it to a derived table, as those are much faster\n- Subqueries can usually be executed independently, allowing us to see what each one is doing so we can mentally make substitutions as we are trying to understand an SQL query\n- use subqueries any time you want to first query a set of data, which you will then query again on\n\t- note: in reality it is not as inefficient as it sounds. This is just a simplification\nex. below, if `dr_recommended` is a table with a column called `activity`, then this query will return all activities in both tables, since they will be \"doctor recommended\" \nselect the `type` column go in the `dr_recommended` table\n- you can use the WITH clause in place of sub-queries\n\n```sql\nselect * \nfrom exercise_logs \nwhere activity in (\n\tselect type from dr_recommended\n);\n```\nex. below, we first query for the highest joindate, then out of the subset of data, we query for the joindate, firstname and surname\n```sql\nselect joindate, firstname, surname\nfrom cd.members\nwhere joindate = (select\nmax(joindate)\nfrom cd.members)\n```","n":0.062}}},{"i":207,"$":{"0":{"v":"Normalization","n":1},"1":{"v":"\n# Normalization\n- Denormalization is the process whereby you put data often used together in a single table to increase performance, at the sake of some database purity. Many find it to be an acceptable trade, even going so far as to design the schema intentionally denormalized to skip the intermediary step of having to join tables.\n- degree of normalization is defined as ~6 levels called Normal Forms (NF)\n\t- getting to level 6 (6NF) is not necessarily better than just getting to level 3 (3NF). \"more normalization\" does come at a cost.\n\t- ex. 6NF requires tables to have \"1 PK, and at most 1 attribute\"\n- purpose is to reduce data redundancy and improve data integrity\n- Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design).\n\t- more normalization means more tables\n\t- more \"normal\" schemas have less attributes when designing a schema, the goal is to find the right balance of normalization: too much: you have a proliferation of tables; too little: you have coupled, repeated, and difficult to query data\n- A hypothetical example of a failure to properly normalize SQL tables would be a hospital database having a table of patients which included a column for the telephone number of their doctor. The phone number is dependent on the doctor, rather than the patient, thus would be better stored in a table of doctors. The negative outcome of such a design is that a doctor's number will be duplicated in the database if they have multiple patients, thus increasing both the chance of input error and the cost and risk of updating that number should it change (compared to a third normal form-compliant data model that only stores a doctor's number once on a doctor table)\n\n## How much normalization?\n- A good rule of thumb for whether or not to normalize is to think about if the attribute might be updated in the row's lifetime. If the attribute probably won't be, then it's fine to denormalize.\n\t- ex. imagine Medium.com, which has the concept of organizations (eg. FreeCodeCamp) that can post blogs. Since we can be reasonably assured that a post will not change the organization it is attached to, it's probably ok to denormalize `organization_id` into `posts` (ie. organization_id becomes a column of the posts table)\n- Normalization can be embraced more when a table has a lower row count.\n- Denormalization can be embraced when a table has many rows and/or has expensively calculated properties that are accessed often.\n\t- we should make use of triggers to automatically update attributes in other tables where data has been duplicated as a result of denormalization. In this case, we can prefix the columns with `gen_` (generated), to be explicit about the fact that this column should not be updated by hand, but is a column that is only updated as a result of the trigger.\n- If we have a write heavy table that is not read often, the overhead of adding denormalized data at write time might decrease performance more than it improves read performance.","n":0.043}}},{"i":208,"$":{"0":{"v":"Keys","n":1},"1":{"v":"\n## Keys\n### Primary Key\n- a foreign key of a table can also be made into its PK\n\t- ex. Imagine we had a `public.user` table that held more public info about a user like `username`, `interests`, and a `private.user_account` which held sensitive info like `password` and `email`. Since it is logical to think of the `user_account` table as just an extension of the `user` table, we can simply use the already existing `id` from the `user` table, and make `user_id` the primary key of `user_account`, which references the `user` table.\n- PK is a type of candidate key.\n\n#### Surrogate Key\n- A surrogate key is a (likely automatically generated) primary key that is chosen because there is nothing else that uniquely identifies a row. Put another way, the only reason it exists is for the purposes of our database.\n\t- By contrast, a key that actually means something outside our database (ex. an item SKU)\n- The danger in just relying on surrogate keys is if we have a situation where there are other columns that should uniquely identify the row, then it leaves us open to developer error\n\t- ex. what if we have an `items` table, and 2 rows are `id` and `SKU`. There's nothing stopping us from inputting the same SKU twice, which result in a duplicated row with different ids.\n\n### Foreign Key\n- for foreign keys, we definitely want one column to uniquely identify a row (likely a primary key), otherwise we would have to reference multiple columns in the other table.\n- foreign keys can reference tables or columns\n- the foreign key is on the many side of a 1:many relationship\n- foreign keys are not unique\n- foreign keys can be null\n\n### Candidate key\n- a column or combination of columns that uniquely identifies a row.\n\t- ex. if the rules of our application dictated that emails in the db must be unique, then email could qualify as a candidate key \n- Any column or column combination that can guarantee uniqueness is referred to as a *candidate key*\n\t- if this uniqueness is achieved by 2 or more columns, then it is referred to as a composite key. Therefore, composite key is a type of candidate key.\n\n#### Differences with PK\n- a table can have multiple candidate keys.\n- candidate key column(s) can have null values\n\n#### Composite key\n- If a table does not have a single column that qualifies for a Candidate key, then you have to select 2 or more columns to make a row unique\n\t- ex. if no EmployeeID or SocialSecurityNumber that could qualify as a candidate key, then you can make FullName + DoB as composite primary key (though there is small risk of duplication still)\n- Using composite keys could save us from having to perform lookups before making insertions.\n\n### Super key\n- if you add any column to a PK, then it becomes a super key\n\t- ex. EmployeeID + FullName\n\n## Constraining tables\n- foreign key constraints ensure that there is another table in existence that has the same column-value\n\t- ex. if we have an attribute `product_no` in an Order table, then we should expect that attribute to reference an `id` from a Product table\n\t- maintains the referential integrity between two related tables\n- when we create a foreign key, we essentially are saying \"a row cannot be entered into this table, unless we have an established reference to the other table\"\n\t- in other words, if the id in the other table (the *referenced table*) doesn't exist, then neither can the row in the *referencing table* (the table with the FK) \n\n## Constraining columns\n- below, we reference columns `c1` and `c2` from `table2`, calling the `b` and `c` in our table (`table1`)\n```\nCREATE TABLE table` (\n  a integer PRIMARY KEY,\n  b integer,\n  c integer,\n  FOREIGN KEY (b, c) REFERENCES table2 (c1, c2)\n);\n```\n\n1:1 and 1:many relationships are achieved by the use of key contraints (foreign keys)\n- ex. User table with an `id` (PK) property can be used as the value for the `owner_id` (FK) property on the Nugget table\n\n\n### Shoestore example\nLets start with a  sensible table: `product` (ex. Air, Jordans). Since we are going to keeping inventory in the store, we will need actual instances of those shoes, which we can call the `item` table. For instance, this table will hold the information about the shoe's size, its particular color, its price etc. (All of this information is specific to a shoe instance, and doesn't make sense to be included in the product table). Next, we might be tempted to add a column to the `product` table indicating what type of shoe it is (business, running etc). What we would find when querying data however, is that 'business' would be repeated many times, which is a sign that it should be its own table. So we create a table called `product_type` and relate it to the `product` table with ids. Next, we will have to consider that we will have to store data on the actual sale of the item (called `sales_item`). There is a distinct but important difference between difference between the `item` table and the `sales_item`. `item` is a table that includes the actual instances of the shoe in inventory. `sales_item` is the item that is being sold. Therefore, it is going to be part of another table `sales_order`. In `sales_item`, we will include information like the specific tax rate that was used (since different people are subject to different tax rates), the discount that was applied, the quantity bought etc. Next, in our `sales_order` table, we will include the form of payment (credit, cash), what time the order was placed etc. \n![8da5a5b7c2d3633de044d693884219ee.png](:/7d7f11b898814196afe51643fe2a6f8d)\n","n":0.033}}},{"i":209,"$":{"0":{"v":"Join","n":1},"1":{"v":"\n### Syntax for joining on a key\nMost common is to use JOIN ... ON\n```sql\n...\ninner join T1 on T2.id = T1.id\n```\n\nWe can also use the USING clause, which is a shorthand that allows you to take advantage of the specific situation where both sides of the join use the same name for the joining column(s)\n- For example, the following produces the join condition ON T1.a = T2.a AND T1.b = T2.b.\n```sql\nselect * from T1\ninner join T2 using (a, b)\n```\nDoing it like this also suppresses redundant columns\n- there is no need to print both of the matched columns, since they must have equal values\n\n### Anti-join\nAn anti-join is meant to keep only the rows that fail a test.\n- ex. imagine we have a result set of students having taken an Economics exam. If we want to filter to include only those students who did not finish the exam (denoted by `null` value in the `score` column), we can filter out those who did in fact finish the exam.\n    - note: since WHERE clause is just a filter, it doesn't care which columns are actually returned by the query. This is why we can just say `SELECT 1`. Postgres doesn't even look at what was selected; only that something was returned from the query.\n```sql\nselect *\n    from students\n    inner join results on students.id = results.student_id\nwhere not exists (\n    select 1\n        from results r\n    where score is not null\n)\n```\n","n":0.066}}},{"i":210,"$":{"0":{"v":"Self Join","n":0.707},"1":{"v":"\nA self join allows us to join a table to itself using an inner or outer join\n- this allows you to create a result set that joins the rows with the other rows within the same table.\n\nIn practice, you typically use a self-join to query hierarchical data or to compare rows within the same table.\n- Imagine we need to compare a column of each row with all other rows in that table, and get a result set of only the rows that pass a certain condition\n\t- ex. we have a table `temperature_readings` that include both the high and low for the day. What we conceptually need to do is take row1 and compare it to every other row. Then we take row2 and compare it to every other row, and so on. Conceptually, this is like a nested for loop, where w1 is the outer loop and w2 is the inner loop:\n```sql\nselect\n     w1.city,\n     w1.temp_lo as low,\n     w1.temp_hi as high,\n     w2.city,\n     w2.temp_lo as low,\n     w2.temp_hi as high\n from weather w1, weather w2\n where w1.temp_lo < w2.temp_lo\n and w1.temp_hi > w2.temp_hi;\n```\nWith self joins, we need to use an alias because you cannot refer to the same table more than once in a query, so we assign it a name when it is joining against itself\n- since we are using 2 versions of the same table, we should give both an alias. For example in the below example, give alias `m` for the manager's version and `e` for the employee's version\n- [why use aliases?](https://www.reddit.com/r/SQL/comments/9e5yi9/why_use_table_alias_noob_question/)\n\nTwo reasons for using a self-join:\n1. query parents/child relationship stored in a table\n\t- ex. in `employee` table, demonstrate relationship between `employee_id` and `reports_to` columns. Here, the employee's `reports_to` would match up with the manager's `employee_id`\n\t\t- `INNER JOIN employees m ON m.employeeid = e.reportsto`\n2. obtain running totals\n\n","n":0.058}}},{"i":211,"$":{"0":{"v":"Outer","n":1},"1":{"v":"\nWhen doing an inner join, if either table is unmatched by the other, then those rows will be excluded from the result set.\n- Outer joins allow us to select rows from one table that may or may not have the corresponding rows in other tables.\n\t- a LEFT JOIN returns only the unmatched rows from the left table (the original table we are joining onto)\n\t\t- \"get me all rows from the left table, even if there are no matching rows in the right table\"\n\t- a RIGHT JOIN returns only the unmatched rows from the right table (the new table we are adding)\n- Making a LEFT JOIN and then putting a column from the outer table in the WHERE clause would give us an error, (spec): since that column would be unselectable.\n\n### Example: songs and artists\nImagine we have songs and artists. Each song has 0 or more artists. This is possible because a song might not have an attributed author.\n\nImagine we query the songs table, then left join artists. This would allow us a result set that includes all the songs without artist. If we had done an inner join, then the songs without an artist would not have ended up in the result set\n\n## Left Outer Join\n- called *left outer* because the table on the left of the join operator will have each of its rows in the output at least once, whereas the table on the right will only have those rows output that match some row of the left table.\n\t- if there is not right-table match, a null value will be substituted in for right-table columns.\n\t- a common way to distinguish the left and right side of the join is to use aliases\n- Left join semantics are to keep the whole result set of the table lexically on the left of the operator, and to fill-in the columns for the table on the right of the left join operator when some data is found that matches the join condition, otherwise using NULL as the column’s value.\n- there are times when we do want to include data where there isn't a match (ex. there is no matching FK for a given PK between 2 tables of a corresponding row). In these cases, we have to use an OUTER JOIN.\n\t- ex. we are doing a left join on \"bucket\", and we have buckets in our db that have no nuggets. this would result in a `null` value for the \"nugget\" column associated with that row.\n\t![2890a1430af9f888455d50224e795554.png](:/2af5c67cf5fb4b898d5323ce1cb0849b)\n- It's the LEFT or RIGHT keyword that makes the JOIN an \"OUTER\" JOIN\n\t- ie, all JOINs designated either LEFT or RIGHT are by definition OUTER JOINs\n\t- We use LEFT, RIGHT or FULL to specify which table should have all of its rows returned, regardless of the condition being met.\n- the first table is the \"left\" table\n- best practice is to avoid Right Outer Joins\n- when joining tables, all rows inclded in the left table will show up on the new table, even if one of the cells is missing, as in prev. example.\n- the left outer join starts selecting data from the left table. it compares the 2 values (specified after `ON`).\n\t- If they are equal, a new row is created that contains columns of both tables and adds the new row to the result set\n\t- if they are not equal, a new row is still created, but the right table's columns will be filled with `null`\n\t- If we want, we can add a WHERE clause to this query to only return rows from the left table that do not having matching rows in the right table\n\t\t- this effectively cuts out the intersection from the venn diagram\n\t\t- ex. `...WHERE tableb_col IS NULL;`\n- if we made a venn diagram of table1 and table2, an outer join would give us the outer parts of the venn diagram Union.\n\t- we will get all of the rows of the left table, but only the \"rows in common\" that the right table has\n\t\t- ex. if the left table has `1`, `2`, `3`, those will always show up no matter what. If the right table has `2`, `3`, `4`, then only `2` and `3` will show up (the rest will be null)\n- Venn diagram illustrates the left join that returns rows from the left table that do not have matching rows from the right table:\n![6457179c137a03db0dd9765c9cd6c988.png](:/69d6642385194490a8d62f93f8cf1bdb)\n\n## Full Outer Join\n- returns a result set that contains all rows from both left and right tables, with the matching rows from both sides if available.\n\t- In case there is no match, the columns of the table will be filled with NULL.\n- similar to the above techniques using `WHERE` clause to cut out the intersection, we can do the same here\n\t- This would give us the rows from one table that do not have the corresponding rows in another table\n![9e04ed55193420bac12aa21a38c81141.png](:/56dbccd24a6d449fa4d6ba8a0427d96a)\n\n### Left vs Right Join\nImagine we have `paymnent_sources`, which has a `payment_method_id` column, and both `coupons` and `credit_cards` has also have a `payment_method_id` column. In other words, a payment_source can reference either a credit_card or a coupon. Let's say we want to query all payment_sources, including all credit_cards and coupons. We would `select from payment_sources`, then `left outer join` the credit_cards table and the coupons table. Therefore, `left join` is the concept of \"I have a baseline group of records, and I want to get all attachments to that baseline, without caring about how each attachment relates to other attachements (e.g. credit_cards to coupons)","n":0.033}}},{"i":212,"$":{"0":{"v":"Lateral","n":1},"1":{"v":"\nSubqueries appearing in FROM can be preceded by the key word LATERAL. This allows them to reference columns provided by preceding FROM items. (Without LATERAL, each subquery is evaluated independently and so cannot cross-reference any other FROM item.)\n\nTable functions appearing in FROM can also be preceded by the key word LATERAL, but for functions the key word is optional; the function's arguments can contain references to columns provided by preceding FROM items in any case.\n\nLATERAL JOINs behave more as a correlated subquery, rather than just a plain subquery.\n- expressions to the right of a LATERAL join are evaluated once for each row left of it - just like a correlated subquery - while a plain subquery (table expression) is evaluated once only.\n\nFor returning more than one column, a LATERAL join is typically simpler, cleaner and faster.\nAlso, remember that the equivalent of a correlated subquery is `LEFT JOIN LATERAL ... ON true`","n":0.081}}},{"i":213,"$":{"0":{"v":"Cross","n":1},"1":{"v":"\nWhile inner and outer joins involve starting with one table and appending rows onto it, CROSS JOINs involve taking 2+ tables and generating a cartesian coordinate table as a result of it.\n![](/assets/images/2021-05-21-17-12-27.png)\n\nEffectively, this gives us a combination of each row in T1 and T2\n- In the above image, T1 has 3 rows and T2 has 3 rows, so the result set of the CROSS JOIN has 9 (3 * 3) rows","n":0.119}}},{"i":214,"$":{"0":{"v":"Constraint","n":1},"1":{"v":"\n# Differences between a column NOT NULL constraint and a CHECK CONSTRAINT not null\na `check constraint` is like a column constraint, except that it's on a table.\n\nAlthough the end result may be the same, there are some differences:\n- check constraints have to be named and belong to the table, while a NOT NULL column is just an option of the latter\n- from a performance point of view, creating an explicit not-null constraint is more efficient.\n\t- only 0.5% performance penalty, so hardly noticeable.\n- you have to remove check constraints before removing the associated columns\n- a `NOT NULL` is written beside the column name when issuing `\\d your_table` on psql, while check constraints are described below a specific session","n":0.092}}},{"i":215,"$":{"0":{"v":"Clause","n":1}}},{"i":216,"$":{"0":{"v":"Update","n":1},"1":{"v":"\nwe can update a table while leveraging the commands that help us create a result set.\n- ex. imagine we discover that all temperature readings after Nov 14 are off by 2 degrees:\n```sql\nUPDATE weather\nSET temp = temp - 2\nWHERE date > '2020-11-14'\n```\n- UPDATE always requires a SET token.\n\n![[dendron://code/sql.query.cte###CTE and UPDATE..FROM]]\n\n### Joining\n```sql\nupdate app_public.payments as payments\n  set stripe_pi_code = book_orders.stripe_pi_code\n  from app_public.book_orders as book_orders\n  where payments.book_order_id = book_orders.id;\n```\n","n":0.124}}},{"i":217,"$":{"0":{"v":"Select","n":1},"1":{"v":"\n# SELECT\n- SELECT is known as a \"projection\" in relational algebra.\n\t- Once you’ve generated your table reference, filtered it, transformed it, you can step to projecting it to another form\n- Within the SELECT clause, you can finally operate on columns, creating complex column expressions as parts of the row.\n- SELECT accepts expressions, allowing us to say `select (temp_lo + temp_hi)/2 as temp_avg...`\n- The SELECT clause may be one of the most complex clauses in SQL, even if it appears so simple. All other clauses just “pipe” table references from one to another\n- In order to understand SQL, it is important to understand everything else first, before trying to tackle SELECT\n\n\n### SELECT DISTINCT\nremove duplicate rows *after* the SELECT has been applied (in other words, after the projection)\n\n### SELECT (inside a WITH)\n- purpose is to break down complicated queries into simpler parts\n\n# Combining result set of 2+ SELECT statements\nThe queries that involve UNION, INTERSECT, or EXCEPT need to follow these rules:\n1. The number of columns and their orders must be the same in the two queries.\n2. The data types of the respective columns must be compatible.\n\n* * *\n\n### Don't use SELECT *\n- By manually listing out each column we want to project, we show the intention of the code. Listing out each column allows for declaring our thinking as developers\n- When we implicitly select all columns, if the columns on a table change (eg. we remove a column), then the implementations where we call `SELECT *` may not be happy with the new dataset returned.\n\t- Maybe the column we just removed was a dependency of that module. If we had manually listed out each column, then the error would have been more clear.\n- It isn't efficient to retrieve all bytes of data, when we don't even need them all.","n":0.058}}},{"i":218,"$":{"0":{"v":"Order by","n":0.707},"1":{"v":"\norder by the column name.\n- SQL doesn't guarantee any ordering of the result set of any query except when you use ORDER BY\n- you can use `order by 1` to sort the view or table by 1st column of query's result.\n\n## Implementing kNN (*k* nearest neighbors)\nSuppose we want to order a list of cities by their distance to Paris:\n- note: `<->` operator is \"distance between\"\n```sql\nselect name, location, country\n    from circuits\norder by point(lng,lat) <-> point(2.349014, 48.864716)\n   limit 10;\n```","n":0.114}}},{"i":219,"$":{"0":{"v":"From","n":1},"1":{"v":"\nThe result of the FROM list is an intermediate virtual table that can then be subject to transformations by the WHERE, GROUP BY, and HAVING clauses","n":0.196}}},{"i":220,"$":{"0":{"v":"Object Relational Mapper","n":0.577},"1":{"v":"\nAn ORM is for translating SQL records into Python objects (or whatever language used on the server). If you think it can handle anything more, like preventing your need to write SQL, managing indexes etc… you are wrong.\n\nIf our server has an ORM built-in (like the one Django has), it is not necessarily a bad idea to use an ORM. Consider that:\n```py\nMyModel.objects.get(id=1)\n```\n\nis equivalent to:\n```sql\nselect mymodel.id, mymodel.other_field, ...\n  from mymodel\n where id=1;\n ```\n\nThe SQL language abstracts away the procedural steps for querying a database, allowing one to merely define what one wants. But certain SQL queries are thousands of times slower than other logically equivalent queries. On an even higher level of abstraction, ORM systems, which isolate object-oriented code from the implementation of object persistence using a relational database, still force the programmer to think in terms of databases, tables, and native SQL queries as soon as performance of ORM-generated queries becomes a concern.","n":0.081}}},{"i":221,"$":{"0":{"v":"Sketch","n":1},"1":{"v":"\n# Styles\n- apply styles to things like text to be able to reuse them easily\n    - when selecting text, it is under the `Appearance` headline\n\n# Layout\n## Smart Layout\n- Create a symbol that when modified (ex. text), will retain its ratio\n    - ex. make a button, group it, then make into a symbol. On the naming\n\twindow, select \"horizontal align\". Now, when you replace the text on\n\tthat button, the horizontal size will automatically adjust\n- Use smart layout any time you create a list of horizontal text (ex. a navbar,\n    or tabs).\n    - Each item should be a symbol that has a smart layout of \"horizontal\n\talign\".\n    - The whole navbar itself should also be a symbol with \"horizontal align\",\n\tmaking the spacing between the items to be consistent even with changes\n\tto the text.\n    - On the sidebar, you can choose to override styles in order to remove an\n\titem from the list. Each item will shift horizontally to take its place.\n\n# Reusing\n- Mainly done through `Layer Styles`, `Text styles` and `Symbols`\n\n## Symbols\n- similar to the concept of components in programming\n- ex. create a button from scratch, similar to grouping them together, we can\n    make them into a symbol to be reused throughout the design\n- to make edits to the master symbol, just click on any symbol to be transported\n    to the master symbol's canvas.\n- [guide to reusing colors](https://www.reddit.com/r/sketchapp/comments/gwft1f/if_you_could_just_edit_these_and_for_the_change/)\n\n### Nested Symbols\n- ability to override the symbols\n- Imagine we have a navbar and want to create an active/inactive state for each\n    button.\n- Create the whole navbar as one symbol, then when editing the master navbar symbol, click\n    into each button and convert it to a symbol, naming it `nav/icon{1,2,3}/off`\n- Duplicate each icon and make the ON state by changing color, then renaming to\n    `nav/icon{1,2,3}/on`\n- This allows us to easily insert each state of a symbol easily.\n- We can quickly swap in on/off states now by clicking on the symbol (from the\n    artboard) and overriding the styles on the sidebar\n- If we want to swap the icon for another, we can just go to the navbar master\n    symbol and modify each nested symbol (the icon), and it will be updated\n    across all instances of the navbar symbol \n    \n#### Thought experiment\nLet’s say that a project requires 3 buttons in 3 different colours. To use only one symbol for all of them at once, we need to be able to change the colour of the button without detaching it from a symbol.\nWhen you put a symbol inside another, you can swap the one inside with any other available symbol of the same size. In Sketch, this function is named nested overrides. This way, if you have multiple colour swatches saved as symbols, you can easily replace one swatch with another.\n\n## Overriding symbols\n- Say we have a vertical scrollable list of airbnb houses for rent, each with common styles. What we can\n    do is create 1, make it a symbol, then insert that symbol into the document.\n    On the sidebar, you can see all of the overrides that are available\n    (adjusting text, adjusting colors, images etc)\n\n* * *\n\n## Mask\n- use to chop a hole out of the object (ex. picture) in the shape of the\n    overlying object.\n    - take a square and hover over a picture, then click 'mask', and you have an\n\timagine in the shape of the square.\n### Alpha mask\n- created when using opacity with gradients\n- allows us to place a rectangle with a gradient at 0%-100% over text, and hit\n    'mask' to impose that gradient on the text.\n- the object on top will apply its gradient onto the object below\n\n# Shortcuts\n- `cmd+1`/`cmd+2` - zoom in/out on object\n- `shift+cmd+l` - lock an object from movement\n- `shift+cmd+h` - hide an object \n- `cmd+[`/`cmd+]` - send layer back/fwd\n    - `cmd+opt+[]` - send to bottom/top\n","n":0.04}}},{"i":222,"$":{"0":{"v":"Security","n":1},"1":{"v":"\nSecurity by obscurity is generally not a thing, since DigitalOcean and AWS etc have known IP ranges that all hackers always target; if you don't ban them they'll eventually brute force you\n- Use ssh keys from day 1 and/or install fail2ban (preferably both)\n    - instead of fail2ban, you can whitelist only your CDN's IP addresses (like Cloudflare), through the use of DigitalOcean firewalls (or something similar). That way, you can’t get DoS’d because everything has to go the CDN route.","n":0.112}}},{"i":223,"$":{"0":{"v":"Malware","n":1},"1":{"v":"\nMalware is a catch-all phrase that includes any kind of bad software\n- ex. malware can be used to install spyware on a host\n\nA virus is a specific type of malware that requires user interaction to infect the user’s device\n- viruses are typically self-replicating. If you open an email attachment that inadvertently runs some malicious code, it will then send off more spam emails. \n\nA worm is another type of malware, and differs from a virus in that it doesn’t require user interaction. \n- ex. User runs a vulnerable network app which the hacker can send malware to. \n\nA Trojan horse is another type of malware, where the malware is hidden as a part of a useful piece of software\n\nA Botnet is a network of infected (\"mind controlled\") hosts that can be used by a hacker to do malicious things, like send out spam email or launch DDOS attacks","n":0.082}}},{"i":224,"$":{"0":{"v":"DDOS","n":1},"1":{"v":"\nDDOS attacks leverage the internet’s ability to reroute traffic (ie packet switching)\n","n":0.289}}},{"i":225,"$":{"0":{"v":"Sdk","n":1},"1":{"v":"\n## SDK\nAn SDK is an interface that is defined by some provider","n":0.289}}},{"i":226,"$":{"0":{"v":"Sanity","n":1}}},{"i":227,"$":{"0":{"v":"Rich Text","n":0.707},"1":{"v":"\nConsider that we can style text when it is in the HTML, such as bolding it, italicizing it, etc. But how does this work with dynamic text? This is not something we can build into our front-end code, because it changes depending on the data\n- ex. Blog posts have their own italicizing, bolding, paragraph breaks etc. built in. That \"formatting\" is coupled with the text itself. Therefore, it must be stored somehow. Sanity accomplishes this by storing content as Rich Text\n\nWhen you query your Sanity project’s API your Rich Text content is returned as Portable Text\n- Other Headless CMSs may store the data in html or markdown.","n":0.097}}},{"i":228,"$":{"0":{"v":"Portable Text","n":0.707},"1":{"v":"\nIn its simplest form, Portable Text is an array of objects of `type` with an array of `children`\n\nPortable Text is an agnostic abstraction of Rich Text.\nIt can be serialized into any markup language\n- ex. HTML, Markdown, XML\n\nPortable Text is built on the idea of rich text as an array of blocks, themselves arrays of children `<span>`s.\n- Each block can have a style and a set of mark definitions\n\n\n```json\n{\n  \"style\": \"h1\",\n  \"_type\": \"block\",\n  \"children\": [\n    {\n      \"_type\": \"span\",\n      \"text\": \"This is a heading\"\n    }\n  ]\n}\n```\n\n### Block\nA section of text, such as paragraph or heading\n\nIncludes:\n- `children`\n- `_type`\n- `style`\n- `markDefs`\n- `listItem`\n- `level`\n\n- includes all styling that is part of the section of text\n    - ex. if a paragraph has 2 italicized words in it, then it will result in 5 objects in the `children` array, but it will be part of the same block.\n\n#### `children`\nThe elements of the `children` array are sections of a block. They maintain the styling of the rich text. For instance, if the following were Rich Text, then we'd end up with a `children` array of length 5:\n```md\nI have to tell *you* what is happening! In fact, it was my **uncle** who shot at us.\n```\n\nIt is split up like this;\n```js\nconst children = [\n    { text: \"I have to tell \", marks: []},\n    { text: \"you\", marks: ['em']},\n    { text: \"what is happening! In fact, it was my \", marks: []},\n    { text: \"uncle\", marks: ['strong']},\n    { text: \" who shot at us\", marks: []},\n]\n```\n\n#### `_type`\nAll blocks must be of a type. The type makes it possible for a serializer to parse the contents of the block.\n\n#### `style`\nStyle typically describes a visual property for the whole block (ie. all children).\n\n#### `markDefs`\nMark definitions is an array of objects with a key, type and some data\n\nMark definitions describe data structures distributed on the children.\n- Mark definitions are tied to spans in `children` by adding the referring `_key` in the child's `marks` array.\n```json\n{\n  \"markDefs\": [\n    {\n      \"_key\": \"some-random-key\",\n      \"_type\": \"link\",\n      \"href\": \"https://portabletext.org\"\n    },\n    {\n      \"_key\": \"some-other-random-key\",\n      \"_type\": \"comment\",\n      \"text\": \"Change to https://\",\n      \"author\": \"some-author-id\"\n    }\n  ]\n}\n```\n\n* * *\n\n### Custom blocks\nCustom blocks is typically images, code blocks, tables, video embeds, or any data structure. \n- Custom blocks should be given a `_type`.\n```json\n{\n  \"_type\": \"image\",\n  \"asset\": {\n    \"_ref\": \"some-asset-reference\"\n  }\n}\n```\n```json\n{\n  \"_type\": \"code\",\n  \"language\": \"javascript\",\n  \"code\": \"console.log(\\\"hello world\\\");\"\n}\n```\n\n* * *\n\n## Full example\n```json\n[{\n    \"_type\": \"block\",\n    \"_key\": \"3628734dd519\",\n    \"style\": \"normal\",\n    \"markDefs\": [{\n        \"_type\": \"link\",\n        \"_key\": \"e556761904ba\",\n        \"href\": \"https://www.portabletext.org\"\n    }],\n    \"children\": [{\n            \"_type\": \"span\",\n            \"_key\": \"3628734dd5190\",\n            \"text\": \"This is a paragraph with a \",\n            \"marks\": []\n        },\n        {\n            \"_type\": \"span\",\n            \"_key\": \"3628734dd5191\",\n            \"text\": \"link\",\n            \"marks\": [\n                \"e556761904ba\"\n            ]\n        },\n        {\n            \"_type\": \"span\",\n            \"_key\": \"3628734dd5192\",\n            \"text\": \".\",\n            \"marks\": []\n        }\n    ]\n}]\n```","n":0.048}}},{"i":229,"$":{"0":{"v":"Serializing","n":1},"1":{"v":"\nSerializing is the process of taking Portable Text (ie. an array of Blocks) and converting to a format that can be rendered to a page, like HTML, React, Markdown etc.\n\n# E Resources\n[Codesandbox with some decent illustrations](https://codesandbox.io/s/portable-text-serializer-demo-all-defaults-p8ms8?file=/src/Page.js)\n[Sanity docs on presenting block text](https://www.sanity.io/docs/presenting-block-text)\n[Sanity's block-content-to-react library](https://github.com/sanity-io/block-content-to-react)\n","n":0.152}}},{"i":230,"$":{"0":{"v":"Images","n":1},"1":{"v":"\n```ts\nimport imageUrlBuilder from '@sanity/image-url'\n\nfunction urlFor (source: SanityImageSource) {\n  return imageUrlBuilder(client).image(source)\n}\n\nreturn (\n  {authorImage && (\n    <div>\n      <img\n        src={urlFor(authorImage)\n          .width(50)\n          .url()}\n      />\n    </div>\n  )}\n)\n```","n":0.213}}},{"i":231,"$":{"0":{"v":"Groq","n":1},"1":{"v":"```\n*[_type == \"post\" && slug.current == $slug][0]\n```\nMeans:\n`*` \n- select all documents\n\n`[_type == 'post' && slug.current == $slug]` \n- filter the selection down to documents with the type \"post\" and those of them who have the same slug to that we have in the parameters\n\n[0] \n- select the first and only one in that list\n\n## Breakdown\n### Filter\nthe first set of `[]` is the filer. We return data the type of data we are seeking, based on what parameters etc.\nex. return data that are of either type `Movie` or `Person`, and `popularity > 15`, or `releaseDate > 2016-04-25`\n\n### Slice\nthe second set of `[]`\nAllows us to determine which items we will get back from the query\nex. first 5\n\n### Information returned\nwhat's found between `{}`\nex. return `title`, `author`, and `publishedDate` from the query for all blog posts.\n\n### Ordering\nwhat's found after the ` | `\n\n# Resources\n[Cheat sheet](https://www.sanity.io/docs/query-cheat-sheet)","n":0.084}}},{"i":232,"$":{"0":{"v":"Document","n":1},"1":{"v":"\nEverything in the Studio starts with the document. A document is what you create and edit in the studio—all the other types you may define live inside the documents. In the default studio configuration, the document-types are the ones that will be listed in the content-column.\n","n":0.147}}},{"i":233,"$":{"0":{"v":"Rxjs","n":1},"1":{"v":"\n# Overview\nThe following table shows what role observables fulfill. When we want to get a single value for a synchronous action, we set the value to a variable. When we want to get a single value for an async action, we use promises. When we want to get the value for an array synchronously, we use an array. Finally, when we want to get the value of an array of values asynchronously, we use observables.\n\n|          |sync    |async     |\n|----------|--------|----------|\n|single    |variable|promise   |\n|collection|array   |observable|\n\nObservables are like arrays because they represent a collection of events but are also like promises as they’re asynchronous: each event in the collection arrives at some indeterminate point in the future.\n- This is distinct from a collection of promises (like `Promise.all`) as an observable can handle an arbitrary number of events, and a promise can only track one thing.\n\nAn observable can be used to model clicks of a button. It represents all the clicks that will happen over the application’s lifetime, but the clicks will happen at some point in the future that we can’t predict.\n```js\nlet myObs$ = clicksOnButton(myButton);\n```\n\n## \"Mapping + Flattening\" Operators\n- ***flatten*** - subscribing inside a subscribe\n- All work mostly in same manner\n    - They map some value to an observable (you are the one in charge of returning an observable value from them, they just map it)\n    - They flatten the observable you return ( they just subscribe to it)\n    - They decide about what to do before / after they flatten (“Flattening Strategy”)\n### mergeMap - the slacker operator\n- simply keep subscribing to every new observable that we return from the map\n- Other than mapping + flattening the observable, it does nothing else.\n- ex. Imagine Netflix shows up-to-date ratings for each movie, retrieved from IMDB's API. We can `mergeMap` the movie into an http request to IMDB to get this data and enhance our UI.\n### switchMap - the \"latest and greatest\" operator\n- unsubscribe from the last mapped observable\n- ex. Imagine we are typing in Google and the autocomplete box shows up. Of course, these suggestions change with each key press. If we use `switchMap`, each previous request will be cancelled if a new one happens. If we'd used `mergeMap`, a request for each keystroke would be made\n### concatMap - the \"wait in line\" operator\n- queue up the observables one after the other, and play their events in that order (i.e. subscribe to the next Observable in the queue only when the previous one is completed).\n- Similar to `mergeMap`, except order matters\n- ex. top 10 list\n### exhaustMap - the \"do not disturb\" operator\n- ex. login button - since we don't want multiple clicks to be registered, we want want to disable mapping while the first http request is on the go, ensuring that we never call the server while the current request is running.\n\n\n# Redux-Observable (library)\n- ***def*** - a redux middlware allowing us to `map` and `filter` actions with RxJS operators.\n    - While javascript `map` and `reduce` allows us to transform arrays, these versions allow us to transform *streams of actions*\n## Epic\n- ***def*** - a function that takes in a stream of actions, and returns a modified stream of actions.\n    - receive `variable$` as input\n- epics can be thought of as a description of what additional actions redux-observable should dispatch\n- epics are analogous to sagas from redux-saga\n\nexample:\n```js\nconst pingEpic = action$ => action$.pipe(\n  ofType('PING'), //equivalent to filter(action => action.type === 'PING')\n  mapTo({ type: 'PONG' })\n);\n\n// later...\ndispatch({ type: 'PING' });\n```\n- pingEpic will listen for actions of type PING and map them to a new action, PONG. This example is functionally equivalent to doing this:\n```js\ndispatch({ type: 'PING' });\ndispatch({ type: 'PONG' });\n```\n- Epics run alongside the normal Redux dispatch channel, after the reducers have already received them. When you map an action to another one, you are not preventing the original action from reaching the reducers; that action has already been through them\n\n## Operators\n- `ofType` - filter by a specific type of action\n\n# Stream\n- ***def*** - a sequence of data elements made available over time\n- Can be thought of as items on a conveyor belt being processed one at a time, rather than in large batches\n- Can also be seen as a sequence of ongoing events ordered in time\n    - ex. number of button clicks in 1 second\n        - All the clicks will be grouped together as a stream\n- The stream is the subject which is being observed\n\n* * *\n### Promise vs Observable\n- Promises handle a single event (ie. the failure or success of an asyn operation).\n- An observable is a function that returns a stream, and we can pass in zero or more events to it. The callback provided to the observable will be called for each event.\n\t- Observables are preferred over promises because they don't care how many events you have, while promises require 1, and only 1.\n- Observable libraries provide methods to help interact with the emitted (returned?) value(s). For instance, we can use `map` to transform each value's output\n- a Promise is eager, whereas an Observable is lazy\n- If you wanted to subscribe to the reactive way of programming, then you could just \"observable all the things\"\n\nAdditionally...\n- Observables are cancellable.\n\n# Questions\n- what is meant by inner/outer observable\n    - [source](https://academind.com/learn/javascript/callbacks-vs-promises-vs-rxjs-vs-async-awaits/)\n\n# Resources\n- [Operators](http://reactivex.io/documentation/operators.html)\n- [thinking in nested streams](https://rangle.io/blog/thinking-in-nested-streams-with-rxjs/)\n","n":0.034}}},{"i":234,"$":{"0":{"v":"Objects","n":1}}},{"i":235,"$":{"0":{"v":"Subscription","n":1},"1":{"v":"\n***subscription*** - an object that represents the execution of an *observable*\n\nMuch like a promise, we need to unwrap our observable to access the values it contains. The observable unwrapping method is called subscribe. The function passed into subscribe is called every time the observable emits a value (in this case, a message is logged to the console anytime the button is clicked).\n\n```js\nlet myObs$ = clicksOnButton(myButton);\nmyObs$\n.subscribe(clickEvent => console.log('The button was clicked!'));\n```\n\nOne thing to note here is that observables under RxJS are lazy. This means that if there’s no subscribe call on `myObs$`, no click event handler is created. Observables only run when they know someone’s listening to the data they’re emitting.\n\nThe function passed into subscribe is called every time the click event happens(the observable emits a value)\n\n- `.create` accepts a subscribe function\n    - `subscribe` accepts an *observer argument*\n```js\n// This first part is the observable, which emits things\nconst myObservable = Observable.create(function subscribe(observer) {\n    observer.next('hey!') //this is emitting a value\n})\n\n// To grab the value, we define an observer\n// (x is the observer)\nconst observer = myObservable.subscribe((x) {\n    console.log(x) // hey!\n})\n```\n","n":0.075}}},{"i":236,"$":{"0":{"v":"Observable","n":1},"1":{"v":"\n- ***Observable*** - A data structure representing the idea of an invokable collection of future values or events\n\t- in other words, they are a representation of any set of values over any amount of time\n- Observables allow us to compartmentalize our eventing flows, encapsulating each action in a single function\n- Observables are lazy Push collections of multiple values\n- An observable is a function that returns a stream\n- An observable is a function that takes an observer and returns a cancellation function\n- `$` is used to indicate that the variable in question is observable\n- variables that reference a stream are denoted with `$` (ex. `action$`)\n    - ex. *observable*\n    - an observer is an object with `next`, `error` and `complete` methods\n- Observables are inert (they just sit there until they are *subscribed* to), while observers stay active and listen for events from the producers\n- The problem is this: we can consider an array where we already know all the values as eager, and an array that receives values (ie. increases length) at a set interval (1s) as lazy. Normally, we perform data processing in an eager way, since the data is processed immediately as it's received, which is instant. What if we are in a position where the array grows over time? It would be beneficial if we could call a method on an array value as it enters the array. In this sense, we need to subscribe to the array to execute a method when the array grows.\n\nobservables are conceptually similar to a fancy event emitter.\n\n- There are 2 sides to an observable: producer and consumer\n    - **Producer** - adds to the array\n        - ex. button clicks add that click event to the array\n    - **Consumer** - calls the function on the new array item\n        - ex. calls `console.log` in response to the new click event\n\n- An `Observer` subscribes (ie. is consumer) to an `Observable`\n    - an observer is a collection of callbacks\n\nObservables are lazy Push collections of multiple values. They fill the missing spot in the following table:\n\n|      | Single   | Multiple   |\n|------|----------|------------|\n| Pull | Function | Iterator   |\n| Push | Promise  | Observable |\n- They *push* with `.next()`\n\n\n## Characteristics\n- They are time-independent (ie. lazy)\n- They are mostly used in asynchronous data streams, like web sockets or multiple concurrent api calls\n\n- Just as promises abstract time away from our concern for a single asynchronous operation, observables abstract time away from a set of data (eg. array)\n    - ex. Observable can be set up to listen for mouseclicks, by pushing onto an array each time the mouse is clicked. The fact that the *subscribed* function exists means that it will fire whenever the array increases in size.\n\n\n- If you combine the functionality of an Observer and an Observable, you get a Subject\n\n- each operator on an Observable returns a new Observable, meaning they are chainable (this is known as a *stream*)\n\n- A map(..) on an array runs its mapping function once for each value currently in the array, putting all the mapped values in the outcome array. A map(..) on an Observable runs its mapping function once for each value, whenever it comes in, and pushes all the mapped values to the output Observable.\n\n**epic** - a collection of observables\n\n```js\nexport default function fetchTeams(action$) {\n    // epic\n    return action$.pipe(\n        //operator\n        ofType('FLASHCARDS_TEAMS_GET'),\n        //operator\n        switchMap(() =>\n            queryGraphQL('flashcards__teams'),\n        ),\n        //operator\n        map(resp => { operator\n            const flattenedData = flattenGraphqlNode(resp).data.teams;\n            return getTeamsSuccess(flattenedData);\n        }),\n        catchError(err => {\n            getError(err);\n        }),\n    );\n}\n```\n\n## Using observables\n- An `observable` is a function that takes in an `observer` (an object with `next`, `error`, and `complete`) and returns cancellation logic (i.e. unsubscribe).\n- `.next()` is called when the observable produces values\n- When an observer subscribes to an observable, the observer will keep receiving values until one of 2 things happens:\n    - there are no more values to be sent (in which case `.complete()` is called)\n    - the observer calls `.unsubscribe()` on the observer\n- fromEvent will turn an event into an observable\n    - `fromEvent(<event to be listened to>, <eventName>)`\n```js\nconst input$ = Rx.Observable.fromEvent(node, 'input')\n  .map(event => event.target.value)\n  .filter(value => value.length >= 2)\n  .subscribe(value => {\n    // use the `value`\n  });\n```\nHere are the steps of this sequence:\n1. Let’s assume the user types the letter “a” into our input (node is a variable that has query selected the html input element)\n2. The Observable then reacts to this event, passing the value to the next observer\n3. The value “a” is passed to `.map()`, which is subscribing to our initial observable\n4. `.map()` returns a new Observable of event.target.value and calls `.next()` on it’s observer\n5. The `.next()` call will invoke `.filter()`, which is subscribing to `.map()`, with the resulting value of the .map() call\n6. .filter() will then return another Observable with the filtered results, calling `.next()` with the value if the length is 2 or above\n7. We get the final value through our `.subscribe()` block\n\n- Each time a new Observable is returned, a new observer is hooked up to the previous Observable (allowing us to pass values along a stream of observers, which do something we've asked, then call `.next()`, then pass the result to the next observer.\n    - Basically, an operator returns a new Observable each time, allowing the stream to continue\n","n":0.034}}},{"i":237,"$":{"0":{"v":"Ruby","n":1}}},{"i":238,"$":{"0":{"v":"Yield","n":1},"1":{"v":"\nIn Ruby, you can pass in a code block to a method\n- From within the method, every time `yield` is called, that code block that was passed in will get executed","n":0.18}}},{"i":239,"$":{"0":{"v":"Rss","n":1},"1":{"v":"\nAn RSS feed is an application with a data layer that stores URLs that return XML data from a feed.\nex. we store `https://reddit.com/r/investing.rss` in the database, and when I open the RSS feed, a GET request is made and all the posts are returned.\n\n### Make a subreddit RSS feed\nSimply add `.rss` to the subreddit URL\n- ex. `https://www.reddit.com/r/hockey.rss`","n":0.132}}},{"i":240,"$":{"0":{"v":"Retool","n":1},"1":{"v":"\nRetool allows us to create internal tools that can hook into our APIs and database. With Retool, we can read/write data in a UI that we don't have to build ourselves.\n- [home](https://retool.com/)\n- [ToolJet: Untested alternative to Retool](https://tooljet.io/)","n":0.164}}},{"i":241,"$":{"0":{"v":"Regex","n":1},"1":{"v":"\n### Types\nThe \"normal\" regex is Perl-Compatible Regex (PCRE)\n- may be contrasted with Basic Regular Expressions (BRE)\n- Vim Regex largely follows BRE\n\n#### BRE\nThe main difference between the two is that BRE tends to treat more characters as literals - an \"a\" is an \"a\", but also a \"(\" is a \"(\", not a special character - and so involves more backslashes to give them \"special\" meaning.\n\n\n- **magic** means we don't have to escape a character for it to take on its special meaning. **nomagic** means we have to escape, because otherwise the character will be taken literally\n    - ***ex.*** with magic, `.` will mean \"stand in for any character\". with nomagic, it will literally mean \"match the `.` character\"\n    - this makes sense, because with magic, a lot of cool stuff is happening that we have no idea how it's being done. Without magic, it's just looking for a character\n- `\\V` Verbatim Switch - only `\\` has special meaning (ie. very nomagic)\n    - prevent regex symbols from taking over - `/\\Va.k.a` (equivalent to `/a\\.k\\.a\\.`)\n    - in regex, `.` means \"match any character\". making a verbatim search removes that functionality\n    - when we use `\\V`, it means for the following search, only `\\` will have a special meaning\n- `\\v` Literal Switch - all characters (except word characters (**[a-zA-Z_]**)) have special meaning (ie. very magic)\n- delimit words - since the word \"the\" appears in \"these\", if we wanted to just search for the word \"the\", we can use delimiters\n    - `/\\v<the>`\n- ignore case - `/search\\c`\n- enforce case - `/search\\C`\n- Anything that matches inside of parentheses is automatically assigned to a temporary silo.\n    - we can reference these matches in any later commands, such as a `:%S` search and replace command\n    - We can reference the captured text within the first set of parens as `\\1` (and `\\2` for the second set of parens, and so on) \n    - `\\0` refers to the entire match, whether or not there were parens\n    - tip: use `%` to not capture the following parens\n        - imagine we want to find and replace all occurrences of a first and last name, then replace it by the format `LAST, FIRST`. Notice the where we use and omit the `%` in order to control which matches are going to the `\\1` and `\\2` registers. Here, we don't care whether \"Drew\" or \"Andrew\" was matched, so we don't bother registering it.\n            - `/\\v(%(And|D)rew) (Neil)`\n            - `:%s//\\2, \\1/g`\n\n### Character Classes (make the letter capital to negate the character (ex. `\\S` for non-whitespace char)\n- whitespace char - `\\s`\n- digit - `\\d`\n- hex-digit - `\\x`\n- word character (alphanumeric + underscore) - `\\w`\n- alphabetic char - `\\a`\n- lowercase char - `\\l`\n- uppercase char - `\\u`\n\n# Word boundaries\n- `pattern` - the regex text that we type into the search field\n- `match` - any text in the document that is highlighted as a result of the search\n- `<`/`>` - match the beginning/end of a word\n\n## Lookaheads/Lookbehinds\n- while the boundaries of a match normally correspond with the start and end of a pattern, we can use `\\zs` and `\\ze` to crop the beginning and end of a match, making the new match a subset of the pattern\n    - ex. search for matches of \"eagle\", but only when it follows the word \"bald\"\n        - `/bald \\zseagle`()\n        - In this example, \"bald eagle\" still forms part of the pattern, but only the word \"eagle\" is matched\n    - ex. search for everything inside quotes, without the quotes themselves\n        - `/\\v\"\\zs[^\"]+\\ze\"`\n\n### Backreference\n- allow you to match a particular pattern, then use a variable that refers back\n    to it\n    - Say we wanted to match an html tag and its content. Since there is\n        repetition in an opening and closing tag (`<div>`/`</div>`), we can use\n        a back reference to capture `div` in a variable, in order to use it\n        later in the regex pattern\n- a related, but distinct term.\n\n# Fine slicing\n#### Character Set\n- match any character specified - `[a-zA-Z0-9]`\n- match either an `f` or an `o` - `(f|o)`\n- search for any 6 of the preceding characters - `{6}`\n    - ex. `/\\v#([a-fA-F0-9]{6})` will match any hex code\n- search for between 4 and 6 matches of the preceding character - `{6,8}`\n- search for at least 6 matches of the preceding character - `{6,}`\n    - In other words, leave upper range unbounded\n- negate the following search - `^`\n- multiple occurrences of the same character\n    - 0 or 1 times (ie. it is optional) -s `?`\n    - 0 or more times - `*`\n    - 1 or more - `+`\n- match any character - `.`\n- match any character 0 or more times - `.*`\n    - doesn't have to be the same character many times to work\n        - ex. `.*dog` matches `yyydog` and `yfwdog`\n\n# Useful Regex Patterns\n- `ag '\\{\\s.*ListView.*from\\s.react-native.'` - search for a non-default\n    exported module from a specific module\n","n":0.035}}},{"i":242,"$":{"0":{"v":"Redux","n":1},"1":{"v":"\nRedux uses a different instance of the store for every request.\nRedux is actually based in part on [[CQRS|general.patterns.CQRS]] and [[event sourcing|general.patterns.event-sourcing]]\n\nre-render performance in Redux is faster in comparison to Context API because every component that consumes the Context will re-render even if the state relevant to that component hasn’t changed.\n\n# Tools\n[Redux Toolkit: Opinionated Redux](https://redux-toolkit.js.org/)","n":0.136}}},{"i":243,"$":{"0":{"v":"Selectors","n":1},"1":{"v":"\n# Selectors \nIf actions are the entry points, selectors are the exit.\n- After dispatching an action, the application can use selectors to get the resulting data from the store after reducers and sagas have done something with it.\n### useSelector\n- equivalent to the purpose of `mapStateToProps`, which is provide easy access to the particular parts of the store that you want. \n\t- when a component has multiple instances, the selector needs to be defined inside the component, so that each instance gets its own selector instance.\n\n## Reselect\n- Will memoize the value that is returned by the selected value\n    - Imagine there is a list of questions in the redux store, and we want to retrieve all questions that have an even-numbered `id`. We could write a reselect selector that gets this array, performs a `filter` on it, memoizes (caches) that function, then return it to us. Then, next time we call that selector, the value is already available to us, rather than having to perform that `filter` operation on the array all over again.\n- If we are just getting data without modification (ex. getting `store.title`), then reselect is not necessary. Only when there is expensive computation is reselct beneficial.\n- `createSelector` will take in 1+ selectors (functions), call them, and passes their return values (the selected part of the store) as arguments to the final arg (an anonymous transform function) of `createSelector`. That function will accept 1 parameter for every selector that came before it. \n\t- If one of the input selectors has changed since the last call, the transform function will be called. Otherwise, it will just return the memoized value.\n    - this function is a thunk (returns a function)\n    - ***ex.***\n    ```\n    export const getFeedbackForYou = createSelector(\n        getByRecipient,\n        (byRecipientState) => byRecipientState.myFeedback,\n    );\n    ```\n    - here, `createSelector` is being called and returns a function that takes state as the argument (since that's what thunks do).\n    - conceptually, we can therefore replace `createSelector(...)` with `(state) => \n\n* * *\n\n- `mapStateToProps` is really just a type of selector when you think about it.\n\t- specifically, it is the selector that (potentially) uses other selectors to get data from the store. It is meant to clump them together for easy access to components that get rendered by the container \n- `state` refers to the data that currently exists in the store. `store` refers to the instance","n":0.051}}},{"i":244,"$":{"0":{"v":"Reducers","n":1},"1":{"v":"\n# Reducers\nreducers are supposed to return the initial state when they are called with undefined as the first argument, no matter the action.\n","n":0.209}}},{"i":245,"$":{"0":{"v":"Mw","n":1},"1":{"v":"\nMiddlewares only wrap store.dispatch()","n":0.5}}},{"i":246,"$":{"0":{"v":"Thunk","n":1},"1":{"v":"\nA thunk is a pure function that returns an impure function \n## The pattern of providing dispatch to a helper function\n- imagine we wanted to dispatch 2 actions that normally occur together, such as an action to show a notification, and an action to hide it 5 seconds later. The simplest approach is to dispatch an action to show it, then dispatch an action 5 seconds later to hide it. But what if we want to use this combination of 2 actions elsewhere in the app? What we can do is create an external function that accepts `dispatch` as the first arg, and the payload that goes to both dispatches as the rest of the args. \n\t- Not only does this give us code reuse, but it also allows us to have better control over *that* action, and only that action. In our first naive method, we have no idea if the notification related to the action being dispatched 5 seconds later is associated with the notification related to the action dispatched instantly. For instance, if a notification is trigged and then milliseconds later another is triggered, we might end up in a race condition where our app doesn't know how to match up the `show` and `hide` actions with the correct notification. \n\n```js\n//Naive method\nthis.props.dispatch({ type: 'SHOW_NOTIFICATION', text: 'You logged in.' })\nsetTimeout(() => {\n  this.props.dispatch({ type: 'HIDE_NOTIFICATION' })\n}, 5000)\n\n// external function to increase code reuse\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(dispatch, text) {\n  // Assigning IDs to notifications lets reducer ignore HIDE_NOTIFICATION\n  // for the notification that is not currently visible.\n  const id = nextNotificationId++\n  dispatch(showNotification(id, text))\n\n  setTimeout(() => {\n    dispatch(hideNotification(id))\n  }, 5000)\n}\n```\n\n## Motivation for Thunks\n- the above solution works but it presents 2 issues:\n\t- we have to pass `dispatch` around everywhere. This means that where we need to call that helper function, we need to have access to `dispatch`, meaning we need to create a container for that component. \n\t- We have to mentally keep track which redux `actions` are just regular action creators, and which are actually more like action helpers (above example. since they do not return an action, they are not action creators)\n\n## What are thunks\n- A thunk is a middleware that gives `dispatch` the ability to accept a function. Normally dispatch can only accept an action (usually dispatch accepts the invocation of an action creator, which returns an action). With Thunk middleware (mw being something that gives enhanced functionality), `dispatch` gains the ability to accept functions.  \n\t- When a function is dispatched, Thunk MW will recognize that and pass `dispatch` as the first argument, allowing us to dispatch any action we want within the function (ie within the thunk)\n- The result is a function that is very similar to the external helper function we made before. There are 2 key differences: \n\t- our new function doesn't accept `dispatch` as the first prop\n\t- our new function *returns* a function of its own, and *that* function accepts `dispatch` as it's first argument\n```js\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  return function (dispatch) {\n    const id = nextNotificationId++\n    dispatch(showNotification(id, text))\n\n    setTimeout(() => {\n      dispatch(hideNotification(id))\n    }, 5000)\n  }\n}\n```\n\n## Using thunks\nwithout the Thunk MW, we could execute this function like so:\n```\nshowNotificationWithTimeout('You just logged in.')(this.props.dispatch)\n```\nbut since we are using the MW, any time a function is dispatched instead of an action object, thunk takes over and calls that function with `dispatch` as its first argument, giving the function access to `dispatch`\n- therefore, we just do this \n```\nthis.props.dispatch(showNotificationWithTimeout('You just logged in.'))\n```\n\nWith thunks, dispatching an asynchronous action (really, a series of actions) looks no different than dispatching a single action synchronously to the component. Which is good because components shouldn’t care whether something happens synchronously or asynchronously. We just abstracted that away. The component only knows about the thunk, and the thunk deals with the async logic\n\nThunks also receive as the second arg the application store. This is useful for if we want to bail out of an API call \n- ex. user has notifications turned off:\n```js\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  return function (dispatch, getState) {\n    // Unlike in a regular action creator, we can exit early in a thunk\n    // Redux doesn’t care about its return value (or lack of it)\n    if (!getState().areNotificationsEnabled) {\n      return\n    }\n\n    const id = nextNotificationId++\n    dispatch(showNotification(id, text))\n\n    setTimeout(() => {\n      dispatch(hideNotification(id))\n    }, 5000)\n  }\n}\n```\n- note:  If you use `getState()` only to conditionally dispatch different actions, consider putting the business logic into the reducers instead.\n\nThunks may also return promises. In fact, Redux doesn’t care what you return from a thunk, but it gives you its return value from `dispatch()`\n- This is why you can return a Promise from a thunk and wait for it to complete by calling `dispatch(someThunkReturningPromise()).then(...)`\n- You may also split complex thunk action creators into several smaller thunk action creators. The dispatch method provided by thunks can accept thunks itself, so you can apply the pattern recursively. Again, this works best with Promises because you can implement asynchronous control flow on top of that.\n\n## When thunks aren't enough, and to reach for Sagas\nFor some apps, you may find yourself in a situation where your asynchronous control flow requirements are too complex to be expressed with thunks.\n- ex. retrying failed requests, reauthorization flow with tokens, or a step-by-step onboarding\n* * *\n[source](https://stackoverflow.com/questions/35411423/how-to-dispatch-a-redux-action-with-a-timeout/35415559#35415559)","n":0.034}}},{"i":247,"$":{"0":{"v":"Saga","n":1},"1":{"v":"\n# What is it?\n- a saga is a daemon that lets us define long-running processes that take actions as they come, and transform or perform requests before outputting actions. This moves the logic from action creators into sagas\n- a saga is a separate thread in the app that's just for side-effects\n\t- while thunks utilize callbacks, a saga thread can be started, paused (`yield`) and cancelled by dispatching actions within generator functions\n- in synchronous redux, a dispatched action is assigned to a reducer. In Async redux, a dispatched action is assigned to a saga. The saga does its side effect (`resourceListReadRequest`), and takes the returned data, and dispatches another action (`resourceListReadSuccess`) which is then picked up by the reducer.\n\n## Reconciling Generator fns and Saga\n- Imagine a saga as a thread that constantly calls `next()` and tries to execute the `yield` lines as soon as it can\n- spec: like a promise, it will wait on that value to \"return\" before it calls `next()` and goes to the next `yield`\n- In Sagas, we are \"yielding\" to the redux-saga middleware\n\t- The MW suspends the saga until the yielded side effect resolves. At this point, the MW calls `next()`\n- When we say `yield call(___)`, we are only describing what we want to happen. We aren't describing the actual outcome. In this sense, it is declarative.\n\n# Types of Saga\n### Worker Saga\n- def - Sagas that perform side effects and dispatch other actions asynchronously.\n\n### Watcher Saga\n- def - Sagas that listen for dispatched actions and call worker sagas in response\n\n### Root Saga\n- def - Sagas that run all watcher sagas in parallel\n\n# Example process\n1. An action `resourceListReadRequest` is dispatched somewhere in the code\n2. A *Watcher Saga* that is designed to listen for `resourceListReadRequest` picks up on the fact that it was dispatched, and notifys the *Worker Saga*\n\n# API Reference\n### Effects\n*def* - an object containing instructions to be fulfilled by middleware. When the MW retrieves an effect yielded by a saga (ie. when a saga executes put, call, take etc), the saga pauses until the effect is done.\n- `call` - call the fn (1st arg) with args (rest args)\n- `put` - dispatch action\n- `take` - block execution of the saga until the provided action is dispatched\n\t- therefore, this is used in a watcher saga\n- `fork` - useful when a saga needs to start a non-blocking task\n\t- Non-blocking means: the caller starts the task and continues executing without waiting for it to complete\n\t- Situations:\n\t\t1. grouping sagas by logical domain\n\t\t2. keeping a reference to a task in order to be able to cancel/join it\n- `takeEvery` - each time a particular action is dispatched, spawn a saga\n\t- `takeEvery` uses `take` and `fork` under the hood\n- `takeLatest` - spawn a saga only for the latest dispatched action of a given type\n\t- ex.  imagine a user is mashing the login button. with Thunk, an API call would be made with each button press. with redux-saga, we get to just take the latest one and ignore the rest\n\n# UE Resources\n[redux-saga primer](https://flaviocopes.com/redux-saga/)\n- [also](https://medium.com/appsflyer/dont-call-me-i-ll-call-you-side-effects-management-with-redux-saga-part-2-cd16f6bcdbcd)","n":0.045}}},{"i":248,"$":{"0":{"v":"Actions","n":1},"1":{"v":"\n# Actions\nAn action creator is found in the /actions directory. When they are called, an action is created. However, it doesn't do anything, it just floats there. The action becomes mobile only once it is dispatched. When store.dispatch is called with the action as its argument, the action is able to reduce the payload into the state\n","n":0.132}}},{"i":249,"$":{"0":{"v":"Redis","n":1},"1":{"v":"\nWith Redis, the data is (ideally) all in memory, making the lookups much faster.\n\nRedis is a good fit in situations where you want to share some transient, approximate, fast-changing data between servers, and where it’s not a big deal if you occasionally lose that data for whatever reason\nex. a good use case is maintaining request counters per IP address (for rate limiting purposes) and sets of distinct IP addresses per user ID (for abuse detection).\nex. Real time Analytics - you can use Redis to store user identities and their transaction details when implementing a real-time fraud detection service.\nex. You can use Redis to queue application (FIFO) tasks that take a long time to complete\n\n- stands for *Remote Dictionary Server*\n- used as a database, cache, message broker, and queue\n- All Redis data resides in-memory RAM\n- Redis delivers response times of less than 1ms, enabling millions of requests per second\n- Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps.\n\nYou can run atomic operations on the contents of the redis cache.\n\nRedis supports [asynchronous replication](https://redis.io/topics/replication)\n\n### Cache invalidation\nOne of the ways to achieve cache invalidation in Redis is through a built-in Pub/Sub system. It can be usd to send invalidation messages to clients listening, so that the client knows it should fetch again if it wants up to date data.\n- This can be made to work but is tricky and costly from the point of view of the bandwidth used, because often such patterns involve sending the invalidation messages to every client in the application, even if certain clients may not have any copy of the invalidated data\n\n## Client-side Caching (called Tracking)\nThere are 2 modes to client-side caching in Redis\n- *default* - server remembers which clients it sends data to. When the data is modified, it sends an invalidation message to the client, letting it know that its data is no longer up to date. This costs memory in the server side, since the server must keep the list of clients in memory.\n- *broadcast* - server does not attempt to remember what keys a given client accessed. Instead clients subscribe to key prefixes such as `object:` or `user:,` and will receive a notification message every time a key matching such prefix is touched.\n    - does not consume any memory on the server side, but instead sends more invalidation messages to clients.\n    - The server does not store anything in the invalidation table. Instead it only uses a different Prefixes Table, where each prefix is associated to a list of clients.\n\nThis is an example of the protocol:\n```\nClient 1 -> Server: CLIENT TRACKING ON\nClient 1 -> Server: GET foo\n(The server remembers that Client 1 may have the key \"foo\" cached)\n(Client 1 may remember the value of \"foo\" inside its local memory)\nClient 2 -> Server: SET foo SomeOtherValue\nServer -> Client 1: INVALIDATE \"foo\"\n```\n\nThis looks great superficially, but if you think at 10k connected clients all asking for millions of keys in the story of each long living connection, the server would end up storing too much information. For this reason Redis uses two key ideas in order to limit the amount of memory used server side, and the CPU cost of handling the data structures implementing the feature:\n- We must create an *invalidation table* which stores rows of clients that have cached a given key.\n    - Such invalidation table can contain a maximum number of entries, if a new key is inserted, the server may evict an older entry by pretending that such key was modified. Once this happens, it sends an invalidation message to the clients.\n\nclients do not need, by default, to tell the server what keys they are caching. Every key that is mentioned in the context of a read only command is tracked by the server, because it could be cached.\n\nThis has the obvious advantage of not requiring the client to tell the server what it is caching. Moreover in many clients implementations, this is what you want, because a good solution could be to just cache everything that is not already cached, using a first-in first-out approach: we may want to cache a fixed number of objects, every new data we retrieve, we could cache it, discarding the oldest cached object. More advanced implementations may instead drop the least used object or alike.\n\n# UE Resources\n[Redis Tutorials by Flavio](https://flaviocopes.com/tags/redis/)\n[Realtime delivery Architecture at Twitter](https://www.infoq.com/presentations/Real-Time-Delivery-Twitter/)","n":0.037}}},{"i":250,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n### Keys\nTry to stick with a schema. For instance `object-type:id` is a good idea, as in \"user:1000\". Dots or dashes are often used for multi-word fields, as in `comment:1234:reply.to` or `comment:1234:reply-to`.\n\n### Hash\n`field` and `value` form a pair. Both are strings\n- If this were Javascript, \n    - the object name would be the Redis `key`, \n    - the object key would be the Redis `field`, \n    - the object value would be the Redis `value`\n\n```\nuser:1000: {\n    \"username\": \"antirez\",\n    \"birthyear\": 1970\n}\n```\n\nMethods:\n- `HGET <key> <field>` / `HSET <key> <field> <value>` - get/set a field+value\n\n### Lists\nRedis lists are implemented via Linked Lists.\n- LinkedLists were used (as opposed to arrays) because for a database system it is crucial to be able to add elements to a very long list in a very fast way.\n- When fast access to the middle of a large collection of elements is important, Sorted Sets should be used\n\nCommon use-cases for Lists\n- Remember the latest updates posted by users into a social network.\n    - ex. Twitter takes the latest tweets posted by users into Redis lists.\n    - imagine your home page shows the latest photos published in a photo sharing social network and you want to speedup access.\n        - Every time a user posts a new photo, we add its ID into a list with `LPUSH`.\n        - When users visit the home page, we use `LRANGE 0 9` in order to get the latest 10 posted items.\n- Communication between processes, using a consumer-producer pattern where the producer pushes items into a list, and a consumer (usually a worker) consumes those items and executed actions. Redis has special list commands to make this use case both more reliable and efficient.\n\nMethods:\n- `LPUSH` / `RPUSH` - add element to the left/right\n- `LRANGE <listname> <fromindex> <toindex>` - extract ranges of elements from lists\n\n### Sets\nunordered collections of strings.\n\nSets are good for expressing relations between objects. For instance we can easily use sets in order to implement tags.\n- A simple way to model this problem is to have a set for every object we want to tag. The set contains the IDs of the tags associated with the object.\n\nMethods:\n`SADD` - add element to set\n\n### Sorted Sets\nsimilar to a mix between a Set and a Hash\n- Like sets, sorted sets are composed of unique, non-repeating string elements, so in some sense a sorted set is a set as well.\n- every element in a sorted set is associated with a floating point value, called the score (this is why the type is also similar to a hash, since every element is mapped to a value).\n\nMethods:\n`ZADD` - add element to set\n\n# E Resources\n[Redis Documentation](https://redis.io/topics/data-types-intro)","n":0.048}}},{"i":251,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\nOpen redis-cli shell - `redis-cli`\n\nGet all keys - `keys *`\nGet the type of a particular key - `type <key>`\nSet new key to hold string value - `set <key> <value>`\nSets field in the hash stored at key to value - `set <key> <field> <value>`","n":0.152}}},{"i":252,"$":{"0":{"v":"React","n":1},"1":{"v":"\n## Rendering\nA client-side rendered React app starts out as a simple html file with virtually nothing but `<script>` tags in its `<body>`. When the browser downloads the html, it runs the scripts, which is the bundle/chunks.\n- Once the browser downloads and parses those scripts, React will build up a picture of what the page should look like, and inject a bunch of DOM nodes to make it so.\n\nWhen we talk about \"re-rendering\" in react, we are talking about the shadow DOM, not the regular DOM. Re-rendering the shadow DOM is highly performant, which is why re-rendering components shouldn't be that big of a concern.\n\n- React splits all work into the “render phase” and the “commit phase”.\n\t- Render phase is when React calls your components and performs reconciliation.\n\t- Commit phase is when React touches the host tree. It is always synchronous.\n\n* * *\n\n- In React, anything other than updating the page is considered a side-effect. If you’re not using React to update state or render HTML, that’s a side effect. It’s any non-React thing.","n":0.076}}},{"i":253,"$":{"0":{"v":"React Typescript","n":0.707},"1":{"v":"\nThe `React.FC` type implicitly defines children as a type. Therefore we might not want to use fc, since we aren’t always using children. This would be implicit, and the argument is that if we are using children it should be explicitly defined in a type\n\n# Resources\n[React+Typescript Cheatsheet](https://github.com/typescript-cheatsheets/react)","n":0.146}}},{"i":254,"$":{"0":{"v":"Tools","n":1},"1":{"v":"\n[Rehooks: premade react hooks](https://github.com/rehooks)","n":0.5}}},{"i":255,"$":{"0":{"v":"Patterns","n":1}}},{"i":256,"$":{"0":{"v":"Composition","n":1},"1":{"v":"\nInstead of this:\n```js\nfunction App() {\n  const [someState, setSomeState] = React.useState('some state')\n  return (\n    <>\n      <Header someState={someState} onStateChange={setSomeState} />\n      <LeftNav someState={someState} onStateChange={setSomeState} />\n      <MainContent someState={someState} onStateChange={setSomeState} />\n    </>\n  )\n}\n```\n\nWe can do this:\n```js\nfunction App() {\n  const [someState, setSomeState] = React.useState('some state')\n  return (\n    <>\n      <Header\n        logo={<Logo someState={someState} />}\n        settings={<Settings onStateChange={setSomeState} />}\n      />\n      <LeftNav>\n        <SomeLink someState={someState} />\n        <SomeOtherLink someState={someState} />\n        <Etc someState={someState} />\n      </LeftNav>\n      <MainContent>\n        <SomeSensibleComponent someState={someState} />\n        <AndSoOn someState={someState} />\n      </MainContent>\n    </>\n  )\n}\n```","n":0.119}}},{"i":257,"$":{"0":{"v":"Batching","n":1},"1":{"v":"\nWith automated batching in React 18, multiple state updates will always be batched.\n\nThe problem is that when we update multiple pieces of state in sequence, React will re-render the component. We need a way to tell React \"hey, don't bother re-rendering until I've completed my whole sequence of state updates.\"\n\nWhen we implement a click handler, we are implementing the same benefits that batching gives us. Only a single re-render would be triggered from the following click handler:\n```js\nfunction handleClick() {\n  setIsFetching(false);\n  setError(null);\n  setFormStatus('success')\n}\n```\n\nHowever, what if we wanted to change the 3 pieces of state *not* in response to an event handler? Imagine we want the state to be updated when a fetch resolves?","n":0.094}}},{"i":258,"$":{"0":{"v":"Lang","n":1}}},{"i":259,"$":{"0":{"v":"Suspense","n":1},"1":{"v":"\nSuspense refers to React’s ability to “suspend” rendering while components are waiting for something, and display a loading indicator\n\nImagine a component that needs to do some asynchronous task before it can render, e.g. fetching some data.\nBefore Suspense, such a component would keep an isLoading state and return some kind fallback (an empty state, skeleton, spinner, ...) based on it.\nWith Suspense, a component can now, during rendering, shout \"HALT! I'm not ready yet. Don't continue rendering me, please - here's a pager, I'll ping you when I'm ready!1\"\nReact will walk up the tree to find the nearest Suspense boundary, which is set by You and contains the fallback that'll now be rendered until the pager is ringing.\n\nIn the traditional way of handling data fetching, the paradigm is: \"wait for Promise to resolve and run some user code\". With suspense, it's \"React will retry rendering when it resolves\". \n- In some ways it's simpler but it takes some \"unlearning\" because the user loses some control they might be used to from traditional Promise-base programming.\n\nsolves the problem of having some of the data, but not being able to render until all data is in. With suspense, we can load some of the UI without all of the data having to be available.\n- Suspense lets you pause rendering a component while it’s loading async data\n\nReact Suspense will specify a fallback component in case the main component suspends. \n\n![28a01e0b811e84e2c3c20a4faca82641.png](:/3b101f50bf264119a5a3d57e387812f4)\n\n## Process for data fetching and rendering in browser\n1. Browser downloads the code\n2. React begins to render the `<Home />` component\n3. React will attempt to use the data needed for the component, then will realize that the data doesn't exist yet. React will then look up the component tree, find its nearest Suspense boundary. Then it will then render the fallback component, and fetch the data simultaneously.\n4. Once data has been fetched (signified by a function like `useQuery` having been completed), React will render `<Home />` (which contains an `<img />` tag)\n5. The browser sees the `<img />` tag and makes a fetch to get that image.\n* * *\n- If we have multiple components within a `<Suspense />`, then we have to wait\n    for all components inside to be ready before anything renders. Conversely if\n    each component is nested within its own `<Suspense />`, then each can render\n    when it is done\n```jsx\n<Suspense fallback={<DotLoader />}>\n    <NewsFeed />\n</Suspense>\n```\n\n- If we want to control the order in which something will render, we can use a\n    `<SuspenseList>` component. This component takes a prop `revealOrder`.\n    - if we set `revealOrder=\"forwards\"`, then we ensure that the first\n        component will always render first. \n    - We might want to do this if component1 and component2 are stacked on top\n        of each other, and component1 has dynamic dimensions (content determines\n        the height). If component2 were to load first, then once component1\n        loads, it would push everything down.\n        - ex. Facebook's `<Composer>` and `<NewsFeed>`\n- \"In the long term, we intend Suspense to become the primary way to read asynchronous data from components — no matter where that data is coming from.\"\n    - from React team\n  ","n":0.044}}},{"i":260,"$":{"0":{"v":"Ref","n":1},"1":{"v":"\n## Ref\n- Ref - a mutable value that is shared by all component renders.\n\t- Think of a ref as a piece of state that doesn't abide by the same re-rendering rules that apply to normal state.\n\t- a ref can be a function or an object\n- ex. Imagine we had 2 counters: one made with state, and the other with refs. As expected, when we increment the state count, the component understands that it needs to re-render, so it does so and reads in the new value of count. When we increment the ref count however, the component understands that it should not update, so we end up still seeing the old value, even though underneath the hood, the count has still risen. Since nothing has caused the component to re-render, this value will remain static. Now imagine that we increment the state count. React will recognize that an updated state means that it needs to re-render. Upon doing so, the current value of the ref will be read.\n- traditionally, refs were meant to let you directly access an html element.\n\t- ex. focus an input field\n- The `useRef()` hook helps create mutable variables inside a functional component that won’t update on every render.\n- a ref can be used when the \"React-way\" of interacting with children (ie. Passing props) doesn't cut it, or we want to manually access DOM nodes or React elements created in the render method\n- The variables created with useRef() persists in between component renders (like state).\n- The nature of refs is that we can change the value of the ref, and it will not cause the component to re-render.\n- Refs have a value called `.current()`, which gives us the value of the ref in the current rendering of the component.\n- use refs To manage focus, text selection, or media playback\n- Most elements inside of your document have a ref attribute, which facilitates the use of useRef to reference elements inside of your HTML.\n- refs created using createRef are not persisted between re-renders. A new ref is always created whenever the component is re-rendered.\n- Updating a ref value is considered a side effect\n\nThis is the reason why you want to update your ref value in event handlers and effects and not during rendering\n\n\n- Ex. Imagine we have a list of users. When we click one, it logs out the username after 1 second. Now imagine that we click on a user, then click on another before the `console.log` occurs.\nIn this scenario, class components would log out user2, while functional components would log out user1.\n\t- This shows that functional components attach the call made to the user selected at the time. If we want to make a functional component that behaves like a class, we can use a ref, which is a value that is shared between all renders (in this sense it's sort of like an instance variable) <-- (in retrospect I think this should read \"static variable\", since it exists across all renders, similar to how static variables exist across all instances of that class; however this information came from React docs)\n\nref QUESTION\nI'm hoping someone can help me understand Refs and their use in React. To be clear, I have a decent high-level understanding of them. For instance, I understand that they are an escape hatch from the regular re-rendering lifecycle of react components. We can use them to access DOM nodes directly, and we can use `useRef` to get a value that persists underneath the re-rendering nature of components.\n\nHowever, I struggle to see how this relates to the use of refs in the modal library I'm using (`react-native-modalize`). This library exports a component called `<Modalize />` which wraps a component, putting it in a modal. In addition to others, `<Modalize />` takes in 2 props: `ref` and `contentRef`. `ref` lets us control the modal via 2 methods `myRef.current.open()` and `myRef.current.close()`. This makes sense, but why do we need refs here? Why couldn't the library be written in a way that we just control the modal with state `isOpen`/`isClosed` (perhaps it is an implementation detail rather than the nature of how modals work?). If the value of refs here is supposedly to persist values through successive renders, how does this relate?\n\n### Forward Ref\n- What if we want to take a **ref** that is currently existing in the component and pass it down the tree?\n\t- ie. we can reference a component rendered by `<Child />` from `<Parent />`\n\t- ex. we have a `<Form />` and `<App />`. We have a ref defined in `App` and want to attach it to an input box inside `Form`. We wrap `Form` with `forwardRef`, enabling the component to receive the `ref`. in `Form`, we pass the ref to the input. Now, we can control that component from `App`.\n- the component that has `forwardRef` in it is the component that is receiving the ref (ie. the child)\n- `forwardRef()` will create a component, accepting 2 arguments: `props` and `ref` (a regular component just accepts props)\n\n#### Example\nImagine we have a CheckoutPage component which renders a PaymentForm. The CheckoutPage handles all logic related to network calls in the app, and PaymentForm handles all form logic, including managing its own state. This is a reasonable separation of concerns. However, what happens if we want to access a piece of state in the parent from the child? Imagine the network call our parent made needed to get the address from the form. The \"React Way\" of accomplishing this is to bring the form state up one level to live in the parent component. However, this breaks our separation of concerns pattern. We can use an escape hatch forward ref in this circumstance.\n\nIllustration:\n```js\nconst Parent = () => {\n    const myRef = useRef();\n    return <Child ref={myRef} />;\n}\n\nconst Child = React.forwardRef((props, ref) => {\n    const [myState, setMyState] = useState('This is my state!');\n    useImperativeHandle(ref, () => ({getMyState: () => {return myState}}), [myState]);\n})\n```\nThen you should be able to get myState in the Parent component by calling: `myRef.current.getMyState()`\n\n#### Combined ref\n- for low-level UI development, it's common to want to use both a local ref and an external one (`forwardRef`).\n\n### UER\n[Dan Abramov on refs](https://overreacted.io/making-setinterval-declarative-with-react-hooks/)","n":0.031}}},{"i":261,"$":{"0":{"v":"Memo","n":1},"1":{"v":"\n# Memo\n- def - a HOC that memoizes the rendered output of a component\n- `Memo` accepts a second arg that lets us determine whether or not the component should re-render (defaults to `true`, meaning it won't re-render)\n- When deciding to update DOM, React first renders your component, then compares the result with the previous render result. If the render results are different, React updates the DOM.\n\t- To be clear, React will always render, but won't always update DOM\n- When we wrap our component with `Memo`, the first render will be memoized. Every subsequent render will involve the usual check of oldProps/oldState vs nextProps/nextState, but instead of rendering the component *then* comparing the output to the DOM, it will just take the memoized output. The logic is: since props or state didn't change, we shouldn't need to re-render it (which may well be the case for some components; in which circumstance we should certainly wrap the component with `Memo`\n- Components using hooks can be freely wrapped in React.memo() to achieve memoization\n- use mostly when component trees get too wide or too deep\n\n## When to use\n1. the component is purely functional (if class, use PureComponent and modify SCU), and given same props, will always return the same component\n2. the component renders often\n\t- often what causes this is a Parent with state/props that change often, and a Child who would receive the same props for a longer period of time. In this case, memoize the Child\n\t\t- ex. we have a `<MoviePage />` component and inside it a `<MovieInfo title={props.title} />`. MoviePage holds the state to determine how many thumbs up a movie has, and it pings the server every second. Therefore, when the number changes, the component re-renders, and thereby forces MovieInfo to render. The issue here is that `props.title` is not going to change each time, so React is needlessly rendering the component each time. \n3. the component often receives the same props in between renders\n4. the component contains a decent amount of components that are subject to conditional rendering (and depend on a prop)","n":0.054}}},{"i":262,"$":{"0":{"v":"Hooks","n":1},"1":{"v":"\nThe value of using Hooks is being able to decouple React-y logic things, like state management and side effects, from the actual component that that it is actually existing in\n","n":0.183}}},{"i":263,"$":{"0":{"v":"useReducer","n":1},"1":{"v":"\nPrefer `useReducer` over `useState` when either:\n1. The state is complex, where it has multiple sub-values (ie. an object)\n2. the next state depends on the previous state\n\n`useReducer` also lets you optimize performance for components that trigger deep updates because you can pass dispatch down instead of callbacks.\n","n":0.147}}},{"i":264,"$":{"0":{"v":"useEffect","n":1},"1":{"v":"\n# useEffect\n- *The question is not “when does this effect run”, the question is “with which state does this effect synchronize with:”*\n\t- `useEffect(fn)` - all state\n\t- `useEffect(fn, [])` - no state\n\t- `useEffect(fn, [these, states])`\n- if one of the variables in the dependency array changes, `useEffect` runs again. If the array is empty the hook doesn't run when updating the component at all, because it doesn't have to watch any variables.\n- if you have no dependency array and are getting infinite loops, see if there are functions being defined each time in the component. It might be a simple fix to memoize them with `useCallback`\n\t- simply adding an empty dependency array is a bandaid solution and is not really addressing the root of the problem.\n- By passing an empty array, we’re saying “only ever do this once”.\n\t- on rare occasions that’s ok, but most of the time you want something in there. The reason is that usually you want to synchronize with something in your code, not just perform the effect once\n\n## Shortcoming of useEffect\n- Compared to class component lifecycle methods, the shortcoming of `useEffect` is that we while we can set new state, we are unable to access current state (because of stale closure)\n```js\nuseEffect(() => {\n    const intervalId = setInterval(() => {\n        setCount(count + 1)\n    }, 1000)\n    return () => clearInterval(intervalId)\n}, [])\n```\n- in this example, `count` is always pointing to the previous reference.\n- we can work around this shortcoming with refs. Essentially, because refs exist between renders, that value is never lost between mounts. We simply take the value from the ref, and update the state with that value:\n```\nuseEffect(() => {\n    const intervalId = setInterval(() => {\n        countRef.current = countRef.current + 1\n        setCount(countRef.current)\n    }, 1000)\n    return () => clearInterval(intervalId)\n}, [])\n```\n\n`this` is mutable state, and the problem with mutable state is that it is always up to date. The good thing about hooks in react is that there is no `this` get retrieve values from. State always stays the same within a given render of a component.","n":0.055}}},{"i":265,"$":{"0":{"v":"useCallback","n":1},"1":{"v":"\n# useCallback & useMemo\nThe purpose of useCallback and useMemo is to prevent unnecessary re-renders and make your code more efficient.\n- In the days of class components, we had a `render()` function. Anything placed inside would get re-calculated on every render, but we could also give the class methods (putting functions outside of `render()`). These methods would not get recalculated on each render, and would lead to performance gains. Since functional components naturally render everything (`render()` method is implicit), they inherently have a performance issue. `useCallback` and `useMemo` exist to alleviate this problem.\n- *useCallback* - Allows you to cache an instance of a function between renders.\n- *useMemo* - Allows you to cache a value between renders.\n\t- the cache is local to the component position w/in the tree\n- You should consider using useCallback and/or useMemo hooks on the following situations:\n1) Processing large amounts of data\n2) Working with interactive graphs and charts\n3) Implementing animations\n4) Incorporating component lazy loading (useMemo specifically)\n\n## useCallback\n- useCallback returns a memoized callback.\n- `useCallback` accepts a function, and returns the same instance of the function being passed instead of creating a new one when a component re-renders, which is the default behavior.\n-  with this hook, a new function is only created when the variables passed to the dependency array change (kind of like `useEffect`). Otherwise, we use the same instance of that function between re-renders of the component (ie. different instances of the same component).\n- Recall that in React, components will only re-render (and will ALWAYS re-render) when there are changes to either its state or changes to the props it receives. This means that not only will the JSX re-render, but the functions defined in the component will be called each time. But what if we want to prevent those functions from being called each time, and only call them when some values change?\n```js\nconst additionResult = useCallback(add(firstVal, secondVal), [firstVal, secondVal])\n```\n- In this example, the `additionResult` doesn't necessarily get called each time the component re-renders. In fact, it only gets called if either `firstVal` or `secondVal` have changed between those 2 renders.\n\n- [when to use](https://kentcdodds.com/blog/usememo-and-usecallback)\n\n## useMemo\n- similar to `useCallback`, but instead of returning a function, it calls the function and gives us the return value of the function (again, only when one of the dependencies change)\n\t- In other words, useMemo calls the passed function only when necessary and it returns a cached value on all the other renders.\n- `useMemo` \"caches\" the value, while `useCallback` \"caches\" the function\n\n### Example\nImagine we had a function that we passed down to a child component.\n```js\nconst getItems = () => {\n\treturn [number, number + 1, number + 2]\n}\n```\n[Fayez example](https://gist.github.com/fayezosaadi/8d54fe30d8bf855c23e6d7c13a31e346)","n":0.048}}},{"i":266,"$":{"0":{"v":"Controlled Components","n":0.707},"1":{"v":"\n***[Controlled Component](https://dev.to/stanleyjovel/simplify-controlled-components-with-react-hooks-23nn)*** - components within a form whose state is controlled with `setState`, rather than the built-in functionality provided by HTML forms\n- ie. an HTML `input` that receives its `value` property from a prop in the component\n```js\nconst ControlledInput = ({ value, eventHandler }) => (\n\t<input value={value} onChange={eventHandler} />\n)\n```\n- It means that when the user types the letter “J” on the input, what is visible is not the same “J”, it may be an identical “J” that comes from the state, or whatever the event handler has put in there.\n- Controlled components are the highly recommended approach","n":0.102}}},{"i":267,"$":{"0":{"v":"Components","n":1},"1":{"v":"\npurity and immutability between components is critical. mutability within a component is fine.\n- ex. pushing onto an array within a component is fine.\n\nWhen React is interpreting JSX, it will treat custom components (`<Form>`) as recursive functions, and regular elements (`<form>`) as the finality of the chain\n- ex. We say \"React, render a `<Sidebar>` for me\". React responds \"ok what's in a `<Sidebar>`?\", and it will go on like this until React gets to the html elements and it has therefore built up the entire component tree.\n\n### Higher-order components\nHigher-Order Components are generic functions, as they only care about the argument being a component. The type of the component’s properties is not important.","n":0.095}}},{"i":268,"$":{"0":{"v":"React Native","n":0.707},"1":{"v":"\n# How React Native Works\n- When we `debug remotely`, we are running our JS code in the browser, as opposed to on the device.\n\t- when we do this, a web socket connection is made between the device and the browser\n\t- Since this is a different JS engine than would be used in production, it's possible that errors occur in one but not the other.\n- You can consider React Native as a browser that instead of rendering divs, spans, h1, inputs it renders instead native components. The nice thing is that the native components run in their own native thread (that means you get native performance) and your javascript runs in it's own thread and orchestrates all the native components.\n- In production, when the application starts, it starts running code from your javascript bundle and that will drive which components are to be created on the screen.\n- In development, React Native uses watchman to detect when you've made code changes and then automatically build and push the update your device without you needing to manually refresh it.\n\n## Building the app (run-ios/run-android)\n- When we run `react-native run-ios`, a list of iOS simulators is searched, and the default one is started.\n- Since by default it is run in Debug mode (ie. development mode), a series of `xcrun` commands are run, culminating in the `xcodebuild` commands being run, which results in the app being built on the device.\n- Once the app is successfully built, a request is sent to the metro bundler URL (API at port 8081 mentioned above). Metro will bundle the javascript in response to the app having been built.\n\t- In Release mode (ie. production), the bundle is pre-packaged, which is why we need to change where the ios device looks for the bundle (`App.Delegate.m` file)","n":0.058}}},{"i":269,"$":{"0":{"v":"Threads","n":1},"1":{"v":"\n## Threads\n- There are 2 main threads in a RN app: the main thread  and the javascript thread\n\t- The main thread runs on native platforms and handles displaying the elements of the UI and processes user gestures.\n\t- The JS thread executes JS code in a separate JS engine \n\t\t- Therefore, it deals with the business logic of the app\n\t\t- Also, it defines the structure and functionality of the UI\n- These 2 threads never communicate directly and never block each other\n- Between these 2 threads is the bridge, which has 3 characteristics:\n\t1. Asynchronous communication between the threads\n\t2. Batched communication between threads\n\t3. Serializable, meaning the threads never operate on the same data— instead exchanging serialized messages\n","n":0.094}}},{"i":270,"$":{"0":{"v":"Performance","n":1},"1":{"v":"\n# Performance\n- adding `shouldComponentUpdate` will drastically help with optimization.\n    Consider this as you are building the app rather than considering it afterwards.\n    - Alternatively, if you're concerned about a component re-rendering too many times, try using React.PureComponent\n- [library to check how many times a component re-rendered needlessly](https://github.com/maicki/why-did-you-update)\n    - the library is archived, but apparently works perfectly. alternatively: [why-did-you-render](https://github.com/welldone-software/why-did-you-render)\n- [Spying the queue (focusing on the data passing over the bridge)](https://callstack.com/blog/react-native-how-to-check-what-passes-through-your-bridge/?ref=hackernoon.com)","n":0.12}}},{"i":271,"$":{"0":{"v":"Metro","n":1},"1":{"v":"\n# Metro Bundler\n- when we run `yarn start` both a node.js server and metro bundler are started.\n- Metro can be thought of as similar to \"webpack for react native\"\n- Metro is a JavaScript bundler. It takes in an entry file and various options, and gives you back a single JavaScript file that includes all your code and its dependencies.\n\t1. Metro builds a dependency graph of all modules required from the entry point (ie, our whole react-native app)\n\t2. Metro transforms modules into a format that React-Native can understand\n\t3. Metro serializes the transformed modules to form a bundle\n\t\t- A bundle is just a bunch of modules combined into a single js file.\n\t4. The bundlefile gets installed on the device, where its code is then executed.\n\t\t- Remember that when you are writing code for a React Native application, your code is not \"translated\" to Java/Swift/whatever. The Native Modules will send events to a Javascript thread, and the JS thread will execute your bundled React Native code.\n- I/O of Metro\n\t- Input - entryfile along with any options\n\t- Output - the bundle\n- Metro bundler is not used for production\n\t- ex. running `react-native run-ios --configuration Release`\n\nThe Bundle\n- The bundlefile can be found at `http://localhost:8081/index.bundle?platform=ios&dev=true&minify=false`. This is the API through which the device will access the bundle\n\t- this is served from memory, therefore does get written into our project directory\n- The bundle may be stored in different formats, such as binary or .bundle file.\n- Metro bundler also translates JSX to standard javascript\n","n":0.064}}},{"i":272,"$":{"0":{"v":"Raycast","n":1},"1":{"v":"[Scripting guide](https://www.raycast.com/blog/getting-started-with-script-commands/)\n","n":0.707}}},{"i":273,"$":{"0":{"v":"Ramda","n":1},"1":{"v":"\nThere are two main guiding principles of Ramda:\n- Data comes last\n- Everything gets curried\n\nThese two principles lead to a style that functional programmers call *point-free* (a.k.a *tacit programming*)\n- Think of point-free code as “Data? What data? There’s no data here.”\n\nWe can make *point-free* transformations by converting functions that take in data to new functions that accept that data and after having already decided what to do with it:\n```\nconst forever21 = age => ifElse(gte(__, 21), always(21), inc)(age)\n// becomes:\nconst forever21 = ifElse(gte(__, 21), always(21), inc)\n```\nnow, the function `forever21` can be used without having to worry about data until the last minute.\n- Note: there is no behavioral difference in these two versions. We’re still returning a function that takes an age, but now we’re not explicitly specifying the age parameter.\n\nRamda `map` and `filter` (maybe more?) automatically convert different datatypes into functors\n- This allows us to take a function that was made for an array, and apply it to an object, or even a string\n```\n// An array\ntransform(['Optimus Prime','Bumblebee','Ironhide','Sunstreaker','Ratchet'])\n//=> [ 'EMIRP SUMITPO', 'EDIHNORI', 'REKAERTSNUS', 'TEHCTAR' ]\n \n// A string\ntransform('Optimus Prime')\n// => [ 'R' ]\n \n// Even an object\ntransform({ leader: 'Optimus Prime', bodyguard: 'Ironhide', medic: 'Ratchet' })\n// => { leader: 'EMIRP SUMITPO', bodyguard: 'EDIHNORI', medic: 'TEHCTAR' \n```\n\n## Math functions\nthese functions take their arguments in what seems like normal order (is the first argument greater than the second?) That makes sense when used in isolation, but can be confusing when combining functions. These functions seem to violate Ramda’s “data-last” principle, so we’ll have to be careful when we use them in pipelines and similar situations. That’s when `R.flip` and the placeholder `R.__` will come in handy.\n\n## Symbol replacements\n### Function methods\n- The following methods are best suited for functions\n    - ex. `either(wasBornInCountry, wasNaturalized)`\n    \n`&&` -> `R.both`\n\n`||` -> `R.either`\n\n`!` -> `R.complement`\n\n### Value methods\n- The following methods are best used with values\n\n`&&` -> `R.and`\n\n`||` -> `R.or`\n\n`!` -> `R.not`\n\n### Inequality\n`gt`, `lt`, `gte`, `lte`\nif we are passing data in, then we most likely want to provide a placeholder for the first arg\nex. using conditionals (`R.ifElse`), `R.pipe`/`R.compose`\n```\nconst forever21 = age => ifElse(gte(__, 21), always(21), inc)(age)\n```\n\n## Debugging Ramda\n[more info](https://blog.carbonfive.com/2017/12/20/easy-pipeline-debugging-with-curried-console-log/)\n\nRamda provides us with a function `R.tap` which we can use to create side effects without interrupting the flow of an existing composition\n- `tap` accepts a function, and returns a function (ex. `console.log`) that may take in the passed in arg\n```\nR.compose(\n    R.tap(x => console.log('REVERSE:',x)),\n    R.map(R.reverse),\n)\n```\nthis allows us figure out what happened after `reverse` was called on the data\n\nControl flow (`if`/`else`) is less necessary in functional programming, but still occasionally useful.\n\n### Data flow\nRemember that Ramda is flexible with how you combine functions, as long as the expected types are received\n```\nconst alwaysDrivingAge = age => ifElse(lt(__, 16), always(16), a => a)(age)\n```\nHere, an age variable goes through the ifElse, and if fails the condition, will proceed to the third arg (else), where age is the first argument of the function\n\n#### Arg order on `pipe`/`compose` when function arg passed in through the data flow\n```\nconst publishedInYear = year => \n    book => \n        book.year === year\nconst titlesForYear = year => \n\tR.pipe(\n\t\t//pIY returns a function that takes a book as its arg\n\t\tR.filter(publishedInYear(year)), \n\t\tR.map(book => book.title)\n\t)\n\nconsole.log(titlesForYear(1934)(books))\n```\n\n#### `R.when`/`R.unless`\nsimilar to `ifElse`, but with only one conclusion. Implicitly, `R.identity` is the second conclusion (ie., it is *else*)\n```\nconst alwaysDrivingAge = age => when(lt(__, 16), always(16))(age)\n```\n\n\n\n## Working with functions that you didn't write\n### `R.apply`\nspread out a single array into individual arguments. Think of it as `spreadArgs()`\nThis is useful for creating a fixed-arity function from a variadic (variable arity) function\n```\nfunction spreadArgs(fn) {\n    return function spreadFn(argsArr){\n        return fn( ...argsArr );\n    };\n}\n```\nThis is related to usingSpread syntax with functions\n```\nconst arr = [1, 2, 3]\nconst fn = (a, b, c)\nfn(...arr) === fn(arr[0], arr[1], arr[2])\n```\n### `R.unapply`\ngather individual args into a single array\n\n### `R.unary`\nCan be used to cut off all arguments past the first. The benefit of this is that if we call a function and pass in too many arguments, the extra ones will just drop off. \nex. map takes a function, and the first argument of map (the iterated value) gets passed to parseInt. Then the second arg (index) gets passed to parseInt, which corresponds to its radix. Since we don't want index to be interpreted this way, we make it a unary function, and the index argument safely falls off\n```\n[\"1\", \"2\", \"3\"].map(R.unary(parseInt)\n```\n\n### `R.flip`\nswap the first and second argument of a function\n\n### `R.__`\nuse a placeholder argument to go in place of a parameter\nWhen we pass data through some sort of chain (`pipe`, `compose`, `ifElse`), that data will be passed through the methods as the argument in the first available spot.\n- ex. if a fn in `pipe` has no args provided, then the data will be the first arg. If it has one arg, then the data will be applied in the second place (and so on). If we use a placeholder `R.__` as the first arg, then the data will be applied to that space, since it is the first available argument.\n- `const forever21 = age => ifElse(gte(__, 21), always(21), inc)(age)`\n\n## Object manipulation\n### `R.assoc`\nupdate (or create) a property of an object\n```\nconst myObj = {\n\tname: 'kyle'\n}\n\nconst newObj = R.assoc('number', '7788713377')\n\nconsole.log(newObj(myObj))\n// { name: 'kyle', number: '77887133' }\n```\n\n### `R.prop`\nread a single property from an object and return the value\n\n### `R.pick`\npick multiple properties of an object and return a new object with just those properties\n- complement of `R.omit`\n\n### `R.has`\nreturn boolean indicating whether or not a property exists on the object\n\n### `R.path`\ndive into nested objects returning the value at the given path\n\n### `R.propOr`/`R.pathOr`\nsearch for the property on an object, allowing you to provide a default value in case the property/path cannot be found on the obj.\n\n### `R.evolve`\ndeclaratively provides transformations to happen on each property of an object:\n- note: cannot add new properties with this\n```\nvar tomato  = {firstName: '  Tomato ', data: {elapsed: 100, remaining: 1400}, id:123};\nvar transformations = {\n  firstName: R.trim,\n  lastName: R.trim, // Will not get invoked.\n  data: {elapsed: R.add(1), remaining: R.add(-1)}\n};\nR.evolve(transformations, tomato); //=> {firstName: 'Tomato', data: {elapsed: 101, remaining: 1399}, id:123}\n```\nalso\n```\n//before\nconst nextAge = compose(inc, prop('age'))\nconst celebrateBirthday = person => assoc('age', nextAge(person), person)\n\n// after\nconst celebrateBirthday = evolve({ age: inc })\n```\n\n## Array manipulation\n`R.nth`\naccess an array at the given index\nequivalent of object's `R.prop`\n\n`R.slice`\nequivalent of object's `R.pick`\n\n`R.contains`\ncheck if array contains given value\nequivalent of object's `R.has`\n\n`R.head`\naccess first element of array\n\n`R.last`\naccess last element of array\n\n`R.append`/`R.prepend`\nFP versions of `.push` and `.unshift`\n\n`R.drop`/`R.dropLast`\nFP versions of `.shift` and `.pop`\n\n`R.insert`\ninsert an element at given index of an array\n\n`R.update`\nreplace an element at the given index of an array\n\n`R.adjust`\nequivalent of object's `R.evolve`, except only works for one element. We would use this function over `R.update` when using a function to update the array element\n```\nconst numbers = [10, 20, 30, 40, 50, 60]\n \nadjust(multiply(10), 2, numbers) // [10, 20, 300, 40, 50, 60]\n```\n\n`R.remove`\nremove element by index\n\n`R.without`\nremove element by value\n\n### `R.reject()`\ncomplement of `R.filter()`. If `filter()` is *filter in*, then `reject()` is *filter out*\n\n### `.reduce()`\n\nwith `initialValue`\n![](/assets/images/2021-03-07-22-18-20.png)\n\nwithout `initialValue`. The first value of the list will act in place of the initialValue and the combining will start with the second value in the list\n![](/assets/images/2021-03-07-22-18-41.png)\n\nIf the array passed to `.reduce()` is empty, then there must be an `initialValue` specified, otherwise there will be an error\n\n### `.map()`\n#### use cases\n- transform a list of functions into their return values:\n```\nvar one = () => 1;\nvar two = () => 2;\nvar three = () => 3;\n\n[one,two,three].map( fn => fn() );\n// [1,2,3]\n```\n- transform a list of functions by composing each of them with another function, and then execute them:\n```\nvar increment = v => ++v;\nvar decrement = v => --v;\nvar square = v => v * v;\n\nvar double = v => v * 2;\n\n[increment,decrement,square]\n.map( fn => compose( fn, double ) )\n.map( fn => fn( 3 ) );\n// [7,5,36]\n```\n\n### `R.chain` (a.k.a. flatMap)\n\niterate over values of a list, performing the provided function on each element, then concatenating all of the results together\n- what results is if we were to perform a function of each element, then flatten that resulting array\n```\nvar firstNames = [\n\t{ name: \"Jonathan\", variations: [\"John\", \"Jon\", \"Jonny\"] },\n\t{ name: \"Stephanie\", variations: [\"Steph\", \"Stephy\"] },\n\t{ name: \"Frederick\", variations: [\"Fred\", \"Freddy\"] }\n];\n\nR.chain(entry => [entry.name, ...entry.variations], firstNames)\n// [\"Jonathan\",\"John\",\"Jon\",\"Jonny\",\"Stephanie\",\"Steph\",\"Stephy\",\n//  \"Frederick\",\"Fred\",\"Freddy\"]\n```\n\n### `R.zip`\nAlternate through 2 arrays and take each value that appears at the same index and put them into their own array. The shorter of the 2 lists is considered the list length\n```\nconst arr1 = [1, 2, 3]\nconst arr2 = ['a', 'b', 'c']\n\nR.zip(arr1, arr2)\n// [[1, a], [2, b], [3, c] ]\n```\n### `R.invoker()` a.k.a. `unboundMethod()`\n\n## Number manipulation\n`R.clamp`\nrestrict a number to be within a certain range\n```\nR.clamp(1, 10, -5) // => 1\nR.clamp(1, 10, 15) // => 10\nR.clamp(1, 10, 4)  // => 4\n```\n\n### `R.complement()`\n implements the same idea for functions as the ! (not) operator does for values.\n nothig good ever comes out of this\n my daring susan\n why cant we jst love each other aagain???\n\n## Fusion (with `.map()`)\nImagine we have multiple functions that each take 1 argument, and each function's return can be passed directly into the next as input:\n```\nfunction truncate(word) {\n\tif (word.length > 10) {\n\t\treturn `${word.substring(0, 6)}...`\n\t}\n\treturn word\n}\n\nfunction upper(word) {\n\treturn word.toUpperCase()\n}\n\n\nfunction append1(word) {\n\treturn word + '1'\n}\n```\nNaively, we could do this:\n```\nconst generateList = arr\n    .map(append1)\n    .map(upper)\n    .map(truncate)\n```\nwhen we pass a function to map, like `.map(fn)`, the element of the list gets passed as the first argument to that function. Here, when we pass `R.pipe`, the element gets passed as first argument to `pipe` (since pipe is ltr)\n```\nconst generateList = arr.map(R.pipe(append1, upper, truncate))\n```\n\n# Lenses\nWith the following more specific ways of creating lenses, `R.lens()` is not often needed\n\n#### `R.lensProp`\nlets us create a lens that focuses on a *non-nested* property of an object\n\n#### `R.lensPath`\nlets us create a lens that focuses on a *nested* property of an object\n\n#### `R.lensIndex`\nlets us create a lens that focuses on an element of an array\n- these are for the times where we know in advance the index that we are interested in, but don't yet have the data (*ex. capitalize first letter*)\n```\nconst toTitle = R.compose(\n\tR.join(''),\n\tR.over(R.lensIndex(0), R.toUpper)\n)\n```\n\n### Three functions for working with lenses\n#### `R.view()`\nread value of the lens\n\n#### `R.set()`\nset the value of the lens\n- note: this does not mutate the original object supplied. In other words, `R.view()` will give us the exact same result before and after #### `R.set()`\n\n#### `R.over()`\napply a transformation function to the lens\n```\nover(nameLens, toUpper, person)\n// => {\n//   name: 'RANDY',\n//   socialMedia: {\n//     github: 'randycoulman',\n//     twitter: '@randycoulman'\n//   }\n// }\n```\n\n\n","n":0.024}}},{"i":274,"$":{"0":{"v":"CLI","n":1}}},{"i":275,"$":{"0":{"v":"Pipe","n":1},"1":{"v":"\n# `R.pipe()`\n`pipe(..)` is identical to `compose(..)` except it processes through the list of functions in left-to-right order:\n`var pipe = reverseargs( compose )`\n\nthe function returned by `pipe` takes the same number of arguments as the first function `.pipe()` is given\n\nwhen we call `operate(3, 4)`, pipe passes the 3 and 4 to the `multiply` function, resulting in 12. it passes that 12 to addone, which returns 13. it then passes that 13 to square, which returns 169, and that becomes the final result of operate.\n```\nconst operate = pipe(\n  multiply,\n  addOne,\n  square\n)\n```\n\nex. improvement with `R.pipe`\n```\n// before\nconst titlesForYear = (books, year) => {\n  const selected = filter(publishedInYear(year), books)\n \n  return map(book => book.title, selected)\n}\n\n// after\nconst titlesForYear = year =>\n  pipe(\n    filter(publishedInYear(year)),\n    map(book => book.title)\n  )\n```\n","n":0.091}}},{"i":276,"$":{"0":{"v":"Compose","n":1},"1":{"v":"\n# `R.compose`\ncomposing two functions and then mapping the resulting function over a functor should be the same as first mapping one function over the functor and then mapping the other one.\n- ie. if you plan to map over the same array multiple times, just compose the functions and then map over the array once with that composed function\n```\n[\"1\", \"2\", \"3\"].map(r.unary(parseint)\n```\nthe data flow is: \n```\narrayvalue <-- map <-- unary <-- parseint\n```\n(p.s., any time you see data flow presented this way, think `r.compose()`\n\n`parseint` is the input to `unary(..)`. the output of `unary(..)` is the input to `map(..)`. the output of `map(..)` is `arrayvalue`. this is the composition of map(..) and unary(..).\n\nhaving a series of functions whose input is the output provided by the previous (inner) function\n\n![](/assets/images/2021-03-09-09-34-17.png)\n\nencapsulating a series of functions within one function\n![](/assets/images/2021-03-09-09-34-28.png)\n\nfunctions compose from right to left (incl. `r.compose()`)\n\n- expl. this is consistent with how we evaluate functions, from inner to outer (ie. right to left)\nex:\n```\n[\"1\", \"2\", \"3\"].map(r.unary(parseint)\n```\n\ncomposition is the wrapper around the big machine, whose contents are the parameters of `r.compose()`. in other words, the components of the machine (under the wrapper) are the individual functions that the origvalue goes through (as an argument). the composed machine returns a new function, so it is in essence a function factory.\nrightmost functions are at the top of the machine.\n![](/assets/images/2021-03-09-09-34-44.png)\n","n":0.068}}},{"i":277,"$":{"0":{"v":"Python","n":1},"1":{"v":"\n**kwargs** - similar to `...args` in javascript\n\n### Self\n- a reference to the current instance of the class\n- `self` is always the first argument of a class method (including the `init` method)\n    - In the `init` method, `self` refers to the newly created object\n    - In other class methods, it refers to the instance whose method was called.\n\n### .init\n- it is a constructor\n- Called when an object is instantiated.\n- It will initialize the attributes of the class\n\n### super\n- give the current object access to the methods from its superclass\n- `super` returns a temporary object of the superclass, which allows us to call that superclass's methods\n- Common use case\n    - building classes that extend functionality of previously built classes\n  \n# Celery\n- Async task queueing system that can allow you to queue up jobs at schedules, or have them queue up on a trigger (ex. on some get request).\n","n":0.083}}},{"i":278,"$":{"0":{"v":"Pulumi","n":1},"1":{"v":"\nA Framework for Infrastructure as Code\nDescribes how infra will look in the cloud\nex. here is what our postgres database looks like\n\nall config is done through pumuli.yaml\n- get access to the encrypted secrets (which we can check in through version control)","n":0.158}}},{"i":279,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Show stacks\n`pulumi stack ls`\n\n#### Switch a stack (preproduction, staging, production)\n`pulumi stack select <stackname>`\n\n#### Show the details of a stack\n`pulumi stack output -j --show-secrets | less`","n":0.196}}},{"i":280,"$":{"0":{"v":"Protocol","n":1},"1":{"v":"\nMost protocols widely used are synchronous (eg. http)","n":0.354}}},{"i":281,"$":{"0":{"v":"HTTP","n":1},"1":{"v":"\nHTTP uses TCP as its underlying transport protocol.\n- The HTTP client first initiates a TCP connection with the server, then once the connection is made, the browser and server processes access TCP through their [[socket|network.sockets]] interfaces.\n\n### Credentials policy\n*Only relevant to browsers*\n- Credentials are cookies, authorization headers or TLS client certificates.\n\nCredentials\n- `include` - send a request with credentials included, even if it's cross-origin\n- `same-origin` - only send credentials if the request URL is on same origin as the calling script\n- `omit` - ensure browsers don’t include credentials in the request\n\n- we can use the `Access-Control-Allow-Credentials` response header to tell browsers to expose the response to frontend JavaScript code.\n\t- to do this, the client will need to set the credentials to `include` (`Request.credentials`)\n\n### Breakdown of connection to server\n1. User enters `http://someschool.edu/somedepartment/home.index` in the address bar of a browser\n2. The browser (ie. HTTP client process) initiates a TCP connection to the server `www.someschool.edu` at port `80`\n\t- associated with the TCP connection, there will be a socket at the client and a socket at the server.\n3. The browser sends an HTTP request to the server via its socket.\n\t- The request includes the path name `/somedepartment/home.index`\n4. The server receives the request and retrieves the object (in this case, HTML) from its storage (eg. RAM or disk), encapsulates the object in an HTTP response, then sends that message to the client via its socket.\n5. The HTTP server process tells TCP to gracefully close the TCP connection.\n\t- \"gracefully\" here means that the connection will only close once the client receives the full communication.\n6. The browser receives the response, and the TCP connection is closed.\n7. The response from the server indicates that the encapsulated object is HTML. The client then extracts the file fromm the response message, examines the HTML file, and finds references to 10 JPEG objects.\n8. Steps 1-5 are then repeated for *each* of the referenced JPEG objects.\n\t- These 10 TCP connections are most likely made in parallel, which we would say are 10 serial TCP connections.\n\t\t- This degree of parallelism is determined by the browser, and configurable by the user.\n\t\t\t- by default most browsers open between 5 and 10 parallel TCP connections.\n","n":0.053}}},{"i":282,"$":{"0":{"v":"Webdav","n":1},"1":{"v":"\nHTTP doesn't allow for writing, modifying, moving, etc of a file; it only allows for querying that file. WebDAV enhances HTTP to allow writing.\n\n\n- an extension of HTTP\n\t- Therefore, gets all of the benefits that HTTP offers, such as encryption, if HTTPS\n\t- Also can use HTTP tools like cURL\n- Since it is an extension of HTTP, it gets access to HTTP verbs. Additionally, it extends these base verbs, giving additional functionality\n\t- ex. COPY, MOVE, MKCOL (make collection, aka directory)\n- a protocol that allows us to create, update, and move documents on a server.\n\t- these are known as *remote web content authoring operations*\n- WebDAV provides a coherent set of methods and headers, and has a system like Express that involves request and response objects\n- ex. perform CRUD operations on information about Web pages, such as their authors, creation dates, etc.\n- The WebDAV protocol enables a webserver to behave like a fileserver","n":0.082}}},{"i":283,"$":{"0":{"v":"Requests","n":1},"1":{"v":"\n## HTTP Requests\n### GET vs. POST\n- there are some limitations to GET requests as follows:\n    - GET requests can be cached\n    - GET requests remain in the browser history\n    - GET requests can be bookmarked\n    - GET requests should never be used when dealing with sensitive data\n    - GET requests have length restrictions\n    - GET requests should be used only to retrieve data\n- Some + points on POST requests:\n    - POST requests are never cached\n    - POST requests do not remain in the browser history\n    - POST requests cannot be bookmarked\n    - POST requests have no restrictions on data length","n":0.1}}},{"i":284,"$":{"0":{"v":"HTTP Request Properties","n":0.577}}},{"i":285,"$":{"0":{"v":"Credentials","n":1},"1":{"v":"\n### Credentials\nFor cross-origin requests, `credentials` allows us (as the client) to specify whether or not credentials should be sent along for the ride in HTTP requests.\n- Therefore this gets set on the client.\n\nCredentials are cookies, authorization headers, or TLS client certificates. Basically, like the email/password credentials we are most familiar with, credentials verify identity and are a way to establish trust.\n\nBoth the client and the server must indicate that they’re opting into including credentials.","n":0.116}}},{"i":286,"$":{"0":{"v":"URL","n":1},"1":{"v":"\nA URL contains the following parts:\n1. The protocol in use\n    - tells the client what conventions to follow when exchanging messages with the server that has the website.\n2. The hostname of the server\n    - tell the client which server to contact\n3. The location of the file\n4. The arguments to the file e.g. query params\n","n":0.136}}},{"i":287,"$":{"0":{"v":"Cross-Origin Resource Sharing","n":0.577},"1":{"v":"\nCORS is an HTTP-header based mechanism that allows a server to indicate any origins (domain, scheme, or port) other than its own from which a browser should permit loading resources.\n- It does this by modifying the response headers returned from the server.\n- ex. if your app uses a headless CMS, then that server must be configured to allow requests from your website `www.mypersonalwebsite.com`.\n\nBy default, if you are trying to access a resource that has a different domain from the one that serves your main website, then CORS will prevent that from happening.\n- To get around this, we give the server a list of allowed domain names that it will allow requests from.\n\nWhen a website can access a resource or execute commands on another domain via HTTP requests, the process is called cross-origin resource sharing.\n- ex. you are on a blogging website, and clicking on one of their buttons sends an HTTP request to your bank's website requesting that all money be transferred out.\n- Because of CORS, this of course cannot occur. So if we want to allow certain domains to send requests, we need to put them on some sort of whitelist.\n\nCORS is a mechanism to use HTTP headers to tell browsers to give a web application running at one origin, access to selected resources from a different origin\n- a cross-origin HTTP request is executed when it requests a resource that has a different origin (domain, protocol, or port) from its own\n- ex. the front-end JavaScript code served from `https://domain-a.com` uses XMLHttpRequest to make a request for `https://domain-b.com/data.json`\n\nIn a browser console, the following code snippet will always result in a CORS error unless:\n- I'm already on google.com\n- Google's server has enabled cross-origin requests from whatever website (hostname) I'm on \n```js\nfetch(\"https://google.com\", function(error, meta, body){\n    console.log(body.toString());\n});\n```\n\n#### Enabling CORS for a host\n\nThe server is the one that enables the cross-origin requests, not the client\n- [Adding CORS support to server](https://enable-cors.org/server.html)\n\n```js\nrouter.use(function (req,res,next) {\n  console.log('/' + req.method);\n  res.header(\"Access-Control-Allow-Origin\", \"https://google.com\");\n  next();\n});\n```\n\nIf you want to make a request from one domain to another, then you need to enable CORS on the server\n\n\n* * *\n\n### Terminology\nOrigin - the User Agent that initiated the request.\n\n* * *\n\n## UE Resources\n[MDN guide](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)","n":0.053}}},{"i":288,"$":{"0":{"v":"headers","n":1},"1":{"v":"\nSince CORS is administered by the server handling the requests, these are response headers. They allow the server to fine-tune its policy toward handling cross-origin requests.\n\n### Access-Control-Allow-Credentials\nThis header gives the server the power to determine if the browser should expose the response to the front-end Javascript code when the request's [[credentials|protocol.http.requests.prop.credentials]] mode is `include`.","n":0.136}}},{"i":289,"$":{"0":{"v":"UDP","n":1},"1":{"v":"\nUDP is a no-frills, lightweight transport protocol, providing minimal services.\n\nUDP is connectionless, so there is no handshaking before the 2 processes start to communicate.\n\nUDP provides an unreliable data transfer service, meaning that when a process sends a message into a UDP [[socket|network.sockets]], UDP provides no guarantee that the message will ever reach the receiving process. Furthermore, there is not even a guarantee that the messages will arrive in the order they were sent.\n","n":0.117}}},{"i":290,"$":{"0":{"v":"TCP","n":1},"1":{"v":"\n- IP is unreliable, in that we can't guarantee that the data that's sent is received without being corrupted. IP cannot even guarantee that the data arrives. TCP is reliable. Interestingly, TCP is built on top of IP. The question is, how is a reliable technology built on top of an unreliable one? The answer is that TCP acts as sort of the manager of a transaction of data from one place to another. If data arrives corrupted, it will repeat the request until data comes in fully-formed\n\t- Since the end-user never knows about the data-loss that might have occurred somewhere along the way, TCP is effectively an abstraction\n\nTCP is said to exist between the [[sockets|network.socket]] of two processes\n\nOn the TCP level, the tuple (source ip, source port, destination ip, destination port) must be unique for each simultaneous connection. That means a single client cannot open more than 65535 simultaneous connections to a single server. But a server can (theoretically) serve 65535 simultaneous connections per client.\n- So in practice the server is only limited by how much CPU power, memory etc. it has to serve requests, not by the number of TCP connections to the server.\n\nThe connection between 2 processes via TCP is full-duplex, meaning the two processes can send messages to each other simultaneously.\n- When the application finishes sending a message, it must tear down the connection\n\n- TCP transports application-layer messages such as HTTP requests.\n- Works by establishing a connection between 2 hosts.\n- Data is guaranteed to be delivered, and in the order it was sent.\n- TCP breaks long messages in to shorter segments.\n","n":0.061}}},{"i":291,"$":{"0":{"v":"SMTP","n":1},"1":{"v":"\n## The Process\n\nThe MTA receives the email from the MSA, which it receives from the MUA\n- Once the MTA gets the email, relaying comes into play— with the MTAs acting as the relays (SMTP relay)\n\t- In relaying, the email is forwarded to other MTAs if the recipient of the email is not hosted locally on the same system as the sender.\n- Once the relaying process has finished, the email is sent to the Mail Delivery Agent (MDA)— which is the last stop before being delivered to a client's mailbox\n- The whole process uses SMTP, except for the final stage, which is between MDA and and MUA, which uses POP3 or IMAP\n\t- In other words, the process of getting mail from the \"mail server\" onto the email client uses POP3/IMAP.\n\n![](/assets/images/2021-04-07-10-12-35.png)\n- MUA - Mail User Agent (email client)\n- MSA - Mail Submission Agent\n- MDA - Mail Delivery Agent\n- MTA - Mail Transfer Agent\n\n### Mail Transport Agent (MTA)\nA mail server can have many names: mail relay, mail router, Internet mailer. But the most common alias is an mail transport agent (MTA)\n- an MTA is responsible for transferring email among users on the internet\n- MTAs query the MX records and select a mail server to transfer emails\n- Usually, MTAs use a store-and-forward model of mail handling. This means that outgoing mail is put into a queue and waits for the recipient’s server response. An MTA will recurrently try to send emails. If the mail fails to be delivered during the established term, it will be returned to the mail client.\n\n#### MTA impact on email deliverability\nThere are three major factors that email deliverability is based on:\n- sender’s reputation\n- infrastructure & authentication\n- content\n\nThe reputation of the domain and IP address the email is sent from is the most important thing.\n- MTAs can protect and strengthen the reputation of the sender.\n\n##### Brand new IPs\nIf you’re building your reputation from scratch, you should not use your virgin IP address at full load. It has no email sending history and thus needs some warming up. An MTA will let you do this and later slowly increase sending capacity\n\n* * *\n\n## Breakdown of SMTP\n1. Alice sends an email to Bob. \n2. In reality, it goes directly to her mail server, where it is placed in a message queue.\n3. The client-side of SMTP (running on Alice's mail server) sees the message sitting in the queue.\n4. In response, it opes up a TCP connection to an SMTP server running on Bob's machine\n5. After the handshaking process is complete, the SMTP client sends Alice's message into the TCP connection.\n6. The message is received by Bob's mail server. The server then places the message in Bob's mailbox.\n","n":0.048}}},{"i":292,"$":{"0":{"v":"FTP","n":1},"1":{"v":"\nThe user interacts with FTP via an FTP user agent.\n- once the hostname is provided by the client, the FTP client process establishes a TCP connection with the FTP server.\n    - after this, the user provides username/password to establish the connection.\n\nOne key difference between HTTP and FTP is that FTP uses two parallel TCP connections to transfer a file: a control connection, and a data connection.\n- *control connection* is for passing control information between the 2 hosts, such as username, password, commands to change remote directory, commands to get/put files etc.\n- *data connection* is for actually sending files back and forth.\n\nUnlike HTTP, FTP is stateful; the server maintains state about the FTP client, most importantly associating the connection with a particular user.\n- also it must maintain state about the user's present working directory of the server.\n- keeping this state significantly limits the total number of sessions that an FTP server can handle.\n\nFTP connections happen on port `21`\n","n":0.08}}},{"i":293,"$":{"0":{"v":"Product Design","n":0.707}}},{"i":294,"$":{"0":{"v":"User Testing","n":0.707},"1":{"v":"\n# User Testing\n- Think of user testing as high-level workflow tests. While atomic components, such as copy on a button would be tested through A/B testing \n- always let users drive the test. Instead of giving them imperative instructions (*click here, enter your email here, then click here*), give declarative tasks (*create a piece of information for your repository)\n- Be careful not to use any verbiage that is specific to the app when giving users your app\n    - ex. for Never Forget, don't use the words nugget and bucket, because it gives them insight that later users won't have the benefit of. If necessary, use more neutral words, like category.\n- As the user is going through your app, watch their face for signs of surprise or confusion, and question them on it. Make sure you understand why the user was feeling what they felt.\n    - Keep good data on this, because if you start to notice a pattern that occurs at expected areas of the app, then you know you have something that needs to be fixed.\n- Parrot people as they tell you things, because it allows them to expand on the feelings that prompted those words. \n    - \"I felt a little confused here\" —\"Oh, you felt confused here?\"\n- As they click on something, ask them how well it matched their expectations of what would happen when clicked on it.\n    - \"was *this* the screen you were expecting to go to after clicking on *that* button?\"\n- Listen for the *value gain* that the user is suggesting, rather than the specific implementation of that feature.\n    - When people make suggestions, focus on the \"why\" behind those suggestions. Don't focus too much on the implementation of their suggestion. Instead, if you put attention on what goal they are trying to facilitate within their suggestion, then you can extract useful potential information regarding the subject that is receiving the suggestions\n    - ex. Say you have an airbnb like product, and while doing user testing, a user suggests having a back and forward button right on the photo, that takes you to the next property in the list. Maybe you disagree with the design decision of putting the arrows on the main image, but you are missing a valuable piece of feedback here: users may want to navigate more quickly between properties, and may want to do it right from a property page. Take that goal in mind, and devise your own solution for it. The solution could be an overall better navigation system, or just placing the forward/back buttons somewhere else on the page. \n\n## UX Research\n- Everything to do with research can be placed on a grid by how qualitative/quantitative the feedback is, vs the origin of that feedback (what people say vs how they act)\n\n![fe3a55ac2df38afe29080c5838a915ba.png](:/5c1bfd2a9fea435ca57b5ae96c45453b)\n\n- Qualitative methods are largely unstructured, tend to be subjective, are at the softer end of science, and are about establishing insights and theories (which we then test, often using quantitative approaches).\n    - are open-ended (“How might you improve this customer journey?”) \n    - Qualitative methods lead to insights\n- Quantitative methods are largely: structured, tend to be objective, are at the harder — more measurable — end of science, and are about testing theories. They tend to be larger sample sizes and can be run in a more hands-off manner. With quantitative research, user behaviors and attitudes are gathered indirectly.\n    - tend to be yes/no (“Do you use this feature?”).\n    - quantitative methods allow you to test those insights brought by the qualitative methods.\n\n# A/B Testing\n- Think of A/B testing as atomic tests. You want to test which copy on a button is better. Which color is better. Which location of the text is better. What you don't want to test is which workflow is better\n    - If you were to A/B test two different workflows, it would be impossible to know the precise reason that one outperformed the other. At the end of the day, you aren't really learning anything valuable.\n\n\n# Misc\nThere is a distinction between brainstorming and asking for feedback. When asking for feedback you need to clearly present a single idea. When you are brainstorming you are trying to come up with new ideas.\n- Make sure all parties involved know what kind of conversation you are having.\n\n# UE Resources\n- [Jobs to be done (product design)](https://jtbd.info/)\n- [UX research cheat sheet](https://www.nngroup.com/articles/ux-research-cheat-sheet/)\n- [user testing tips](https://www.smashingmagazine.com/2017/11/improve-user-testing/)\n","n":0.037}}},{"i":295,"$":{"0":{"v":"Job to be Done","n":0.5},"1":{"v":"\nA JTBD is a prototype that one aspires to be. This prototype is designed by a company. The company will design a product that creates in you a \"desire\" to get a certain job done. By virtue of the process, the product then becomes the only means of fulfilling that new need and thereby \"getting the job done\".\n- Progress can only happen when we attach and integrate new ideas and new products into our lives. This is the basis for why people gain new jobs to be done. \n- When people start to understand there is a new *job to be done*, they start imagining the product integrated in their life, along with all of the benefits that the product provides.\n- Having a JTBD is a process. A JTBD is not a thing that consumers have, but rather something that they participate in (an analogy is falling in love, which is something we participate in.)\n- A JTBD is distinct from a task (which would be something like \"search a song and add it to your playlist\").\n- A JTBD describes \"a better me\". It answers the question, “How are you better since you started using [product]?”\n![fdf33f277db063fad011a2c2fda8b79c.png](:/8f843f4b9f7743bcb5c3c3a3b9386bf7)\n- People use only one solution at a time for a JTBD.\n- People that are in the process of discovering a solution to a JTBD engage in a mental simulation of that product helping them become better.\n- People don't want the product — they want help making their lives better.\n- JTBD is a useful framework because it helps you keep your mind on what core value you add to the customer's life. This drives innovation in the right direction. \n    - If you were a company that made horse-drawn carriages before cars were made, you should have realized at some point that the job to be done for your customers was to have specific (and imaginable) benefits that arise from being able to travel fast. As long as your product is doing the best job it can in satisfying that customer's job to be done, they will remain a client. As cars come along, customers would have much to gain by switching from your horse-drawn carriage, since the car helps the customer become more efficient in fulfilling his job to be done.\n- Linkedin using JTBD\n![](/assets/images/2021-10-27-20-09-48.png)\n    - All these options aren't describing the product, but are rather describing progress (how can you improve as a result of using the product).\n","n":0.05}}},{"i":296,"$":{"0":{"v":"Typography","n":1},"1":{"v":"\n*\"Typography is the craft of endowing human language with a durable visual form. Typography is clothing for words\"*\n- Type needs to be designed at:\n    - the macro, page-level, considering the overall structure of the page the typographic hierarchy\n    - and at the micro text-level, considering the details\n        - spacing, what words get bolded to draw focus.\n- Consider typographic pairings, a bold sans-serif typeface for headings catches the eye, coupling this with a classic serif for body copy helps aid legibility\n- to pair typefaces, find a ‘superfamily’ and build around that\n    - Consider contrasting typefaces for headings and body copy\n    - generally serif for body, and sans-serif for heading\n\n# Nice fonts\n- bryantLG \n\n# UE Resources\n- [selecting typeface for body](https://practice.typekit.com/lesson/selecting-typefaces-for-body-text/)\n- [using shading on text](https://practice.typekit.com/lesson/using-shades/)\n","n":0.091}}},{"i":297,"$":{"0":{"v":"Copy","n":1},"1":{"v":"\n![](/assets/images/2021-07-03-21-10-39.png)\nIt is interesting what ExpressVPN does here. They tell you what they believe, which gives you added confidence in the company/product, since you can better predict what their activities would look like, based on their claimed beliefs. This has the added benefit of giving people the ability to make future assumptions, since they now know a little more about the company's philosophies.\n\nLook for opportunities to add video game terminology and concepts to your apps\n- Ex. \"Unlock\" is reminiscent of levels of a video game\n","n":0.109}}},{"i":298,"$":{"0":{"v":"Color","n":1},"1":{"v":"\n# UE Resources\nhttps://learnui.design/blog/color-in-ui-design-a-practical-framework.html\n","n":0.577}}},{"i":299,"$":{"0":{"v":"Ux","n":1},"1":{"v":"\n# Introducing things the user hasn't seen before\nWhen prompting a user with a decision that is potentially impactful, it is a good idea to provide them with an option that says \"decide later\", then show them where to find that information when they need it\n- this example is from Apple when hooking up an external hardrive, when it asks if you'd like to set up Time Machine to sync with this HD\n\n### Introducing new features\n1. have a tooltip that pops up saying \"new feature coming soon: unlock phone with face\". then there is a button saying \"im interested\". now you have a reason to contact them again with the features. release\n2. when showing a user of a feature that they have not yet explored, it is better to direct them to the normal place where they will enable that feature, rather than just allowing them to turn it on or off right then and there\n\t- ex. in the Quora app, on the front page there was a link that said \"dark mode is here!\". when you click on it, instead of turning dark mode on, it opens the sidebar, and then at the bottom of the sidebar, there is an arrow pointing to the dark mode switch. this has the benefit of teaching users where to find that option so that in the future they already have this knowledge\n\n# UX Process\n- the UX process can be distilled down as follows. It is iterative and cyclical:\nObservation → Idea Generation → Prototyping → Testing\n\nwe need to understand the user's goals and motivations, then design experiences that deliver these high-level goals.\n- ex. With Never Forget, it's better to make the navigation items focused around \"actions\", since that maps to a goal of the user. They want to...\n    1. create a nugget\n    2. browse their nuggets\n    3. explore nuggets\n    4. learn their nuggets\n\n- Ensure every page or screen has a clear call-to-action\n    - Ask yourself, “What is the user trying to achieve?” and design with that in mind\n\n- on making a search box, ask yourself \"is it obvious that the user knows what they are searching for? even on first visit?\n- if searching is something they wouldn't understand on first visit and there is nothing to search anyway, consider hiding the search bar and give some information about what they'd search for instead\n    - ex. once you add some nuggets of knowledge, you can start to search through them here!\n\n## Pillars of design\n### Visual grammar\n- everything in a design can be broken down into points, lines and planes (plane - anything with a surface)\n- Every component should serve its purpose. Take away as much as you can while still serving the goal of the application.\n- When building UI components try and focus on re-using elements and minimizing visual complexity.\n    - With a core set of simplified and elegant components built, focus on combining these to create easily understandable interfaces that ease user interactions.\n### Language & Typography\n#### Language\n- Ask yourself: What’s the message? Then find the right words to communicate that message.\n- Use clear and concise language\n    - The words should be unambiguous (given it's context in the project) and should be as close as possible to describing what the user should expect to happen when engaging with (ex. clicking on) that content.\n- words can help set a tone and voice, as well as establish and reinforce the personality of a brand\n- **Macrocopy** - the words that relate to the heart of the brand. They are closer to the \"why\", and can be considered closely related to the content that marketing would use.\n    - One way of defining macrocopy is to develop a ‘brand dictionary’: a palette of words that defines the brand you’re working on\n    - ex. Consider two different car brands: smart and Mercedes. smart’s brand dictionary might include: agile, expressive, dynamic, fun and friendly. Mercedes’ brand dictionary might include: precision, luxury, stylish, engineered and efficient\n- **Microcopy** - the functional copy. These are the words we use to help guide users through the application.\n    - Microcopy should reflect your brand’s values but is more likely to be consistent from one project to another\n    - ex. Everyone understands what ‘Add to basket’ means, so don’t confuse your users by inventing your own terminology.\n\n### Narrative Design\n- consider rhythm when designing the user flow. Think of it as a narrative. We are taking the user through a journey, and we want to control the intensity of the information that is presented to them.\n    - Consider a James Bond movie. It starts off in full-action, but then it slows down, allows the coals to slow burn in the heat, building up pressure, as it gradually gets back into full-action.\n    - Too much too fast, and the user will be overwhelmed. Too slow, and the user will get bored and will be confused as to the value that is being added to them. Trick is to find the balance.\n\n* * *\n\n### Flexibility-Usability tradeoff\n- As a system gets more functionality and becomes more flexible to the user's desires, there is a natural tradeoff of ease of use of the product (in other words, it becomes more complex to navigate)\n- The ability of an audience to anticipate future uses of a product is a key indicator of how they will value flexibility versus usability in design. When an audience can clearly anticipate its needs, more specialized designs that target those needs will be favored. When an audience cannot clearly define its needs, more flexible designs that enable people to address future contingencies will be favored.\n\n# Concepts\n### Optimistic UI\n- ex. when you send a message in Whatsapp, there is a period of loading between the time you hit \"send\" and the time that message appears in the chat. A non optimistic UI will not show anything in the chat area until the request has completed, while an optimistic one will show the message instantly in the chat, but will show a loader next to it, to indicate the request is still in motion.\n\n# User-Centric design\n### Communicating chronology from a user's perspective\njoplin forums have beautifully user-centric way of showing chronology of comments. Most sites will have a date associated with a comment. However, Joplin augments this by also inlcuding a tag that says \"13 days later\". This effectively gives you the element of chronology that is often ignored when a computer centric, more abstract thing like the date is only given. Showing a date gives no sense of chronology.\n\n### Result-focused (declarative) menu buttons\n[this site](https://vincit.github.io/objection.js/guide/models.html#examples) has done an interesting thing. At the top, there is a link with a star on it, so that people can \"Star\" the repo. However, it simply links to the repo on github. There is an interesting distinction here, in that the maker of the site is defining an action, even though the user cannot actually execute that action on the website. In other words, they need to be shot off to another site in order to take that action, or \"star\" that github repo. This is an example of being more \"declarative\" in your thinking when designing user interfaces. Make things to be more \"action\" focused, rather than what it is you are doing. In the linked website, imagine that the button had just read \"Github\". Fewer people would have clicked \"star\". By giving the user an action to do, they took it.\n- It is generally a better idea to focus buttons on the result, rather than the action. This becomes more true as the idea gets more abstract.\n\n# Allowing user to discover new things in the app\n### Steam Browse set\nSteam has an interesting feature on their app. you get the concept of a browse set, which is a list of 10 new games, shown to you based on your interests. After you've seen all ten, if you want to see more, you can get another set of ten\n\nThis is interesting because it gives a sense of completion to the app. Think of it in terms of WoW daily quests. It gives you a reason to log in to complete your task for the day.\n\nThis is a good idea to get people in NF to add nuggets to their repo. This also blends nicely with the fact that If our pricing model is based around a threshold, then this will encourage users to get to that limit quicker\n\n### Helping users understand why they may not be able to find something\n- User's tend to look for things to do in an application. Because of the way your business logic is set up, the user may not be able to do a particular action. This is the moment where we need to show the user what they *would have* seen, had that business logic been satisfied\n\t- ex. on Freelancer.com, you can't review a freelancer until the payment has been released from the project. Because this is not an entirely obvious thing (in fact other sites do let you do this), freelancer.com should have a highlighted-out section of the freelancer's profile. Otherwise, the user spends a lot of time looking around the site for the ability to rate, and will never find a trace of it. This is a bad UX, and one that could be drastically improved.\n- The idea here is that users want to figure something out. It is easy to focus on only one half of this sentence. Most of the time when you hear \"help users accomplish goals\", we often forget that this desire is being fulfilled if we put up markers along the way that help users navigate their way.\n\n* * *\n\nwhat really counts is not the number of clicks it takes me to get to what I want (although there are limits), but rather how hard each click is—the amount of time that elapses between each click\n- ex. if there are only 2 buttons to click, but I really have to think about what to choose, that is still a bad experience\n\n100ms is the roof for an imperceivable delay.\n- up to 300ms is still considered \"fast\", but the delay can be noticed\n\n# Notable UX Designs\n- [Big Apple Hotdogs](http://www.bigapplehotdogs.com/)\n- [Jira](https://www.atlassian.com/software/jira/mac)\n- iOS \"Shortcuts\" app\n- Mongo Cloud (Atlas) SaaS\n- Blinkist\n- Raspberry Pi\n- Dropbox\n- DigitalOcean\n\t- Specifically the wizards to create droplets, clustered dbs etc.\n\n# UE Resources\n- [wow moment 1](https://www.forentrepreneurs.com/time-to-wow/)\n- [wow moment 2](https://www.appcues.com/blog/finding-your-products-first-wow-moment)\n- [laws of ux](https://lawsofux.com/)\n- [activity centered design](http://bokardo.com/archives/activity-centered-design/)\n- [ux design series (current progress)](https://www.smashingmagazine.com/2018/01/comprehensive-guide-ux-research/)\n- [guide to microcopy](https://www.wheelhousedmg.com/blog/a-brief-guide-to-ux-writing-microcopy-and-content-design/)\n","n":0.024}}},{"i":300,"$":{"0":{"v":"laws","n":1},"1":{"v":"\n### Hick's Law\n\"The time it takes to make a decision increases with the number and complexity of choices.\"\nApplication of law:\n- Distinguish essential content from secondary content. By enabling users to find a path through fewer choices, you’ll reduce their cognitive burden.\n- ex. When creating navigation instead of providing an endless list of choices, focus on just a few.\n\n### Fitt's Law\nThe time it takes to acquire a target is a function of the distance to and size of the target.\n- ie. The farther away a target is — a button on a screen, for example — the larger it needs to be for a user to be able to reach it easily.\n\n### Miller's Law\nThe average person can only keep 7±2  items in their working memory. In short: there’s only so much we can hold in our heads in a short space of time.\n- Miller’s law is particularly important when we consider how we organize and group information, and is where chunking can come in useful\n- Where possible look for groups of information that can be broken down and chunked, enabling them to be held more easily in users’ working memory.\n\n","n":0.073}}},{"i":301,"$":{"0":{"v":"UI","n":1},"1":{"v":"\n\n- Mimic familiar objects and environments in abstract contexts (e.g., software interfaces) to imply the way in which new systems can be used\n- A satisfactory area alignment can be achieved by positioning an object along the axis of alignment such that an equal amount of area or visual weight hangs on either side — if the object had mass, it would be balanced on the axis.\n- quotes should be aligned based on the text edge and not on the quotation marks\n- When objects are simple and symmetrical, align based on their edges; otherwise, align based on their areas.\n- When designs involve simple and recognizable patterns, consider removing or minimizing the elements in the design that can be supplied by viewers.\n- Angularly-shaped (sharper) objects are more effective at attracting attention and engaging thought; contoured objects are more effective at making a positive emotional and aesthetic impression.\n- Whitespace should be as intentional as content\n\n# Design Principles\n- When designing an interface for users to do things, consider where you expect their eyes to naturally be. Position things in such a way that it takes advantage of where their eyes will be.\n    - Imagine a simple login page. There it a field for username and password, a link below those fields that say \"forgot password?\", and a button in the top right concerner saying \"Log in!\". The natural movement of the eyes for this layout is to look at username, look at password. with their eyes now being at password, they must jump all the way to the top right corner. \"Don't make me think\" states don't make them needlessly thing. A login is simple. The user should be able to do it without even using text anywhere on the page. Don't make them read the text on such an engrained activity as logging in.\n\n# Quotes\n- “Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away”\n\n# UE Resources\n- [7 Rules for creating gorgeous UI design](https://learnui.design/blog/7-rules-for-creating-gorgeous-ui-part-1.html#rule-1-light-comes-from-the-sky)\n\n# UI Inspiration\n- [reddit list](https://www.reddit.com/r/androiddev/comments/4cezsx/where_do_you_usually_go_for_app_ui_design/)\n- [collectui](https://collectui.com/)\n- [UI Design Daily](https://uidesigndaily.com/)\n- DiscordApp\n","n":0.054}}},{"i":302,"$":{"0":{"v":"components","n":1},"1":{"v":"\n## Component design principles\n- set your touch target (ie. buttons) size to 9 mm square or greater (48×48 pixels on a 135 PPI display at a 1.0x scaling).\n- Keep the number of character per line within 60-80 (30–40 for mobile)\n\n* * *\n## Atomic components\n### Navbars\n- when the page the user is on is where an action is accomplished, a navbar is not needed. Presumably, the user came to this part of the app with intention and thus does not need to navigate freely to other parts of the app so easily. In lieu of the missing navbar, there should be a back button available at the top of the screen so the navbar is realistically only 1 navbar click away.\n\n### Icons\n- when picking which icon to represent which idea, think more in terms of what the outcome of the icon helps you accomplish, rather than the method it uses to get there\n    - Google Slides uses the icon of a highlighter to symbolize the act of highlighting text. Makes sense why they would do that, but it's misguided. While you are using a piece of software you and want to accomplish a goal like highlighting text, your eyes naturally start looking for a symbol that shows the result. In other words, they start looking for an icon perhaps with text and a colored background. Instead, they should have been looking for a symbol that is easily mistaken with a pen. This creates friction, and understandably a lot of friction. \n\n### Lists\nanything that can be a bulleted list probably should be.\n","n":0.062}}},{"i":303,"$":{"0":{"v":"Postgraphile","n":1},"1":{"v":"\nPostGraphile automatically detects tables, columns, indexes, relationships, views, types, functions, comments and more. It builds a GraphQL server that is highly intelligent about your data, and that automatically updates itself without restarting when you modify your database.\n\nPostGraphile provisions, sets up and tears down a PostgreSQL client automatically for each GraphQL query\n- Setup involves beginning a transaction and setting the relevant session variables, e.g. using your JWT or the pgSettings function\n\n### Smart Comments\n- in postgres, we can make user-friendly remarks on a table called `comments` by using the COMMENT statement\n- Postgraphile leverages this feature to be able to alter functionalities by adding smart comments as comments on a table\n\n## Function\n**Computed Column vs. Custom Query**\n- We can differentiate between computed columns and custom queries by observing that computed columns must accept the table they belong to as a first arg. Of course, to be able to create a computed column we need a table to attach it to.\n- Consider the expansive nature of computed columns. They allow us to augment our existing tables with data that doesn't normally belong there.\n\n### Computed column\n- def - a psuedo column that we can attach to a table that will automatically be reflected in the graphql schema\n- in a function, running `setof nugget` will return a connection\n\n### Custom Queries\n- similar to computed columns, but instead of the function being callable as a node's field (ex. the `buckets` field on `nugget`), the function is callable from the root-level\n- ex. `all_nuggets_with_bucket_id` function\n\n## Auth\n- Postgraphile can generate JWTs easily from inside your PostgreSQL schema.\n\t- to do this, we define a `jwtToken` composite type and we pass it to `jwtPgTypeIdentifier`, and now every time that type is returned from a postgres function, it will be signed with the jwtSecret, and return it as a jwt token as part of the graphql response.\n- when the server receives a jwtToken from the request's authorization headers, like so:\n```\n{\n  \"aud\": \"postgraphile\",\n  \"role\": \"app_user\",\n  \"user_id\": 27\n}\n```\nit will automatically run this code:\n```\nset local role app_user;\nset local jwt.claims.role to 'user_login';\nset local jwt.claims.user_id to '27';\n```\n\n## Exposing HTTP requests to Postgres\n- `pgSettings` lets us set the jwt within postgres' `current_setting` while having access to the request\n\t- this function fires on each request, and everything returned by it will be applied to `current_setting` (with `set_config`)\n\t- ex. we can get the userId from the request and update the value of `user_id` within `current_setting(...)`\n\t- `pgSettings` is a function that can be async\n- instead of passing an object, we can pass `pgSettings` function that will get executed on each request.\n- Everything returned by `pgSettings` is applied to the current session with `set_config($key, $value, true)`\n- You can use `pgSettings` to define variables that your Postgres functions/policies depend on\n\t- When adding variables for your own usage, the keys must contain either one or two periods (`.`)\n\t\t- Variables without periods will be interpreted as internal Postgres settings, such as role, and will be applied by Postgres\n\t\t- All settings are automatically reset when the transaction completes\n\t- Here's an example of switching the user into the Postgres 'visitor' role, and applying the application setting jwt.claims.user_id:\n\t```\n\tpgSettings: async req => ({\n\t    'role': 'visitor',\n\t    'jwt.claims.user_id': req.user ? req.user.id : undefined,\n\t    //...\n\t}),\n\t```\n- role is overridden after pgSettings is applied\n\t- But only if `pgDefaultRole` is set or there's a role property in the claim of the JWT\n\n## Exposing HTTP requests to resolvers\n- `additionalGraphQLContextFromRequest` is an optionally asynchronous function that has access to the req and res objects from your HTTP library (Express)\n\t- The result returned from the function is merged into the GraphQL context object which is passed as the third argument to every GraphQL resolver\n\t- `additionalGraphQLContextFromRequest` let us perform asynchronous actions if we need to, for example looking up the current user in the database with Passport. Once all that is done, we can return an object from this function that will merge with the existing `context` object so the resolvers can access it.\n\n## Graphile Workers\n- allows you to run jobs (e.g. sending emails, performing calculations, generating PDFs, etc) \"in the background\" so that your HTTP response/application code is not held up\n[repo](https://github.com/graphile/worker)\n\n## Extending Graphql Schema (makeExtendSchemaPlugin)\n- when we use `makeExtendSchemaPlugin`, we can define types and resolvers that will get merged into the existing ones generated by Postgraphile\n- the callback returns an object with 2 keys:\n\t1. `typeDefs`\n\t2. `resolvers`\n\t\t- an object whose keys are graphql types, which resolve to an object with `key`-`value` pair of `field`-`resolver function`\n- the `build` argument is supplied to the `makeExtendSchemaPlugin` callback, and it contains lots of information and helpers defined by various plugins\n\t- includes the introspection results, inflection functions, and SQL helper (`build.pgSql`, an instance of `pg-sql2`, a query builder)\n\n* * *\n### Custom mutations/queries\n- By default Postgres assumes all functions will mutate the database. Therefore, if we want the postgres function to show up as a query, we need to mark it `stable`\n- when naming custom functions that get back the user some data, we need to name it as if it were a simple property on an object. We don't want to name is something like `getUsers`. Instead, we want to simply call it `users`. This makes more sense when viewing it from the graphiql perspective and querying via graphql.\n\n* * *\n\n### Pooling\n- if we are using postgraphile, `rootPgPool` Postgraphile doesn't know about it, as we don't pass it to the postgraphile engine. Instead, it is used in the `additionalGraphQLContextFromRequest` callback.\n\n* * *\n\n### Scalar Types\nPostGraphile generates the following scalar types:\n- BigFloat, BigInt, BitString, Boolean, CidrAddress, Date, Datetime, Float, Int, InternetAddress, Interval, JSON, KeyValueHash, MacAddress, MacAddress8, String, Time, UUID\n\n## Resources\n### Quality Repos\n[graphile/starter - lots of really quality code. check out how some of the lower level db config functions work](https://github.com/graphile/starter)\n### Quality Docs\n[lots of overall good info, inc high level setting up the SQL](https://github.com/graphile/postgraphile/blob/v4/examples/forum/TUTORIAL.md)","n":0.033}}},{"i":304,"$":{"0":{"v":"Utils","n":1}}},{"i":305,"$":{"0":{"v":"Gql","n":1},"1":{"v":"\nThe gql helper is responsible for turning the human-readable GraphQL schema language you write into an abstract syntax tree ([[AST|graphql.ast]]) that the application can understand.\n\nthe `gql` helper differs slightly from the one from `graphql-tag`, namely in how the placeholders work.\n- This `gql` function is designed to work with PostGraphile's inflection system, so you can embed strings directly\n- We can also embed other gql tags directly\n\n```js\nconst nameOfType = \"MyType\"; // Or use the inflection system to generate a type\n\n// This tag interpolates the string `nameOfType` to allow dynamic naming of the\n// type.\nconst Type = gql`\n  type ${nameOfType} {\n    str: String\n    int: Int\n  }\n`;\n\n// This tag interpolates the entire definition in `Type` above.\nconst typeDefs = gql`\n  ${Type}\n\n  extend type Query {\n    fieldName: Type\n  }\n`;\n```","n":0.091}}},{"i":306,"$":{"0":{"v":"Embed","n":1},"1":{"v":"\nIf we want to embed a raw js value (function, string, object etc.) into a document, we can use the `embed` function within the `gql` tagged template literal\n- most common use case is to pass the value to a directive","n":0.158}}},{"i":307,"$":{"0":{"v":"Schema","n":1},"1":{"v":"\n## makeExtendSchemaPlugin\nAllows us to extend the graphql schema that is generated by Postgraphile\n- It does this by allowing us to define additional Graphql Types and Resolvers and merge them into our schema.\n- To accomplish this, the callback we pass to `makeExtendSchemaPlugin` should return the `typeDefs` (schema definition) and the `resolvers` function\n\n### typeDefs\nThis allows us to specify how our Graphql schema will be extended.\n- ex. Will we be adding a new subscription on it? A new InputType?\n\n### build\nThe `build` argument to the `makeExtendSchemaPlugin` contains information and helpers defined by various plugins in the Postgraphile ecosystem\n- most importantly, it includes:\n\t1. introspection results (`build.pgIntrospectionResultsByKind`)\n\t2. inflection functions (`build.inflection`)\n\t3. an SQL helper (`build.pgSql`, an instance of `pg-sql2`\n\n### selectGraphQLResultFromTable\nThis helper populates data that is returned from our resolver\n- It should *not* be used to retrieve data for our resolver to process.\n\t- Instead use `context.pgClient` directly.\n\nThis helper should not be called more than once per resolver (which wouldn't make sense anyway)","n":0.081}}},{"i":308,"$":{"0":{"v":"Views","n":1},"1":{"v":"\nWe can use postgres views in such a way that it is exposed on the graphql schema to be consumed, as if it were an actual table.\n\nImagine we have the following:\n```sql\nCREATE TABLE app_public.films (\n  id serial PRIMARY KEY,\n  name text,\n  release_year int,\n  kind text\n);\n\nCREATE VIEW comedies AS\n    SELECT *\n    FROM app_public.films\n    WHERE kind = 'Comedy';\n```\n\nThe view `comedies` is able to be queried directly on the Graphql schema:\n```\n{\n  comedies(first: 20) {\n    name\n    releaseYear\n  }\n}\n```","n":0.117}}},{"i":309,"$":{"0":{"v":"Directives","n":1}}},{"i":310,"$":{"0":{"v":"Requires","n":1},"1":{"v":"\n# @requires\nWhen extending a schema, it's often because you want to expose data from Node.js that would be too difficult (or impossible) to access from PostgreSQL\n- in other words, we come to a situation where we don't want to rely on Postgres to process and give us data, but instead want to rely on Node to do the job for us.\n\n## USD to CAD example\n- Imagine in our database we store the price of a product in USD. However, we want to be able to query the `priceInCad`, directly as a field on the graphql schema:\n```\nquery Product {\n\tproduct {\n\t\tpriceInCad\n\t}\n}\n```\n\nWe can accomplish this by making a custom Postgraphile plugin, which would extend the Graphql schema to include the `priceInCadCents` field on the `Product` type:\n```\nconst MyForeignExchangePlugin = makeExtendSchemaPlugin(build => {\n  return {\n    typeDefs: gql`\n\t\t\textend type Product {\n\t\t\t\tpriceInCadCents: Int! @requires(columns: [\"price_in_us_cents\"])\n\t\t\t}`,\n    resolvers: {\n      Product: {\n        priceInCadCents: async product => {\n          // Note that the columns are converted to fields, so the case changes\n          // from `price_in_us_cents` to `priceInUsCents`\n          const { priceInUsCents } = product;\n          return await convertUsdToCad(priceInUsCents);\n        },\n      },\n    },\n  };\n});\n```\n\nwe include the `@require` directive to show that in order to calculate this field, we need the postgres column `price_in_us_cents`","n":0.071}}},{"i":311,"$":{"0":{"v":"Pgsubscription","n":1},"1":{"v":"\nProvided by the `@graphile/pg-pubsub` library\n\nThis directive allows us to embed a function that will calculate the PostgreSQL topic to subscribe to based on the arguments and context passed to the GraphQL field","n":0.177}}},{"i":312,"$":{"0":{"v":"Pubsub","n":1},"1":{"v":"\nGraphile defines a library `pg-pubsub`\n\nThe main high-level purpose of this library is provide our custom plugins with realtime data, which allows us to add subscription fields to our API.\n- The library uses Postgres' `LISTEN`/`NOTIFY` to provide realtime features\n\nThis library also gives us the `@pgSubscription` directive that we can put on graphql fields\n\n\n[[Pub-Sub|general.architecture.pub-sub]]","n":0.139}}},{"i":313,"$":{"0":{"v":"Philosophy","n":1},"1":{"v":"\n### Default Heuristic\nInstead of searching for the best option, use a technique we call the default heuristic. The premise of this heuristic is that when the cost of acquiring new information is high and the consequence of deviating from a default choice is low, sticking with the default will likely be the optimal choice.\n- It should be any option that gives you very high confidence that it will work.\n- It is something you’ve used before.\n- Something you understand well.\n- Something that has proven itself to be a reliable way for getting things done in the space you’re operating in.\n- It doesn’t necessarily have to be the theoretical best choice.\n- It doesn’t have to be the most efficient. Or the latest and greatest.\n- It simply needs to be a reliable option to get you to your ultimate desirable outcome.\n- It should be very unlikely to fail you; you have to be confident that it’s a very safe bet. In fact, that’s the only requirement.\n","n":0.078}}},{"i":314,"$":{"0":{"v":"Data","n":1},"1":{"v":"\n# Push vs Pull data creation\n- Pull and Push are two different protocols that describe how a data Producer can communicate with a data Consumer.\n![](/assets/images/2021-03-07-22-37-12.png)\n\nNote:\n- A regular function is a lazily evaluated computation that synchronously returns a single value on invocation.\n- A generator function is a lazily evaluated computation that synchronously returns zero to (potentially) infinite values on iteration.\n- A Promise is a computation that may (or may not) eventually return a single value.\n- An Observable is a lazily evaluated computation that can synchronously or asynchronously return zero to (potentially) infinite values from the time it's invoked onwards.\n\n**Pull**\n- Consumer determines when it receives data from the data Producer\n- The Producer itself is unaware of when the data will be delivered to the Consumer.\n- ex. every javascript function is a pull system\n\t- The function itself is the producer, and the calling code is the consumer. The reason the calling code is called the consumer is because the calling code \"pulls\" out a *single* return value\n\n**Push**\n- Producer determines when to send data to the consumer, and the consumer is unaware of when it will receive that data.\n- ex. Promises and Observables are a push system, since the promise (a producer) delivers a resolved value to the registered callbacks (the consumers).\n\t- it is the Promise which is in charge of determining precisely when that value is \"pushed\" to the callbacks.\n\t- ex. An Observable is a Producer of multiple values, \"pushing\" them to Observers (Consumers).\n","n":0.064}}},{"i":315,"$":{"0":{"v":"Automation","n":1},"1":{"v":"\n# Automation\n![](/assets/images/2021-03-07-22-40-00.png)\n- benefits of automation (not related to time savings):\n\t- prevents you from having to context switch often\n\t- provides you with self-documented code that demonstates how to accomplish a certain task\n","n":0.18}}},{"i":316,"$":{"0":{"v":"Postgres","n":1},"1":{"v":"\nPostgres is a [[DBMS|db.DBMS]].\n- \"SQL tells the database what information you want, and the DBMS determines the best way to provide it.\"\n\nThink of postgres as a micro service that is stateful.\n- Further, don’t think of it as a storage layer, but rather as a concurrent data access service. The service is capable of handling data processing\n\t- nm. think of The Matrix, where Neo awakes in the pod near the start of the movie. If each pod were a piece of data, the mechanical arms that move and manipulate the pods would be Postgres.\n\t\t- In actuality, it would be both— the whole system. Limiting our thinking to just the data also limits our thinking in terms of what we consider Postgres to be capable of.\n\nWhen we think of Postgres only as our data layer, we tend to overlook the power that Postgres can actually bring to the table.\n\nIn PG, data consistency is maintained by using a multiversion model (Multiversion Concurrency Control, MVCC).\n- This means that each SQL statement sees a snapshot of data (a database version) as it was some time ago, regardless of the current state of the underlying data.\n- [source](https://www.postgresql.org/docs/current/mvcc-intro.html)\n\n### Performance\nIt's possible to scale Postgres to storing a billion 1KB rows entirely in memory - This means you could quickly run queries against the full name of everyone on the planet on commodity hardware and with little fine-tuning.\n- Postgres can easily handle 10,000 insertions per second.\n\nIf we have the SQL do the data related heavy lifting, there will often be a net gain in performance. Mostly, it is because round-trip times and latency along with memory and bandwidth resources usage depend directly on the size of the result sets.\n\nIt's rarely a mistake to start with Postgres and then switch out the most performance critical parts of your system when the time comes.\n\n# UE Resources\n[Exercises for learning SQL with PG](https://pgexercises.com/)\n","n":0.057}}},{"i":317,"$":{"0":{"v":"Views","n":1},"1":{"v":"\n# View\n- A view is like a derived table, but instead of it only living within the context of a single query, it exists more in perpetuity \n\t- in other words, it is the result set of a stored query\n\t- users of the database can query the view just as they would query any other table\n- while a view is a virtualized table that represents the result of a particular db query, when a query is made against a view, that query is converted into a query that can be made against the base table (in other words, the underlying base table(s) that the view is based off of)\n\t- *materialized view* - in contrast to the way a view is queried, a materialized view actually takes up storage space, making it a proper table (and not just a conceptual table). When you query a materialized view, the result set is cached and stored (or, materialized) as a sort of pseudo table \n\t\t- can be thought of as a snapshot of a query set\n\t\t- makes queries more efficient at the expense of space  \n","n":0.074}}},{"i":318,"$":{"0":{"v":"Tools","n":1},"1":{"v":"\n- [SchemaSpy: tool for documenting database](http://schemaspy.org/)\n- [Citus: Convert into distributed database](https://www.citusdata.com/)\n\t- To your application it still looks like a single database, but then under the covers it’s spread across multiple physical machines and Postgres instances.\n","n":0.169}}},{"i":319,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\nPostgreSQL includes many SQL tests to validate its query parser, optimizer and executor. It uses a framework named the *regression tests suite*, based on a very simple idea:\n\n1. Run an SQL file containing your tests (using psql)\n2. Capture its output to a text file that includes the queries and their results\n3. Compare the output with the expected one that is maintained in the repository with the standard diff utility\n4. Report any difference as a failure\n\nYou can have a look at PostgreSQL repository to see how it’s done, as an example we could pick [src/test/regress/sql/aggregates.sql](https://github.com/postgres/postgres/blob/master/src/test/regress/sql/aggregates.sql) and its matching expected result file [src/test/regress/expected/aggregates.out](https://github.com/postgres/postgres/blob/master/src/test/regress/expected/aggregates.out).\n- Implementing regression testing like this is trivial, since the driver is only a thin wrapper around executing standard applications such as *psql* and *diff*\n\nThe idea would be to always have a setup and a teardown step in your SQL test files, wherein the setup step builds a database model and fills it with the test data, and the teardown step removes all that test data.\n\n# Tools\n[RegreSQL: Regression testings for Postgres](https://github.com/dimitri/regresql)\n","n":0.076}}},{"i":320,"$":{"0":{"v":"Pgtap","n":1},"1":{"v":"\npgTAP is a suite of database assertion functions that make it easy to write [[TAP|testing.TAP]]-emitting unit tests \n- These tests can be written in 2 ways:\n    - scripting-style unit testing, typical with TAP test frameworks\n    - xUnit-style test functions\n\nWith pgTAP, we can just compare values directly in the database. There is no need to do any extra work to get the database interface to talk to the database, fetch data, convert it, etc. You just use SQL\n\nExample test:\n```sql\n-- Start a transaction.\nBEGIN;\nSELECT plan( 2 );\n\\set domain_id 1\n\\set src_id 1\n\n-- Insert stuff.\nSELECT ok(\n    insert_stuff( 'www.foo.com', '{1,2,3}', :domain_id, :src_id ),\n    'insert_stuff() should return true'\n);\n\n-- Check for domain stuff records.\nSELECT is(\n    ARRAY(\n        SELECT stuff_id\n          FROM domain_stuff\n         WHERE domain_id = :domain_id\n           AND src_id = :src_id\n         ORDER BY stuff_id\n    ),\n    ARRAY[ 1, 2, 3 ],\n    'The stuff should have been associated with the domain'\n);\n\nSELECT * FROM finish();\nROLLBACK;\n```\n\n## Schema testing\nThere are a wealth of assertion functions to test the schema\n`has_table`, `col_is_pk`\n","n":0.081}}},{"i":321,"$":{"0":{"v":"Terms","n":1},"1":{"v":"\n### Operation\n`select`, `insert`, `update`, `delete`\n","n":0.447}}},{"i":322,"$":{"0":{"v":"Tablespaces","n":1},"1":{"v":"\n# Tablespace\n- Tablespaces are where PostgreSQL stores the database objects. Therefore, it is an abstraction between the physical and logical layers (ie. what the data looks like on disk, and what the data looks like in sql-format.)\n- Tablespaces allow you to move your data to different physical locations across drivers easily by using simple commands.\n\t- ex. 2 different objects in one schema might have 2 different underlying tablespaces.\n- By default, PostgreSQL provides you with two tablespaces:\n\t1. The `pg_default` is for storing user data.\n\t2. The `pg_global` is for storing system data.\n- A common use of tablespaces is for performance\n\t- ex. an index that is used often can be tablespaced on a faster SSD. \n\t- ex. a table with cold data that is rarely accessed can be tablespaced on a slower magnetic HDD.\n- At the file layer (of data storage), a single file can only correspond to a single tablespace.\n- Using tablespaces, we can provision different locations for our data, based on the idea of classifying data as hot, warm or cold (determined by frequency that data will be needed)\n\t- This would involve us having a different tablespace in each storage group (ie. a different path to the physical localtion). This means that moving data from hot to warm involves changing the tablespace that data is associated with.\n\nhttps://techchannel.com/SMB/9/2012/storage-groups-hot-warm-cold#:~:text=The%20classifications%20are%20often%20referred,stored%20on%20even%20slower%20storage.\n","n":0.068}}},{"i":323,"$":{"0":{"v":"Table","n":1}}},{"i":324,"$":{"0":{"v":"Joins","n":1},"1":{"v":"\n# JOIN\n- a JOIN is a query that accesses multiple rows of the same or different tables at one time\n```sql\nSELECT * FROM student_grades\n\tJOIN students ON student_grades.student_id = students.id\n```\n- every JOIN must have an ON\n\t- if the column names are identical in both tables, you can use USING\n- we may alias the tablename for one of 2 reasons: firstly, it's convenient, and secondly we might join to the same table several times, requiring us to distinguish between columns from each different time the table was joined in.\n```sql\nselect bks.starttime as start, facs.name as name\n\tfrom\n\t\tcd.facilities facs\n\t\tinner join cd.bookings bks\n\t\t\ton facs.facid = bks.facid\n\twhere\n\t\tfacs.facid in (0,1) and\n\t\tbks.starttime >= '2012-09-21' and\n\t\tbks.starttime < '2012-09-22'\norder by bks.starttime;\n```\n- any time you write `FROM table1, table2` (ie. including 2 tables instead of 1), you are writing a JOIN, even though it doesn't explicitly say `JOIN`\n- all filtering of rows and columns should be done before the JOIN\n\n- Joins are fairly expensive, which might be a reason to denormalize.\n![83fe4afa6ddc9f8d5b3665cfde631660.png](:/cad05cee711947d682fb8e53e85ab34c)\n\n## Inner Join\n- create new tables with common data between the two reference tables.\n- Will not include rows where one of the relevant piece of data are missing\n\t- ex. if joining `students` tables and `cohorts` table, and one student doesn't have a `cohort_id`, then that student won't be included\n- The common columns are typically the primary key columns of the first table and foreign key columns of the second table\n- with inner joins, order doesn't matter\n- the inner join will examine each row in the first table and compare each row's value (specified after `ON`). If these values are equal, the inner join creates a new row that contains columns from both tables\n\t- ex. `... FROM table1 INNER JOIN table2 ON table1_name_col = table2_name_col`\n- ~99% of the time, we want to use an INNER JOIN\n- if we made a venn diagram of table1 and table2, an inner join would give us the intersect.\n\t- ie., performing an inner join will give us the columns(?) in common between the 2 tables\n![820988c3645cc0df3da155abc938228f.png](:/066d1e23ea0a45738633ae20ae3f38ee)\n- you can join a single table with itself. You may do this if you want to derive information from the table (that is not explicitly stored), while using it to determine\n\t- ex. below we are getting \"the members who have bee recommended by another member\". since we are using data within the same table to do the determination, we need to join with itself.\n```sql\nselect\nrecs.firstname as firstname,\nrecs.surname as surname\nfrom cd.members mems\njoin cd.members recs\non recs.memid = mems.recommendedby\norder by surname, firstname\n```\n","n":0.05}}},{"i":325,"$":{"0":{"v":"Inheritance","n":1},"1":{"v":"\n## Table Inheritance\n- Imagine we had 2 data sets: cities and capitals. A naive approach might be to create 2 tables called `non_capitals` and `capitals`, then join them together in a UNION\n\t- This approach is bad because problems arise once we need to update multiple rows at once.\n- Instead, a better solution is to use inheritance:\n```\nCREATE TABLE cities (\n  name       text,\n  population real,\n  elevation  int     -- (in ft)\n);\n\nCREATE TABLE capitals (\n  state      char(2) UNIQUE NOT NULL\n) INHERITS (cities);\n```\n- here, a row of `capitals` will inherit all columns from its parent, and will gain an additional column `state`\n- a table can inherit from more than one other table\n- using the ONLY clause, we can prevent the query from running over tables below the specified table\n\t- ex. in the capital cities example, if we query `SELECT * FROM ONLY cities...`, then we prevent capitals from showing up in the result set.\n","n":0.082}}},{"i":326,"$":{"0":{"v":"System","n":1}}},{"i":327,"$":{"0":{"v":"Information Schema","n":0.707},"1":{"v":"The information_schema is a schema that contains a set of views that contain information about the objects defined in the database.\n\nThe information_schema is part of the SQL standard, unlike the [[system catalog|pg.system.catalog]]\n\n- As a consequence of this, you will not find information on Postgres-specific features in the information schema. An example of this is the fact that we can get all user-defined types on `information_schema.user_defined_types`, but we cannot get enums. Enums aren't part of the SQL spec, and therefore are not findable through the information_schema.\n","n":0.108}}},{"i":328,"$":{"0":{"v":"System Catalog","n":0.707},"1":{"v":"The system catalogs are just regular tables that are updated automatically in response to events.\n\n- ex. the `pg_database` catalog is updated in response to `CREATE DATABASE MY_DB`\n\nThe system catalogs are the place where a RDBMS stores:\n\n- schema metadata, such as information about tables and columns\n- and internal bookkeeping information\n\nSystem catalogs are _not_ part of the SQL standard, and are core to Postgres, unlike the [[information schema|pg.system.information-schema]], which is SQL-compliant\n\n","n":0.12}}},{"i":329,"$":{"0":{"v":"Seed","n":1},"1":{"v":"\n### Generate_series\n```sql\nINSERT INTO app_public.entities (id)\nSELECT gen_random_uuid()\n    FROM generate_series(1, 10) x\n;\n\nINSERT INTO app_public.organizations (id, entity_id, owner_id, display_name)\nSELECT gen_random_uuid() AS id, e.id AS entity_id, gen_random_uuid() AS owner_id, random() AS name\n    FROM app_public.entities e\n;\n```\n\n# UE Resources\n[Use generate_series](https://www.citusdata.com/blog/2018/03/14/fun-with-sql-generate-sql/)\n[For fake data generation, have a look at pyramation's faker]\n","n":0.152}}},{"i":330,"$":{"0":{"v":"Scaling","n":1},"1":{"v":"\n### Scaling PG horizontally vs vertically\nThe ability to scale horizontally is impacted by the rate of change in the primary database. The primary can be making many changes concurrently. Those changes are serialized, written to the WAL files, and the WAL information is forwarded to the secondary systems. The secondary systems *must* process this data serially. This induces *lag* into the replicant. It will “fall behind” the primary. When will it catch up? It is unpredictable. We know that it will eventually catch up, assuming that the rate of change on the primary is not constant. If you have a system which can tolerate the inherent lag in replication, then Postgres can scale quite well. However, if your workload can not tolerate this lag, you would think it does not scale well at all.\n\nIf it is about read-heavy workload then you should just add replicas. Add as many replicas as you need to handle the whole workload. You can balance all the queries across the replicas in the round robin fashion.\n\nIf it is about write-heavy workload then you should partition your database across many servers. You can put different tables on different machines or you can shard one table across many machines. In the latter case you can shard a table by a range of the primary key or by a hash of the primary key or even vertically by rows. In each of the cases above you may lose transactionality, so be careful and make sure that all the data changed and queried by a transaction be resided on the same server.\n\nHorizontal scaling in relational databases is hard to achieve because when you have tables (or shards of the same table) across the different cluster nodes, joins usually become very inefficient. Additionally, there is a problem of replication and keeping ACID guarantees while ensuring that all replicas have fresh data\n\n### Streaming Replication\n- streaming replication allows us to continuously apply WAL XLOG records to standby servers, so that they are kept current\n\t- WALs are write-ahead logs and are synonymous with, which are XLOG are transaction logs (X stands for transaction)\n","n":0.053}}},{"i":331,"$":{"0":{"v":"Query","n":1}}},{"i":332,"$":{"0":{"v":"Common Table Expression","n":0.577},"1":{"v":"\nA CTE is basically a way to set a computed result set to a variable, which we can then later use (eg. to join against)\na CTE is a temporary result that we can reference with another SELECT.\n- A CTE always returns a result set\n- They are used to simplify queries\n\t- ex. you could use one to eliminate a derived table from the main query body.\n\nA good use-case for a CTE is when we imagine that our result set is grouped by date. Imagine that we want to get a result set showing sales by date. We could certainly query a single table and order by date, but what happens when there are no sales on a particular day? We would end up with gaps in our result set. To solve this, we can make a CTE and join our tables to it.\n```sql\nwith v_date as (\n\tselect current_date\n)\n```\n\n### CTE and UPDATE..FROM\nWe can use a CTE in combination with `UPDATE..FROM` to update every row in the result set of a different query (the CTE)\n- Here, we are updating the `book_orders.purchase_status` column on book_orders that have been `PENDING` for more than 24 hours:\n```sql\nWITH old_books AS (\n\tselect * from app_public.book_orders as book_orders\n\twhere book_orders.purchase_status = 'PENDING'\n\tand created_at < NOW() - interval '24 hours'\n)\nupdate app_public.book_orders set purchase_status = 'EXPIRED'\nfrom old_books\n```\n\n[more](https://www.essentialsql.com/introduction-common-table-expressions-ctes/)\n","n":0.069}}},{"i":333,"$":{"0":{"v":"Pub Sub","n":0.707},"1":{"v":"\nVanilla Postgres allows us to define workers which watch a new events channel, and attempt to claim a new job whenever one is pushed to the channel.\n- Postgres also lets other services watch the status of the events with no added complexity.\n","n":0.154}}},{"i":334,"$":{"0":{"v":"Psql","n":1}}},{"i":335,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\nAll psql commands are just convenience wrappers around underlying SQL statements that are being executed\n- ex. `\\l+` executes an sql query on the `pg_catalog.pg_database` table, joining the `pg_catalog.pg_tablespace` table\n\n## Connecting to psql (from shell)\nEither:\n```\npsql -d mydb -h localhost -p 5432 -U kyle\n```\nor simply:\n```\npsql -d postgresql://kyle@localhost:5432/mydb\n```\nnote: `@` symbol must be entered as `%40` to properly interpret it.\n- ex. if hostname=`kyle@db1464.azure.com`, I connect like `psql -d postgresql://kyle%40db1464.azure.com`\n\n# CLI (psql/pgcli)\nshow help for a command - `\\h <sql command>`\n\nconnect to db - `\\c mydb`\nshow schemas - `\\dn`\ndisplay tables in \"public\" schema - `\\dt public.*`\ndisplay views in \"public\" schema - `\\dv public.*`\ndisplay table signature `\\d <TABLENAME>`\ndisplay types `\\dT <SCHEMA>`\ndisplay enum values ` \\dT+ <TYPE>`\ndisplay roles - `\\du`\ndisplay schema - `\\dn`\ndisplay functions - `\\df`\ndisplay function definition - `\\ef app_public.register_user`\ndisplay extensions - `\\dx`\nread in commands from a sql file- `\\i`\n- ex. `\\i migration.sql`\n- this is an alternative to shell-level `psql -h localhost -p 5432 -U kyletycholiz -d f1db < migration.sql`\nset variable in psql - `\\set DATABASE_OWNER nf_dev`\nset variable's value from command line - `psql --variable \"DATABASE_OWNER=nf_dev\"`\nlist all databases, and show their respective size on disk - `\\l+`\n\ndisplay info about current connection - `\\conninfo`\n- current user, database, port etc.\n\ncycle command history - `<C-r>`\n\n## Named Queries\nCreate a new named query\n```\n\\ns <name> select * from ...\n```\n\nCall named query\n```\n\\n <name>\n```\n\nDelete a named query\n```\n\\nd <name>\n```\n\n### Positional Parameters\nNamed queries support shell-style parameter substitution. Save your named query with parameters as placeholders (e.g. $1, $2, $3, etc.):\n\n```\n\\ns user_by_name select * from users where name = '$1'\n```\nWhen you call a named query with parameters, just add the parameters after the query's name. You can put quotes around arguments that include spaces.\n\n```\n\\n user_by_name \"Skelly McDermott\"\n```\n","n":0.061}}},{"i":336,"$":{"0":{"v":"Copy","n":1},"1":{"v":"\n## \\copy\n- This is the client-side version of the copy command. It gives us a distinct benefit, which is the source file is required on the client side. In other words, we can execute the copy command on the same machine as where the source file is stored; we don't need to upload the file to the postgres server first.\n- copy is much faster than running `insert into...`\n\n","n":0.121}}},{"i":337,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Copy from CSV into Postgres table\n1. In psql, create a new table that matches the format of the csv headers\n2. In psql, run the COPY command:\n```\nCOPY cars\nFROM '/Users/kyletycholiz/Downloads/cars.csv' \nDELIMITER ',' \nCSV HEADER;\n```\nnote: if we don't want to have the table columns match the csv headers, we can specify like so:\n```\nCOPY cars(id, name)\n...\n```\n","n":0.137}}},{"i":338,"$":{"0":{"v":"Prod","n":1}}},{"i":339,"$":{"0":{"v":"Logs","n":1},"1":{"v":"\nPostgreSQL implements the `application_name` parameter, which you can set in the connection string and with the SET command within your session. It is then possible to have it reported in the server’s logs, and it’s also part of the system activity view `pg_stat_activity`.\n\nIt is a good idea to be quite granular with this setting, going as low as the module or package level, depending on your programming language of choice. It’s one of those settings that the main application should have full control of, so usually external (and internal) libs are not setting it.\n- doing this, we can find out which part of the code is sending a specific query we can see in the monitoring, in the logs or in the interactive activity views.\n\n","n":0.089}}},{"i":340,"$":{"0":{"v":"Plpgsql","n":1},"1":{"v":"\nPL/pgSQL is an interpreted language, which means that the function body is not evaluated for syntactical soundness as in compiled programming languages like Pascal or C++. Therefore you can write a function which only reveals any errors when it is invoked.\n\nThink of sql commands as having the potential to return rows. In this sense, they are like functions.\n- in javascript we can do something like this:\n```js\nconst returnedValueFromFn = fn()\n```\n\n- in plpgsql we can do something like this:\n```sql\nselect * from users returning * into v_users;\n```\n\nSince these sql commands are like functions, we can imagine that `v_user_id` is an argument to the following query:\n```sql\nselect * from users where id = v_user_id;\n```\n- When executing a SQL command in this way, PL/pgSQL may cache and re-use the execution plan for the command\n\n# E Resources\n[Return multiple fields without making new type](https://stackoverflow.com/questions/4547672/return-multiple-fields-as-a-record-in-postgresql-with-pl-pgsql)","n":0.086}}},{"i":341,"$":{"0":{"v":"Types","n":1},"1":{"v":"\nWe can get the type of a column like so:\n```\nSELECT pg_typeof(now());\n+--------------------------+\n| pg_typeof                |\n|--------------------------|\n| timestamp with time zone |\n+--------------------------+\n```\n","n":0.236}}},{"i":342,"$":{"0":{"v":"Record","n":1},"1":{"v":"\nA type that holds a single row from a result-set (therefore, not just tables)\n\n`record` type is similar to `row-type`, but `record` doesn't have a predefined structure. The structure only gets established when the `select` or `for` statements assin an actual row to it.\n\nTo access a field in the record, you use the dot notation (.) syntax like this:\n```sql\nrecord_variable.field_name;\n```\n\na record is not a true data type. It is just a placeholder.\n- Also, a record variable can change its structure when you reassign it\n","n":0.11}}},{"i":343,"$":{"0":{"v":"Statements","n":1}}},{"i":344,"$":{"0":{"v":"Raise","n":1},"1":{"v":"\n`RAISE` can be an easy and powerful way to self-document the code.\n\n## Levels\n- DEBUG\n- LOG\n- INFO\n- NOTICE\n- WARNING\n- EXCEPTION (default level)\n - raises an error, aborting current transaction\n\n## Where and when messages get sent\nraising with different levels will generate messages of different priority levels.\n- Where or not a message of a given priority will be sent to the client or not is determined by the `log_min_messages` and `client_min_messages` configuration variables.\n\t- This file also determines if we write to server log\n\n## Body\nAfter specifying the level, we can provide a simple string literal, which can be followed by optional argument expressions to be inserted into the string\n```\nRAISE NOTICE '% %', v_job_id, v_job_name;\n```\n\n### Using\nWe can attach additional info to the error by combining `using` with an option, one of:\n- MESSAGE\n- DETAIL\n- HINT\n- ERRCODE\n\t- Specifies the error code (SQLSTATE) to report, either by condition name or directly as a five-character SQLSTATE code\n- COLUMN\n- CONSTRAINT\n- DATATYPE\n- TABLE\n- SCHEMA\n```\nRAISE EXCEPTION 'Nonexistent ID --> %', user_id\n      USING HINT = 'Please check your user ID';\n```\n","n":0.077}}},{"i":345,"$":{"0":{"v":"Perform","n":1},"1":{"v":"\nallows us to do the work of a SELECT statement, but then immediately discard the result\n- useful for when we are doing side-effects with no useful result.\n\n```sql\nPERFORM create_mv('cs_session_page_requests_mv', my_query);\n```\n\n```sql\nPERFORM graphile_worker.add_job(\n\t'user__audit',\n\tjson_build_object(\n\t\t'type', 'reset_password',\n\t\t'user_id', v_user.id,\n\t\t'current_user_id', app_public.current_user_id()\n\t));\n```\n","n":0.174}}},{"i":346,"$":{"0":{"v":"Assert","n":1},"1":{"v":"\n# Assert\n`ASSERT` is a convenient shorthand for inserting debugging checks, but is not useful for reporting ordinary messages and errors.\n\n- we can give `ASSERT` a value that is always expects to evaluate to true.\n\t- if it does, `ASSERT` does nothing further.\n\t- if it doesn't, an `ASSERT_FAILURE` exception is raised.\n- if we don't pass a message, a default \"assertion failed\" will be used.\n```sql\nASSERT v_user is not null, 'user not found, check the users table';\n```\n","n":0.117}}},{"i":347,"$":{"0":{"v":"Setof","n":1},"1":{"v":"\nWhen our plpgsql function returns a `setof` rows, we must follow a different procedure:\n- the items that are returned by the function are specified by the `RETURN NEXT` or `RETURN QUERY` commands.\n- at the end, a final `RETURN` command with no argument, to indicate the function has finished execution.\n\n`RETURN NEXT` and `RETURN QUERY` do not actually cause execution to return from the function call. Instead, they simply append zero or more rows to the function's result set.\n- As successive RETURN NEXT or RETURN QUERY commands are executed, the result set is appended to further. The final `RETURN` command causes control to exit the function, leaving us with the dataset that has been constructed up until that point\n\t- note: naturally, we can omit the final `RETURN` if we are at the last line anyway\n","n":0.087}}},{"i":348,"$":{"0":{"v":"For Loops","n":0.707},"1":{"v":"\n# For Loops\n```sql\ndo\n$$\ndeclare\n\trec record;\nbegin\n\tfor rec in select title, length\n\t\t\tfrom films\n\t\t\twhere length > 50\n\t\t\torder by length\n\tloop\n\t\traise notice '% (%)', rec.title, rec.length;\n\tend loop;\nend;\n$$\n```\n","n":0.218}}},{"i":349,"$":{"0":{"v":"Diagnostics","n":1}}},{"i":350,"$":{"0":{"v":"Performance","n":1},"1":{"v":"\nThe vast majority of slow queries found in the wild are still queries that return way too many rows to the application, straining the network and the server's memory\n\n## Query Plans \nPostgreSQL devises a query plan for each query it receives. Choosing the right plan to match the query structure and the properties of the data is absolutely critical for good performance, so the system includes a complex planner that tries to choose good plans. You can use the EXPLAIN command to see what query plan the planner creates for any query\n\n### EXPLAIN ANALYZE\n\"EXPLAIN ANALYZE is the most powerful tool at our disposal for understanding and optimizing SQL queries\"\n- this is a command that accepts a statement such as SELECT ..., UPDATE ..., or DELETE ..., executes the statement, and instead of returning the data provides a query plan detailing what approach the planner took to executing the statement provided.\n\nThis is how you might go about using explain to understand run time characteristics of your queries:\n```\nexplain (analyze, verbose, buffers)\n<query here>;\n```\n\n* * *\n\nThe combination of Bitmap Index Scan and Bitmap Heap Scan is much more expensive than reading the rows sequentially from the table (a Seq Scan)\n\nSeq Scan nodes often indicate an opportunity for an index to be added, which is much faster to read\n\n[source](https://thoughtbot.com/blog/reading-an-explain-analyze-query-plan)\n\n# Tools\n[Explain Depesz: explain analyze made readable](https://explain.depesz.com/)\n","n":0.067}}},{"i":351,"$":{"0":{"v":"Operators","n":1},"1":{"v":"\noperators are declared in the system catalog `pg_operator`\n- Every entry in pg_operator includes the name of the procedure that implements the operator and the class OIDs of the input and output types.\n\nTo view all variations of the “||” string concatenation operator, try:\n```\nSELECT oprleft, oprright, oprresult, oprcode\nFROM pg_operator WHERE oprname = '||';\n\noprleft|oprright|oprresult|oprcode\n-------+--------+---------+-------\n     25|      25|       25|textcat\n   1042|    1042|     1042|textcat\n   1043|    1043|     1043|textcat\n(3 rows)\n```\n","n":0.128}}},{"i":352,"$":{"0":{"v":"Union","n":1},"1":{"v":"\n\"Take 2+ tables with equal number of columns and combine them into a single result set.\"\n- ie. combines results of 2+ SELECTs into a single result set.\n- \"append\" could have also been an appropriate name\n\nUNION is conceptually opposite of JOIN; it expands the result set in the other direction (y instead of x)\n- JOIN is for taking 2 tables and combining their columns, while UNION is for taking 2 tables and combining their rows(ie. records)\n```sql\nSELECT select_list_1\nFROM table1\nUNION\nSELECT select_list_2\nFROM table2\n```\n- The number and the order of the columns in the select list of both queries must be the same.\n- The UNION operator by default removes all duplicate rows from the combined data set (`UNION ALL` will bring them back)\n- In practice, you often use the UNION operator to combine data from similar tables, which are not perfectly normalized\n- ex. with 2 tables: `popular_movies`, `top_rated_movies`, return the movies found in both tables, removing any duplicates along the way\n![8d2bb4d874e9ee042506b53f0a5eb8cd.png](:/202885018e6045e4acd744771788c297)\n","n":0.08}}},{"i":353,"$":{"0":{"v":"Intersect","n":1},"1":{"v":"\nReturns only the rows that are available in both result sets.\n- ex. with 2 tables: `popular_movies`, `top_rated_movies`, return only the movies that appear in both tables\n\nTODO:\n![b4e5d3a9f4dd1214eb4273e9d24fa8c7.png](:/da1f2c53c1e349bb944f4f33172bb9b6)\n","n":0.196}}},{"i":354,"$":{"0":{"v":"Except","n":1},"1":{"v":"\nLike UNION and INTERSECT, EXCEPT returns distinct rows from the first (LHS) query that are not in the output of the second (RHS) query.\n- ex. with 2 tables: `popular_movies`, `top_rated_movies`, return the movies that are popular, but not top rated.\n\t- assuming we have `SELECT * FROM popular_movies` first\nTODO: fix this\n![daddae8a308e1bf9405e038b101affbd.png](:/d8dc07ccdd01418c816ddaf6951dfdd7)\n","n":0.141}}},{"i":355,"$":{"0":{"v":"Any","n":1},"1":{"v":"\n`any` compares a scalar value (ie. a base type, like `text` or `number`) to a set of values returned by a subquery.\n\nThe form is:\n```\nexpression operator ANY(subquery)\n```\nIn this syntax:\n- The subquery must return exactly one column.\n- The `ANY` operator must be preceded by one of the following comparison operator =, <=, >, <, > and <>\n- The `ANY` operator returns true if any value of the subquery meets the condition, otherwise, it returns false.\n\nNote: `SOME` is a synonym for `ANY`, meaning that you can substitute `SOME` for `ANY` in any SQL statement.\nNote: The `= ANY` is equivalent to IN operator.\n","n":0.101}}},{"i":356,"$":{"0":{"v":"Lang","n":1},"1":{"v":"\n- *quotes* - Double is for identifiers (tables, columns, schemas, functions); single is for values (mostly strings)\n","n":0.243}}},{"i":357,"$":{"0":{"v":"Views","n":1},"1":{"v":"\n# View\n- Views are named queries stored in the database. For queries that we perform often, we can effectively give them an alias. The result set of that query is conceptually placed into a table, which is referencable by the name we give it.\n- ex. Imagine we have a join that we perform often. We can put it in a view and get that data like so:\n```\nCREATE VIEW myview AS\n    SELECT city, temp_lo, temp_hi, prcp, date, location\n        FROM weather, cities\n        WHERE city = name;\n\nSELECT * FROM myview;\n```\n- Being liberal with use of Views is a good practice, as they allow us to encapsulate details of the structure of our tables behind consistent inferfaces\n\t- This value shows itself as the structure of our tables change, yet we continue to have a consistent inferface.\n- Besides the read-only views, PostgreSQL supports updatable views.\n","n":0.085}}},{"i":358,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n## Data Types\n### Composite Type\n- a type defined by the \"signature\" of the row. In other words, the very combination of columns included in a row make up a **composite type**\n\t- therefore, every column in a given table is an example of a composite type. In fact, every time we create a table, a composite type representing that row's composition is made.\n- PostgreSQL allows composite types to be used in many of the same ways that simple types can be used.\n\t- ex. a column of a table can be declared to be of a composite type.\n\t\t- spec: for instance `first_name` and `last_name`?\n- In Postgres, columns can be of composite types, meaning that we can have a type defined as (email: string, username: string), and have a column in a table called `identity` and have a column with that type.\n- When used in a function, composite types can be thought of as objects, as we would access the value of each subtype with `$1.key`\n\n### Text\n- don't use VARCHAR. just use TEXT, and add length limits\n\n#### Citext\n- Case insensitive text\n- behaves like text, except that it lower-cases everything before comparing it\n\t- Therefore, we may want to store things like email or postal code in `citext` rather than `text`.\n\n### Serial\n- SERIAL is the postgres equivalent of autoincrement in other flavors of SQL\n\t- Consider using uuid's though.\n\n### Range\n- We can specify a range in a single column (ex. timestamps, price etc).\n- The real benefit is that you can then have constraints\n\t- ex. certain time stamps can’t overlap\n- To work with ranges, use the [range operators](https://www.postgresql.org/docs/9.3/functions-range.html)\n[Demo](https://wiki.postgresql.org/images/7/73/Range-types-pgopen-2012.pdf)\n","n":0.062}}},{"i":359,"$":{"0":{"v":"Timestamp","n":1},"1":{"v":"\nFor timestamp with time zone, the internally stored value is always in UTC\n- An input value that has an explicit time zone specified is converted to UTC using the appropriate offset for that time zone.\n\t- If no time zone is stated in the input string, then the system's TimeZone parameter is used.\n\n`+00:00` indicates an `hour:minute` timezone offset\n\n### Timezone\nTimezone can be changed like:\n- `set timezone to 'Europe/Paris';`\n\n### Timestamp vs Timestamptz (with timezone)\nImagine we use `timestamptz`. Now 2 users that are haflway around the world from each other can insert data into the database, and the same exact timestamp will be recorded.\n\nFurthermore, the time that appears in our database will actually change if we change the timezone that is set in the pg client:\n```sql\nset timezone to 'Europe/Paris'\n-- ts looks like this: `2021-04-29 17:51:42.316944+02`\n\nset timezone to 'Pacific/Tahiti'\n-- ts now looks like this: `2021-04-29 05:54:15.419514-10`\n```\n\nTstz can be thought of a way of abstracting timezones. If I am looking at a set of data, I can directly compare all timezones because of this normalization.\n- ex. `2021-04-29 08:51:42.316944-07` and `2021-04-29 08:52:15.419514-07` are the timestamps recorded of 2 users in different timezones, 1 minute apart from each other.\n\nIf you manage an application with users in different time zones and you want to display time in their own local preferred time zone, then you can set timezone in your application code before doing any timestamp related processing, and have PostgreSQL do all the hard work for you.\n\nEven when using timestamps with time zone, PostgreSQL will not store the time zone in use at input time, so there’s no way from our tstz table to know that the entries are at the same time but just from different places.\n\n### Functions\n###### now()\n- returns the timestamp of the current transaction.\n\t- Therefore, the result is equal if we call `now()` in different parts of the transaction.\n\n###### clock_timestamp()\n- returns us the actual timestamp of when the code is being executed.\n\t- Therefore, if we use this in a transaction, the actual time an operation took place *will* be recorded\n\n# References\n[Timezone List](https://www.postgresql.org/docs/8.1/datetime-keywords.html#DATETIME-TIMEZONE-SET-TABLE)\n","n":0.054}}},{"i":360,"$":{"0":{"v":"Range","n":1},"1":{"v":"\n## Inclusive/Exclusive Bounds\n- `[]` is used to represent inclusive values\n- `()` is used to represent exclusive values\n- ex. `SELECT '[3,7)'::int4range` is a range that includes 3, but discludes 7\n\n## Omitting Upper/Lower Bounds\nWe can omit upper bounds with `[7,]`\nWe can omit lower bounder with `[,7]`\n","n":0.151}}},{"i":361,"$":{"0":{"v":"Point","n":1},"1":{"v":"\nPoint is a [geometric type](https://www.postgresql.org/docs/9.4/datatype-geometric.html), which are types that represent two-dimensional spatial objects\n\nPoints are the fundamental two-dimensional building block for geometric types, and are specified in one of 2 ways:\n```\n(x, y)\nx, y\n```\n","n":0.177}}},{"i":362,"$":{"0":{"v":"Numeric","n":1},"1":{"v":"\n`decimal` is an alias for `numeric`\n\nthere are 2 values inherent to a `numeric` value\n1. Precision - total $ of significant digits in a number (ie. size of integer part plus size of decimal part)\n2. Scale - size of decimal part. (therefore integers have scale of 0)\n- these 2 values should be declared explicitly\n\nexample:\n```sql\nnumeric(3,2)\n```\nmeans \"3 digits total, with 2 occurring past the decimal\"\nie. `1.00` and `0.23`, but not `1.235`\n","n":0.121}}},{"i":363,"$":{"0":{"v":"Money","n":1},"1":{"v":"\nDon't use money.\n- Instead use `decimal` (aka. `numeric`) with 2 units of forced precision\n\nif you only care about prices to 2 decimal places, then you shouldn't really care about using anything more precise than numeric type with 2 precision decimal places. However, this would coerce incoming values like `44.423` to be rounded to `44.42`, and the amounts could be very far off. Therefore, if we are controlling the prices coming in, and we always know that it will be to 2 decimal places then it shouldn't be a problem.\n- If we are really concerned about this, then we can use microcurrencies, which store the value as a bigint 1 millionth of the numeric value. This allows lots of precisio\n","n":0.092}}},{"i":364,"$":{"0":{"v":"Json","n":1},"1":{"v":"\nJSON is primarily intended to store whole documents that do not need to be manipulated inside the RDBMS\n- Updating a row in Postgres always writes a new version of the whole row. That's the basic principle of Postgres' MVCC model. From a performance perspective, it hardly matters whether you change a single piece of data inside a JSON object or all of it: a new version of the row has to be written.\n- Ideally, JSON documents should each represent an atomic datum that business rules dictate cannot reasonably be further subdivided into smaller datums that could be modified independently.\n\n## Working with JSON\n### Functions\nimagine we pass a serialized json object as an argument to a postgres function like so:\n```\n{\n\tusername,\n\tavatar_url,\n\temail,\n\tfirst_name\n}\n```\nWe can then unpack that arg into a *declared* postgres variable:\n```sql\ndeclare\n\tv_email = profile_object ->> 'email';\n-- imagine this as `email = profile_object.email`\n-- `v_email :=` syntax is identical\n```\n\n#### -> vs ->>\n`->`\nReturns the field as `JSON`\n- because of this, we must use this for chaining to access deeply nested fields:\n```sql\nSELECT info -> 'items' ->> 'product' as product\n```\n\n`->>`\nReturns the field as `text`\n\n#### Contains (`@>`)\nWe can check the JSON to see if it contains certain key-value pairs:\n```sql\nselect media_items\nfrom nuggets\nwhere media_items @> '{ \"type\": \"text\" }'\n```\n\n#### JSON vs JSONB\nIn general, most applications should prefer to store JSON data as jsonb, unless there are quite specialized needs, such as legacy assumptions about ordering of object keys.\n\n##### JSON\n- JSON - stores an exact copy of the input text, which processing functions must reparse on each execution\n\t- Because the json type stores an exact copy of the input text, it will preserve semantically-insignificant white space between tokens, as well as the order of keys within JSON objects\n- JSON values can be manipulated, but must be cast to `text` first.\n\n##### JSONB\n- JSONB - stored in a decomposed binary format that makes it slightly slower to input due to added conversion overhead, but significantly faster to process, since no reparsing is needed\n\t- also supports indexing on GIN and GIST index types.\n\t- does not preserve the order of object keys\n- gives us capability to query into our JSON document for quick lookups\n\n* * *\n\n# E Resources\nhttps://www.postgresqltutorial.com/postgresql-json/\n\n# UE Resources\n[PG plugin to query jsonb data](https://github.com/postgrespro/jsquery)","n":0.053}}},{"i":365,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Update statement to change jsonb value\n```sql\nUPDATE test\nSET data = replace(data::TEXT,': \"my-name\"',': \"my-other-name\"')::jsonb\nWHERE id = 1;\n```\n\n#### Use variable/function within JSON insertion\nEasiest way is to just use string concatenation (`||` or `concat()`)\n\nIf value is integer:\n```sql\n('[{\"specCode\": {\"name\": \"Telephone Number\", \"text\": \"TEL_NUM\"}, \"specValue\": {\"code\": null, \"text\":' || tel || '}}]')::json\n```\n\nIf value is not integer (difference in quotes used):\n```sql\n('[{\"specCode\": {\"name\": \"Telephone Number\", \"text\": \"TEL_NUM\"}, \"specValue\": {\"code\": null, \"text\":\"' || tel || '\"}}]')::json\n```\n\n#### Overwriting json value\nHere, we move values from the `list_price` and `paid_price` columns into a combined jsonb column:\n```sql\nupdate app_public.invoice_items as invoice_items\n\tset\n\t\tamount = concat('{\"list_price\":', invoice_items.list_price, ',\"paid_price\":', invoice_items.paid_price, '}')::jsonb\n\twhere id = ii_row.id;\n-- result {\"list_price\": 1465, \"paid_price\": 817}\n```","n":0.099}}},{"i":366,"$":{"0":{"v":"Interval","n":1},"1":{"v":"\nAllows us to store and manipulate a period of time.\n```sql\ninterval '2 months ago';\ninterval '3 hours 20 minutes';\ninterval '2 month - 1 day';\n```\n\nInternally, PostgreSQL stores interval values as months, days, and seconds. The months and days values are integers while the seconds can field can have fractions.\n\nThe interval values are very useful when doing date or time arithmetic. For example, if you want to know the time of 3 hours 2 minutes ago at the current time of last year, you can use the following statement:\n```sql\nSELECT\n\tnow(),\n\tnow() - INTERVAL '1 year 3 hours 20 minutes'\n             AS \"3 hours 20 minutes ago of last year\";\n```\n\n### Proper types for selected columns\n```sql\nselect album.title as album,\n\tsum(milliseconds) * interval '1 ms' as duration\n\t...\n```\n","n":0.093}}},{"i":367,"$":{"0":{"v":"Enum","n":1},"1":{"v":"\nEnums in postgres are non-DDL-safe","n":0.447}}},{"i":368,"$":{"0":{"v":"Date","n":1},"1":{"v":"\n#### current_date\nspec: when doing \"math\" on dates that don't have a time attached to them, then midnight is used:\n```sql\nselect count(*)\nfrom app_public.users\nwhere created_at > current_date\nand created_at < current_date + interval '1 day'\n```\nIf current_date gets rounded back to midnight, then, the first `where` is saying \"give me all the rows created since midnight\", and the second `where` is saying \"until midnight 24 hours later\"\n\n### Extract\nusing the `extract` function, we can get parts of a date out of a date object\n```sql\nextract('isodow' from date) as dow\n-- 4 (ISO day of week)\nextract('isoyear' from date) as year\n-- 2002 (ISO year)\nextract('week' from date) as week\n-- 52\n```\n\nWe can combine this with `trunc_date` to extract out sub-parts:\n```sql\nselect extract('year' from date_trunc('decade', date)) as decade\n```","n":0.094}}},{"i":369,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Create enum\n```sql\ncreate type mood as enum ('SAD', 'OK', 'HAPPY');\n```\n\n### Add value to enum","n":0.267}}},{"i":370,"$":{"0":{"v":"Triggers","n":1},"1":{"v":"\n# Trigger\n- a callback that is executed whenever a table (or view) is modified. Triggers can also be set to listen for specific user actions much like a callback.\n\t- Can also be set up to be executed by using the INSTEAD OF condition.\n- 2 main types of trigger:\n\t1. row-level trigger\n\t2. statement-level trigger\n\t- the difference between these two is in how many times each would be called in response to an event. \n\t\t-  ex. if you issue an UPDATE statement that affects 20 rows, the row-level trigger will be invoked 20 times, while the statement level trigger will be invoked 1 time.\n- examples\n\t- restrict DML (actions that modify the db) operations to business hours \n\t- Automatically generate derived column values\n- You can think of a trigger like a middleware that sits between the user's request (that interacts with the db) and the sql server. Every time our db is interacted with, listeners are able \"intercept\" the query and act on it\n- Most triggers are only activated by either INSERT or UPDATE statements.\n- We can specify columns on a trigger, which will cause the trigger to only fire if those columns are operated on (ex. we update those columns)\n- If multiple triggers of the same kind are defined for the same event, they will be fired in alphanumerical order.\n\t- which is why it is useful to prepend them with numbers\n\n## Syntax\n- A trigger that is marked `FOR EACH ROW` is called once for every row that the operation modifies.\n- a trigger that is marked `FOR EACH STATEMENT` only executes once for any given operation\n- `WHEN` allows us to determine whether or not the trigger should be fired\n\t- In row-level triggers the `WHEN` condition can examine the old and/or new values of columns of the row\n\t- ex. only execute the function if `OLD.balance` does not equal `NEW.balance`\n\t\t- `WHEN (OLD.balance IS DISTINCT FROM NEW.balance)`\n\t- ex. only execute function if anything has changed\n\t\t- `WHEN (OLD.* IS DISTINCT FROM NEW.*)`\n- `CONSTRAINT` allows us to adjust the timing of when the trigger actually fires.\n\t- The trigger can be fired either:\n\t\t- At the end of the statement which caused the triggering event\n\t\t- At the end of the containing transaction, which is the `COMMIT` (called deferred triggers)\n\t- Each constraint has its own IMMEDIATE or DEFERRED mode.\n\t- only available `AFTER ROW`\n\n# Parts of a Trigger\n- 2 parts to a trigger: the trigger itself, and the function that is executed by the trigger\n\n## Trigger\n\n### Before/After Triggers\nThe trigger can be specified to fire before the operation is attempted on a row, or after the operation has completed (after constraints are checked and the INSERT, UPDATE, or DELETE has completed)\n- Trigger can also be set to fire *instead of* having the operation be performed. This only works on views\n\n#### Before\n- the constraints will not-yet have been checked, allowing us to perform some action before the actual operation has taken place.\n\t- ex. Imagine we want to keep only one of the user's credit cards specified as `is_primary`. Because of the uniqueness constraints applied on the table (allowing only one `is_primary` card per user), we must set `is_primary` to `false` for all of our cards, any time the user attempts to insert a new card where `is_primary` is true. \n\t\t- If we had instead ran the trigger *after* the `insert`, then we might have gotten an error, since we are potentially trying to insert a new card with `is_primary = true`, while one may already exist.\n- If the trigger fires before the event, the trigger can skip the operation for the current row, or change the row being inserted (for INSERT and UPDATE operations only)\n- in `BEFORE`, the `WHEN` condition is evaluated just before the function is executed\n\t- Therefore, using `WHEN` (in the trigger itself) has the same effect as testing the same condition at the beginning of the triggered function with `TG_WHEN`\n\t- The implication of this is that the `NEW` value seen by the function is the current value, and not the value that would exist *following* the operation\n\t\t- Also consider that while this \"current value\" normally means \"prior to the operation being performed\", there is the possibility that previous triggers in the chain have already executed, potentially changing what \"current value\" means to us.\n\t- Another implication is that in `BEFORE`, a trigger's `WHEN` condition cannot examine the columns of the `NEW` row that is to be inserted/updated (because they wouldn't have been set yet)\n\n#### After\n- rows will be impacted with consideration to the constraints\n- If the trigger fires after the event, all changes, including the effects of other triggers, are \"visible\" to the trigger.\n- in `AFTER`, the `WHEN` condition is evaluated after the row update occurs.\n\t- The `WHEN` condition here also determines whether an event is queued to fire a trigger, following the operation.\n\t\t- Therefore, if `WHEN` does not return true, we do not have to queue an event; nor to re-fetch the row at the end of the statement\n\t\t\t- This can result in significant speedups in statements that modify many rows, if the trigger only needs to be fired for a few of the rows.\n\n## Trigger Function\nDepending on if we are using row-level triggers or table-level triggers, the function will be called for each row that is affected, or will be called once per table.\n\nA trigger function must return either `NULL` or a row value having exactly the structure of the table the trigger was fired for.\n- ie. the the row being returned must have the same type of the table.\n\n- a trigger function returns a `trigger`\n- if a trigger function updates a row in the same table that the trigger is for and the operation is the same, then it will trigger a recursive infinite loop, since the trigger function will cause the trigger to fire in response to the \n\t- ex. we have a trigger that is set to fire on update of `users` table, and the trigger function itself updates a row in that table. This update, will cause the trigger to fire again, and so on.\n\n### Return value of trigger functions\n`BEFORE`\n- if we return `null` from the function, we are signalling to the trigger manager signal that we want to skip the rest of the operation for this row, meaning 2 things happen:\n\t1. subsequent triggers will be short-circuited \n\t2. the operation will not occur for this row.\n- if we return a non-`null` value, then the operation proceeds with that value. \n\t- returning a row value that is different from the original value of `NEW` will alter the row to be inserted/updated\n\t\t- Therefore, if we want the trigger to succeed normally without altering the `NEW` row value, then we must return `NEW`\n\t\t\t - This means that we could alter single columns in the row, with `NEW.col = X`, and proceeding to return `NEW`\n- a statement-level trigger fired `BEFORE` always ignores the return value of the trigger function, so it might as well be `null`.\n\n`AFTER`\n- return value of a row-level trigger (or statement-level trigger) fired `AFTER` will always be ignored, so it might as well be `null`\n\n### Trigger Local Variables\nA function called by a trigger receives data about its calling environment (called `TriggerData`)\n- prefixed by `TG_`\n\n`NEW` \n- variable holding the new database row for INSERT/UPDATE operations in row-level triggers\n- type: `record`\n- in statement-level triggers and `DELETE` operations, this value is `null`\n\t- this shows why we can return `null` from the trigger function on `DELETE` operations. There is nothing to return to the table!\n\t- must still return a non-`null` value from a DELETE trigger function, or we will short-circuit the trigger action. Normally, `OLD` is returned, since `NEW` is null\n\n`OLD`\n- variable holding the old database row for UPDATE/DELETE operations in row-level triggers\n- type: `record`\n- in statement-level triggers and `INSERT` operations, this value is `null`\n\t\n`TG_WHEN`\n- evaluates to BEFORE, AFTER, or INSTEAD OF, depending on the trigger's definition.\n- type: `text`\n\n`TG_OP`\n- evaluates to INSERT, UPDATE, DELETE, or TRUNCATE telling for which operation the trigger was fired.\n- type: `text`\n\n`TG_ARGV`\n- the arguments passed in to the trigger's function call\n- type: `text[]`\n\nAlso\n`TG_TABLE_NAME`, `TG_TABLE_SCHEMA`, `TG_NARGS` (# of args)\n\n","n":0.027}}},{"i":371,"$":{"0":{"v":"Transactions","n":1},"1":{"v":"\nTransactions have a status to them. They might be in a \"running\" state, or they might be in a \"failed\" state.\n\nIf a function call has failed within a transaction block and we try to commit, we will instead rollback. Observe:\n```sql\nBEGIN;\nSELECT 1/0; -- ERROR: division by zero\nCOMMIT; -- error detected, ROLLBACK executed\n```\n\nPostgres transactions are isolated to individual clients, meaning that the same client must be the one to execute all code in a transaction block.\n- In `node-postgres`, we cannot use `pool.query` because of this, (spec:) because a pool may be seen as multiple different clients to the postgres server.\n\nIn Postgres\n- To execute a transaction in Postgres, use BEGIN / COMMIT / ROLLBACK\n- in Postgres, DDL commands are transactional, except when the commands are \"high-caliber\", such as creating and deleting DATABASE, TABLESPACE, CLUSTER\n- PostgreSQL supports multi-level transactions on save points level\n\t- If an error occurs inside a transaction, PostgreSQL rolls back the whole transaction but demands a command to complete the current transaction (COMMIT, ROLLBACK, ABORT)\n- Postgres treats every SQL statement as being executed within a transaction implicitly. If we do not issue a BEGIN command, then each statement will be surrounded with BEGIN..COMMIT\n\nTransactions come with a computational load. For example, if we are populating a database for the first time, we should disable AUTOCOMMIT, and instead only commit all inserts one time. This has the added benefit of being able to rollback the inserts if there is some sort of error.","n":0.065}}},{"i":372,"$":{"0":{"v":"Variables","n":1},"1":{"v":"\n# Transaction Variables\n- Transaction variables are variables that are cleared when the transaction exits\n```\ncurrent_setting('my_app.user_id', TRUE)\n```\n","n":0.258}}},{"i":373,"$":{"0":{"v":"Schemas","n":1},"1":{"v":"\n# Schema\nA schema is a logical namespace that can contain database objects such as tables, views, indexes, data types, functions, stored procedures and operators\n- by default, we use the `public` schema, if none is specified\n- a schema holds all objects, except for roles and tablespaces.\nA schema is essentially a namespace: it contains named objects (tables, data types, functions, and operators) whose names can duplicate those of other objects existing in other schemas. Named objects are accessed either by \"qualifying\" their names with the schema name as a prefix, or by setting a search path that includes the desired schema(s). A CREATE command specifying an unqualified object name creates the object in the current schema (the one at the front of the search path, which can be determined with the function current_schema).\n- \"a schema is a namespace that contains named database objects such as tables, views, indexes, data types, functions, stored procedures and operators.\"\n- A database can contain one or multiple schemas and each schema belongs to only one database.\n- ex. you may have sales schema that has staff table and the public schema which also has the staff table. When you refer to the staff table you must qualify it as follows:\n\t- public.staff\n\tOr\n\t- sales.staff\n- you can use one schema for the tables that will be exposed to GraphQL, another for the tables that should be completely private (e.g. where you store the bcrypted user passwords or other secrets never to be exposed!)\n\n## Good practice for Schema building:\n**app_public** - tables and functions to be exposed to GraphQL (or any other system) - it's your public interface. This is the main part of your database.\n**app_hidden** - same privileges as `app_public`, but it's not intended to be exposed publicly. It's like \"implementation details\" of your app_public schema. You may not need it often.\n**app_private** - SUPER SECRET STUFF 🕵️ No-one should be able to read this without a SECURITY DEFINER function letting them selectively do things. This is where you store passwords (bcrypted), access tokens (hopefully encrypted), etc. It should be impossible (thanks to RBAC (GRANT/REVOKE)) for web users to access this.\n- security definer effectively changes the role for the function being executed\n\n#### \"User\" tables\n- we should have 2 different tables about users. One should be on the private schema (`user_account`), where password and email will be kept. The other should be on the public schema, where everything else about the user is kept. The two tables have a 1:1 relationship, where the `private.user_account` table has a reference to the `public.user` table.\n\t- [good example](https://github.com/dijam/graphile-jwt-example/blob/master/db.sql)\n\n* * *\n\nAccording to the SQL standard, the owner of a schema always owns all objects within it. PostgreSQL allows schemas to contain objects owned by users other than the schema owner. This can happen only if the schema owner grants the CREATE privilege on their schema to someone else, or a superuser chooses to create objects in it.","n":0.046}}},{"i":374,"$":{"0":{"v":"Roles","n":1},"1":{"v":"\n# Role\n- A role is an entity that can own database objects and have database privileges\n\t- can be considered a \"user\", a \"group\", or both depending on how it is used\n\t\t- in other words, a user role can be part of a group role\n\t- must have CREATEROLE privilege or be a database superuser to create roles.\n- When a user logs in, we want them to make their queries using a specific role\n- any time we are running `CREATE USER` or `CREATE GROUP`, we are running `CREATE ROLE` under the hood.\n\t- minor difference: `CREATE USER` also logs the user in, so a `role` having the LOGIN attribute can be thought of as a user \n\t\t- the ability to log in just means the role can be input along with a password as part of the connection string: `postgres://admin@localhost/mydb`\n\t- In the past, there were users and groups. Now, there are just roles. Roles have the ability to log in, have the ability to inherit from other roles, \n\t\t- basically, we moved from the Unix paradigm of users and groups, to the OOP paradigm of having inheritance.\n\t\nAttributes\n- attributes define privileges and info for a role \n\t- ex. login, perform superuser, database creation, role creation, password, etc\n\t\t- by default, roles don't get the ability to log in\n- the attributes are listed after the WITH clause, though that word is optional \n```\nCREATE ROLE kyletycholiz WITH\nLOGIN\n...\n```\n\nDefault roles\n- the `postgres` user is automatically created when we install Postgres. It is a superuser that we log into postgres with\n\t- all server processes work on behalf of this user\n\t- all database files belong to this user\n- roles that start with with pg_ are system roles.\n\n- roles are defined at the database cluster level, and so are valid in all databases in the cluster.\ncluster → database → schema → table\n\n## Grant\n- When an object is created, it is assigned an owner. \n\t- The owner is normally the role that executed the creation statement.\n\t- The right to modify or destroy an object is always the privilege of the owner only\n- When we use grant to grant access to a role, the now-empowered role can do 2 things: they can do everything that the source role could do, or they can actually control and become the role that it gained its powers from (this would mean that if an `admin` role became `user_login` role, it would no longer be able to perform any action that have admin-only privileges) \n\nGRANT has 2 variants:\n1. grant privileges on a database object\n\t- inc. table, column, view, sequence, database, foreign-data wrapper, foreign server, function, procedural language, schema, or tablespace\n2. grant membership in a role\n- below, we define a role `user_admin`, then give the role `postgres` the ability to do anything that `user_admin` can do.\n\t- from another viewpoint, `postgres` is gaining the power to do everything that `user_admin` can do.\n```\nCREATE ROLE user_admin;\nGRANT user_admin to postgres;\n```\n- GRANTing on a database doesn't grant rights to the schema within. Similarly, GRANTing on a schema doesnt grant rights to tables within.\n\t- ex. if we run `grant usage on schema app_public to user_guest`, we grant the role `user_guest` the right to know that the schema exists, but it doesn't give it the right to interact with the tables within.\n\t\t- if we want to grant read rights to a table, then we need to run `grant select on table <TABLENAME> to user_guest`\n\t- If you have rights to SELECT from a table, but not the right to see it in the schema that contains it, then you can't access the table\n\t\t- Therefore, the user must first be granted rights to the schema, otherwise the rights on the table are useless. \n\t\t- Likewise, if the user had been granted rights to just the schema, but not the tables within, they would still be locked out. \n\t\t\t- It's like a directory tree. If you create a directory somedir with file somefile within it then set it so that only your own user can access the directory or the file (mode rwx------ on the dir, mode rw------- on the file) then nobody else can list the directory to see that the file exists. If you were to grant world-read rights on the file (mode rw-r--r--) but not change the directory permissions it'd make no difference. Nobody could see the file in order to read it, because they don't have the rights to list the directory. If you instead set rwx-r-xr-x on the directory, setting it so people can list and traverse the directory but not changing the file permissions, people could list the file but could not read it because they'd have no access to the file. You need to set both permissions for people to actually be able to view the file. \n- the public schema has a default GRANT of all rights to the role public, which every user/group is a member of. So everyone already has usage on that schema.\n- opposite of GRANT is REVOKE\n*GRANT USAGE* - grants access to objects contained in the specified schema \n\n## Roles & Auth\n- all auth happens through postgres roles and permissions. Postgres is in charge of authenticating requests. \n- Postgres permissions work as a whitelist and not a blacklist (except for functions). \n\t- There are very few defaults, and if we want to blow them away entirely, use `alter default privileges`\n- when an authenticated user makes a request, the role will be changed for that user, having the effect of restricting queries (can be seen in `current_user` variable)\n\t- `current_user` is the username of the current execution context \n- Because we have the concept of default roles, we set a default (such as `user_guest` that every user will get by default. When the user demonstrates a verified JWT token, that role gets changed to one with more authorization rights (such as `user_login`).\n- JWTs can be generated in postgres with `pgjwt` extension\n","n":0.032}}},{"i":375,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n##### Change role\n```sql\nset role postgres;\n```\n","n":0.447}}},{"i":376,"$":{"0":{"v":"Listen/Notify","n":1},"1":{"v":"\n# Overview\nPostgres has a system of asynchronous messages and notifications, implemented by `listen` and `notify` keywords.\n- This means that as soon as a connection is established with PostgreSQL, the server can send messages to the client even when the client is idle.\n\t- This method of communication is also carried out with the `COPY` command.\n\nCommonly, the channel name is the same as the name of some table in the database, and the notify event essentially means, \"I changed this table, take a look at it to see what's new\"\n\n# Notify\n`notify` provides a simple interprocess communication (IPC) mechanism for a collection of processes accessing the same PostgreSQL database.\n\nWhen `notify` is used to signal the occurrence of changes to a particular table, a useful programming technique is to put the `notify` in a rule that is triggered by table updates.\n- In this way, notification happens automatically when the table is changed, and the application programmer cannot accidentally forget to do it.\n\nIn a transaction, `notify` events are not delivered until the surrounding transaction has been committed.\n\n# Listen\nWhen we execute `listen`, we are registering the current postgres session as a listener on the specified notification channel.\n- when the specified channel is `notified` (either by the current session, or another one connected to the same db), all sessions subscribed to that channel will receive the message, and will in turn pass it on to the client whom it is connecting.\n\nThe payload passed to the client includes 3 things:\n1. Notification channel name\n2. Session server's PID\n3. Payload string (`''` if unspecified)\n\nA session's listen registrations are automatically cleared when the session ends.\n\nThe method a client application must use to detect notification events depends on which PostgreSQL API it uses (ie. libpq, libpgtcl)\n\n# `pg_notify(channel_name text, payload text)`\nWe can also send a notification using the `pg_notify` function\n- The function is much easier to use than the `notify` command if you need to work with non-constant channel names and payloads.\n\n# Example\n```\npsql# listen my_notification_channel;\nLISTEN\n\npsql# notify my_notification_channel, 'foo';\nNOTIFY\nAsynchronous notification \"my_notification_channel\" with payload \"foo\"  ⏎\nreceived from server process with PID 40430.\n```\n\n# UE Resources\n[Dmitiri Fontaine on Listen-Notify](https://tapoueh.org/blog/2018/07/postgresql-listen-notify/)\n","n":0.054}}},{"i":377,"$":{"0":{"v":"Indexes","n":1},"1":{"v":"\nPostgres has multiple types of index: B-tree, Hash, GiST, SP-GiST and GIN\n- B-tree (balanced tree) is default, and is appropriate for most use-cases\n\nWhether to create a single-column index or a multicolumn index, take into consideration the column(s) that you may use very frequently in a query's WHERE clause as filter conditions.\n- ie. add an index on the columns that we typically use WHERE on\n\nIndexing is often thought of as a data modeling activity. When using PostgreSQL, some indexes are necessary to ensure data consistency (the [[C|db.acid.consistent]] in ACID). Constraints such as UNIQUE, PRIMARY KEY or EXCLUDE USING are only possible to implement in PostgreSQL *with* a backing index. When an index is used as an implementation detail to ensure data consistency, then the indexing strategy is indeed a data modeling activity.\n- In all other cases, the indexing strategy is meant to enable methods for faster access methods to data. Those methods are only going to be exercised in the context of running a SQL query\n\nIn the case of UNIQUE, PRIMARY KEY and EXCLUDE USING, the reason why PostgreSQL needs an index is because it allows the system to implement visibility tricks with its MVCC implementation \n- this fact is what prevents these 2 (concurrently ran) transactions (which should conflict on UNIQUE constraint) from being accepted:\n```sql\nt1> insert into test(id) values(1);\nt2> insert into test(id) values(1);\n```\n\n### Indexing comes at a cost\nDetermining *what not* to index is probably more important than determining *what* to index\n- An index duplicates data in a specialized format made to optimise a certain type of searches. When we `COMMIT;`, every change that is made to the main tables of your schema must have made it to the indexes too.\n\t- As a consequence, each index adds write costs to your DML queries: INSERT, UPDATE and DELETE now have to maintain the indexes too, and in a transactional way.\n\nAvoid indexes on...\n- small tables.\n- Tables that have frequent, large batch update or insert operations.\n- columns that contain a high number of NULL values.\n- Columns that are frequently manipulated should not be indexed.\n\n* * *\n\n[[Index Docs|db.strategies.indexing]]\n\n# UE Resources\n- [Indexes under the hood, using B-trees](https://rcoh.me/posts/postgres-indexes-under-the-hood/)\n- [provides a quick-n-dirty test at the bottom of the article, to see impact of indexes on millions of rows](https://www.cybertec-postgresql.com/en/postgresql-now-vs-nowtimestamp-vs-clock_timestamp/)\n","n":0.052}}},{"i":378,"$":{"0":{"v":"Strategy","n":1},"1":{"v":"\nYou can begin your indexing needs analysis by listing every query that averages out to more than 10 milliseconds, or some other sensible threshold for your application. The only way to understand where time is spent in a query is by using the EXPLAIN command and reviewing the query plan\n","n":0.141}}},{"i":379,"$":{"0":{"v":"Partial Index","n":0.707},"1":{"v":"\nA partial index is an index built over a subset of a table, and therefore only contains entries for that subset.\n- The subset is determined by a conditional expression given by us (called the predicate).\n\n## Purpose\n- To avoid indexing common values\n    - Since a query searching for a common value (ie. one that occurs for more than a few percent of all table rows) will not use the index anyway, there is no point in keeping those rows in the index at all. Doing this reduces the size of the index, increasing the speed of queries that do ue the index.\n","n":0.1}}},{"i":380,"$":{"0":{"v":"Grant","n":1},"1":{"v":"\nThe GRANT command has two basic variants: one that grants privileges on a database objects, and one that grants membership in a role.\n\nWhen you create a new database, any role is allowed to create objects in the public schema. To remove this possibility, you may issue immediately after the database creation:\n\n```sql\nREVOKE ALL ON schema public FROM public;\n```\nafter the above command, only a superuser may create new objects inside the public schema, which is not practical. Assuming a non-superuser foo_user should be granted this privilege, this should be done with:\n\n```sql\nGRANT ALL ON schema public TO foo_user;\n```\n","n":0.103}}},{"i":381,"$":{"0":{"v":"Func","n":1}}},{"i":382,"$":{"0":{"v":"Window","n":1},"1":{"v":"\nA window function performs a calculation across a set of table rows. Each row is somehow related to the current row.\n- Window Functions differ from aggregate functions in that they do not cause rows to become grouped into a single output row. Instead, the rows retain their separate identities.\n\t- Behind the scenes, the window function is able to access more than just the current row of the query result.\n- ex. Imagine we want a result set of all employees, and have their individual salaries compared against the average of their individual departments:\n```\nSELECT \n\tdepname, \n\tempno, \n\tsalary, \n\tavg(salary) \nOVER (PARTITION BY depname) \nFROM empsalary;\n```\nwould result in:\n```\n  depname  | empno | salary |          avg          \n-----------+-------+--------+-----------------------\n develop   |    11 |   5200 | 5020.0000000000000000\n develop   |     7 |   4200 | 5020.0000000000000000\n develop   |     9 |   4500 | 5020.0000000000000000\n develop   |     8 |   6000 | 5020.0000000000000000\n develop   |    10 |   5200 | 5020.0000000000000000\n personnel |     5 |   3500 | 3700.0000000000000000\n personnel |     2 |   3900 | 3700.0000000000000000\n sales     |     3 |   4800 | 4866.6666666666666667\n sales     |     1 |   5000 | 4866.6666666666666667\n sales     |     4 |   4800 | 4866.6666666666666667\n(10 rows)\n```\n\n### Over\nThe OVER clause determines exactly how the rows of the query are split up for processing by the window function\n- the OVER clause is what causes it to be treated as a window function, and therefore computed across the window frame.\n- A window function call always contains an OVER clause directly following the window function's name and argument(s). ","n":0.064}}},{"i":383,"$":{"0":{"v":"Custom Functions","n":0.707},"1":{"v":"\nA function is a reusable block of SQL code that returns a scalar value of a set of rows.\n- Functions are transactional by nature. If there is an error somewhere in the function, then the function will be rolled back.\n\t- If a function is called within a transaction block, and the executing code does not reach the concluding `COMMIT`, then all code executed within the function will roll back as well.\n\t- Any `BEGIN...EXCEPT` blocks within the function operate like savepoints like the `SAVEPOINT` and `ROLLBACK TO <SAVEPOINT>` SQL statements.\n- SQL functions execute an arbitrary list of SQL statements, returning the result of the last query in the list.\n\t- In the simple (non-set) case, the first row of the last query's result will be returned\n\t- Alternatively, an SQL function can be declared to return a set, which allows *all* rows of the last query's result to be returned\n- allows us to write functions that can interact on the database. For instance, we can create a function that combines `first_name` and `last_name` to give us `fullName`\n- Since PostgreSQL permits function overloading, the function name alone does not uniquely identify the function to be called;\n\t- the parser must select the rigt function based on the data types of the supplied arguments.\n- Functions and operators in PostgreSQL support polymorphism and almost every part of the system can be extended.\n- By default, functions can be executable by public\n- anonymous IIFEs can be invoked anywhere in the SQL by escaping the function identifiers:\n```sql\nDO \\$\\$\nBEGIN\n  EXECUTE 'GRANT appname TO ' || user;\n  EXECUTE 'GRANT appname_authenticator TO ' || user;\nEND;\n\\$\\$ LANGUAGE PLPGSQL;\n```\n\n## Using functions in queries\n- say we have a function that returns a composite type:\n```sql\nCREATE FUNCTION new_emp() RETURNS emp AS $$\n    SELECT ROW('None', 1000, 25, 'yoyo')::emp;\n$$ LANGUAGE SQL;\n```\n- here we are returning a single column that is a composite type of the signature: string, int, int, string. We also coerce it to the composite type related to the whole `emp` table. This means that the `emp` table has all 4 of those rows, and the act of us coercing means the output of the function can be used anywhere that an explicit `emp` type is required.\n- we could make a SELECT statement to get back a one-column table by using `SELECT new_emp()`\n- since the composite type is also a sort of virtual table, we can use the function as a \"table function\": `SELECT * FROM new_emp();`, which will return 4 columns.\n\n## Syntax\n- use `$$` to open and close the function:\n- stable means that this function does not mutate the database\n\nTerms\n- `setof` - Sets emulate rows of tables.\n\t- `returns setof` and `returns table(column)` are equivalent\n\t- When an SQL function is declared as returning SETOF sometype, the function's final query is executed to completion, and each row it outputs is returned as an element of the result set.\n\t\t- This feature is normally used when calling the function in the FROM clause, since everything after FROM would get interpreted as if it were a base table. In other words, we can add some columns to a base table, and query it as if those added rows were permanent\n\t\t\t- ex. imagine having a function that determined if your salary was above $100,000. we can use a function to get the boolean result and attach it to the base table, so that we can use `FROM employee, getAboveHundred(emp1)\n```sql\nCREATE FUNCTION add(a int, b int) returns int as $$\n select a + b\n$$ language sql stable;\n```\n\n# PL/PGSQL\n### Declare\n- Declare a variable of specified type\n\n### Function Declarations\n#### Strict\n- this means that if the function receives null input, then the function won't be called and the output will automatically be null as well.\n\t- ex. imagine we have a `register_user` function, that takes name, email and password as inputs. If the function does not receive `name`, we want it to fail.\n\n#### Security Definer/Invoker\n- Security Definer - \"the privileges of the function use the privileges (security clearance) of the definer\"\n- Security invoker - \"use privileges (security clearance) of the user who invoked the function\n- this means that the function is created with the privileges of the pg role that created it.\n- This is a way to heighten the rights of the function, and can be thought of similar to sudo, instead of rights being elevated to superuser, rights are only elevated to the creator of the function itself\n\t- ie. if we run our migrations with user `postgres`, then using `security definer` will give our function the rights of `postgres`. This is helpful if we want to insert into a table that is part of a very strict schema, such as `app_private`\n- we can use `security definer` to by pass RLS\n\n### UE Resource\n[insert returning into variable](https://www.xspdf.com/resolution/50004699.html)\n","n":0.036}}},{"i":384,"$":{"0":{"v":"Signature","n":1},"1":{"v":"\n# Return value\n- functions can return base types (string, int) and composite types (collection of columns), or sets of base types and composite types\n- we can return any type from a function. Since the whole row of a table is by definition a composite type, we can specify `returns nugget`. This will specify that the function must return all columns in the `nugget` table.\n\t- If we don't want to return all columns, we can always use the `ROW` construct to specify which columns we want to include in the row signature\n\t- The select list order in the query must be exactly the same as that in which the columns appear in the table associated with the composite type.\n\t- You must typecast the expressions to match the definition of the composite type\n- If the function is defined to return a base table, the table function produces a one-column table (with the column named after the function. If the function is defined to return a composite type, the table function produces a column for each attribute of the composite type.\n\t- using `setof` will return multiple columns\n- the type that follows `returns` has to match up with whatever is SELECTed during the query.\n\t- ex. if we declare `returns bucket`, then we'd better be using `SELECT * FROM bucket`\n\t\t- since in this example we are returning a base table, a one-column table is the result.\n\n### Returning a Set\n- we can also use `RETURNS TABLE(columns)` syntax to return a set\n\t- equivalent to using one or more `OUT` parameters plus marking the function as returning SETOF record (or SETOF a single output parameter's type, as appropriate)\n\t- It is not allowed to use explicit OUT or INOUT parameters with the RETURNS TABLE notation — you must put all the output columns in the TABLE list.\n\n### Output Params\n- an alternate way to define a function's signature (inputs and outputs)\n- The value of output parameters is that they provide a convenient way of defining functions that return several columns, which is why functions can have multiple outputs\n```\nCREATE FUNCTION sum_and_product (IN x int, IN y int, OUT sum int, OUT product int)\n```\n- What has essentially happened here is that we have created an anonymous composite type for the result of the function\n\t- if we wanted to be more explicit, we could have declared a composite type `sum_prod`, made up of the `sum` column and the `product` column, and declared that the function `returns sum_prod`.\n\n## Set returning function\n- *set returning functions* are functions that possibly return more than one row\n\t- currently, `series generating functions` are the only type of `set returning functions`\n- `generate_series(<start>, <stop>, <step>)`\n\t- because this function returns a result set, we can use the function after FROM\n- ex. imagine running:\n```\ngenerate_series(date :'start',\n\t\t\t    date :'start' + interval '1 month'\n\t\t\t\t\t\t\t  - interval '1 day',\n\t\t\t\tinterval '1 day'\n) as calendar(entry)\n```\nwhich would return a set of dates, starting from `start` (a variable we defined earlier), and increasing by intervals of 1 day.\n\t- this would be useful if we have a set of data by year, and have some years where there is no data. Instead of skipping those rows, we might want to display zeros instead. When we join this result set with our data, joining on the `date` column will result in `null` for the missing years. Using `coalesce`, we can default all nulls to zero.\n\n# Params\n## Named params\n2 ways:\n\n```sql\nSELECT * FROM app_private.really_create_user(\n\tusername := $1\n)\n```\n\nor:\n```sql\nselect app_private.really_acquire_book(\n\tpayment_intent_id => $1\n)\n```","n":0.042}}},{"i":385,"$":{"0":{"v":"Builtin","n":1}}},{"i":386,"$":{"0":{"v":"Rank","n":1},"1":{"v":"\n`RANK()` lets us assign a rank to every row according to a partition we define with PARTITION BY\n- ex. imagine we had a result set of every person in a company along with their salaries. We want to rank the top earning employees by position (eg. top 3 salespeople; top 3 programmers etc). To get this result set, we can RANK, partitioning over the position_title\n```sql\nselect \n    salary\n    rank() over (partition by position_title order by salary) from employees\n```\n\nThe `RANK()` function can be useful for creating top-N and bottom-N reports","n":0.107}}},{"i":387,"$":{"0":{"v":"Lag","n":1},"1":{"v":"\n`lag` allows us to access access rows that appeared before the current row that we are accessing. We give `lag` an offset and we get back the row that appeared `x` spots before the current one.\n- Naturally, this makes `lag` useful in being able to compare the values of the current and previous rows.\n\nThe following gives us the previous row.\n```sql\nlag(invoice, 1)\n```","n":0.128}}},{"i":388,"$":{"0":{"v":"Coalesce","n":1},"1":{"v":"\n# Coalesce\n- `coalesce` accepts unlimited params, and always returns the first non-null value\n\t- In this way, you can think of the final param as the default value.\n- Embracing `null` allows us to easily inline defaults\n\t- You can chain a bunch of inputs and it returns the first non-null value\n\t- ex. imagine having a result set that shows us how much revenue was earned each year since 2010. We should define a column `coalesce(income, 0) as income` in our select statement, which will return us a 0 if there is no income for the relevant year. This is a nicer alternative than to just omitting rows without data.\n\t- ex. Lets say you have an input parameter with a default, you can the just wrap the usages of that parameters like this: coalesce (_parameter, default_value) = whatever it tests against\n\t- ex. Imagine we have a postgres function that accepts 2 args: a `start_date` and an `end_date`, and returns the amount of time that an employee worked at a company. If they currently work there still, the client passes `null` for the value of the `end_date`. In our function, we would have a smart failover that defaults the null value to `now()`.\n\t- ex. imagine we have a column `discount` that defaults to `null`. We want to use this `discount` column when determining subtotal, so we need to use coalesce to provide us with a default value of 0:\n\t```sql\n\tSELECT\n\t\tproduct,\n\t\t(price - COALESCE(discount,0)) AS net_price\n\t```\n","n":0.065}}},{"i":389,"$":{"0":{"v":"Agg","n":1},"1":{"v":"\n# Aggregate Functions\n- called aggregate functions because they are functions that act on multiple rows and give us a single output\n\t- ex. sum, avg, max, min\n- used to extract information about whole groups of rows, and allow us to easily ask questions like:\n1. What's the most expensive facility to maintain on a monthly basis?\n```sql\nselect max(monthlymaintenance)\nfrom cd.facilities;\n```\n2. Who has recommended the most new members?\n3. How much time has each member spent at our facilities?\n4. what is the most popular type of shoe?\n- Aggregate functions only work in SELECT or HAVING clauses\nTherefore, an aggregate function will take a group of data, perform some function on it, and have a single output that it will print in our result set. An aggregate function can perform transformation on the data, or it can also perform a calculation on it. In either case, the aggregate function is making a consideration of all rows of data, and deciding what to do with it that would result in a single piece of data (int, array etc) that will hold the results of that calculation.\n- This is why when we perform aggregate functions, we need to use ORDER BY, because we need to tell the query what we want to apply the aggregate function to.\n\t- Since the aggregate function results in an aggregate set of data *based on something*, we need to know what that \"based on\" actually is. Is the data getting combined *based on* age? Is it getting combined *based on* nugget (ie. combining buckets from multiple rows in an effort to make a json array)?\n- When you hear \"aggregate function\", you should think of taking data from multiple rows and combining (likely integers) or transforming (likely objects/arrays) it in some way\n\n[some useful examples](hashrocket.com/blog/posts/faster-json-generation-with-postgresql)\n\n# Pitfalls\n- because of the order of execution, aggregate functions cannot be used in the WHERE clause (see queries/WHERE)\n\t- This restriction exists because the WHERE clause determines which rows will be included in the aggregate calculation; so obviously it has to be evaluated before aggregate functions are computed\n","n":0.055}}},{"i":390,"$":{"0":{"v":"Json","n":1},"1":{"v":"\n### row_to_json\nturns the table into json\n- each row becomes a json object, and the cumulative of these rows makes up the json array\n- If the query `select * from nugget` gives us the data, then the whole below query gives us that same data as json\n```\nselect row_to_json(nugget)\nfrom (select * from \"nugget\") as nugget\n```\n### array_agg\nIt aggregates its argument into a PostgreSQL array\n- accepts a set of values and returns an array. Each arg becomes an element in the array\n\t- we can determine the order of the array with ORDER BY\n- ex. Imagine we had 2 many:many tables (`actors` and `movies`) connected through a junction table `movies_actors`.\n\t- We are interested in viewing which actors were in which movie.\n\t- Assuming that the `movies` table is the table we are joining from, if we were to query the movies, we'd get duplicate rows with the same movie and actor names (since each row is a combination of the two, effectively having the same amount of rows as the junction table).\n![d26ee15403ed820f7c410d859e7c5e56.png](:/20ede76c8a314c528f1b4716c372db0d)\n\t- What we want instead is to show a different movie in each row, and consolidate the actors appearing in that movie into an array\n![939e260f1cc0aab3f7f7e6477164e5ac.png](:/9d7a1c247b064dc285cd0597a477d28a)\n\n### json_agg\n- similar to `array_agg`, except that it puts the elements into a json array\n- we could use `json_agg(json_build_object())` if we wanted to build an array of objects\n\nBefore\n```sql\nSelect\n    title,\n    first_name || ' ' || last_name as name\nfrom film\njoin film_actor using (film_id)\njoin actor using (actor_id)\ngroup by title, first_name, last_name\norder by title;\n```\n\nAfter\n```sql\nSELECT\n     title,\n     ARRAY_AGG (\n\t     first_name || ' ' || last_name\n\t     ORDER BY first_name\n     ) actors\n FROM\n     film\n INNER JOIN film_actor USING (film_id)\n INNER JOIN actor USING (actor_id)\n GROUP BY\n     title\n ORDER BY\n     title;\n```\n\n### json_build_object\nAllows us to provide key-value pairs, thereby returning us an object\n- if we want to provide an array as the value of a key (ex. `buckets` property of a `nuggets` object), we can provide a SELECT statement in place of the value\n```sql\nSELECT\n    json_build_object(\n        'id',id,\n        'name',name,\n        'comments',(\n            SELECT json_agg(json_build_object(\n                'id', comments.id,\n                'mood', comments.mood,\n                'subject', comments.subject,\n                'content', comments.content,\n                'created_at', comments.created_at\n            ))\n            FROM user_comment_map JOIN comments ON comment_id=comments.id\n            WHERE user_id=users.id\n        )\n    )\n```\nthis works because\n\n### json_strip_nulls\n- lets us exclude fields where the value is null\n```sql\nSELECT json_strip_nulls(json_build_object('name', p.name, 'birthday', p.birthday))\nFROM person p;\n```\n\n### json_populate_record\n- allows us to give postgres a string of json, which it will then convert into SQL row-format.\n- the first arg we pass to\n\n### jsonb_pretty\nprint json pretty in output\n\n## jsonb manipulation methods\n\n### jsonb_set & jsonb_insert\nWith these methods, it is not possible to add values at the root of the json object.\n\nsignature:\n```\njsonb_set/jsonb_insert(target         jsonb,\n          path           text[],\n          new_value      jsonb,\n          create_missing boolean default true)\n```\n#### jsonb_insert\nAllows us to insert into JSON array (while preserving all of the original values)\n","n":0.048}}},{"i":391,"$":{"0":{"v":"Count","n":1},"1":{"v":"\n#### count(*)\nCounts the number of rows returned by a `select *` statement\n\n#### count(colname)\nCounts the number of rows, but does not consider `null` values of the specified column\n\n#### count(DISTINCT colname)\nCounts the number of rows, but does not consider `null` values of the specified column, and does not consier duplicate values\n- ex. if we are selecting from a `users` table and there is a column `role`, we can `select count(distinct role)...` and we will get a small list of role possibilities\n\nWe often use the COUNT() function with the GROUP BY clause to return the number of items for each group. For example, we can use the COUNT() with the GROUP BY clause to return the number of films in each film category.\n","n":0.091}}},{"i":392,"$":{"0":{"v":"Fdw","n":1},"1":{"v":"\n### Foreign Data Wrappers (FDW)\n[FDW](https://thoughtbot.com/blog/postgres-foreign-data-wrapper)\n","n":0.447}}},{"i":393,"$":{"0":{"v":"Row Level Security (RLS)","n":0.5},"1":{"v":"\nThe main hypothesis is that we should be able to prevent access to specific rows of data based on a policy. That means our application logic only has to worry about `SELECT * FROM my_table` and RLS will handle the `WHERE user_id = my_user_id` part automagically.\n- To put it another way: our queries should only contain the clauses requested by our interfaces and not the filters and conditions demanded by access control in a multi-tenant data store.\n- The current PG role that is accessing the table must have been `grant`ed permission to use it. Otherwise, RLS errors will arise when we try to alter something in the table as that role, because we won't be able to access the table from the outset.\n\na function marked as `SECURITY DEFINER` will bypass RLS\n\n# Row Level Security (RLS)\n- While GRANT is the privilege system of Postgres, Tables can also have Row Security policies that restrict (on a per-user basis) which rows can be interacted with (INSERT, UPDATE, DELETE, SELECT)\n\t- Policies are created using the CREATE POLICY command\n\t- spec: `grant` means \"I am giving *this* user the privilege to use *this* table\". `create policy` means \"I am specifying the requirements of any particular row that must be satisfied before it is accessed by user\"\n- The basis of row level security is to create policies that define how a user interacts with rows within a given table.\n- By default, tables do not have any policies, and RLS must be opted-in for each table.\n\t- When RLS is enabled, all rows are by default not visible to any roles (superusers still have access)\n- If the value in parentheses after USING evaluates to true, then the user gets permission\n- ex. imagine we have a chat app, and we want to ensure a user can only see messages sent by him, and messages intended for him. Also, we want to ensure that users cannot modify the `message_from` column to make it seem that the message is coming from someone else:\n```sql\nCREATE TABLE chat (\n  message_uuid    UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n  message_time    TIMESTAMP NOT NULL DEFAULT now(),\n  message_from    NAME      NOT NULL DEFAULT current_user,\n  message_to      NAME      NOT NULL,\n  message_subject VARCHAR(64) NOT NULL,\n  message_body    TEXT\n);\n\nCREATE POLICY chat_policy ON chat\n  USING ((message_to = current_user)\n  OR (message_from = current_user))\n  WITH CHECK (message_from = current_user)\n```\n- RLS can be implemented using jwt claims to verify that the user is who they say they are, if we do not want to use `current_user`:\n\t- the second arg true means \"return null if the setting is missing\"\n```sql\nCREATE POLICY chat_policy ON chat\n  USING ((message_to = current_setting('request.jwt.claim.email', true))\n  OR (message_from = current_setting('request.jwt.claim.email', true)))\n  WITH CHECK (message_from = current_setting('request.jwt.claim.email', true))\n```\n- When request contains a valid JWT with a role claim (`jwt.claims.role`), we should switch to the role with that name for the duration of the HTTP request\n\n### RLS Policy using external tables\n- What if we want to enable RLS where `user_id = current_user_id()`, but the current table does not keep a `user_id` column?\n- If we are adding an RLS policy to T1, but the policy depends on a `JOIN`able table T2, then T2 must have `grant`ed privileges to the PG role accessing the table.\n\n```sql\ncreate policy t2_policy_update on t2 for update using (\n  exists (\n    select 1\n      from t1\n      inner join t0\n        on t1.t0id = t0.id\n      where t0.u = session_user\n        and t1id = t1.id\n  )\n)\n```\nSubqueries in RLS policies respect the RLS policies of the tables they reference\n\n[source](https://stackoverflow.com/questions/41354818/postgresql-row-level-security-involving-a-view-or-a-select-with-join)\n\n## Per-command Policies\n### UPDATE\n- Since `UPDATE` involves pulling an existing record and replacing it with a new modified record, `UPDATE` policies accept both a `USING` expression and a `WITH CHECK` expression\n  - `USING` determines which records the `UPDATE` command will see to operate against\n  - `WITH CHECK` defines which modified rows are allowed to be stored back into the table.\n    - If the updated value fails the `WITH CHECK` expression, there will be an error.\n    - If only a `USING` clause is specified, then it will be used for both `USING` and `WITH CHECK` cases (ie. `WITH CHECK` is implemented for us implicitly)\n- Typically an `UPDATE` command needs to read data from columns in the relation being updated (e.g. in a `WHERE` clause, or `RETURNING` clause, or right side of a `SET` clause).\n  - In cases such as these, `SELECT` rights are required on the relation being updated, in addition to the `UPDATE` right.\n\n* * *\n\n## Anatomy\n### WITH CHECK vs USING\n- USING is applied before any operation occurs to the table’s rows\n\t- ex. in the case of updating a nugget, one could not update a row that does not have the appropriate user_id in the first place\n\t- must use USING with DELETE commands because a delete changes no rows, and only removes current ones.\n\t- USING implicitly runs a WITH CHECK with the same clause that USING received, meaning that the verification operation runs both before and after the data is inserted.\n- WITH CHECK is run after an operation is applied, so if it fails, the operation will be rejected\n\t- ex. in the case of an insert, Postgres sets all of the columns as specified and then compares against WITH CHECK on the new row\n\t- must use WITH CHECK with INSERT commands because there are no rows to compare against before insertion\n\n### Permissive or Restrictive\nRLS policies can be either permissive or restrictive\n- permissive (default) - in consideration of all RLS policies, only 1 must pass\n- restrictive - in consideration of all RLS policies, all must\n```\ncreate policy select_all on table_name as permissive using (true)\n```\n\n* * *\n\n### Infinite Recursion\nImagine we are making a RLS policy for `select` on a given table. If we then try and `select` that same table within the `using()` function, we will get an infinite recursion as a result.\n\n* * *\n\n### Check if RLS enabled\n```sql\nselect relname, relrowsecurity, relforcerowsecurity\nfrom pg_class\nwhere oid = 'your_table_name_with_schema'::regclass;\n```\nor\n```sql\nselect * from pg_tables where tablename = 'your_table_name_without_schema'\n```\n\n# UE Resources\n[Good information about RLS](https://info.crunchydata.com/blog/a-postgresql-row-level-security-primer-creating-large-policies)\n[RLS using columns from other tables](https://medium.com/@ethanresnick/there-are-a-few-faster-ways-that-i-know-of-to-handle-the-third-case-with-rls-9d22eaa890e5)\n[](https://blog.crunchydata.com/blog/a-postgresql-row-level-security-primer-creating-large-policies)\n","n":0.032}}},{"i":394,"$":{"0":{"v":"Internals","n":1},"1":{"v":"\n# UE Resources\n- [Internals of Postgres](http://www.interdb.jp/pg/)\n","n":0.408}}},{"i":395,"$":{"0":{"v":"Objects","n":1},"1":{"v":"\n# Object-nature of Postgres\n- PostgreSQL is an object-relational database management system (ORDBMS)\n- a postgres database itself is an object. It contains other objects, such as tables, views, functions, and indexes. since a database is just an object, multiple postgres databases can be stored inside a single postgres server.\n- By making everything an object, Postgres is highly extensible, allowing us to build:\n\t- data types\n\t- functions\n\t- aggregate functions\n\t- operators\n\t- index methods\n- A special feature of PostgreSQL is table inheritance, meaning that a table (child table) can inherit from another table (parent table) so when you query data from the child table, the data from the parent table also shows up.\n","n":0.096}}},{"i":396,"$":{"0":{"v":"Extensions","n":1},"1":{"v":"\nWhen installing new PG extensions, we often need to restart the DB. Extensions need to be registered in `shared_preload_libraries`\n\n## PGXS\nWe can compile extensions using a tool called `pgxs`\n- PGXS is a build infrastructure for extensions\n- it allows us to build extensions against an already installed server.\n\n## Misc\n- When trying to install an extension, if we get the error message `cannot find \"postgres.h\"`, this means that to build the extension from source, the postgres header files are needed. There is a package called `postgres-server-dev-<VERSION>`. This will install all header files for us necessary for server development.\n\n## Commands\n- show installed extensions - `\\dx` OR `SELECT * FROM pg_extension`\n- show available extensions - `SELECT * FROM pg_available_extensions`\n\n# Notable Extensions\n### pg_stat_statements\nAllows us to achieve a general system-wide analysis to help us with things like indexing strategy.\n\nIt’s possible to have a list of the most common queries in terms of number of times the query is executed, and the cumulative time it took to execute the query.\n","n":0.079}}},{"i":397,"$":{"0":{"v":"Pgloader","n":1},"1":{"v":"\nPgloader can operate in 2 ways:\n1. Load data from files (ex. CSV)\n2. Migrate the whole db to Postgres (ex. Mysql to Postgres)\n\nEasiest way is to use the docker container:\n```\ndocker run --rm --name pgloader dimitri/pgloader:latest pgloader SOURCE TARGET\n```\n\n# Resources\n[Docs](https://pgloader.readthedocs.io/en/latest/)\n","n":0.162}}},{"i":398,"$":{"0":{"v":"Connection","n":1},"1":{"v":"\n# Connecting to Postgres\n- When a new user connects to the database, postgres will fork a new process.\n\t- After this point the client and the new process communicate directly, without intervention from the original process.\n\t- This shows the dependency on processes rather than threads, giving us more stability at the cost of higher connection start-up costs.\n\t- each connection into Postgres is going to consume about 10 MB of memory from your database\n\t- These start-up costs can be overcome with pooling\n- In postgres, each client connecting to the database has its own process \n\t- connection can be verified by running `select true as \"Connection test\";`\n- *Connection String* - the database url used to connect to the database (`postgres:///...`)\n\t- if we specify the connection string as `postgres:///mydb`, then we are implicitly passing our computer's username and no password, which would be equivalent to `postgres:///kyletycholiz@localhost:5432/mydb` \n\t\t- spec: are these default variables determined by env variables like PGHOST?\n","n":0.081}}},{"i":399,"$":{"0":{"v":"Pools","n":1},"1":{"v":"\n### Why?\n- Connecting a new client to the PostgreSQL server requires a handshake which can take 20-30 milliseconds. During this time passwords are negotiated, SSL may be established, and configuration information is shared with the client & server. Incurring this cost every time we want to execute a query would substantially slow down our application.\n\n- The PostgreSQL server can only handle a limited number of clients at a time. Depending on the available memory of your PostgreSQL server you may even crash the server if you connect an unbounded number of clients. \n\n- PostgreSQL can only process one query at a time on a single connected client in a first-in first-out manner. If your multi-tenant web application is using only a single connected client all queries among all simultaneous requests will be pipelined and executed serially, one after the other. No good!\n\nThe client pool allows you to have a reusable pool of clients you can check out, use, and return. You generally want a limited number of these in your application and usually just 1. Creating an unbounded number of pools defeats the purpose of pooling at all.\n\n### Implementing\nYou must always return the client to the pool if you successfully check it out, regardless of whether or not there was an error with the queries you ran on the client. If you don't check in the client your application will leak them and eventually your pool will be empty forever and all future requests to check out a client from the pool will wait forever.\n\n### Connection Pooling\n- a pool sits between the postgres frontend (ex. postgraphile, postgres-node) and the postgres server. The pooler speaks the postgres language, so understands all incoming queries.\n\t- Therefore, the server sees the requests as coming from the pg pool, and the postgres frontend sees the pool as handling the requests. \n\t\t- Therefore,\n\t\t\t- without a pool, if we had 20 db connections and 8 were idle, all 20 would be eating up postgres resources (memory)\n\t\t\t- with a pool, those 8 idle connections remain in the pool and don't connect to the database, freeing up resources for us. From the database perspective, there are only 12 connections.\n\t- the frontend/backend paradigm here is no different from in web developent. In postgres, we have users who interact with the database. The client generates a query, and that query gets sent off to the postgres backend, where it interprets the request, performs the actions, then returns the data to the frontend. \n- when a client is connected to the pool, we say it is a \"virtual connection\", and if the pooled client is connected to the database, it is a \"physical connection\" \n- 2 main options: `pgpool-II` and `pgBouncer`\n\t- pgBouncer does one job and does it well. pgpool-II is more of a swiss army knife, with load balancing included\n\t\t- `pgpool-II` seems to be more out-dated\n\t- [comparison](https://scalegrid.io/blog/postgresql-connection-pooling-part-4-pgbouncer-vs-pgpool/)\n\n#### pgBouncer\n- When PgBouncer receives a client connection, it first performs authentication on behalf of the PostgreSQL server.\n\t- when a password is provided, one of 2 things happens:\n\t\t1. PgBouncer first checks the userslist.txt file – this file specifies a set of (username, md5 encrypted passwords) tuples. If the username exists in this file, the password is matched against the given value. No connection to PostgreSQL server is made. \n\t\t2. If passthrough authentication is setup, and the user is not found in the userslist.txt file, PgBouncer searches for an auth_query. It connects to PostgreSQL as a predefined user (whose password must be present in the userslist.txt file) and executes the auth-query to find the user’s password and matches it to the provided value.\n- When the client executes an SQL statement:\n\t1. PgBouncer checks for a cached connection\n\t2. if found, it returns the connection to the client. Otherwise, it creates a new connection\n![a891779b4493983dd662f11960423a72.png](:/bde888d39a1c41fa99e619df6db6f56b)\n","n":0.04}}},{"i":400,"$":{"0":{"v":"Clause","n":1},"1":{"v":"\n## SQL Clause Execution Order\n`FROM ➡ WHERE ➡ GROUP BY ➡ HAVING ➡ SELECT ➡ DISTINCT ➡ ORDER BY ➡ LIMIT`\n\nin other words...\n- The system first executes the FROM clause i.e. it creates the data set on which the WHERE and SELECT clauses are to be run. This step includes any joins specified in the FROM clause.\n- Then, it eliminates rows from this data set by using the filters specified in the WHERE clause.\n- It then groups the data set by the columns specified in the GROUP BY clause, and finally runs the SELECT clause. This is why you can never use the aliases specified in the SELECT clause to filter in the WHERE clause - because the aliases haven’t been declared yet.\n","n":0.09}}},{"i":401,"$":{"0":{"v":"Where","n":1},"1":{"v":"\nWHERE determines which rows will be included in the aggregate calculation\n- this show that the WHERE clause must be evaluated before aggregate functions are computed.\n```sql\n-- WRONG\nSELECT city FROM weather WHERE temp_lo = max(temp_lo);\n\n-- CORRECT\nSELECT city FROM weather\n    WHERE temp_lo = (SELECT max(temp_lo) FROM weather);\n```\n![](/assets/images/2021-03-09-16-30-49.png)\n- In contrast with HAVING, WHERE selects input rows before groups and aggregates are computed\n\t- thus, it controls which rows go into the aggregate computation\n\n- We usually try to keep the where clauses as simple as possible for PostgreSQL in order to be able to use our indexes to solve the data filtering expressions of our queries.\n- the OR operator is more complex to optimize for, in particular with respect to indexes.\n","n":0.093}}},{"i":402,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Delete from one table while using values from another as the condition\nHere, we delete from the `users` table where the `user.id` can be found in the result set. We are doubling up the result set with `UNION`, in an effort to find users that are test users\n\n```sql\ndelete from app_public.users\nwhere id in\n\t(\n\t\tselect user_id from app_public.user_emails where email like 'testuser%@example.com'\n\tunion\n\t\tselect user_id from app_public.user_authentications where identifier = '123456%'\n\t)\n```","n":0.123}}},{"i":403,"$":{"0":{"v":"Select","n":1},"1":{"v":"\n### SELECT INTO\n- SELECT INTO creates a new table and fills it with data computed by a query.\n- The data is not returned to the client, as it is with a normal SELECT.\n- The new table's columns have the names and data types associated with the output columns of the SELECT.\n\n#### with `record` type\n```sql\ndo\n$$\ndeclare\n\trec record;\nbegin\n\t-- select the film\n\tselect film_id, title, length\n\tinto rec\n\tfrom film\n\twhere film_id = 200;\n\n\traise notice '% % %', rec.film_id, rec.title, rec.length;\n\nend;\n$$\nlanguage plpgsql;\n```\n","n":0.116}}},{"i":404,"$":{"0":{"v":"Returning","n":1},"1":{"v":"\ntakes in same arguments as the SELECT statement.\n- however, cannot execute aggregate functions like SELECT statements can.\n","n":0.243}}},{"i":405,"$":{"0":{"v":"On Conflict","n":0.707},"1":{"v":"\nThe newly added ON CONFLICT clause allows to specify an alternative to\nraising a unique or exclusion constraint violation error when inserting.\nON CONFLICT refers to constraints that can either be specified using a\ninference clause (by specifying the columns of a unique constraint) or\nby naming a unique or exclusion constraint.  DO NOTHING avoids the\nconstraint violation, without touching the pre-existing row.\n","n":0.131}}},{"i":406,"$":{"0":{"v":"Like","n":1},"1":{"v":"\n# LIKE\n- use if you want to match a string against your query\n\t- ex. return all emails that have *@gmail* in them\n\t`...WHERE email LIKE '%gmail.com'`\n","n":0.2}}},{"i":407,"$":{"0":{"v":"Insert","n":1},"1":{"v":"\nThere are generally three methods in PostgreSQL with which you can fill a table with data:\n1. Use the `INSERT INTO` command with a grouped set of data to insert new values.\n2. Use the `INSERT INTO` command in conjunction with a `SELECT` statement to insert existing values from another table.\n3. Use the `COPY` (or `\\copy`) command to insert values from a system file.\n\n### RETURNING\n- allows us to get back the columns that were successfully inserted into the table\n\t- ex. INSERT INTO\n- Using this method, we can get back default values (like id or timestamps)\n- This method is also useful to insert rows into a variable, which we can then simply return from the whole function\n```sql\ndeclare\n  v_person forum_example.person;\nbegin\n\tinsert into forum_example.person (first_name, last_name) values\n\t\t(first_name, last_name)\n\t\treturning * into v_person;\n```\n","n":0.089}}},{"i":408,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### Inserting a composite type (a whole row at once)\n```sql\ninsert into stats.daily_registration_historical (registration_date, count_registrations)\nselect * from app_private.daily_registration;\n```\n\n##### Insert all default values\n```sql\ninsert into users default values\n```\n\n##### Insert multiple rows\n```sql\ninsert into countries (code) values ('CA'), ('US'), ('FR')\n```\n\n##### Bulk insert\nImagine you need to split a table in two, and move half of the rows from the first to the second:\n```sql\ninsert into second_table (col1, col2) select a, b from first_table;\n```","n":0.123}}},{"i":409,"$":{"0":{"v":"Having","n":1},"1":{"v":"\nThe WHERE clause allows you to filter rows based on a specified condition. However, the HAVING clause allows you to filter groups of rows according to a specified condition.\n- In other words, the WHERE clause is applied to rows while the HAVING clause is applied to groups of rows\n- The WHERE clause applies the condition to individual rows before the rows are summarized into groups by the GROUP BY clause. However, the HAVING clause applies the condition to the groups after the rows are grouped into groups.\nex. from a People table, we can SELECT income, and \"filter\" in the rows that have income > 1000000: `... HAVING income > 1000000;`\nIn contrast with WHERE, HAVING selects group rows after groups and aggregates are computed. Thus, the WHERE clause must not contain aggregate functions; it makes no sense to try to use an aggregate to determine which rows will be inputs to the aggregates.\n- The HAVING clause almost always contains aggregate functions.\n","n":0.079}}},{"i":410,"$":{"0":{"v":"Group by","n":0.707},"1":{"v":"\n# GROUP BY\nGROUP BY allows us to divide rows returned from SELECT into groups. It is only applicable to aggregate functions\n- GROUP BY groups rows sharing a property so that an aggregate function can be applied to each group\n\t- for each group, we can apply an aggregate function, ex `SUM()`\n\nGROUP BY allows implementing much the same thing than `map`/`reduce` do in Javascript\n- ie. map your data into different groups, and in each group reduce the data set to a single value.\n\nGROUP BY produces a new table reference with only as many columns as are listed after the GROUP BY clause\n- Note that other columns may still be available as arguments of aggregate functions\n- ex. `GROUP BY \"nugget\".id, \"nugget\".title` will reduce the number of available columns in all subsequent logical clauses (inc. SELECT) to 2.\n\t- This is the syntactical reason why you can only reference columns from the GROUP BY clause in the SELECT clause.\n\nIf we simply GROUP BY an id and don't apply any aggregate function, then the functionality will be the same as DISTINCT (remove duplicate rows)\n\t- this is why GROUP BY only really becomes valuable once we add an aggregate function\n\n#### Multiple columns in GROUP BY\n`GROUP BY X` means put all those with the same value for X in the one group\n`GROUP BY X, Y` means put all those with the same values for both X and Y in the one group.\n\n## Examples\n### Average salary by position\n- ex. imagine we have an `employee` table and we are interested in average salaries. Naturally, we have different employee positions, so if we ran the aggregate function AVG, we would get back an average of every salary:\n```sql\nSELECT avg(salary)\nFROM \"employee\"\n```\nsince the average of all salaries is not that interesting, we want to get the average of salaries GROUPed BY position\n```sql\nSELECT position, avg(salary)\nFROM \"employee\"\nGROUP BY position\n```\n- note: if we didn't have the GROUP BY clause here, we would get an error\n\t- here, GROUP BY basically says \"let's see the aggregate data *per* position\"\n- Sometimes we may see syntax such as `GROUP BY 2`. Here, 2 refers to the second column, and would be equivalent to writing out the actual column name.\n","n":0.053}}},{"i":411,"$":{"0":{"v":"Delete","n":1}}},{"i":412,"$":{"0":{"v":"Delete","n":1},"1":{"v":"\n#### Delete using join\n`DELETE JOIN` isn't supported, but we can imitate the functionality:\n```sql\nDELETE FROM table_name1\nUSING table_expression -- the table(s) we are \"joining\"\nWHERE condition\nRETURNING returning_columns;\n```","n":0.204}}},{"i":413,"$":{"0":{"v":"Cascade","n":1},"1":{"v":"\n# CASCADE\n- whenever rows in the parent (referenced) table are deleted/updated, the respective rows in the child (referencing) table with matching FK will be deleted/updated as well\n\t- put another way, not only do we delete an object (ex. a schema), but we delete everything contained within that object (ex. tables)\n- ex. if we mark the `nugget_id` of `nugget_bucket` table as CASCADE, when a row in the `nugget` table is deleted, all rows in `nugget_bucket` table that reference that nugget will also be deleted\n","n":0.11}}},{"i":414,"$":{"0":{"v":"As","n":1},"1":{"v":"\n# AS\n- alias a column name as you are selecting it\n\t- ex. `SELECT title AS bucket_title...`\n- alias a tablename\n    - ex. `SELECT ___ from users as u`\n\n- the alias only exists during execution of the query\n- if you use column aliases in the SELECT clause, you cannot use them in the WHERE clause.\n","n":0.137}}},{"i":415,"$":{"0":{"v":"Alter","n":1}}},{"i":416,"$":{"0":{"v":"Alter","n":1},"1":{"v":"\n### Alter column to have unique constraint\n`ALTER TABLE foo ADD UNIQUE (column_name);`\n\n### add/drop column constraint\n```sql\nalter table invoice_items alter column invoice_id set not null;\nalter table invoice_items alter column invoice_id drop not null;\n```\n\n### Add non-nullable column to a table\nThis can be tricky, since running\n```sql\nalter table invoice_items add column payment_id uuid not null ...\n```\n\nwill result in an error, since we are trying to modify existing rows in a table to have a `null` value for the new column\n\nInstead, we have a bit of a workaround, setting some junk default value to get around the constraint.\n```sql\nalter table mytable add column mycolumn character varying(50) not null default 'foo';\n-- ... some work (set real values as you want)...\nalter table mytable alter column mycolumn drop default;\n```\n\nIn the case where we are adding a non-nullable FK, we can take a bit of a different approach:\n```sql\nalter table invoice_items add column if not exists payment_id uuid references payments;\n-- ... some work (set real values as you want)...\nalter table invoice_items alter column payment_id set not null;\n```\n\n#### Alternative approach: check constraint\n[[Differences between a column NOT NULL constraint and a CHECK CONSTRAINT not null|dendron://code/sql.constraint#differences-between-a-column-not-null-constraint-and-a-check-constraint-not-null]]\n\nThe trick here is that you can issue a `NOT VALID` option when adding a check constraint. This will tell PostgreSQL that you are aware that the constraint may not be valid on existing data and that it does not have to check it. Subsequent inserts or updates, however, will be enforced.\n```sql\nalter table invoice_items add constraint payment_id_not_null check (payment_id is not null) not valid;\n```\nAfter you have done your operations, then we can `validate` the sql:\n```sql\nalter table invoice_items validate constraint payment_id_not_null;\n```\nThis will scan the table and ensure that the constraints are all passing","n":0.061}}},{"i":417,"$":{"0":{"v":"Admin","n":1},"1":{"v":"\n# Overview\n- When you install an instance of Postgres (ex. in Node), there will be a corresponding Postgres server, since postgres uses a client/server model.\n\t- if we use different ports, we can have multiple Postgres servers on a single physical server\n\n## Processes\n### Postgres Server\n- the server program is called `postgres`\n- purposes:\n\t- manages database files\n\t- accepts connections from clients\n\t- performs operations on behalf of clients\n\n### Postgres Client\n- the client is the application that wants to perform database operations\n- clients are diverse in nature, and could be a CLI, GUI, or a library within a web server (like `pg` in node.js)\n\n- Postgres' value is about concurrency and isolation\n\t- The idea: what happens when 2+ people are trying to do the same thing concurrently? how is that handled?\n\t\t- This value is provided by the concept of ACID\n\t\t- The fact that we have transactions in postgres means that we are ACID-compliant.\n","n":0.082}}},{"i":418,"$":{"0":{"v":"Users","n":1},"1":{"v":"\n# Database Users\n- each database cluster has a set of database users, which are distinct from the users that the OS of the server manages.\n- database users are global across a cluster installation.\n- users own database objects, such as tables. These owners can assign priveleges on those objects to other users.\n- freshly initialized postgres systems will always contain a predefined user with ID 1, which has the same name as the OS user that initialized the db cluster. However, it is often a best practice to name this user `postgres` instead.\n\t- this is a superuser\n\t- To create more users, we need to connect as this user first.\n","n":0.097}}},{"i":419,"$":{"0":{"v":"Settings","n":1},"1":{"v":"\n# System Administration\n- there are 2 functions (getter/setter) that allow us to query and alter run-time configuration parameters:\n\t1. `current_setting` - returns the current `value` of the setting associated with the provided `key`\n\t2. `set_config` - pass the setting name and the new value.\n\t- these functions are executed with a SELECT statement. The result is displayed as a table.\n","n":0.132}}},{"i":420,"$":{"0":{"v":"Search Path","n":0.707},"1":{"v":"\n# Search path\n- Implicitly, every object in postgres gets operated on as a path. running `...into nuggets`, postgres runs under the hood `...into neverforget.public.nuggets`.\n- We can run `SHOW search_path;` to get the path\n\t- the first element of the output `$user` indicates that a schema with the same name as current user (ex. a schema called `kyletycholiz`) should be searched first\n\t- the first schema listed (default `public`) is the default location for creating new objects \n\t\t- when objects are referenced without qualifying a schema (ex. `...into nuggets`), the search_path is traversed, starting with public, and onto the next one (by default only 1, so we need to specify more) \n- to modify the `search_path`, we can run `SET search_path TO myschema,public;`, which will inform postgres that `myschema` should be searched first when there is no specified schema.\n","n":0.086}}},{"i":421,"$":{"0":{"v":"Dir","n":1},"1":{"v":"\n# Database File\n- known by the environment variable PGDATA, this is where all the data for a database cluster is stored.\n\t- likely `/etc/postgresql/<VERSION>/main/`\n\n### base/\n- each directory within `base/` is used by a database node in our cluster. Each directory is named after the database's OID\n\t- the files inside are the actual data of the relations (ie. tables, indexes, sequences)\n- database examples: `postgres`, `template0`, `template1`, `neverforget`, which can be seen from psql with `\\l`\n\t- the `postgres` database is not required to exist. It is simply a default that is connected to if no database is specified in the connection string.\n\t- `CREATE DATABASE` works by copying the `template1` database (`template0` can be thought of as a more primitive and stripped down version of `template1`\n\n### pg_hba.conf\n- a file that controls the authentication of clients to connect to the postgres database\n\t- HBA stands for host-based authentication.\n- after making changes to this file, to put them into effect run `SELECT pg_reload_conf();` or `pg_ctrl reload` with superuser.\n- In `psql`, run `SHOW hba_file;`\n","n":0.078}}},{"i":422,"$":{"0":{"v":"Auth","n":1},"1":{"v":"\n# Authentication\n- to connect to a PG databse, there are a few different ways to have users authenticate themselves\n- the default authentication method can be found in the `pg_hba.conf` file.\n\t- therefore, if we want to change the default method from peer authentication to md5 (password), we change it here (remember to restart the service)\n\n## Peer Authentication\n- by default, `psql` tries to connect to the postgres database over UNIX sockets. The default authentication method is *peer authentication*, which requires the current UNIX user to have the same username as `psql`\n\t- spec: Therefore, to connect with peer authentication, we need to be logged in on UNIX as the same username as the postgres username we are trying to connect with\n\t- ex. if on UNIX we are logged in as user `kyletycholiz`, then simply executing `psql` without arguments will try and log us in as the postgres user `kyletycholiz`. If this user doesn't exist in postgres, then we will get a peer authentication error.\n- works by obtaining the client's OS username from the kernel, and using it as the allowed database username.\n- Only supported for local connections.\n","n":0.074}}},{"i":423,"$":{"0":{"v":"Acid","n":1},"1":{"v":"\n### How Postgres handles Transactions and Concurrency\nPostgres handles these 2 problems with MVCC (multi-version concurrency control)\n- When you update or delete a row, Postgres doesn't actually remove the row. When you do an UPDATE or DELETE, the row isn't actually physically deleted. For a DELETE, the database simply marks the row as unavailable for future transactions, and for UPDATE, under the hood it's a combined INSERT then DELETE, where the previous version of the row is marked unavailable. These new versions of rows are generally referred to as the \"live\" rows, and the older versions are referred to as \"dead\" rows.\n\n## UE Resources\n[Concurrency control](https://www.postgresql.org/docs/current/mvcc-intro.html)\n","n":0.098}}},{"i":424,"$":{"0":{"v":"Pg Xl","n":0.707},"1":{"v":"\nPostgresXL is a horizontally scalable SQL database cluster\n- It is equipped to handle write-intensive workloads that depend on atomic transactions which are potentially distributed\n\t- OLTP\n- PostgresXL allows you to either partition tables across multiple nodes, or replicate them.\n\t- Partitioning (or distributing) tables allows for write scalability across multiple nodes as well as massively parallel processing (MPP) for Big Data type of workloads.\n\t\t- MPP is using many computers to perform a set of coordinated computations in parallel\n\n## Components of Postgres-XL\n### Global Transaction Monitor\n- ensures cluster-wide transaction consistency\n\t- responsible for issuing transaction ids and snapshots as part of its Multi-version Concurrency Control.\n\n### Coordinator\n- manages the user sessions and interacts with GTM and the data nodes\n\t- parses and plans queries, and sends down a serialized global plan to each of the components involved in a statement.\n\n### Data node\n- where the actual data is stored.\n\t- The distribution of the data can be configured by the DBA.\n- warm standbys of the data nodes can be configured to be failover-ready.\n","n":0.078}}},{"i":425,"$":{"0":{"v":"Paradigm","n":1}}},{"i":426,"$":{"0":{"v":"Oop","n":1},"1":{"v":"\n## Deciding when it should be a class\nThe fact that something sounds like an object does not automatically mean that it should be an object in your program. Reflexively writing classes for every concept in your application tends to leave you with a collection of interconnected objects that each have their own internal, changing state. Such programs are often hard to understand and thus easy to break.\n- This is in reference to a program that simulates a village with roads between houses, and a robot that delivers parcels to each house. The program has to track the state of the parcel delivery status, as well as the robot's location in the town. Consider that if our instinct was to have classes for the robot, one for a parcel, maybe one for places, we would have to hold state in objects of different classes. The robot's current location would probably be held in the `Robot` class, while the delivery status of a parcel would probably be held in the `Parcel` class. Furthermore, a location would probably hold the state of all the parcels that are currently delivered to it. Having this state spread out everywhere makes the program hard to understand (and know which of these objects will need to have their state updated).\n  - [source](https://eloquentjavascript.net/07_robot.html#p_XP2aQths2D)\n\nWhen considering how granular we should make our classes, it makes sense to think high-level. Ask the question, \"how can I represent the state of this program in a central and simple way, where I can update the state in a single place in the event of a business-level action (such as moving location, dropping off a letter etc)\". If we had a postman program, which mails a stack of letters to different householders, a natural inclination might be for us to create classes for postman, village etc. Instead, we should consider what the minimal scope of state is. In this example, we only really need to keep track of how many letters there are, and where the current location is. Realistically, all we need is a `VillageState` class.\n```js\nclass VillageState {\n  constructor(place, parcels) {\n    this.place = place;\n    this.parcels = parcels;\n  }\n\n  move(destination) {\n    if (!roadGraph[this.place].includes(destination)) {\n      return this;\n    } else {\n      let parcels = this.parcels\n        .map(p => {\n          if (p.place != this.place) return p;\n          return {place: destination, address: p.address};\n        })\n        .filter(p => p.place != p.address);\n      return new VillageState(destination, parcels);\n    }\n  }\n}\n```\n\nLet's apply the above lesson to TicTacToe. If we were to blindly make every item into it's own object, we would have classes for `Game`, `Board`, `Player`, `Square`. Resisting that temptation, think about how we might go about centralizing the state and the methods that change them. For instance, we don't really need a `Square` class. The duty of a `Square` class presumably is to keep track of the square's current state (empty, X, 0). But this could just as easily be managed by the same class that represents the `Board`. If we used a `Square` class, then we would have to reach into it every time we want to make a calculation about the state of the game.\n- spec: when deciding on what classes to make, think in terms of what state you need to store, and at what level would be appropriate?\n  - ex. in TicTacToe, it might make sense to model a `Player` class, if you are going to use it to determine each player's currently occupied squares (ex. `[0, 4, 7]`). Alternatively, you may decide that that data can be safely stored in the `Game` object as 2 properties, `player1Choices` and `player2Choices`.\n\nIn OOP, dependencies are things that you need to provide to the constructor of the class to create an instance of it.\n\n* * *\n\n## Example: Implementing TicTacToe\nCreate a player, game, and board class. The player class will choose moves, the board class will hold the state of the moves, and the game class will run the game and declare the winner.\n\nIf you want to make this project more challenging then make sure the game class can accept either human or computer players (and the code is exactly the same from the game class's perspective). This will allow you to practice class inheritance and also you can learn to build a simple ai. For the ai you can start with a simple computer that picks randomly. Once you get that done you can basically build a tree structure which builds all the possible future outcomes and make sure the computer selects the most favorable one.\n\n# E Resources \n[TicTacToe Repo in OOP: a great breakdown to understand OOP, without clutter of anything not related to TicTacToe](https://github.com/Anna-Myzukina/Tic-Tac-Toe-OOP)\n","n":0.036}}},{"i":427,"$":{"0":{"v":"Polymorphism","n":1},"1":{"v":"\n# UE Resources\n- [Good explanation](https://stackoverflow.com/questions/154577/polymorphism-vs-overriding-vs-overloading)","n":0.447}}},{"i":428,"$":{"0":{"v":"Overriding","n":1},"1":{"v":"\nWhen we have a method in the child class and a method in the parent class, and they both have the same name. In this case, the method defined in the child would override the parent's version.\n\nOverriding implements Runtime Polymorphism","n":0.158}}},{"i":429,"$":{"0":{"v":"Overloading","n":1},"1":{"v":"\nWhen two or more methods in the same class have the same name but different parameters, it’s called Overloading.\n- This is distinct from `Overriding` which is when we have a method in the child class and a method in the parent class, and they both have the same name. In this case, the method defined in the child would override the parent's version.\n\nOverloading implements Compile time polymorphism","n":0.122}}},{"i":430,"$":{"0":{"v":"Encapsulation","n":1},"1":{"v":"\n\nEncapsulation is essentially about protecting the private data of an object such that they can only be accessed or mutated via the public API exposed by the same object.\n\nEncapsulation refers to the bundling of data with the methods that operate on that data, all the while restricting of direct access to some of an object's components\n\nEncapsulation is used to hide the values or state of a structured data object inside a class, preventing direct access to them by clients in a way that could expose hidden implementation details or violate state invariance maintained by the methods.\n\nTraditionally, JavaScript developers used `_` to prefix the properties or methods that they intended to be private.","n":0.094}}},{"i":431,"$":{"0":{"v":"Imperative","n":1},"1":{"v":"\nA language is imperative when they make explicit references to the state of the execution environment\n- think about how calling .open() on a modal relates to this, versus setting a variable isOpen that is listened to by the modal. .open() is explicitly referencing the state of the environment \n","n":0.143}}},{"i":432,"$":{"0":{"v":"Functional","n":1},"1":{"v":"\n# Overview\n\"function\" is a reference to how functions work in math.\nEx. f(x) = 2x² + 5 can be plotted on a graph to make a parabola. In this sense, it can be thought of as a map. An x value of 5 maps to a return value of 55. \nThough FP rarely deals with graphs like this, think of it as input values mapping to output\n![](/assets/images/2021-03-09-09-35-55.png)\n\nfunctional languages are closer to math than the more popular imperative languages.\n\nBased on idea of Referential Transparency\n\n### Referential transparency\nThe notion that a function could be replaced by its return value and it wouldn't impact the functionality of the program.\nIf satisfied, this is a clear sign that a function is pure.\nas you're reading a program, once you've mentally computed what a pure function call's output is, you no longer need to think about what that exact function call is doing when you see it in code, especially if it appears multiple times.\nThat result becomes kinda like a mental const declaration, which as you're reading you can transparently swap in and not spend any more mental energy working out.\n\n\n## Abstracting (Generalizing) Functions\nSimilar to how partial application and currying (see Chapter 3) allow a progression from generalized to specialized functions, we can abstract by pulling out the generality between two or more tasks. The general part is defined once, so as to avoid repetition. To perform each task's specialization, the general part is parameterized.\n\nconsider:\n```\nfunction saveComment(txt) {\n    if (txt != \"\") {\n        comments[comments.length] = txt;\n    }\n}\n\nfunction trackEvent(evt) {\n    if (evt.name !== undefined) {\n        events[evt.name] = evt;\n    }\n}\n```\nthe repetition (generality) between the 2 functions is: storing a value in a data source. the uniqueness (specialty) of them is that one sticks the value on the end of an array, while the other sets the value as a property on an object.\n\nabstracting:\n```\nfunction storeData(store,location,value) {\n    store[location] = value;\n}\n\nfunction saveComment(txt) {\n    if (txt != \"\") {\n        storeData( comments, comments.length, txt );\n    }\n}\n\nfunction trackEvent(evt) {\n    if (evt.name !== undefined) {\n        storeData( events, evt.name, evt );\n    }\n}\n```","n":0.055}}},{"i":433,"$":{"0":{"v":"Transducer","n":1},"1":{"v":"\n# Transducer\n[read more](https://www.jeremydaly.com/transducers-supercharge-functional-javascript/)\ndef: A transducer is a function which takes in a reducer, and returns another reducer\ndef: A transducer takes an object or array, iterating through each value, transforming each element with a composition of transformer functions\n\nThe transduce function is really just a reduce function with an additional argument upfront:\n```js\n// With reduce\nR.reduce(R.flip(R.append), [], autobots)\n\n// With transduce\nR.transduce(transform, R.flip(R.append), [], autobots)\n```\nthe above reduce function iterates the elements of our `autobots` array and appends them to the accumulator\n\nWhen we use `transduce()`, we are passing each item from our list into our transformation function before passing it to our reducing function. So basically, transduce is just a way for us to transform items while reducing them. We are, in fact, *transducing*\n\ntransducers are a generic and composable way to operate on a collection of values, producing a new value or new collection of new values\n\nIn this example, we can see the power of `transduce`. The first way, we need to iterate over `autobots` a total of 3 times. However, when we use `transduce` as in the second way, we only need to iterate over it once.\n- The traditional way, is that we perform `map` on each element of the array, then `map` on each element again, then `filter` on each element. With transduce, we perform `map` on the first element, then `map` again on the first element, then `filter` on the first element. The resulting value is then mapped to the output array, then the second element is performed on.\n```js\nlet autobots = ['Optimus Prime','Bumblebee','Ironhide','Sunstreaker','Ratchet']\n \n// Filter for autobots that contain 'r', uppercase, then reverse\nlet transform = R.compose(\n  R.filter(x => /r/i.test(x)),\n  R.map(R.toUpper),\n  R.map(R.reverse)\n)\n \n// BEFORE\ntransform(autobots)\n// => [ 'EMIRP SUMITPO', 'EDIHNORI', 'REKAERTSNUS', 'TEHCTAR' ]\n\n// AFTER\nR.transduce(transform, R.flip(R.append), [], autobots)\n// => [ 'EMIRP SUMITPO', 'EDIHNORI', 'REKAERTSNUS', 'TEHCTAR' ]\n```\n\nThe word ‘transducer’ itself can be split into two parts that reflect this definition: \n- `transform` — to produce some value from another (ex. `reduce`/`map`).\n- `reducer` — to combine the values of a data structure to produce a new one.\n- this is a really cool concept, because it allows us to abstract away any implementation detais that we\n\n`.map` and `.filter` can be implemented with `.reduce`. Therefore, we see that `.reduce` is a more general function, and `.map` and `.filter` are more specific implementations of `.reduce`. Transduce allows us to treat a reduce-like construct as a *higher order function*, meaning we can pass in the *reducing function* (which is the function found in a reducer)\n- This means we could create `.map`/`.filter` with a transducer\n[more info](https://www.jeremydaly.com/transducers-supercharge-functional-javascript/)\n","n":0.049}}},{"i":434,"$":{"0":{"v":"Recursion","n":1},"1":{"v":"\nIn general, recursive code helps us avoid complex nested loops and is most useful for tasks that can be defined in terms of similar subtasks.\n\nEach recursive function consists of 2 parts:\n1. Base Case: The base case is where further calls to the same function stop, i.e, it does not make any subsequent recursive calls.\n2. Recursive Case: The recursive case is where the function calls itself again and again until it reaches the base case.\n    - Each child function call returns result to its parent function.\n\nBoth recursion and iteration depend on a condition which determines whether to stop execution or continue it.\n\n![[general.lang.functions#memory-allocation-of-recursive-functions,1]]\n\n## Methods to understand flow of recursive function\n### Visualizing through a stack\nThe concept of a stack is critical in recursion. When you start visualizing your function calls through a stack and how it returns when reaching a base case - the concept of recursive calls and their output becomes easier to comprehend.\n\nRecursing:\n![](/assets/images/2021-10-09-15-16-13.png)\nReturning values to lower stacks:\n![](/assets/images/2021-10-09-15-16-42.png)\n\n### Making a recursive tree\nConsider that each function call must return before subsequent code can be run. In the first recursive step (ie. \"iteration 1\"), the recursing function does not actually return until the base-case is complete. In fact, it is the *final* call of that function to actually return.\n![](/assets/images/2021-10-09-21-07-35.png)\n![](/assets/images/2021-10-09-21-11-00.png)\n* * *\n\n### Parallels with mathematics\nMentally, what's happening is similar to when a mathematician uses a Σ summation in a larger equation. We're saying, \"the max-even of the rest of the list is calculated by `maxEven(...restNums)`, so we'll just assume that part and move on.\"\n```js\nfunction maxEven(num1, ...restNums) {\n\tvar maxRest = restNums.length > 0 ? maxEven(...restNums) : undefined;\n\n\treturn (num1 % 2 != 0 || num1 < maxRest) ? maxRest : num1;\n}\n\nconsole.log(maxEven(1, 6, 3))\n```\n\nOnce the base case is met, the final return value makes its way back up all layers of the call stack to give us that value\n```js\nfunction foo(x) {\n    if (x < 5) return x;\n    return foo( x / 2 );\n}\n```\n\n![](/assets/images/2021-03-09-09-41-09.png)\nRecursion is declarative for algorithms in the same sense that Σ is declarative for mathematics.\n\nusing reverseString as an example\n![[paradigm.functional.recursion.cook#reverse-a-string,1:#*]]\n\nspec: consider what we are doing here, conceptually. We are taking the last character, putting it at the front, and calling the function again with the last character popped off. Each time we recurse, we build up the new string. That is, each \"iteration\" adds one character to the new string. That whole idea can be summed up with just this part of the function: `string[string.length - 1] + ...` (ie. the first part of the return value). Assuming you understand the logic taking place each \"iteration\" (ie. get the last character of a truncated (by one character) string), then you don't really need to focus on the recursive-function-call part of the return value. This logic taking place is reflected in the value that is passed to the recursive function. Once you've internalized this, then you can inherently understand the values you are working with. Given `string[string.length - 1] + ...`, we know that we are just adding the last character to the front.","n":0.045}}},{"i":435,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Factorial function\nImplementing a factorial is a good starting-point for coming to grips with recursion. It is a simple implementation, and the logic lends itself well to a recursive solution.\n\n```js\nfunction factorial(num) {\n    if (num === 1) return 1\n    return num * factorial(num - 1)\n}\n```\n\nGiven `factorial(5)`, each \"iteration\" looks like this:\n1. return 5 * factorial(4)\n2. return 4 * factorial(3)\n3. return 3 * factorial(2)\n4. return 2 * factorial(1)\n5. return 1\n\nConsider that when we see `factorial(num - 1)`, we really don't know what this value is going to be for each iteration until the very end. This is part of what makes it mentally challenging to parse. If we work backwards and plug in the return value of each step into the return value of the previous step's call to `factorial()`, then it's more clear how the value from the final \"iteration\" bubbles up.\n\n\n### Reverse a string\nWe start from the end of the string. We take the last character of the string string[string.length - 1] and call the function reverseString() again, but with the last character removed. When this child function returns, this string[string.length - 1] will be appended at the beginning of the returned string.\n```js\nfunction reverseString(string) {\n  // Base case\n  if (string === \"\") {\n    return string;\n  }\n\n  // Recursive case\n  else {\n    // take the last character and add it to the reverse of the previous characters\n    return string[string.length - 1] + reverseString(string.substr(0, string.length - 1));\n  }\n}\n```\n\n### Simulate reduce with recursion\neach call of the function separates the first argument from the rest, takes something from that first argument, then calling the function again with the remaining arguments (all but first)\n```js\nfunction sum(num1,...nums) {\n    if (nums.length == 0) return num1;\n    return num1 + sum( ...nums );\n}\n```\n```js\nfunction maxEven(num1,...restNums) {\n    var maxRest = restNums.length > 0 ?\n            maxEven( ...restNums ) :\n            undefined;\n\n    return (num1 % 2 != 0 || num1 < maxRest) ?\n        maxRest :\n        num1;\n}\n```","n":0.057}}},{"i":436,"$":{"0":{"v":"Purity","n":1},"1":{"v":"\n## Purifying functions\n- Use immutable data structures (ex. Immutable.js)\n- Sometimes you can just shift the side effects out of a function to the part of the program where the call of that function happens. The side effect wasn't eliminated, but it was made more obvious by showing up at the call-site.\n\n```js\nfunction addMaxNum(arr) {\n    var maxNum = Math.max( ...arr );\n    arr.push( maxNum + 1 );\n}\n\nvar nums = [4,2,7,3];\n\naddMaxNum( nums );\n\nnums;       // [4,2,7,3,8]\n```\nto\n\n```js\nfunction addMaxNum(arr) {\n    var maxNum = Math.max( ...arr );\n    arr.push( maxNum + 1 );\n}\n\nvar nums = [4,2,7,3];\n\naddMaxNum( nums );\n\nnums;       // [4,2,7,3,8]\n```","n":0.105}}},{"i":437,"$":{"0":{"v":"Lens","n":1},"1":{"v":"\n# Lenses\nThink of a lens as something that focuses (zooms in) on a specific part of a larger data structure\n- Similar to stores  in Redux\n\nAnother way is to see lenses as little drones that copy a part of an object for us, and delve into it to change the properties, all without mutating the original \n\nGiven a lens there are essentially three things you might want to do\n- view the subpart\n- modify the whole by changing the subpart\n- combine this lens with another lens to look even deeper\n\nLenses can be handy if we have a somewhat complex data structure that we want to abstract away from calling code. Rather than exposing the structure or providing a getter, setter, and transformer for every accessible property, we can instead expose lenses. Client code can then work with our data structure using view, set, and over without being coupled to the exact shape of the structure.\n\nIt is a special `type` that combines a *getter* and a *setter* function into a single unit\n\nThe first fn is the getter, while the second is the setter\n","n":0.075}}},{"i":438,"$":{"0":{"v":"Currying","n":1},"1":{"v":"\n# Currying\nTransforming a function of N arity (num of args) into N functions of 1 arity.\n\nPartial application\nApplying a certain number of args toward completion of the function (when the last argument is called)\nApplication === calling a function and applying it's return value\n\nJavaScript engine does its job in two phases: memory creation (declaring variables/functions and hoisting them), and execution (initializing variables and actually running through code)\n\n## Why use these techniques?\nThe first and most obvious reason is that both currying and partial application allow you to separate in time/space (throughout your codebase) when and where separate arguments are specified, whereas traditional function calls require all the arguments to be present at the same time. If you have a place in your code where you'll know some of the arguments and another place where the other arguments are determined, currying or partial application are very useful.\n\nAnother layer to this answer, specifically for currying, is that composition of functions is much easier when there's only one argument. So a function that ultimately needs three arguments, if curried, becomes a function that needs just one, three times over. That kind of unary function will be a lot easier to work with when we start composing them. But the most important layer is specialization of generalized functions, and how such abstraction improves readability of code.\n\nR.partial says \"you give me a function and any number of arguments you want, then I'll just keep letting you add in as many arguments as you want either indefinitely or until all parameters of the function have been satisfied with arguments\"\n![](:/7548f55bd87c4c279f38a15ebac02ed5)\n\n### Partial application of composed functions\nallows us to compose multiple functions together, while returning a function that will accept more. This has the benefit of allowing us to compose more and more specific functions.\nexpl. `unique` and `words` will get partially applied to `compose`\n\n```\nconst filterWords = partialRight( compose, unique, words )\nconst biggerWords = filterWords(skipShortWords)\n```\n\n![Untitled Diagram.jpg](:/593b0123095a48c98e17db790864665f)\n\n### Currying composed functions\nsince compose has right-to-left ordering, we normally `R.curry(R.reverseArgs(R.compose), ..)`\n\n*Using partials with `.pipe()`*\n`var filterWords = partial( pipe, words, unique )`\n\n#### Functor\nsomething that can be mapped over\nex. array, object\n- a functor that holds values of type A, when mapped over with a function that takes a value of type A and returns a value of type B, the result must be a functor that holds values of type B\n\n#### Applicative\na subtype of functors for which additional functions are defined.\n[more info](https://medium.com/@JosephJnk/an-introduction-to-applicative-functors-aea966799b1d)\n","n":0.05}}},{"i":439,"$":{"0":{"v":"Open API","n":0.707},"1":{"v":"\nOpenAPI is a specification for describing REST API formats. Swagger provides tools for implementing that specification. These tools can be used at different stages of the API lifecycle.\n\nAn OpenAPI file (json or yml) allows you to describe your entire API, including:\n- Available endpoints (`/users`) and operations on each endpoint (`GET /users`, `POST /users`)\n- Operation parameters Input and output for each operation\n- Authentication methods\n- Contact information, license, terms of use and other information.\n\nThe value of OpenAPI is that it allows APIs to describe their own structure. This allows us to:\n- Design-first users: use Swagger Codegen to generate a server stub for your API. The only thing left is to implement the server logic – and your API is ready to go live!\n- Use Swagger Codegen to generate client libraries for your API in over 40 languages.\n- Use Swagger UI to generate interactive API documentation that lets your users try out the API calls directly in the browser.\n- Use the spec to connect API-related tools to your API. For example, import the spec to SoapUI to create automated tests for your API.\n\nDefinition example:\n```\nopenapi: 3.0.0\ninfo:\n  title: Sample API\n  description: Optional multiline or single-line description in [CommonMark](http://commonmark.org/help/) or HTML.\n  version: 0.1.9\nservers:\n  - url: http://api.example.com/v1\n    description: Optional server description, e.g. Main (production) server\n  - url: http://staging-api.example.com\n    description: Optional server description, e.g. Internal staging server for testing\npaths:\n  /users:\n    get:\n      summary: Returns a list of users.\n      description: Optional extended description in CommonMark or HTML.\n      responses:\n        '200':    # status code\n          description: A JSON array of user names\n          content:\n            application/json:\n              schema: \n                type: array\n                items: \n                  type: string\n```","n":0.062}}},{"i":440,"$":{"0":{"v":"Objection","n":1},"1":{"v":"\nObjection is a relational query builder. \n- get all the benefits of an SQL query builder but also a powerful set of tools for working with relations.\n\n# Parts\n- define models declaratively, and map the relationship between models (1:many etc)\n- QueryBuilder - Every method that allows you to fetch or modify items in the database returns an instance of the QueryBuilder\n\t- therefore most important component in Objection\n\nAll instance methods start with the character $ to prevent them from colliding with the database column names.\n\n## Model\nconsider that in an app, there are different layouts of data:\n1. it could be the layout that exists in the database itself\n2. it could be the layout that exists when the user gets that data back from the database (ie, on the client)\ntherefore, any time we read or write to a database, we are converting data\n\nthere are 4 methods on a model that are used to transform data. They exist by default, but can be overridden. \nthese \"converter methods\" will be called each time data is tranformed: \n1. when we are writing data, we are converting it to the database layout, therefore `$formatDatabaseJson`\n2. when we are reading data, we are converting it to our internal data layout, therefore `$parseDatabaseJson`\n3. when we give data for a query, (for example `query().insert(req.body)`) or create a model explicitly using `model.fromjson(obj)` the `$parsejson` method is invoked\n\t- When you call `model.toJSON()` or `model.$toJson()` the `$formatJson` is called.\n- Note: Most libraries like express and koa automatically call the toJSON method when you pass the model instance to methods like response.json(model). You rarely need to call toJSON() or $toJson() explicitly.\n- All properties that start with $ are also removed from database and external layouts.","n":0.06}}},{"i":441,"$":{"0":{"v":"Nosql","n":1},"1":{"v":"\nRelational databases assume that the relationships are of similar importance, document databases assume that relationships form a hierarchical structure and relationships between documents are less important\n\nIf your data cannot be represented on literally a sheet of paper, NoSQL is the wrong data store for you. And I don't mean sheets of paper with references that say \"now turn to page 64 for the diagram\", no, I mean a sheet of paper per document. That is what a normalized record looks like in a document store.\n\nHorizontal scaling is a distinct benefit of NoSQL, which is why companies like Netflix and Spotify use document databases.\n- RDBMSs more lend themselves to vertical scaling, which can get costly.\n\nNoSQL databases fit better into the whole paradigm of distributed computing, and NoSQL databases make the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution but requires data to be easily spread across multiple servers to scale up. Using commodity (affordable, smaller) hardware on-site or in the cloud saves you the hassle of additional software, and NoSQL databases like Cassandra are designed to be scaled across multiple data centers out of the box, without a lot of headaches.\n\nGenerally, NoSQL databases sacrifice ACID compliance for scalability and processing speed\n\nGoing from SQL to NoSQL is easier than from NoSQL to SQL\n\nWhen all the other components of our application are fast and seamless, NoSQL databases prevent data from being the bottleneck.\n- Big data is contributing to a large success for NoSQL databases, mainly because it handles data differently than the traditional relational databases.\n\n# Types of NoSQL Databases\n## Document-Based Databases\nDocument-based databases store the data in JSON objects. Each document has key-value pairs like structures:\n\nThe document-based databases are easy for developers as the document directly maps to the objects as JSON is a very common data format used by web developers. They are very flexible and allow us to modify the structure at any time.\n\nEx. Mongo, Couch, Couchbase\n\n## Key-Value Database\nHere, keys and values can be anything like strings, integers, or even complex objects. They are highly partitionable and are the best in horizontal scaling. They can be really useful in session oriented applications where we try to capture the behavior of the customer in a particular session.\n\nkey-value stores, in general, always maintain a certain number of replicas to offer reliability.\n\nEx. DynamoDB, Redis, Cassandra\n\n## Wide Column-Based Database\nThis database stores the data in records similar to any relational database but it has the ability to store very large numbers of dynamic columns. It groups the columns logically into column families.\n- For example, in a relational database, you have multiple tables but in a wide-column based database, instead of having multiple tables, we have multiple column families.\nCassandra or key-value stores, in general, always maintain a certain number of replicas to offer reliability.\n\nEx. Cassandra\n\n# Implementations\n\n## Cassandra\nA key-value store approach to NoSQL\n\nCassandra's approach to data availability is as follows: Instead of having one master node, it utilizes multiple masters inside a cluster. With multiple masters present, there is no fear of any downtime. The redundant model ensures high availability at all times.\n\nCassandra is designed to manipulate huge data arrays across multiple nodes. \n\nIn contrast to the relational database organizing data records in rows, Cassandra’s data model is based on columns to provide faster data retrieval. The data is stored in the form of hash.\n\ndesigned to be scaled across multiple data centers out of the box, without a lot of headaches.\n\n## DynamoDB (Amazon)\n![[aws#dynamodb,1]]\n\n## Elasticsearch\nWhen to use ElasticSearch?\n- If your use case requires a full-text search, Elasticsearch will be the best fit\n- If your use case involves chatbots where these bots resolve most of the queries, such as when a person types something there are high chances of spelling mistakes. You can make use of the in-built fuzzy matching practices of the ElasticSearch\n- Also, ElasticSearch is useful in storing logs data and analyzing it\n\n## CouchDB\nLike MongoDB, Couch is a document-oriented NoSQL databases, but Mongo and Couch diverge significantly in their implementations. \n- CouchDB uses the semi-structured JSON format for storing data. Queries to a CouchDB database are made via a RESTful HTTP API, using HTTP or JavaScript. \n- MongoDB uses BSON, a JSON variant that stores data in a binary format. MongoDB uses its own query language that is distinct from SQL, although they have some similarities. \n\nLike Mongo, Couch is schemaless.\n\nCouchDB and MongoDB differ in their approach to the [[CAP theorem|deploy.distributed.CAP-theorem]] \n- CouchDB favors availability and partition tolerance\n    - CouchDB uses eventual consistency. Clients can write to a single database node, and this information is guaranteed to eventually propagate to the rest of the database. \n- MongoDB prefers consistency and partition tolerance.\n    - MongoDB uses strict consistency. The database uses a replica set to provide redundancy but at the cost of availability. \n\nAs of this writing, Google projects the cost of deploying CouchDB on GCP at $34.72 per month. This estimate is based on a 30 day, 24 hours per day usage in the Central US region, a VM instance with 2 vCPUs and 8 GB of memory, and 10GB of a standard persistent disk.\n\n## Couchbase\nEvery Couchbase node consists of a data service, index service, query service, and cluster manager component. Starting with the 4.0 release, the three services can be distributed to run on separate nodes of the cluster if needed.\n\nIn the parlance of CAP Theorem, Couchbase is typically run as a CP system (consistency & partition tolerant)\n\nProvides a SQL-like query language called `N1QL` for manipulating JSON data stored in Couchbase.\n\n## PouchDB\nPouchDB was created to help web developers build applications that work as well offline as they do online.\nIt enables applications to store data locally while offline, then synchronize it with CouchDB and compatible servers when the application is back online, keeping the user's data in sync no matter where they next login.\nInspired by Couch","n":0.032}}},{"i":442,"$":{"0":{"v":"Node","n":1},"1":{"v":"\n# How it works\nNode.js is a JavaScript runtime environment that processes incoming requests in a loop, called the event loop.\nNode runs as a single process. Other languages may handle \"concurrency\" by having multiple threads handle tasks. However, Node achieves this by having async. When an async action happens, the javascript code will not get blocked, and will continue to execute until the response has come. This makes node a very event-based language (hence its usefulness in the web word, which is fundamentally made up of requests). \n- concurrency here is used liberally, since Node does not run its code concurrently at all. Alas, this is how Node solves the problem, which may be solved using multiple threads (concurrency) at a time. \n\nNode is built on top of v8, the engine that converts javascript to native machine code. These core javascript features form the basis of what Node is. Because Javascript is missing native features that server languages typically have, there comes a need of having to fulfill those tasks. Web server functions (such as I/O, networking, streams etc.) are fulfilled by having isolated modules that each have a single responsibility. Node.js is made up of many of these modules, all built on top of the base language of Javascript and its underlying v8 engine.\n- ques:does this mean that all the language features of javascript are also available in node? Is there a global object? what about all of the javascript features that are considered useless, like exec? \n\nNode.js connects the ease of a scripting language (JavaScript) with the power of Unix network programming\n\nNode.js uses an event loop for scalability, rather than processes or threads\n\n## V8\nWhen it was first introduced, javascript was a simple scripting language, and processed commands in real time. This had performance implications, and made it very slow. This was the inspiration for V8, which did just in time compiling, making it a much faster and more capable language.\n\nWith Node, the idea was \"what if we took the V8 engine, and instead of using it in the context of the web, we allowed it to run terminal applications\"\n## Resources\n### UE Resources\n[Tutorial: creating a native node module with C++](https://medium.com/@marcinbaraniecki/extending-node-js-with-native-c-modules-63294a91ce4)\n[Tutorial: creating streams in node](https://github.com/substack/stream-handbook)","n":0.052}}},{"i":443,"$":{"0":{"v":"Types","n":1}}},{"i":444,"$":{"0":{"v":"Stream","n":1},"1":{"v":"\nhttps requests and responses are streams in Node\n","n":0.354}}},{"i":445,"$":{"0":{"v":"Buffer","n":1},"1":{"v":"\na `Buffer` is similar to an array of integers, but corresponds to a raw memory allocation outside the V8 heap.\n```js\nconst buffer = [11111111, 11011001, 10010001]\n```\n\nThe `Buffer` class can set its encoding so we can put a `String` into it. This enables it to translate from `String` of UTF16 characters into an array of bytes. Once you have a `String` in byte form you can use it in computing communication.\n- In Javascript, a `String` is a collection of characters in UTF16 encoding. \n    - Under UTF16 encoding a single character may consist of multiple bytes (at least one, usually not more than four)","n":0.099}}},{"i":446,"$":{"0":{"v":"Serial Port","n":0.707},"1":{"v":"\nSerial Port is a Node library that allows us to connect and communicate with an external device via the device's serial ports\n\n![Serial Port](/assets/images/2021-03-20-19-47-20.png)\n\n### Raspberry Pi + Roomba/Arduino\nRoomba has a serial port, meaning we can connect it with a Raspberry Pi via the Pi's GPIO pins\n\nOn the Pi, we would install Node SerialPort, and run `npx @serialport/list` to scan the devices that are connected via the GPIO pins.\n- From there, we open a serial port, we open a REPL, and we define a new port, which uses one of the connected devices and specifies some options\n```\n$ npx @serialport/terminal -p /dev/tty.usbmodem14301                                                                    \nOpening serial port: /dev/tty.usbmodem14301 echo: true\n```\n```\n$ npx @serialport/repl                                                                                                  \nport = SerialPort(\"/dev/tty.usbmodem14301\", { autoOpen: false })\nglobals { SerialPort, portName, port }\n```","n":0.092}}},{"i":447,"$":{"0":{"v":"package.json","n":1},"1":{"v":"\n# package.json\n## Scripts\n- every script we define has `pre` and `post` version to it\n    - ex. if we define a `install` script, then there is a `preinstall` that is implicitly defined\n\n## Dependencies\n### Peer Dependencies\n- peer dependencies are not automatically installed like dependencies and devDependencies are.\n\t- Instead, when we list a package as a peerDependency, we expect it to be provided from the host (the host would be our application)\n- peer dependencies are most likely to be used only when creating npm packages.\n- ex. `react-dom` lists `react` as a peerDependency, since `react-dom` is useless unless we have `react` installed.\n\t- if `react-dom` had listed `react` as a dependency, then effectively we would have 2 different versions of react in our project.  \n\t- by listing `react` as a peerDependency, `react-dom` is basically saying \"hey npm, I don't have react as a dependency, but I do need react to work. If someone tries to install me, but they don't have react installed, spit a warning at them, cause I ain't gonna work properly\"\n\n[more info](https://flaviocopes.com/npm-peer-dependencies/)\n\n ","n":0.077}}},{"i":448,"$":{"0":{"v":"Modules","n":1},"1":{"v":"\n### node_modules\n- in `node_modules`, we have a folder `.bin/`. This is where the binaries (executables) of our dependencies (listed in `node_modules`) are stored.\n\t- normally these binaries are symlinks to the binaries that are stored within each package's directory in `node_modules`\n\t- when are within a project and run a command that is not globally installed, it looks in the `.bin/` directory for that executable\n\t\t- ex. running `jest test` in the project will look for an executable called `jest` within `node_modules/.bin` and execute it.\n- the [node module resolution algorithm](https://nodejs.org/api/modules.html#modules_loading_from_node_modules_folders) is recursive, meaning when looking for package A, it looks in local `node_modules/A`, then `../node_modules/A`, then `../../node_modules/A`, and so on.\n- unless we use strict versioning (ie. omitting `^` in package.json.dependencies) the version listed in package.json is not the version our package uses. Rather, it defines a range that is allowed to be installed. To see the actual version, check yarn.lock\n\n### Package.json\n[Adding comments to package.json (of course, a workaround; not actual comments)](https://stackoverflow.com/questions/14221579/how-do-i-add-comments-to-package-json-for-npm-install)","n":0.08}}},{"i":449,"$":{"0":{"v":"Winston","n":1},"1":{"v":"\nWinston is a logger library designed with support for multiple transports (containers for the logs)\n- you can store errors in one transport, and normal logs in another. This is nice if you want to integrate other services. For instance what if we want our database to consume and store these errors, but don't care about storing other log levels?","n":0.13}}},{"i":450,"$":{"0":{"v":"Stream","n":1},"1":{"v":"\nA stream is a way for us to incorporate buffering into the process of writing files\n- We may add multiple avenues for this data to go in, releasing us from the restriction of only being able to insert one set of data in a single file\n\t- This gives the ability for a stream to be able to `pipe` its output to both stdout and a file\n\nThere are 4 types of streams in Node.js:\n1. **Writable**: streams to which we can write data. For example, fs.createWriteStream() lets us write data to a file using streams.\n2. **Readable**: streams from which data can be read. For example: fs.createReadStream() lets us read the contents of a file.\n3. **Duplex**: streams that are both Readable and Writable. For example, net.Socket\n4. **Transform**: streams that can modify or transform the data as it is written and read. For example, in the instance of file-compression, you can write compressed data and read decompressed data to and from a file.\n\nIn a Node.js based HTTP server, `request` is a readable stream and `response` is a writable stream\nThe fs module lets you work with both readable and writable file streams\n\nWhenever you’re using Express you are using streams to interact with the client, also, streams are being used in every database connection driver that you can work with, because of TCP sockets, TLS stack and other connections are all based on Node.js streams.\n\n### How to create a readable stream\nWe first require the Readable stream, and we initialize it.\n```js\nconst Stream = require('stream')\nconst readableStream = new Stream.Readable()\n```\n\nNow that the stream is initialized, we can send data to it:\n```js\nreadableStream.push('ping!')\nreadableStream.push('pong!')\n```\n\n#### Async iterator (`for await`)\n\n#### Pipe vs. Write\n`pipe` is like positioning the pipes that will change the flow of water, before anything has actually gone through those pipes. `write` is like turning on the water.\n\n### Built-in streams\na request to an HTTP server and process.stdout are both stream instances.\nAll streams are instances of EventEmitter.\n\n# UE Resources\n[Async Iterator](https://nodesource.com/blog/understanding-streams-in-nodejs/)","n":0.056}}},{"i":451,"$":{"0":{"v":"Staging","n":1},"1":{"v":"\nIf we look in `node_modules/`, we may notice a `.staging/` directory\n\nThis directory is for those dependencies that are getting downloaded. so for the temporary basis it keeps all those dependencies under \".staging\" folder. Once all gets downloaded properly then it will showcase them under node_modules only.\n","n":0.147}}},{"i":452,"$":{"0":{"v":"Path","n":1},"1":{"v":"\n# Path\n## Path.join\n- concatenate each argument to get a full URL\n\t- therefore, it can be relative or absolute (using `__dirname`) depending on what args we pass \n- depending on if the OS is Unix-based or Windows, different delimiters will be used, abstracting this away from us. \n\n## Path.resolve\n- attempts to resolve a sequence of paths from RTL, with each subsequence path prepended until an absolute directory is formed. \n\t- if an absolute directory is not able to be formed, then the args will be put on the end of the current working directory. \n- this method will treat the first argument as the root directory \n- when called without arguments, it will return the working directory (which may happen to be equivalent to `__dirname`)\n- it will always result in an absolute URL ","n":0.087}}},{"i":453,"$":{"0":{"v":"Morgan","n":1},"1":{"v":"\nMorgan is a HTTP request logger middleware for Node.js.\n\n### Token\nWhen morgan logs to the console, the structure of the content that gets logged in determined by the tokens used\n\nA standard way to use morgan is to use the preset tiny:\n`app.use(morgan('tiny'))`, which is equivalent to:\n`morgan(':method :url :status :res[content-length] - :response-time ms');`\n- The part following `:` is the token.\n\n#### Custom Tokens\nWe can create our own tokens with `morgan.token(nameOfToken, callback)`, where the callback returns the value that will stand in for the token name.\n\nTokens can be configured to accept custom arguments:\n```js\napp.use(morgan(':method :host :status :param[id] :res[content-length] - :response-time ms'));\n\nmorgan.token('param', function(req, res, param) {\n    return req.params[param];\n});\n```\n\nMorgan can be combined with Winston to great effect","n":0.096}}},{"i":454,"$":{"0":{"v":"Child Process","n":0.707},"1":{"v":"\n### Spawn\nThe spawn function will spawn a new process of git log type. The first argument of the function represents a path for an executable file that should start the process, and the second argument is an arguments vector that will be given to the executable. The returned process object will hold a property for each std type represented as a Stream: .stdin - WriteStream, .stout - ReadStream and finally .stderr - ReadStream.\n```js\nconst { spawn } = require('child_process')\n\nspawn('git', ['log'])\n```\n\nIf we would like to run git log through a Node process and print it to the console we would do something like the following:\n```js\nspawn('git', ['log']).stdout.pipe(process.stdout)\n```\n\nOr we can do:\n```js\nspawn('git', ['log'], {\n  stdio: 'inherit' // Will use process .stdout, .stdin, .stderr\n})\n```\n\n\nhttps://www.freecodecamp.org/news/node-js-child-processes-everything-you-need-to-know-e69498fe970a/","n":0.092}}},{"i":455,"$":{"0":{"v":"Ngrok","n":1},"1":{"v":"\n# Ngrok\nNgrok will create a secure tunnel on the local machine from a given port (ex. `8000`) to a url hosted on the internet at their domain (ex. `https://e2210e647fe4.ngrok.io`)\n- when a request somewhere on the internet hits an endpoint of that remote url, ngrok will forward that request on through the tunnel to the local machine.\n\t- ex. Stripe sends a webhook post request to `https://e2210e647fe4.ngrok.io:8000/webhooks/stripe`. Ngrok sees this, and passes it along to `localhost:8000/webhooks/stripe`, where the `/webhooks/stripe` endpoint defined in the application server can then handle the request","n":0.107}}},{"i":456,"$":{"0":{"v":"Nginx","n":1},"1":{"v":"\n## Overview\n- Nginx has 1 master process, and multiple worker processes.\n\t- The master's job is to read configuration files, and to manage the worker processes.\n\t- The worker processes handle the requests.\n- Nginx uses an event-based model to distribute requests among workers\n- The # of workers is specified in the config file, and may either be fixed, or adjustable based on how many cores the CPU has. \n\n### Config file\n- The nginx config file `nginx.conf` is stored either in `/usr/local/nginx/conf`, `/etc/nginx`, or `/usr/local/etc/nginx`.\n- nginx consists of modules which are controlled by directives specified in the configuration file\n\t- Directives can either be simple directives or block directives\n\t\t- simple ends with `;`, block uses `{}`\n- If a directive can have other directives inside, it is called a Context \n\t- ex. `events`, `http`, `server`, `location`\n- If a directive is not placed within a Context, then it is considered to be in the Main Context. \n\t- The `events` and `http` directives reside in the Main Context, `server` in `http`, and `location` in `server`.\n\n## Blocks\n- Nginx divides the configurations meant to serve different content into Blocks, which live in a hierarchical structure.\n- Each time a client request is made to the server, Nginx begins a process of determining which hierarchical block should be used to handle the request.\n\n### Server Block\n- Defines a virtual server used to handle requests of a defined type\n\t- each Server Block functions as a separate virtual web server instance\n- Based on the domain name, port and IP address requested, we can configure multiple server blocks to handle each combination.\n- The `server_name` and `listen` directives are used to determine which server block should be used to fulfill a request. They are used to bind to tcp sockets.\n\t- With `listen`, we can use a lone IP, a lone port, or a combo of the two. If we only specify one, then defaults are used\n\t\t- default port: 80\n\t\t- default IP: 0.0.0.0\n\t- the `server_name` directive is only evaluated when nginx needs to distinguish between server blocks that match to the same level of specificity in the `listen` directive. Put another way, it is a \"specificity tie-breaker\" \n\t\t- in other words, if `example.com` is hosted on `192.168.1.10:80`, a request will always be served by a server block that specifies `listen 192.168.1.10`, even if there is another server block that specifies `server_name example.com`\n\t- Finally, if further specificity is needed, then the Host header from the request (which contains the URI that the client was trying to reach) is used. \n\t\t- When using wildcard matching, the longest match beginning with a wildcard is used\n\t\t\t- ex. if the request has a Host header of `www.example.com`, and we have 3 server blocks with `server_name` of `*.example.com`, `www.example.*` and `*.com`, `*.example.com` would win out.\n- With server blocks, we can run more than one website on a single host\n- in Apache, called *VirtualHost* \n\n### Location Block\n- Lives within a Server Block (or nested in other location blocks).\n- Determine how Nginx should handle the part of the request that comes after IP:port (ie. the URI).\n- Similar to how Nginx has a specificity-based process for determining which server block will process the request, Nginx has an algorithm to determine which location block within the server should be used for handling requests.\n- Location blocks take the following form:\n```\nlocation <optional_modifier> <location_match> {\n}\n```\n- The `location_match` defines what Nginx should check the request URI against.\n- The `optional_modifier` affects the way Nginx will attempt to match the location block.\n\t- ex. check for prefix match (default), check for exact match (`=`), check for case-sensitive Regex (`~`)\n- The URI specified after `location` will be added to the path specified in the *root directive*\n\t- ex. if we specify `root /var/www/` and the location block specifies `/images/`, then the path to the requested file on the local FS will be `/var/www/images`\n- ex. Imagine we had a server block:\n```\nserver {\n    location / {\n        root /data/www;\n    }\n\n    location /images/ {\n        root /data;\n    }\n}\n```\nin response to a request with URI starting with `/images/`, the server will send files from the `/data/images` directory. \n\n## Tasks of Nginx\n### Serving Static Content\n- Nginx can be configured to serve static content, such as HTML and images.\n- this involves setting up of a server block inside the http block with two location blocks.\n\t- multiple server blocks are able to be added, each specifying a different port.\n```\nhttp {\n\tserver {\n\t\t\n\t}\n}\n```\n\n### Reverse Proxy Server\n- When Nginx proxies a request, it sends the request to a specified proxied server, fetches the response, and sends it back to the client\n\t- it is possible to proxy requests to another HTTP server (eg. another Nginx server) or to a non-HTTP server (eg. express.js)\n\t\t- We use a specified protocol like FastCGI to do this\n- We can establish a proxy server by using the `proxy_pass` directive within a *location block*.\n\t- The value of `proxy_pass` is the address of the proxy server:\n```\nlocation /some/path/ {\n    proxy_pass http://www.example.com/link/;\n}\n```\n- In this config, all requests processed to `/some/path/` to be sent to the proxy server at `http://www.example.com/link/`.\n\t- ex. the request with the URI of `/some/path/page.html` will be proxied to `http://www.example.com/link/page.html`\n- to pass a request to a non-HTTP server, the appropriate `*_pass` directive should be used\n\t- ex. `fastcgi_pass`\n\n## Debugging\n- upon changing nginx.conf, we need to reload the nginx server with `nginx -s reload`.\n- logs are stored at either `/usr/local/nginx/logs` or `/var/log/nginx`\n\n### Pitfalls\n- A *root directive* should occur outside of the location block. We can then use another *root directive* within a *location block* if we want to override it.\n\t- Conversely, if you were to add a root to every location block then a location block that isn’t matched will have no root. Therefore, it is important that a root directive occur prior to your location blocks, which can then override this directive if they need to.\n- [source](https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/)","n":0.032}}},{"i":457,"$":{"0":{"v":"Conf","n":1},"1":{"v":"\nnginx.conf building blocks \n  - worker process    : should be equal to number cores of the server (or auto)\n  - worker connection : 1024 (per thread. nginx doesn't block) \n\n  - rate limiting     : prevent brute force attacks.\n  - proxy buffers     : (when used as proxy server)limits how much data to store as cache\n                         gzip /brotil or compression\n  - upload file size  : it should match php max upload size and nginx client max body size.\n  - timeouts          : php to nginx communication time.\n  - log rotation      : error log useful to know the errors and monitor resources\n  - fastcgi cache     : very important to boost the performance for static sties.\n  - SSL Configuration : there are default setting available with nginx itself \n                        (also see ssl performance tuning).\n\nExample nginx.conf: \n```\nuser www-data;                                   \nload_module modules/my_favourite_module.so;      \npid /run/nginx.pid;\n                                                    | Alternative global config for \n                                                    | [4 cores, 8 threads, 32GB RAM] \n                                                    | handling  50000request/sec\n                                                    |\nworker_processes auto;                           | worker_processes 8;\n                                                    | worker_priority -15;\ninclude /etc/nginx/modules-enabled/*.conf;       | \nworker_rlimit_nofile 100000;                     | worker_rlimit_nofile 400000;                                  \n                                                    | timer_resolution 10000ms;\n                                                    |\nevents {                                         | events {\n    worker_connections 1024;                       |     worker_connections 20000;                       \n    multi_accept on;                               |     use epoll;\n}                                                |     multi_accept on;\n                                                    | }\n\nhttp {               ←  global config            \n    index index.php index.html index.htm;          \n    # Basic Settings                               \n\n    sendfile on;                                   \n    tcp_nopush on;\n    tcp_nodelay on;\n    sendfile_max_chunk 512;\n    keepalive_timeout 300;\n    keepalive_requests 100000;\n    types_hash_max_size 2048;\n    server_tokens off;\n\n    server_names_hash_bucket_size 128;\n    # server_name_in_redirect off;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    ##\n    # SSL Settings\n    ##\n\n    #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE\n    #ssl_prefer_server_ciphers on;\n    #rate limit zone\n\n    limit_req_zone $binary_remote_addr zone=one:10m rate=3r/m;\n    #buffers\n\n    client_body_buffer_size 128k;\n    client_max_body_size 10m;\n    client_header_buffer_size 32k;\n    large_client_header_buffers 16 256k;\n    output_buffers 1 32k;\n    postpone_output 1460;\n    #Porxy buffers\n    proxy_buffer_size 256k;\n    proxy_buffers 8 128k;\n    proxy_busy_buffers_size 256k;\n    proxy_max_temp_file_size 2048m;\n    proxy_temp_file_write_size 2048m;\n\n    ## fast cgi PHP\n    fastcgi_buffers 8 16k;\n    fastcgi_buffer_size 32k;\n    fastcgi_connect_timeout 300;\n    fastcgi_send_timeout 300;\n    fastcgi_read_timeout 300;\n    #static caching css/js/img\n\n    open_file_cache max=10000 inactive=5m;\n    open_file_cache_valid 2m;\n    open_file_cache_min_uses 1;\n    open_file_cache_errors on;\n    #timeouts\n\n    client_header_timeout 3m;\n    client_body_timeout 3m;\n    send_timeout 3m;\n\n    # Logging Settings\n\n    log_format main_ext ‘$remote_addr – $remote_user [$time_local] “$request” ‘\n    ‘$status $body_bytes_sent “$http_referer” ‘\n    ‘”$http_user_agent” “$http_x_forwarded_for” ‘\n    ‘”$host” sn=”$server_name” ‘\n    ‘rt=$request_time ‘\n    ‘ua=”$upstream_addr” us=”$upstream_status” ‘\n    ‘ut=”$upstream_response_time” ul=”$upstream_response_length” ‘\n    ‘cs=$upstream_cache_status’ ;\n\n    #access_log /var/log/nginx/access.log main_ext;\n    error_log /var/log/nginx/error.log warn;   Read more on nginx error log&common errors\n\n    ##\n    # Gzip Settings #brotil\n    ##\n\n    gzip on;\n    gzip_disable “msie6”;\n\n    gzip_vary on;\n    gzip_proxied any;\n    gzip_comp_level 6;\n    gzip_buffers 16 8k;\n    gzip_http_version 1.1;\n    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript application/x-font-ttf font/opentype image/svg+xml image/x-icon;\n    ##\n    # Virtual Host Configs\n    ##\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;   \n}\n\nserver {             ← Domain level \n    listen 0.0.0.0:443 rcvbuf=64000 sndbuf=120000 backlog=20000 ssl http2;\n    server_name example.com www.example.com;\n    keepalive_timeout         60;\n    ssl                       on;\n    ssl_protocols             TLSv1.2 TLSv1.1 TLSv1;\n    ssl_ciphers               'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS:!RC4';\n    ssl_prefer_server_ciphers on;\n    ssl_session_cache         shared:TLSSL:30m;\n    ssl_session_timeout       10m;\n    ssl_buffer_size           32k;\n    ssl_certificate           /etc/letsencrypt/live/example.com/fullchain.pem;\n    ssl_certificate_key       /etc/letsencrypt/live/example.com/privkey.pem;\n    ssl_dhparam           /etc/ssl/certs/dhparam.pem;\n    more_set_headers          \"X-Secure-Connection: true\";\n    add_header                Strict-Transport-Security max-age=315360000;\n    root       /var/www;\n\n    location {         ← Directory level \n        root /var/www;\n        index index.php index.html;\n    }\n\n    location ~ .php$ {  \n    fastcgi_keep_conn on;\n    fastcgi_pass   unix:/run/php5.6-fpm.sock;\n    fastcgi_index  index.php;\n    fastcgi_param  SCRIPT_FILENAME /var/www$fastcgi_script_name;\n    include fastcgi_params;\n    fastcgi_intercept_errors off;\n    fastcgi_buffer_size 32k;\n    fastcgi_buffers 32 32k;\n    fastcgi_connect_timeout 5;\n    }\n\n    location ~* ^.+.(jpg|jpeg|gif|png|svg|ico|css|less|xml|html?|swf|js|ttf)$ { \n        root /var/www;\n        expires 10y;\n    }\n\n}\n```\n\n- /etc/nginx/conf.d/*: user defined config files\n\nSee also:\nhttps://github.com/trimstray/nginx-admins-handbook\nhttps://github.com/tldr-devops/nginx-common-configuration","n":0.044}}},{"i":458,"$":{"0":{"v":"Nextjs","n":1},"1":{"v":"\ncreate pre-rendered react websites, offered from SSR\n\njsx is rendered already into html on the server, and is sent to the client to be displayed\n- vanilla-react will do everything at runtime.\n\nhelps with SEO\n\nnext takes care of routing for us\n- we define new pages in the `pages/` directory, and next picks up on them automatically, giving us the url out of the box.\n\t- next defines a component `<Link />` we can use to handle routes\n\nnext is smart and will not re-render the same content, if it has rendered it already\n\nIf we had a function in a Next.js component that referred to the `window` object, we would get errors. This is because that code is being run on the server, which of course has no concept of the browser's global variables. However, if we were to use `window` in the `useEffect` call, we would indeed get access to the `window` object. This shows that `useEffect` is still done client side, even though functions are understood server-side\n","n":0.078}}},{"i":459,"$":{"0":{"v":"Styled Components in Nextjs","n":0.5},"1":{"v":"\n## SSR Styled components in Next\nStyled-components supports concurrent SSR (server side rendering) with stylesheet rehydration. The basic idea is that when your app renders on the server, you can create a ServerStyleSheet and add a provider to your React tree which accepts styles via a context API. This doesn’t interfere with global styles, such as keyframes or createGlobalStyle and allows you to use styled-components with React DOM’s various SSR APIs.:w\n","n":0.12}}},{"i":460,"$":{"0":{"v":"Rehydration","n":1},"1":{"v":"\nspec: In traditional server-side rendered applications (think Express-Pug, Ruby on Rails), each value in the HTML is generated on the server, and sent to the client. That means if the page changed, it's because the server sent some new HTML to the client. With Next, the initial render is done via the server at compile time, but the same React code used to generate that HTML is also included on the client (ie. the bundle.js is included in Nextjs, just like it is in vanilla React)\n- This compiled client-side javascript code (originally React code) gets run on the client, building up a picture of what the world should look like. Then it is compared to the HTML in the document. This process is **Rehydration**.\n\n### Render vs Rehydration\nIn a render, vanilla React detects when there are changes to state or props. When there are, React updates the DOM.\nIn a rehydration, React assumes the DOM won't change; It's just trying to adopt the existing DOM.\n\n# E Resources\n[Quality resource on rehydration. Uses the backdrop of a dynamic Navbar to explain](https://www.joshwcomeau.com/react/the-perils-of-rehydration/)","n":0.075}}},{"i":461,"$":{"0":{"v":"Prerendering","n":1},"1":{"v":"\nThere are 2 types of pre-rendering that Next offers:\n- static generations - The `about` page is identical no matter who views that page. - Server-side rendering - HTML is generated by the server on each request.\n- Client-side rendering - as vanilla React does, we can still opt to use CSR if advantageous.\n\n### Static Site Generation (SSG)\nThe idea is \"Why does the html need to be generated on the client, when we can leverage the server to do it beforehand? (ie. at build time)\"\n- this is the default, thus recommended option\n\nIf your page shows frequently updated data, and the page content changes on every request, you *must* use SSR.\n\nStatically generated pages can be cached by CDN\n\n`getStaticProps` is run at build time.\n\nExamples:\n- Marketing pages\n- Blog posts and portfolios\n- E-commerce product listings\n- Help and documentation\n\n### Server-Side Rendering (SSR)\nHere, Next.js pre-renders a page on each request. It will be slower because the page cannot be cached by a CDN, but the pre-rendered page will always be up-to-date. \n\nTo use Server-side Rendering for a page, you need to export an async function called `getServerSideProps`. This function will be called by the server on every request.\n\n#### Usage\n```ts\nfunction Page({ data }) {\n  // Render data...\n}\n\n// This gets called on every request\nexport async function getServerSideProps() {\n  // Fetch data from external API\n  const res = await fetch(`https://.../data`)\n  const data = await res.json()\n\n  // Pass data to the page via props\n  return { props: { data } }\n}\n\nexport default Page\n```\n\nWhen you run `yarn build`, it generates 1 HTML document for every route on your site. Every side page, every blog post, every store item — an HTML file is created for each of them, ready to be served up immediately.\n\n`getServerSideProps` is run on every request\n","n":0.059}}},{"i":462,"$":{"0":{"v":"Page","n":1},"1":{"v":"\n### NextPage\nWe can type them using the `NextPage` type.\n```js\nconst Page: NextPage<Props> = (props: Props) => (\n  <main>Your user agent: {userAgent}</main>\n)\n```\n\n## API\n### `getInitialProps`\nEnables SSR, allowing us to do initial data population.\n- ie. sending the page with the data already populated from the server.\n- carries SEO-related benefits\n\nThis method can be used to asynchronously fetch some data, which then gets passed through `props`\n- The first time this method runs, it will be on the server, each time the user navigates to a different route (using `next/link` or `next/router` components), `getInitialProps` will be run on the client.\n\n`getInitialProps` is being migrated away from in favor of `getStaticProps` or `getServerSideProps`.","n":0.098}}},{"i":463,"$":{"0":{"v":"Methods","n":1},"1":{"v":"\n### `getInitialProps`\nThis function is called and returns props to the react component before rendering the templates in `/pages`. This is a perfect place for fetching the data you want for a page.\n\n`getInitialProps` works only in files in the pages folder and are used for routing, i.e it will not be called for react components that are included in these pages\n\nAdding a custom getInitialProps in your `_app.js` will disable Automatic Static Optimization in pages without Static Generation.","n":0.115}}},{"i":464,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Prevent render of component until data arrives\nFor example, a header that must show either a login button or a profile dropdown depending on if the user is logged in or not. Until we know, we should render nothing:\n\nbecause the HTML file is built at compile-time in Next, every single user gets an identical copy of that HTML, regardless of whether they're logged in or not. Only once the js bundle is parsed and executed can we render the proper header state.\n```js\nconst Header = () => {\n    if (typeof window === 'undefined') {\n        return null;\n    }\n\n    const user = getUser();\n    if (user) {\n        return (\n        <AuthenticatedNav\n            user={user}\n        />\n        );\n    }\n    return (\n        <nav>\n            <a href=\"/login\">Login</a>\n        </nav>\n    );\n};\n```\n","n":0.092}}},{"i":465,"$":{"0":{"v":"Network","n":1},"1":{"v":"\n# Physical Networks vs. Overlay Networks\n## Physical Network\n- physically, there are connections between nodes in a network. The network essentially is the sum of its physical components that connect computers to one another.\n\t- physical components include: cables, wires, routers, repeaters\n\t- multiple physical networks can make up one big virtual network.\n\t\t- ex. a university spread across many buildings will have many different phsyical networks, though all computers will be able to see and communicate with one another because of the overlayed virtual network that exists on top of the physical one.\n\n## Overlay Network\n- the overlay network is created using logic at the software level to determine where the connections between the nodes are.\n- a network may have multiple virtual layers built on top of the backbone physical one. \n- while in a physical network, each node is connected to another through physical means, such as a cable. In an overlay network, nodes are connected to one another using network addresses. \n- overlay networks *encapsulate* the data before sending it over the network, and unwrapped upon reaching the destination.\n- an overlay network needs to employ a protocol that determines the rules that the hosts of a network must all abide by (IP, VPN, P2P)\n\n# UE Resources\n[Networking tutorials from Flavio Copes](https://flaviocopes.com/tags/network/)","n":0.069}}},{"i":466,"$":{"0":{"v":"Tools","n":1}}},{"i":467,"$":{"0":{"v":"Traceroute","n":1},"1":{"v":"\n# Traceroute\n- see a list of all nodes (routers and end node) that your packets traveled through on their way to an origin server. Each step is a *hop*\n\t- Also tells us how long each jump took (the response time b/w nodes) (RTT; round trip time)\n- RTT tells us how long it took to get to that node (from the previous one) and return to your computer\n\t- There are three RTT columns because the traceroute sends three separate signal packets so that we may be able to spot inconsistencies in the route.\n- The final column has the router IP\n- `*` in the traceroute means that packets were lost\n- consistency of RTTs between columns is what we are looking for when analyzing a traceroute.  \n- when we use traceroute, the packet gets sent to the first router, which sends a packet back to the source. Then the packets continue on to the next router, which again sends a packet back to the source. This pattern continues until we reach the destination node.\n\t- ex. if there are 5 routers between a source node and destination node, then the source will send 6 packets into the network, which each packet addressed to the destination node.\n\t\t- The difference of course with traceroute over a regular packet, is that when an individual router along the chain receives the packet, it does not forward it along. Instead, it sends it back to the source. In this way, the source can determine the route that the packets took to reach the destination.\n\nIssues:\n- if the RTT from one hop to another greatly increases, and continues to increase until the destination, this indicates a problem with the node that first took a long time to respond. \n\t- This often accompanies packet loss (`*`)\n- if the RTT spikes on only one node, then subsequent hops have lower RTT, that doesn't indicate an issue. It just means the slow router gave your packets a lower priority.\n- if the RTT jumps then remains consisistent at that levels, this does not indicate an issue\n- By default, traceroute uses high UDP ports for tracing hosts. Sometimes firewalls block these UDP ports. \n\t- use `-P` flag to use different protocols: (`-P ICMP`, `-P TCP`, `-P UDP`)","n":0.052}}},{"i":468,"$":{"0":{"v":"Socket","n":1},"1":{"v":"\nA socket is the interface between the application layer and the transport layer, and can be thought of as the API between the application and the network\n\nAs application developers, we can think of the socket as the dividing point between what we have control over and what we *don't* have control over.\n- realistically, the developer does have control over 2 things on the transport layer\n    1. The choice of transport protocol\n    2. transport-layer parameter, allowing control over things like maximum buffer size, maximum segment sizes etc.\n\nIt is not accurate to say that programs communicate with each other (eg. that an Express server program communicates with a Postgres server program). Realistically, the programs are running as processes on their respective hosts, and it is these processes that communicate with each other, via sockets.\n- anal: If the processes were houses, then the sockets would be the doors that enable access to the processes\n\nthe socket's uniqueness is determined by five factors:\n- the local IP address\n- the local port number\n- the remote IP address\n- the remote port number\n- the transfer protocol (TCP/UDP)\n\n* * *\n\n### How can more than 65,535 clients connect to a server?\nWhile it's true that a machine can only open 65,535 ports, this does *NOT* mean that only 65,535 clients can connect to a server at a time. A server listens only on one port and can have large numbers of open sockets from clients connecting to that one port.\n- On the TCP level the tuple (source ip, source port, destination ip, destination port) must be unique for each simultaneous connection. These 4 factors determine the socket's uniqueness.\n    - That means a single client cannot open more than 65535 simultaneous connections to a single server. But a server can (theoretically) serve 65535 simultaneous connections per client.\n\nIn practice the server is only limited by how much CPU power, memory etc. it has to serve requests, not by the number of TCP connections to the server.","n":0.056}}},{"i":469,"$":{"0":{"v":"Servers","n":1},"1":{"v":"\nAll servers need static IPs\n","n":0.447}}},{"i":470,"$":{"0":{"v":"Edge","n":1},"1":{"v":"\n# Edge Server\n- def - a server that acts as a gateway so that one network can access another\n\t- In other words, the edge server enables the two networks to communicate.\n- Therefore, the ability for clients to make a network connection to other clients is bottlenecked by the number of edge servers between the two clients;\n- Generally speaking, the farther the connection must travel, the greater the number of networks that must be traversed.\n![](/assets/images/2021-03-11-15-51-29.png)\n- \"edge\" refers to the philosophy of geographically placing the data close to the server (or proxy server) that requests it \n- Edge servers are contrasted with Origin Servers, which is your actual web server (ex. Express).\n- spec: caches want to live as close as possible to the client, while edge servers tend to live further out, closer to the internet (but still closer to the client than the origin server)\n","n":0.083}}},{"i":471,"$":{"0":{"v":"Clusters","n":1},"1":{"v":"\n# Cluster\n- a group of computers that function so closely together that you may consider them to be a single computer\n- a cluster will designate each task to a different node so that the responsibility is split\n- Each node would run its own instance of the OS\n\n### Stretched cluster\nA stretched cluster is a deployment model in which two or more host servers are part of the same logical cluster but are located in separate geographical locations.","n":0.115}}},{"i":472,"$":{"0":{"v":"Osi Model","n":0.707},"1":{"v":"\n- The OSI model is a framework for understanding how communications work in a computing system. It is an abstract representation, since no attention is paid to the implementation details of each layer. Instead, each layer simply describes its function and purpose. Put another way, it defines what input it expects, and what output it gives. \n\t- Since each layer interfaces directly with the layers above and below it, the consistence of the input and output offered by each layer is the only thing that matters (ex. as long as L1 receives 0's and 1's and delivers frames, all other details are inconsequential)\n\n## The Protocol Layers\n### L7 Application\t\n- consists of network applications and their application-layer protocols\n- primary user interface with communication system.\n- PDU - messages\n- HTTP, FTP, DNS, SMTP, POP3, SSH, IRC, TLS/SSL, NFS (network FS)\n### L6 Presentation \n- Supports the functionality of the application layer by providing services such as formatting and translation of data.\n- provide data encryption and data compression.\n- Data representation (compression, decompression) and encryption\n- ex. SSL, SSH, IMAP, FTP, TLS, MPEG, JPEG\n### L5 Session\n- Maintains the transmission path by synchronizing packets and controlling access to the medium by the Application layer.\n- controls the connections between computers\n- provides for delimiting and synchronizing of data exchange, including the means to build \n- ex. API, sockets, HTTP sessions\n### L4 Transport\n- Ensures the quality of transmission and determines the best route for transmission of data using the Network layer below.\n- concerned with providing reliable communication over an unsecured network\n- PDU - segments (TCP), datagrams (UDP)\n- goal: deliver the data to the right software application\n\t- ex. TCP, UDP\n### L3 Network \n- Finds a route for transmission of data (packets) between 2 routers/hosts, and establishes and maintains the connection between two connected nodes.\n- goal: pass data chunks over multiple connected networks\n- PDU - packet\n- ex. IPv4, IPv6, ICMP, ARP, NAT\n### L2 Data Link \n- Creates, transmits, and receives packets. Controls the Physical layer.\n- Concerned with sharing multiple access channels\n- They are used to deliver frames on a LAN\n- PDU - frames\n- goal: organize the 1s and 0s into chunks of data, and get them to the right place on the wire.\n- ex. wifi, ethernet, bluetooth, VLAN, port forwarding procol\n### L1 Physical \n- Converts data into bits for transmission and converts received bits into usable data for the layers above it.\n- PDU - bits\n- goal is to send 0s and 1s across a wire\n- ex. fiber optic, copper wire, coaxial cable, wireless, modem, repeaters, ethernet (physical portion), USB\n\n### The Internet Protocol\n- The IP stack consists of L1, L2, L3, L4, L7\n- When an HTTP request is sent, the protocol is established by piggybacking on the TCP connection that had already been made. This TCP connection is enabled by following the internet protocol. This is the point at which the internet protocol determines which routes datagrams\n- The fact that there are 2 layers that are openly missing from the internet stack poses an interesting question: why are they not there? The reason is that the internet leaves these layers up to the application developer. The application developer can use any implementation of L5 and L6 that they choose in order to achieve their goals.\n\n#### Routers\n- As data is sent out, it starts at L7 and makes its way down to L1. At L1, it is connected to the link-layer switch and goes up to L2, before going back down to L1. Then it reaches the router, which goes up to L3, then back down to L1, to be repeated depending on the number of subsequent routers. At the final router, the the router's L1 communicates with the L1 of the destination host, as it makes its way back up to L7.\n\n### Airplane trip analogy\n- As we look at the process of planning and taking an airplane, it becomes apparent that there are different layers to the entire process. In fact, each layer appears two times in the whole process— in reverse order:\n1. buy ticket\n2. check baggage\n3. load at gate\n4. takeoff on runway\n5. airplane routing (travel)\n6. land of runway\n7. unload at gate\n8. pickup baggage\n9. complain about ticket.\n\n- each layer implements some functionality, and we can see that the opposite action was performed in reverse order.\n- We can also see that each layer provides service to the layer below it.\n\t- ex. the act of checking baggage only makes sense to a ticketed person. \n\t- ex. the idea of unloading at a gate only makes sense to a person on a landed plane.\n- we notice that we can replace any layer in the model, as long as the functionality remains the same. In this way, layers are interfaces to each other, and don't care about each other's implementation— only the outcome (ie. output) it provides.\n\n### Encapsulation\n- as data travels each layer from L7 down to L1, additional information is added. As we pass the information encapsulated in the HTTP request down to L4, the transport layer takes the information of L7 and adds its own information to it. This information that was added is then used by the transport layer of the next node in the chain (likely the destination host). This process of encapsulation continues on down layer by layer until L1.\n\t- This idea of encapsulation thus demonstrates what is fundamentally different between each PDU: a message (L7) is an encapsulated datagram (L4). In other words, the datagram encapsulates the message. A datagram is fundamentally a message, plus some other information (provided by the layer). Therefore, at each layer, the PDU has 2 types of fields: header fields, and payload fields (the payload is just a packet from the layer above).\n\n### Protocol Data Unit (PDU)\n- each layer has the concept of a Protocol Data Unit, which is the format that the data exists in within the current Layer. In other words, it is what an atomic unit of data is called at each layer.\n\t- All PDUs are composed of a header and payload\n\t\n### Miscelaneous\n- the very fact that there are layers means that we can treat it as a modular chain, and swap out one L3 implementation for another (such as wifi for ethernet)\n\t- therefore L2 doesn't care if we are using IP or IPX on L3, just like L3 doesn't care if L2 uses wifi or ethernet\n- The price we pay by layering is that we have to map between 32 bit IP addresses (L3) and 48 bit MAC addresses (L2).\n\t- The ARP protocol exists to solve this very problem\n- The OS may be a participant in any or all of the layers\n\t- ex. at L1, signal processing can be offloaded to a host CPU and that requires a driver which interfaces with the operating system","n":0.03}}},{"i":473,"$":{"0":{"v":"Lan","n":1},"1":{"v":"\nIPs ending in zeros are not actual hosts, but indicate the start of a block (since real IP addresses end in a number between 1 and 254)\n- 255 reserved for masking\n\n### Subnet\n- a subnet is a logical division of an IP network\n\t- an IP network is any network that uses IP address to communicate to other devices.\n\t\t- ex. LAN, the internet, enterprise network.\n\t- you might do this for logical reasons (ex. firewalling) for physical reasons (ex. smaller broadcast domains)\n- to be useful, a router is connected to two or more IP subnets\n- IP subnets exist to allow routers to choose appropriate destinations for packets.\n- Traffic is exchanged between subnetworks through routers\n- subnetting is the process of breaking down a single IP address block into smaller subnetworks called subnets\n\t- the reason we need to subnet is to efficiently distribute IP addresses to reduce wastage\n- machines on the same subnet will have the same first 3 digits of their IP address (known as the *most significant bit-group*)\n\t- this means that the IP address is made up of 2 parts: the subnet number and the host identifier.\n- subnetting is analogous to the concept of *zoning* in city planning\n\n#### Subnet Mask\n- a subnet mask allows a computer/router to determine the portion of the IP address which refers to the network, and the portion that refers to the host machine\n\t- the network portion of an address is represented by 1s (255) in the subnet mask, and the host portion is represented by 0s\n\t- a subnet mask can also tell us the number of hosts within a network\n\t- anal. just like our home address consists of a street name (network ID) and a number (network host), the mask's job is to determine where in the IP address one begins and the other ends.\n- each octet of a subnet mask can either be 255 or 0. When the octet on the mask is 255, that means that the when trying to connect to another node, it is going to go through the router to try and resolve that IP address\n\t- ex. if the host has IP=`168.25.4.6` and mask=`255.255.255.0`, that means that it will only try and hit nodes within the LAN if the IP address starts with `168.25.4`. If the host tried to connect to `168.25.8.2`, it would default to going through the gateway (router)\n\t\t- If the mask=`255.255.0.0`, that means the host will try to hit the node locally only if the IP address starts with `168.25`\n\t\t- if at any point the 255 is \"triggered\" (ie. the external node doesn't satisfy the mask's requirement for enabling local searching), then the external node is said to be \"outside the mask\"\n- When we apply the subnet mask to the IP address, we get the *routing prefix*\n- ex. we have a network with IP=`135.68.2.0`. In that network are 2 host computers with IP=`135.68.2.1` and IP=`135.68.2.2`. The network portion of each host's IP is `135.68.2`\n\t-  a 0 at the end of an IP address indicates that it is a network address\n- anal. house addresses are composed of a streetname and a number\n- with a mask of 255.255.255.0, since only one of the octets of bits refers to the hosts in a network, there can only be 256 nodes within that network\n\t- if the mask were 255.255.0.0, there could be 65,536\n\t- in reality we have to subtract 2 from that total, since there are 2 reserved IP addresses: the Network ID and the Broadcast IP address.\n- a subnet mask is needed to tell us how many computers a node within a subnet has to go through before it gets to another node on that same network (ex. the computers at a LAN party).\n\t- When trying to send a message across the network, the subnet mask will tell us if we can access that node via the current network, or if we can only connect to it through the router (ie. it is on the internet)\n- Another way of looking at the subnet mask is that it tells us which octets of an IP address are devoted to telling us which network the nodes are located in.\n\t- when a mask is 255.255.255.0, then anything is on the same network as the host if the first 3 octets of the IP address are identical.\n\n*Default Subnet Masks*\n- There are 3 classes:\nClass A - 255.0.0.0\nClass B - 255.255.0.0\nClass C - 255.255.255.0\n\n### Modem\n- portmanteau of \"modulator-demodulator\"\n- purpose is to convert data from a digital format to a format that is conducive to transmission over a physical layer\n- Modems can be used with almost any means of transmitting analog signals, from light-emitting diodes to radio\n- Any communication technology sending digital data wirelessly involves a modem.\n\t- ex. satellite, WiFi, WiMax, mobile phones, GPS, Bluetooth and NFC.\n\n### Demilitarized Zone (DMZ)\n- a subnetwork that sits between the network and the router. the DMZ exists so that we can control exposure to certain parts of a network. Anything in it is exposed to untrusted networks (like the internet). The idea is that the DMZ is all that can be accessed externally, so the purpose is added security\n\n### Dynamic Host Configuration Protocol (DHCP)\na server hosted within a network that dynamically assigns IP addresses\n- it is therefore the service that manages the IP address pool in a network\n\n**DHCP Reservation** - set aside an IP address and map it to a specific MAC address. Whenever a device with the specified MAC address enters the network, it is assigned with the specified IP address.\n\n#### Static IP\n- on the device receiving IP assignment (ie. not the router), we can bypass the dynamic assigning of IP addresses by creating a entry in `/etc/dhcpcd.conf` (Linux)\n- Generally accepted best practice is to assign a static address on the device that needs it - ex. a NAS - rather than rely on a DHCP server to give you the address you are expecting.\n\t- This shows that there are 2 ways to achieve a predictable IP address for devices on a LAN\n\n##### Linux approach\nrun `hostname -I`\nappend `ip=YOURIP` to end of `/boot/cmdline.txt`\n\n### Localhost vs LAN\n- localhost (an alias for `127.0.0.1`) is an IP address that is used to test the computer's networking protocols without actually using the LAN that the computer is attached to.\n\t- Called a loopback address, and it is analogous to hooking up an outbound cable out of one end of a machine and into the other (as opposed to that cable being hooked up to a router).\n- On the other hand, the private IP (`192.168.X.X`) is created by your network (the router), and allows us to communicate with other devices in the same network.\n- anal. Imagine there were 2 postal services: 1 for your street (local), and one for the whole world (global). When you write a letter, you give it to your local postman, and he connects it to the global postal network. If you are sending your letter to the localhost network, then it is like writing a letter and handing it to yourself. If you are sending your letter to the `192.168` address, then it is like putting it in a mailbox, where your local postman proceeds to deliver it to you.\n\t- In this analogy it's important to note that the letter never reaches the global post network (ie the internet).\n\n## Routing Schemes\n### Broadcast\ntransfer a message to all recipients simultaneously. broadcasting refers to transmitting a packet that will be received by every device on the network\n- can exist aso low as L2\n\t- meaning we can broadcast on ethernet\n- broadcasting is not implemented on IPv6, since it is considered wasteful to broadcast a message to all nodes, when perhaps only a few need to know about it.\n\t- Instead, IPv6 uses multicast\n\n### Multicast\ngroup communication where data transmission is addressed to a group of destination computers simultaneously.\n- can be 1:many or many:many distribution\n- may exist on L7 (application) or L3 (network assisted).\n\t- if done on L3, the sender of the data can send it in a single transmission.\n- multicasting limits the pool of receivers to those that join a specific multicast receiver group.\n\n* * *\nthe combination of IP and port is an interesting thing. We have the IP address of a node (L3), and a TCP port that corresponds to a particular service running on that machine. This demonstrates how both are addresses for two different layers: L3 and L4. In other words, when the data is heading for the IP address, it is a packet. When it is heading for the port, it is a segment.\n\n\n* * *\n## UE Resources\n[explanation of VLAN](https://serverfault.com/questions/188350/how-do-vlans-work)","n":0.027}}},{"i":474,"$":{"0":{"v":"Router","n":1},"1":{"v":"\n# Routers\n- routers are unique in that they have 2 IP addresses: a public WAN-facing one, and a private LAN-facing one.\n- Routers perform the \"traffic directing\" functions on the Internet, forwarding packets from one network to another\n\t- in other words, data packets are forwarded through the networks of the internet from router to router until they reach their destination computer (with routing algorithms determining the choice of route.)\n\t- A router is like a railroad junction: from one incoming track, there are multiple possible destinations, so which one does it choose? You have to configure that (configurable through port forwarding)\n- Each router has a prior knowledge only of networks attached to it directly\n- routers gain knowledge of the topology of the network when the routing protocol shares the information of who the router's neighbors are. Each neighbor then shares this information with *their* neighbors, and so on until the whole network is revealed. \n- routing protocols are layer management protocols for L3, regardless of their transport mechanism\n\t- in other words, data may very well travel over L2 or L4\n- **DSL router** - a residential-grade router designed to create LANs and connect them to a WAN (which is provided by the ISP)\n\t- aka residential gateway\n- a residential router uses a modem to connect the LAN to the WAN\n\n### How routers determine which route to take\n- Each packet contains an address in its header, which has a hierarchical structure (like a postal code)\n\t- Each router has a forwarding table which is used to compare against a part of the address to determine the next step in the packet's journey\n\t- Specifically, the routing table is like a hashmap that makes portions of the destination address to outbound links.\n- each time a packet arrives at a router, the router consults its routing table.\n\t- This routing table contains the network ID and the host ID\n\n### Routing Table\nWhenever a node needs to send data to another node on a network, it must first know where to send it. If a direct connection can't be made, then the data has to be sent via other nodes along a route to the destination node.\n- Imagine a node somewhere along this chain receives a packet of data. It has no idea where it came from or where it's going. A routing table solves this problem, as it gives each node in the chain the address for the destination node. \n\t- Effectively, the router says \"I don't know how to deal with 192.168.0.34, but I know that 192.168.0.254 (a router) knows, so if I get a packet destined for that address, I'll just pass it along to that router, since he knows how to deal with it.\" \n- A routing table is a database that keeps track of paths and uses these to determine which way to forward traffic.\n- A routing table is a data file in RAM that is used to store route information about directly connected and remote networks.\n\n* * *\n\n### Core routers\n- core routers are the supercomputers of the internet\n- designed to operate on the internet backbone, as opposed to on the edge of a network (edge router, ex. home network).\n- the core router's purpose is to forward ip packets along. \n- edge routers connect to core routers\n\n* * *\n\n### Seeing all nodes on a local area network:\nWe can see all nodes on a local area network with:\n- `sudo nmap -sn 192.168.1.0/24`\n\t- the 24 is CIDR notation, signifying that we will scan from 192.168.1.0 to 192.168.1.255\n\t\t- the inclusion of 24 means we are scanning an *address block* ^RUsJkF0C\n","n":0.041}}},{"i":475,"$":{"0":{"v":"Port Triggering","n":0.707},"1":{"v":"\n# Port Triggering\n- Generally, port triggering is used when the user needs to use port forwarding to reach multiple local computers. \n\t- port triggering is also used when applications need to open incoming ports that are different from the outgoing port.\n- Port triggering is used by network administrators to map a port or ports to one local computer. \n\t- Port triggering is considered to be dynamic because ports are opened when they are needed and closed when they aren’t in use.\n- When using port triggering, the router is used to monitor traffic within the network. The user specifies a trigger port that sends outbound data. The router then logs the IP address of computers that have sent traffic to that port. The router then opens an incoming port or ports before forwarding the traffic to that location.\n","n":0.085}}},{"i":476,"$":{"0":{"v":"Port Knocking","n":0.707},"1":{"v":"\n# Port Knocking\n- port knocking is a technique to externally open a port on a firewall.\n- This is done by externally generating a connection attempt on a set of prespecified closed ports. Once the correct sequence of connection attempts is received, the firewall rules are dynamically relaxed to allow the external host to connect over the specified port. \n\t- this is similar to a secret handshake\n- to implement port knocking, we implement a daemon that watches the firewall log for connection attempts. If the attempted sequence is correct, then the daemon sends instructions to modify the firewall rules for the external host.\n\t- daemon examples: *knockd*\n- anal. Imagine we had a 9 pane glass window in a house, and we have a friend on the inside, while we remain on the outside. In private, my friend and I agree that if I tap each pane in a sequence, then he will open up the 3rd pane. For instance, let's imagine the code is: 1, 5, 9, 2, 2, 3. If I don't tap the panels in order, then my friend will simply ignore me. If I am correct however, he will open the 3rd panel\n\t- here, each panel represents a port, I represent an external host, and my friend represents the router/firewall of the network I am trying to access.\n","n":0.068}}},{"i":477,"$":{"0":{"v":"Port Forwarding","n":0.707},"1":{"v":"\n# Port Forwarding\nthe act of directly forwarding data packets from one interface (or physical link) to another. This is not really proxying; instead, think of railroad tracks: without port forwarding the track ends at a station, but with port forwarding the track seamlessly continues to the next station.\n- If I were to try and ssh into my home computer from halfway around the world by using my home's public IP address, my home router would receive the request, and not know what to do with it, since the private IP address would then be needed to complete the send.\n- We can specify to a router \"hey, when you receive requests from the internet with ssh (port 22), I need you to pass them along to 192.168.1.74\" \n\t- We give the router a forwarding IP address (the internal IP that the packets are destined for) and a port. The router external IP and port number together serve as a unique identifier, and we specify what happens when the router receives the combination  \n\t\t- \"upon receiving a request on port 8000, forward that request on to 196.168.1.74\"\n- we need to specify an **external port**, which is the port on the router that is open and facing the internet.\n\t- some numbers will be in use already, such as services from email and web server.\n\t- pick port above 5000\n- Port forwarding settings are found in the router (using the modem's IP) \n\n## External port range\n- normally identical to the internal port range. For security purposes, we may change this.\n- We may not like the fact that people on the outside can see what ports are being used. \n\t- ex. 5432 is the normal port for postgres. Recognizability like this introduces security issues.\n- To get around this issue, we can employ external port ranges. We can specify 11111 as the external port, and 5432 as the internal port, and now whenever a request comes in with port 11111, it will get forwarded to the specified internal IP and the internal port. \n\n## Internal port\n- port of the service running on the internal IP host. \n\t- ex, we are running an express server on port 8000\n- we may want to expose port 22 (SSH), port 21 (FTP) \n\n# Resources\n[Caddy Reverse Proxy: Setting up for home network](https://caddy.community/t/using-caddy-as-a-reverse-proxy-in-a-home-network/9427)","n":0.051}}},{"i":478,"$":{"0":{"v":"Nat","n":1},"1":{"v":"\n# Network Area Translation (NAT)\n- a NAT allows us to map a router's public internet-facing IP address to 1+ local IP addresses on a LAN\n\t- the NAT lives on the router.\n- the NAT was invented to solve a problem presented by the IPv4 protocol – a shortage of IP addresses\n- when you are at a home network and access a webpage, the request goes from your local machine to the router, where the router translates your machine's IP address to the router's address. Therefore, the server that you connected to only sees your router's IP address as the origin of the request. \n- The NAT is the precise reason why if you search something on Google, the results don't show up on your dad's computer. \n- A side effect of NAT is that machines on the internet cannot initiate communications to local machines; they can only respond to communications initiated by them\n\nNAT Table\n- allows devices on a private network to access a public network\n- each row in the table maps one private address to one public address.\n- When the router receives an outbound request from a host in the LAN, it changes the request headers to have the public IP of the router, and it creates an entry in the NAT table.\n\t- When an external request comes in from the internet to the private network, the router needs to know where to forward those packets, so it looks in the NAT Table to find out which host to send it to. \n- each pairing of our private host IP and external public IP is called a *connection*\n\n## Hole Punching\n- hole punching is a technique to establish a direct connection between two parties, where one or both are behind firewalls using NAT\n- To punch a hole, each client connects to an unrestricted third-party server that temporarily stores private and public IP address and port information for each client. The server then relays each client's information to the other, and using that information, each client tries to establish direct connection; \n\t- once there is a successful connection using valid port numbers, each router accepts and forwards the incoming packets on to the host node on the LAN\n- Hole punching is agnostic to which layer the hole punching is performed at. Therefore, there are different types of hole punching occurring at different layers:\n\t- ICMP hole punching\n\t- UDP hole punching\n\t- TCP hole punching\n- Technologies that use hole punching include:\n\t- VoIP, online games, P2P networking, Skype\n\nWhy is this needed?\n- networked devices with privately/publicly available IP addresses can connect easily. However, when one or both of the hosts are behind different firewalls, we need to implement hole punching to make the connection \n","n":0.047}}},{"i":479,"$":{"0":{"v":"Isp","n":1},"1":{"v":"\n## Networking at ISP Level\n### Peering\nPeering involves two IPSs coming together to exchange traffic with each other freely, and for mutual benefit.\n- Normally, ISPs of competing size have peer-agreements in place, ensuring that all traffic is forwarded freely, since each ISP will make money from their customers anyway\n\n### Transit\nContrasted with Peering, this is when a bigger ISP charges a smaller ISP for the right to use its network in order to have access to the larger internet\n\n* * *\n\nAn ISP is not necessarily a telecommunications company. In fact, it can be a university, or a private company providing internet access directly to its employees.\n\n## Tiers\n![](/assets/images/2021-03-11-17-31-26.png)\n### Tier-1 ISP\n- known as Internet Backbone networks.\n\t- ex. Sprint, Verizon, AT&T etc.\n- link speeds are often higher (sometimes as fast as 2.5-10 Gbps)\n- They are directly connected to all other tier-1 ISPs\n- They are connected to a large number of tier-2 ISPs\n- They are international in coverage.\n\n### Tier-2 ISP\n- usually have regional or national coverage\n- connects to only a few of the tier-1 ISPs.\n- routes traffic through one of the tier-1 ISPs to gain access to global internet.\n\t- pay tier-1 ISPs for this right.","n":0.073}}},{"i":480,"$":{"0":{"v":"Internet","n":1},"1":{"v":"\n# What is the Internet?\nThe internet is a network of networks. \n- the first \"network\" in this phrase are the ISPs\n\nThe Internet is a collection of separate and distinct networks\n\t- The relationship between these networks is defined by one of the following:\n\t\t1. Transit (or pay) – The network operator pays money (or settlement) to another network for Internet access (or transit).\n\t\t2. Peer (or swap) – Two networks exchange traffic between their users freely, and for mutual benefit.\n\t- Therefore, in order for a network to reach any specific other network on the Internet, it must either:\n\t\t1. Sell transit (or Internet access) service to that network (making them a 'customer'),\n\t\t2. Peer directly with that network, or with a network which sells transit service to that network\n- the internet is based on the principle that any Internet user can reach any other Internet user as though they were on the same network (*global reachability*)\n\t- Therefore, any Internet connected network must by definition either pay another network for transit, or peer with every other network which also does not purchase transit.\n- Think of the internet as a huge tree with lots of leaves and branches. Your router is the stem of a single leaf (your home). It connects that leaf to the rest of the tree. It doesn't need to know where everything else is, it just needs to know how to transfer data between the twig it is on and the leaf it is connected to. The twig knows what it's connected to, and so on. Some parts of the tree connect directly to many other parts. For example, the trunk is connected to every branch. But, it still has to pass information through those branches and rely upon them to get it to the right twig and leaf.\n\nThe Internet is literally a network of networks, and it’s bound together by BGP. BGP allows one network (say Facebook) to advertise its presence to other networks that form the Internet. As we write Facebook is not advertising its presence, ISPs and other networks can’t find Facebook’s network and so it is unavailable.\n\nThe individual networks each have an ASN: an Autonomous System Number. An Autonomous System (AS) is an individual network with a unified internal routing policy. An AS can originate prefixes (say that they control a group of IP addresses), as well as transit prefixes (say they know how to reach specific groups of IP addresses).\n\nCloudflare's ASN is AS13335. Every ASN needs to announce its prefix routes to the Internet using BGP; otherwise, no one will know how to connect and where to find us.\n\n\n\n## How data moves through a network of links and switches\nThere are two approaches to moving data through a network of links and switches: **circuit switching** and **packet switching**\n\n### Circuit Switching\n- with *circuit-switched networks*, the resources needed along a path are reserved for the duration of the communication session. this is less efficient, since the circuit is still reserved even when no data is transmitted (ex. on phone call, the line is reserved even when no one is talking)\n\n### Packet Switching\n- with *packet-switched networks*, the resources are not reserved, and the session's messages use the resources on demand, and therefore may have to wait for access to a communication link\n\t- here, resources would be buffers and the link transmission rate\n- **Packet Switching** is the process of grouping data to be placed into a packet and sent over a network. \n- a **packet switches** comes in two types: routers and link-layer switches.\n\t\n### Examples\n- anal: We have 2 restaurants: one that takes reservations and one that doesn't. The first has more initial setup, since we have to call to make the reservation— though there is less work to do once we arrive (circuit-switched). Meanwhile, the second takes less time to setup since we don't have to phone ahead, but we have to wait longer once we actually arrive at the restaurant (packet-switched).\n\t- In the non-reservable restaurant, the time spent waiting to get a table is analogous to a *queueing delay* that affects the packets. *Packet loss* would be analogous to arriving at the restaurant and encountering a big line and subsequently being asked to leave.\n- ex: telephone networks are circuit-switched networks, since of you want to send data over a line, you must first establish a connection between sender and receiver. also, once a connection is made, a constant transmission rate is reserved for the duration of the connection\n- ex: the internet on the other hand is a packet-switched network, since when data is sent over a network no bandwidth is reserved. if one of the links is congested because other packets need to be transmitted at the same time, and our packet will have to wait in a buffer at the sending side of the transmission link\n","n":0.035}}},{"i":481,"$":{"0":{"v":"Web","n":1},"1":{"v":"\nA web page consists of objects (HTML file, JPEG images etc), which is addressable with a single URL.","n":0.236}}},{"i":482,"$":{"0":{"v":"Switch","n":1},"1":{"v":"\n## Network Switch\n- While routers server as an interface for devices at L3, a network switch uses MAC addresses to forward packets at L2 to a device.\n- ex. Switches for Ethernet are the most common form of network switch\n- switches are built into residential routers\n","n":0.149}}},{"i":483,"$":{"0":{"v":"Smb","n":1},"1":{"v":"\nSMB is a communication protocol for providing shared access to files, printers, and serial ports between nodes on a network\nSBM is a proprietary protocol used by the Microsoft Windows network file system.","n":0.177}}},{"i":484,"$":{"0":{"v":"Repeater","n":1},"1":{"v":"\n## Repeater\n- a networked device that broadcasts the same data out of each of its ports and lets each device decide what data they need\n- since repeaters amplify a signal, they need a source of electric power\n- Repeaters operate on L1, since they do not try to interpret the data being transmitted\n- anal. in Lord of the Rings, Gondor has a series of beacons that can be lit to communicate to the whole kingdom that war has been declared. \n\t- Therefore, each repeater (beacon) receives a signal and retransmits it (by lighting up their own beacon)\n","n":0.102}}},{"i":485,"$":{"0":{"v":"P2p","n":1},"1":{"v":"\n# P2P\n- P2P networks works don't have a server, which would provide centralized coordination in a traditional client-server model Instead, it is a distributed application architecture that partitions tasks or workloads between peers\n- Each peer has equal privileges and each has equal power.\n- each peer designates a porion of their resources as available for consumption by other nodes in the network. \n\t- ex. can share resources like processing power, disk storage or network bandwidth\n- Peers are both suppliers and consumers of resources, in contrast to the traditional client-server model in which the consumption and supply of resources is divided. Put another way, each node acts as both a client and a server.\n- P2P networks implement an overlay network, whose nodes are a subset of the nodes found in the physical network.\n- data is still exchanged over TCP/IP, but P2P nodes can communicate directly at L7 by using the logical overlay links (each logical overlay link corresponds to a physical link of the underlying network.)\n- ex. WiFi Direct, Skype\n- Because there is no server, other strategies must be taken to accomplish tasks that would otherwise be handled by the server, such as login\n\t- To carry out login duties, a P2P network has what's called a Login Server\n\n## Place in history\n- P2P networks became popular with Napster, because the more traditional way to use the internet is more heavily regulated and corporately-owned. This means that Napster could skirt a lot of the authorities on the matter. \n","n":0.064}}},{"i":486,"$":{"0":{"v":"Ip","n":1},"1":{"v":"\n# Internet Protocol\nIP addresses consist of 32 bits, which is why the address is broken down into 4 parts of 8 bits each.\n- 255 represents 8 bits.\n\n### IP Address Spaces\n- there are 2 main IP Address Spaces: public and private.\n\t- public are routable on the internet, meaning every device on the internet needs its own pubic IP\n- the public address space is further divided into 5 classes:\nClass A\t0.0.0.0 – 126.255.255.255\nClass B\t128.0.0.0 – 191.255.255.255\nClass C\t192.0.0.0 – 223.255.255.255\nClass D\t224.0.0.0 – 239.255.255.255\nClass E\t240.0.0.0 – 255.255.255.255\n\t- A,B,C - devices directly connected to internet\n\t\t- ex. L3 switches, routers, firewalls, servers\n\t- D - multicast traffic\n\t- E - experimental\n- The private address space is divided into 3 classes:\nClass A—10.0.0.0/8 network block\t10.0.0.0 – 010.255.255.255\nClass B—172.16.0.0/12 network block\t172.16.0.0 – 172.31.255.255\nClass C—192.168.0.0/16 network block\t192.168.0.0 – 192.168.255.255\n\n### Address block\n![[dendron://code/network.lan.router#seeing-all-nodes-on-a-local-area-network,1]]","n":0.088}}},{"i":487,"$":{"0":{"v":"Bridge","n":1},"1":{"v":"\n## Bridging\n- A network bridge is a networking device that creates a single network from multiple networks.\n\t- This process is called *network bridging*\n- Bridging differs from routing, since routing allows multiple networks to communicate independently and yet remain separate.\n\t- On the other hand, bridging connects two separate networks as if they were a single network\n- Occurs at L2","n":0.131}}},{"i":488,"$":{"0":{"v":"VPN","n":1},"1":{"v":"\n# VPN\na VPN connects your PC, smartphone, or tablet to another computer (called a server) somewhere on the internet, and allows you to browse the internet using that computer’s internet connection\n- So, instead of connecting to the internet via your ISP, you connect to it via your VPN\n- so is it basically a network with a dedicated server that executes the web searches, and delivers that content to the client\n- the VPN is the thing on the server than enables the network to exist\n\nA VPN is similar to a proxy server, but where a proxy server can only redirect web requests, a VPN connection is capable of routing and anonymizing all of your network traffic.\n- also, a VPN works on the operating system level, meaning that it redirects all your traffic, whether it’s coming from your browser or a background app.\n\nex. If a company were to have 2 branches, they would not be able to communicate with one another with only their private IP addresses. They could choose to connect through the internet, but that would not be preferable, since traffic should be limited to those in the company. Instead, the company can use a VPN to bridge the two private networks. \n- alternatively, an IP tunnel can be used.\n- with either method, the result is that we are encapsulating the packets within a protocol layer during trasmission across the public network.\n","n":0.066}}},{"i":489,"$":{"0":{"v":"Nas","n":1},"1":{"v":"\n- access NAS with `http://192.168.0.101:5000/`\n\n# Resources\n[nas solution that contains services like SSH, FTP etc](https://www.openmediavault.org/)","n":0.267}}},{"i":490,"$":{"0":{"v":"Mysql","n":1},"1":{"v":"\n### Users\n- list all users - `SELECT User FROM mysql.user;`\n- connect as user \"monica\" - `mysql -u monica -p`\n- change user password - `ALTER USER 'userName'@'localhost' IDENTIFIED BY '<my-password>';`\n\n### Databases\n- change database - `use monica;`\n\n### Tables\n- show tables - `show tables;`\n- drop table - `drop table <table-name>`\n- show columns - `show columns from <table-name>`","n":0.136}}},{"i":491,"$":{"0":{"v":"Mongoose","n":1},"1":{"v":"\nNote: do not use arrow functions in mongoose, since it prevents binding\n# Hooks (a.k.a. *Middleware*)\nHooks are useful for atomizing model logic and avoiding nested blocks of asynchronous code.\nOther use cases:\n- complex validation\n- removing dependent documents\n    - (removing a user removes all his blogposts)\n- asynchronous defaults\n- asynchronous tasks that a certain action triggers\n    - triggering custom events\n    - notifications\n## pre-hooks\nThere are two types of pre-hooks:\n### Serial\nThe MW functions are executed one after another\n- Note: calling `next()` does not immediately stop execution of the function. To do this, we would need to call `return`:\n```\nvar schema = new Schema(..);\nschema.pre('save', function(next) {\n  if (foo()) {\n    console.log('calling next!');\n    // `return next();` will make sure the rest of this function doesn't run\n    /*return*/ next();\n  }\n  // Unless you comment out the `return` above, 'after next' will print\n  console.log('after next');\n});\n```\n### Parallel\nThe hooked method does not get executed until `done` is called by each middleware:\n```\nvar schema = new Schema(..);\n\n// `true` means this is a parallel middleware. You **must** specify `true`\n// as the second parameter if you want to use parallel middleware.\nschema.pre('save', true, function(next, done) {\n  // calling next kicks off the next middleware in parallel\n  next();\n  setTimeout(done, 100);\n});\n```\n## post-hooks\nFunctions that are executed *after* the hooked method and all of its `pre` middleware have completed.\n\nDo not directly receive control flow\n- i.e. no `next` or `done` callbacks are passed to it\n\n`post` hooks are a way to register traditional event listeners for these methods\n- ex. when the `save` pre-hook happens, this function will execute:\n```\nschema.post('save', function(doc) {\n  console.log('%s has been saved', doc._id);\n});\n```\n\n## Types of middleware\n### Document MW\n`this` refers to the document itself\n\nHook methods:\n- `init`\n    - initialize a document without setters\n    - Called internally after a document is returned from mongodb.\n- `validate` (a.k.a. *pre-save*)\n    - Executes registered validation rules for this document.\n    - if a validation rule is violated, save is aborted\n- `save`\n    - Saves this document\n    - calling this (as a pre-hook) will trigger `validate`, though `validate` will execute before `save`\n- `remove`\n    - Removes the document from database  \n### Model MW\n`this` refers to the model (schema)\n### Query MW\n`this` refers to the query\n- `count`\n- `find`\n- `findOne`\n- ...\nMongoose will not execute a query until `then` or `exec` has been called on it. The power of this comes when we want to build complex queries (ex. using `populate`/`aggregate`)\n- Note: `.then()` in Mongoose are not actually `promises`. If you need a fully-fledged promise, use `.exec()`\n```\nUser.find({ username }) // will not execute\n\n// callback\nUser.find({ name: 'John' }, (err, res) => {}) // Will execute\n\n// .then()\nUser.find({name: 'John'}).then(); // Will execute\n\nPromise.all([User.find({name: 'John'}), User.find({name: 'Bob'})]) // Will execute all queries in parallel\n\n// .exec()\nUser.find({name: 'John'}).exec(); // Will execute returning a promise\n```\n### Aggregate MW\nAggregate MW executes whe you call `exec()` on an aggregate object\n`this` refers to the aggregation object (`<Model>.aggregate`)\n\nIf any middleware calls `next()` or `done()` with an argument of type `Error`, the flow is interrupted and the error is passed to the callback\n\n# Populate\nLets us reference documents in other collections\n- similar to JOIN in SQL dbs\n\n*population* is the process of automatically replacing the specified paths in the document with document(s) from other collection(s).\n\nIn models, we give the `ref` option to a field (property within a document) to indicate which model to use during population.\n```\nconst userSchema = new mongoose.Schema({\n    username: String,\n    posts: [{\n        type: mongoose.Schema.Types.ObjectId, //an array of object ids\n        ref: 'Post' //which model to use\n    }]\n})\n\nconst postSchema = new mongoose.Schema({\n    content: String,\n    author: {\n        type: mongoose.Schema.Types.ObjectId,\n        ref: 'User'\n    }\n})\n```\nLater, when we are within our controller, we will be using `.populate()` on the fields that have `type = mongoose.Schema.Types.ObjectId`:\n```\nUser.findOne({ username: 'Kyle' })\n    .populate('posts') //we want populate to work with the posts field in the user collection\n    .exec((err, posts) => { // similar to .then()\n        if (err) return err\n        console.log('populated user:', posts)\n    }\n```\nThis query will return the specific document within the user collection where `username = 'Kyle'`, and it will *populate* the posts field with all posts made by 'Kyle':\n```\n{\n    _id: 123,\n    username: 'Kyle',\n    posts: [\n        \"s8fm39m\",\n        \"c83ncm8\",\n        \"w822m02\"\n    ]\n}\n```\n# Schemas\n## Instance Methods\nHere, instance refers to the document (since a document is an instance of a model)\n\nInstance methods are defined like so:\n```\n// define a schema\nvar animalSchema = new Schema({ name: String, type: String });\n\n// assign a function to the \"methods\" object of our animalSchema\nanimalSchema.methods.findSimilarTypes = function(cb) {\n    return this.model('Animal').find({ type: this.type }, cb);\n};\n```\nNow, all `animal` instances will have a `findSimilarTypes` method available on them:\n```\nvar Animal = mongoose.model('Animal', animalSchema);\nvar dog = new Animal({ type: 'dog' });\n\ndog.findSimilarTypes(function(err, dogs) {\n    console.log(dogs); // woof\n});\n```\nImagine we were making a Medium.com clone. Our Article model would have a field called `claps`. We could define an instance method called `clap()`, which when executed, would increment the field `claps`\n## Static Methods\nWhereas instance methods are defined on the instance (document), static methods are defined on the Model itself.\n```\n//static method\nconst fido = await Animal.findByName('fido');\n\n//instance method\nconst dogs = await fido.findSimilarTypes();\n```\nThe previous two could not be swapped (ex. `Animals.findSimilarTypes()`), since it would not make sense. \n- Since `Animals` is a model and it has no `type`. This naturally would only exist on an instance of the model\n\n## Query Helper\nInstance methods for Mongoose queries:\n```\nanimalSchema.query.byName = function(name) {\n    return this.where({ name: new RegExp(name, 'i') });\n};\nvar Animal = mongoose.model('Animal', animalSchema);\n\nAnimal.find().byName('fido').exec(function(err, animals) {\n    console.log(animals);\n});\n\nAnimal.findOne().byName('fido').exec(function(err, animal) {\n    console.log(animal);\n});\n```\n## Virtual\nLets us define `getters` and `setters` that don't get persisted to the database\n- Imagine we need a variable `fullName`, but on the `User` model, we only store `first` and `last`. The naive way would be to concatenate these 2 variables each time. Instead, lets define a virtual so that we can use this \"pseudo-field\" in our application:\n```\npersonSchema.virtual('fullname').get(function() {\n    return `${this.first} ${this.last}`\n}\n```\n\n### Accessing the parent documents from the child document\n- ex. Tour has *1:many* relationship with reviews\n- We can add a pre-hook MW function onto the tour model\n```\nparentSchema.virtual('reviews', { //the name of the virtual field\n  ref: 'Review', //the child model\n  foreignField: 'tour', //name of the field in the child model that contains the reference to the current (parent) model\n  localField: '_id' //name of the field where the ID is stored on the current (parent) model.\n});\n```\nThen, to actually populate this virtual field, we just have to use the `populate()` method within our `getTour` handler.\n\n## Nested routes\n- Having 2+ resources at the same url\n- \n\n## Improving Read Performance with Indices\n- see bottom of https://medium.com/@SigniorGratiano/modelling-data-and-advanced-mongoose-175cdbc68bb1\n- this also allows us to carry out logic such as \"each user can only review a tour one time\" \n\t- ` reviewSchema.index({ tour: 1, user: 1 }, { unique: true });`\n\n# E Resources\n[Data modeling and Referencing other collections](https://medium.com/@SigniorGratiano/modelling-data-and-advanced-mongoose-175cdbc68bb1)","n":0.031}}},{"i":492,"$":{"0":{"v":"Mongo","n":1},"1":{"v":"\n# Normalizing data\n- `normalized` - when data is normalized, it means that instead of having documents called `Folders` that has a field called `files` (an array of actual objects— not just the references to the objects), we have documents that are completely separated from one another, and they just make reference to one another.\n\t- a.k.a Referencing and Embedding\n- denormalized:\n```\nfolder = {\n  name: \"tagA\",\n  files: [\n\t\t\t  {\n\t\t\t    title: \"Tut #1\",\n\t\t\t    author: \"bezkoder\"\n\t\t\t  },\n\t\t\t  {\n\t\t\t    title: \"Tut #2\",\n\t\t\t    author: \"zkoder\"\n\t\t\t  }\n\t\t\t]\n}\n```\n- normalized:\n```\nfolder = {\n  name: \"tagA\",\n  tutorials: [\"238fnnc3\", \"edn38bsn39\"]\n}\n```\n\n## When to Normalize/Denormalize\nif we were to break \"one to many\" relationships down, we could have:\n\t- *1:few* - ex. 1 movie has a few awards\n\t- *1:many* - ex. 1 movie has potentially 1000s of reviews\n\t- *1:tonnes* - ex. 1 chat app has millions of messages\n- We need to distinguish between these relationships when we need to determine whether to normalize or denormalize\n\t- note: This granularity isn’t necessary in relational databases.\n\n### Deciding based on relationship type\n- For *1:few*, we always use denormalized (embedded) data\n- For *few:few* we can just embed\n\t- what about *few:many*?\n- For *1:tonnes* and *many:many*, almost always use normalized (referenced) data\n- *1:many* could go either way\n\n### Deciding based on how data will be interacted with\n- If the data is mostly read and does not change quickly, we should probably embed\n\t- ex. photo gallery related to a movie: once we gather the photos, we probably won’t be updating them too frequently.\n- If the data is updated frequently, we should probably reference\n\t- ex. user generated reviews and ratings of movies. Those vote counts are going to be changing all the time, and we don’t want to query the whole movie document every time a vote is cast. for this reason, it makes sense for reviews to be its own collection with its own documents that reference its parent\n- Imagine Instahop, where each tour guide is also a user. If we were to embed the tourguide directly into the `tour` document, changing any fields in the guide's document would be a pain, since we'd have to update it both in the `tour` document and the `user` document\n\t- Child referencing would be best used here\n\n- in Mongoose, `populate()` is how to denormalize the data as we are sending it to the client\n\n# Referencing other collections\n## Types of referencing\n### Child referencing (child ignorance/parent holds the child reference)\n- best for *1:Few* relationships\n\n### Parent referencing (parent ignorance/child holds the parent reference)\n- best for *1:Many*/*1:Tonnes* relationships\n\n\n### Two-way referencing\n- best for *Many:Many* relationsips\n\n- If we have a Nuggets collection and a Buckets collection, we have a many-to-many relationship between the two. We could Have a field in the Buckets model called `nuggets`, which would be an array of nugget ids. This would be sufficient for some cases, but what if we wanted to get a list of the buckets that a particular nugget belongs to, while we are looking at a nugget? To make this easier, we should have a field in the Nuggets model called `buckets`. This makes things easier in a way, but means that if we want to put the nugget in another bucket, we need to update both the document in the Buckets collection and the document in the Nuggets collection. In other words, using this schema design means we can no longer change the bucket field from the Nugget collection in a single atomic update.\n\n# Aggregation pipelines\n\n# Transaction\n- allows us to execute a series of actions, and have all changes roll back if one of the actions cannot be executed\n\t- ex. in a banking app we have 2 users. one user sends money to another. one action would be to debit the first user's account, and credit the other user's account. Without transactions, if the second action failed, then the first user would be debited while the second would not be credited.\n\n# Indexing\n- Indexing a property will greatly speed up queries where the property is compared for equality at the cost of slower insertions.\n\t- indexes come with a performance cost, so you need to be confident that you’re building the right ones to support your application.\n- When deciding what indexes to create, you need to consider a number of factors, including your query shapes, query volume, read-to-write ratio, and the size of your database\n\n## Purpose\n- Mongo implements indexes to support efficient execution of queries.\n- If there weren't indexes, Mongo would have to scan each document in a collection, and select the ones that match the query statement.\n- Since indexes exist, Mongo is able to limit the amount of documents that it must inspect as part of the querying process.\n\n# Sharded Cluster\n- spec: instead of Mongo deploying a single instance of your database (on a single computer), your database is deployed on different machines (the cluster), and are sharded\n\t- sharded means that not all of the data related to the application will be stored in a single database. Multiple will be used, each one storing different data\n- the `mongos` provides the interface between the client and the sharded cluster\n\t- From the perspective of the application, a mongos instance behaves identically to any other MongoDB instance\n- MongoDB shards data at the collection level, distributing the collection data across the shards in the cluster.\n\t- In other words, the data of a collection is partitioned and stored in different clusters\n- MongoDB uses a *Shard Key* to determine where the data should be split for storage across the different shards.\n\t- Therefore, the shard key consists of a field or multiple fields in the documents.\n\n\n# UE Resources\n- [$ variable](https://docs.mongodb.com/manual/reference/operator/projection/positional/)\n- [design patterns](http://thetechnick.blogspot.com/2016/06/mongodb-design-patterns.html)","n":0.033}}},{"i":493,"$":{"0":{"v":"Realm","n":1},"1":{"v":"\n## Partitioning\nMongo needs a way to know which data it should give the user access to. Of course, any one user should not receive all of the data in the entire application, only what is owned by them. When using RealmDB, each partition is composed of Realms.\n- A realm is a collection of Realm objects that all share a *partition key*. Each client subscribes to a different set of realms, which passively synchronize changes as network availability allows.realm\n- each partition value (ex. one userId) maps to a different realm. Therefore, Documents that share the same partition value belong to the same realm\n- Most common realms would be defined by userId or teamId, meaning all the data with the same userId will be partitioned and synced with the RealmDB on the user's phone.\n- You can combine these partitioning strategies to create applications that make some data publicly available, like announcements or an organizational chart, but restrict other data to privileged users.\n\t- To combine strategies, you can use a generic partition key field name like `_partitionKey` with a partition value structured like a query string. For example: `_partitionKey: \"user_id=abcdefg\"`\n\t\t- Structuring your partition values like a query string lets you take advantage of multiple partitioning strategies at once, providing the power of user, team, and public realms all at once\n\nSince all documents that share the same partition value also share the same permissions for each user, you should select a key whose unique values correspond with permissions within your application. Consider which documents each user needs to read, and which documents each user needs to write. What separates one user’s data from another? The concepts of ownership (which users can change which data?) and access (which users can see which data?) decide how you should partition your data.\n","n":0.058}}},{"i":494,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n# Shell commands\n- `show dbs` - list all dbs\n- `use <never-forget>` - use the Never Forget db\n- `show collections` - show all collections in the current db\n- `db.users.find()` - get all documents from the collection users\n- `db.insertOne({})`\n","n":0.164}}},{"i":495,"$":{"0":{"v":"Atlas","n":1},"1":{"v":"\n# Atlas\n- the cloud-based database-as-a-service (DBaaS) of Mongo. Basically, this solution lets Mongo take care of all the deployment details. The MongoDB instance is hosted on their servers, and we just need to use those servers, and pay a service charge for doing so. This is an alternative to self-hosting those MongoDB instances and taking care of all of those details yourself.\n\t- This is comparable to how Firebase works. You delegate your having to create a backend to Firebase, meaning Google will host your servers, and all you need to do is communicate with them.\n- Atlas will use a cloud-provider, like AWS, Azure, or Linode","n":0.098}}},{"i":496,"$":{"0":{"v":"Aggregation","n":1},"1":{"v":"\n## Aggregation\nprocess data records and return computed results.\ngroup values from multiple documents and give the ability to perform operations on the grouped data, returning a single result\n\nCan be done in 3 ways:\n[more info](https://docs.mongodb.com/manual/aggregation/#aggregation-map-reduce)\n- Aggregation pipeline\n- Map-reduce function\n- single purpose aggregation methods\n","n":0.156}}},{"i":497,"$":{"0":{"v":"Mobile","n":1},"1":{"v":"\n[Basecamp Mobile Hybrid Architecture](https://medium.com/signal-v-noise/basecamp-3-for-ios-hybrid-architecture-afc071589c25)\n","n":0.5}}},{"i":498,"$":{"0":{"v":"Memory","n":1},"1":{"v":"\n### Memory Hierarchy\n- Because of physical laws, larger storage devices are slower than smaller storage devices.\n\t- ex. a disk drive on a typical system might be 100 times larger than the main memory, but might take 10,000,000x longer to read a word from disk than from memory. Similarly a typical register file (CPU) stores only a few hundred bytes of information, as opposed to millions of bytes in main memory. However, the processor can read data from the register file almost 100x faster than from memory.\n- The main idea of memory hierarchy is that the storage at one level serves as a cache for storage at the next lower level\n\n### Memory Leak\nin languages without garbage collection, memory leaks occur when we fail to free up memory (deallocate) after a pointer no-longer serves its purpose (such as when we never reference that variable again until end of execution)\n\n### Memory Cell\nThe memory cell is the fundamental building block of memory.\nThe memory cell itself is an electronic circuit that stores a single bit of binary information (ie. 0 or 1)\n- This is achieved by sending either a high voltage signal (1) or low signal (0)","n":0.072}}},{"i":499,"$":{"0":{"v":"Stack","n":1},"1":{"v":"\n# Stack\nContrast with [[heap|memory.heap]]\n- A place in memory where variables declared by a function are stored\n- Any time we create a variable In a normal way, we are putting it on the stack\n- Every time a function declares a new variable, it is \"pushed\" onto the stack. Then every time a function exits, all of the variables pushed onto the stack by that function are deleted.\n- when a function exits, all of its variables are popped off of the stack. Thus stack variables are local in nature\n- variables are declared, stored and initialized during runtime.\n- Storage is temporary, and when The computing task is complete, the memory location will be erased\n- Data structure is linear\n- Data is physically located together (contiguous blocks)\n- Variable de-allocation is automatic\n","n":0.089}}},{"i":500,"$":{"0":{"v":"Pointer","n":1},"1":{"v":"\n# Pointer\nA pointer is a variable that stores both a memory address and the type of data that resides at that memory location.\n- Obtaining the value stored at that location is known as *dereferencing the pointer*\n    - anal: The index within a textbook has page numbers which reference pages in the book. Dereferencing the pointer would be done by flipping to that page and reading the text.\n\t- the type of the pointer tells the compiler what operations can be performed through that pointer\n- a pointer is a very thin abstraction built on top of a language's own addressing capabilities\n- Each unit of memory in a system is assigned a unique address (memory location).\n\t- A pointer is an object that stores this address.\n\t\t- This is similar in concept to how we dont store images in a database, we just store its url location\n\t- Conceptually, the fact that memory is just stored in blocks makes memory itself a very large array.\n\t- arrays work a bit differently in that the variable points to the address (location) of the first character in the array \n- pointers are used partly because they are sometimes the only way to express a computation, and partly because they usually lead to more compact and efficient code than can be obtained in other ways\n\t- They are especially efficient when used in repetitive operations\n- pointers are used for constructing *references*, which in turn are fundamental to constructing nearly all data structures\n- using a pointer to retrieve the value that is stored at the relevant memory location is called **dereferencing the pointer**\n- anal:if storage were a reference book, then the page number in the index would be the pointer, and flipping to that page and reading the text would be dereferencing that pointer.\n- the format of a pointer is dependent on the underlying computer architecture (since pointer's pertain to physical memory locations)\n- Pointers and arrays are closely related\n\t- Any operation that can be achieved by array subscripting can also be done with pointers\n\t- consider that when we declare an array `x` of length 10, we are reserving 10 contiguous memory cells. We can create a pointer that points to the first element with `ptr = &x[0];`. Now, we can copy the contents of that first element to `y` with `y = *ptr;`.\n\t\t- Since elements in an array are contiguous, by defintion `ptr+1` points to the second element in the array, and `*(ptr+1)` refers to the *contents* of `x[1]` \n\t- ex. a char occupies a single byte, a short occupies 2 contiguous bytes, a long occupies 4 contiguous bytes, and so on.\n\t\t- Naturally, this means that contiguous groups of memory cells can be manipulated\n- A pointer is a group of cells (often two or four) that can hold an address\n\n## Example\nImagine we executed the following code:\n```\nint a = 5;\nint *ptr = NULL;\n```\nassuming that `a` is stored at `0x8130` and `ptr` at `0x8134`, our memory would look like:\n| Address | Value |\n|---------|------------|\n| 0x8130  | 0x00000005 |\n| 0x8134  | 0x00000000 |\n\nnow this code is run:\n```\nptr = &a;\n```\nAnd our memory looks like this:\n| Address | Value |\n|---------|------------|\n| 0x8130  | 0x00000005 |\n| 0x8134  | 0x00008130 |\n\nnow we can dereference `ptr`:\n```\n*ptr = 8;\n```\nAnd our computer takes the contents of `ptr` (0x00008130), locates the address, and assigns 8 to that location, yielding this memory:\n| Address | Value |\n|---------|------------|\n| 0x8130  | 0x00000008 |\n| 0x8134  | 0x00008130 |\n","n":0.042}}},{"i":501,"$":{"0":{"v":"Heap","n":1},"1":{"v":"\n# Heap\nContrast with [[stack|memory.stack]]\n- A place to store global variables. \n- The heap is not automatically managed for you\n- Data structure is hierarchical\n- Variables need to be deallocated manually\n- Garbage collection runs on the heap\n- The heap is slower, but it can also store much more data than the stack\n- In C, `malloc` and `calloc` are methods used to interact with the heap. Once memory has been allocated on the heat, we must use `free()` to deallocate that memory. Failure to do this results in what’s called memory leaks. \n- Pointers must be used to access memory on the heap\n- allocating memory is done on the heap, not the stack (as with other variables)\n- dynamic memory allocation can only be made through pointers and names (variable names) can't be given\n\t- `malloc()` is for allocating memory blocks from the heap in C\n","n":0.084}}},{"i":502,"$":{"0":{"v":"Buffer","n":1},"1":{"v":"\nA Buffer is an in-memory collection of raw bytes\n- Because computers store data in bytes, a simple way to think of a Buffer is to think of it as an array of bytes:\n```js\nconst buffer = [11111111, 11011001, 10010001]\n```\n\nThe point of a Buffer is to simply store anything as an array of bytes. The reason this is important is because every thing in computing communicates in bytes.\n\n- It can be thought of as a temporary place to put things that need to be worked on or processed, \n    - ex. like a to-do pile of work on your desk: Instead of just doing work the moment it’s given to you, you have people put it in a folder on your desk so you can work on everything at a steady pace\n- Buffering is a *technique*, not one specific place in the computer.\n\n### with Streams\na Buffer is what you get from or put to a Stream. The stream then reads or writes the buffer to the input or output target that the Stream is connected to.\n\n### Examples\n\n#### Movies\nThe rate at which you download a file may fluctuate but if the playback of that video did too it would be very awkward to watch. A buffer is a temporary storage place for some of that video so the downloading process can put it somewhere as it comes in and the playback process can play it at a steady rate.\n- For video, what you do is to first retrieve (for example 10 seconds) and then start to play. If the network drops some packet (and the data along with it) you can ask for it to be retransmitted to fix the problem.\n- if the buffer empties without having an end-of-file marker (a special flag designed to tell the playback device that no more data is needed), the program won’t know what data to display/playback next. Thus in the context of video/audio, buffering is when the video/audio buffer is filled with data when it has processed all the data in the buffer without receiving an end-of-file marker.\n- When you see a “buffering” message in the middle of playing a video, that means that the internet got so slow that the player ran out of video to display. A smart player might notice that 10 seconds (or whatever) of buffered video wasn’t enough, given how flaky the internet is at this location is, so maybe it’ll bump up the buffer to 20 or 30 seconds.\n\n#### Keyboard\nWhen you type on your keyboard, the keystroke data is put into a buffer. Then depending on what program, text box, etc. you are using, the processor processes the data and makes the appropriate updates to where it is needed (like showing it on screen, applying any commands or special processes to it like copy and paste functions, storing the information in RAM/ROM, etc.) Typically each process has a separate buffer which your CPU can access, handle reading from, writing to, and processing, as well as share information between other buffers. i.e. your keyboard has one buffer, your mouse another, your display another, your web browser another etc. and your processor handles information sharing between all of them.","n":0.044}}},{"i":503,"$":{"0":{"v":"Mac","n":1}}},{"i":504,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n## Utility\ncmd shift 4 - take a screen shot\ncmd shift 5 - take video recording\n\n## Navigation\nctrl + cmd + f - toggle fullscreen","n":0.209}}},{"i":505,"$":{"0":{"v":"Applescript","n":1},"1":{"v":"\nAppleScript is a scripting language for doing inter-application communication (IAC) using Apple events (ex. open a file, save a file). Most often, these actions are synchronous.\n\nWhereas [[Apple events|mac.apple-events]] are a way to send messages into applications, AppleScript is a particular language designed to send Apple events\n- the AppleScript language is designed on the natural language metaphor\n\nAppleScript can send and receive Apple events to applications, and can act as a connector between different apps.\n\nAppleScript relies on the functionality of applications and processes to handle complex tasks\n\nAppleScript can be compared to a Unix shell in terms of its purpose\n\nWhile not all apps are considered scriptable, any app with a graphical user interface responds to Apple Events at a minimal level. This is because OS X uses Apple Events to instruct all apps to perform core tasks such as launching, quitting, opening a document, and printing\n\nA handler in AppleScript is equivalent to a function/method in Javascript\n\nThe heart of the AppleScript language is the use of terms that act as nouns and verbs that can be combined. For example, rather than a different verb to print a page, document or range of pages (such as printPage, printDocument, printRange), AppleScript uses a single \"print\" verb which can be combined with an object, such as a page, a document or a range of pages.\n```\nprint page 1\nprint document 2\nprint pages 1 thru 5 of document 2\n```\n\n### Example Simple Web Gallery\n1. Open a photo in a photo-editing application (by sending that application an Open File Apple event).\n2. Tell the photo-editing application to reduce the resolution of the image\n3. Tell the photo-editing application to save the changed image in a file in some different folder (by sending that application a Save and/or Close Apple event).\n4. Send the new file path (via another Apple event) to a text editor or web editor application\n5. Tell that editor application to write a link for the photo into an HTML file.\n6. Repeat the above steps for an entire folder of images (hundreds or even thousands of photos).\n7. Upload the HTML file and folder of revised photos to a website, by sending Apple events to a graphical FTP client, by using built-in AppleScript commands, or by sending Apple events to Unix FTP utilities.\n\n# E Resources\n[Good guide that goes over Mac Scripting](https://developer.apple.com/library/archive/documentation/LanguagesUtilities/Conceptual/MacAutomationScriptingGuide/index.html#//apple_ref/doc/uid/TP40016239-CH56-SW1)","n":0.052}}},{"i":506,"$":{"0":{"v":"Dictionary","n":1},"1":{"v":"\nMac apps publish a dictionary of addressable objects and operations. Applescript leverages this dictionary to be able to communicate with those programs.\n- These dictionaries can be viewed with the Script Editor program, and then File > Open Dictionary (`cmd+shift+o`)\n- At its core, this scripting dictionary is a `.sdef` file that is stored in the app bundle.\n\nEvery scriptable app implements its own scripting features and exposes its own unique terminology through a scripting dictionary\n\n### Types of Terminology\nSuite - A suite is a grouping of related commands and classes\n- includes terminology supported by most scriptable apps, such as an `open` command, a `quit` command, and an `application` class.\n\nCommand - A command is an instruction that can be sent to an app or object in order to initiate some action.\n- ex. `delete`, `make`, `print`\n\nClass - A class is an object within an app, or an app itself\n- ex. Mail has an application class, a message class, and a signature class, among others\n\nProperty - A property is an attribute of a class. \n- ex. the message class in Mail has many properties, including `date received`, `read status`, and `subject`.\n\n### Concepts\n#### Inheritance\ndifferent classes often implement the same properties. \n- ex. in Finder, the file and folder classes both have creation date, modification date, and name properties\n- Rather than defining these same properties multiple times throughout the scripting dictionary, Finder implements a generic `item` class\n    - any properties of the `item` class also apply to the `file` and `folder` classes\n\n#### Containment\nClasses of a scriptable app reside within a certain containment hierarchy. The application is at the top level, with other classes nested beneath.\n- ex. Finder contains `disks`, `folders`, `files`, and other objects.\n- ex. Mail contains `accounts`, which can contain `mailboxes`, which can contain other `mailboxes` and `messages`.\n\n","n":0.059}}},{"i":507,"$":{"0":{"v":"Events","n":1},"1":{"v":"\nApple events are a high-level message-based form of Interprocess Communication (IPC), used to communicate between local or remote application processes\n\nAn Apple Event contains:\n1. Attributes describing how the event should be handled\n2. optional parameters to the event handler that receives the event\n\nExample\n- When we drag-n-drop a file onto the TextEdit.app icon in Finder, what happens is that Finder commands TextEdit to open that file by sending it an `odoc` (open doc) event with a list of file identifiers as its parameter\n![](/assets/images/2021-05-07-09-48-28.png)\n\nWith proper bindings, Apple events can be created and sent with programming languages\n- ex. from our client application, we might call `iTunes().play()`, causing a hook/Play event to be sent from the client application to iTunes, instructing it to start playing. Applications may respond to an incoming Apple event by sending a reply event back to the client application\n\n## Apple Event Object Model (AEOM)\nThe AEOM is a view-controller layer that provides a user-friendly representation of the application's internal data, allowing clients to identify and manipulate parts of that structure via Apple events\n- An incoming Apple event representing a particular command (get, set, move, etc.) is unpacked, and any object specifiers in its parameter list are evaluated against the application's AEOM to identify the user-level object(s) upon which the command should act\n- The command is then applied these objects, with the AEOM translating this into operations upon the application's implementation-level objects\n    - These implementation-level objects are mostly user-data objects in the application's Model layer\n\n![](/assets/images/2021-05-07-09-57-56.png)\n\n- The AEOM represents user data as a tree-shaped object graph, whose nodes are connected via 1:1 and/or 1:many relationships.\n- AEOM objects are identified by high-level queries (comparable to XPath or CSS selectors), not low-level chained method calls.\n- While the Apple Event Object Model is sometimes described by third-parties as being similar to DOM, this is inaccurate as AEOM operates at a much higher level of abstraction than DOM.\n\nThe AEOM is a tree-like structure made up of objects. These objects may contain descriptive attributes such as class, name, id, size, or bounds; for example:\n\n```\nfinder.version\nitunes.playerState\ntextedit.frontmost\nfinder.home\ntextedit.documents\nitunes.playlists\n```\n\nUnlike other object models such as DOM, objects within the AEOM are associated with one another by relationships rather than simple physical containment\nRelationships between objects may be one-to-one:\n```\nfinder.home\nitunes.currentTrack\n```\nor one-to-many:\n```\nfinder.folders\nitunes.playlists\n```\n\nto show how how this tree-like structure is not related to containment (as the DOM is), consider that the following object specifiers all identify the same objects (files on the user's desktop):\n- the first specifier describes the files' location by physical containment; the other two use other relationships provided by the application as convenient shortcuts\n\n```\nfinder.disks[\"Macintosh HD\"].folders[\"Users\"].folders[\"kyletycholiz\"].folders[\"Desktop\"].files\n\nfinder.desktop.files\n\nfinder.files\n```\n\nHere is the AEOM for a simple hypothetical text editor:\n![](/assets/images/2021-05-07-10-08-32.png)","n":0.048}}},{"i":508,"$":{"0":{"v":"Os","n":1},"1":{"v":"\n### Launch Agent\n- a launch agent is a simple script used by `launchd` that causes the system to run programs at system startup.\n\t- ex. postgres, Dropbox","n":0.196}}},{"i":509,"$":{"0":{"v":"Launchd","n":1},"1":{"v":"\nThe filename must be the same as the `Label` key value:\n`local.script.backup-everdo.plist`\n```xml\n<key>Label</key>\n<string>local.script.backup-everdo</string>\n```\n\n# Cook\n#### List all\n`launchctl list`\n\n# Resources\n[Quick n' dirty guide](https://www.splinter.com.au/using-launchd-to-run-a-script-every-5-mins-on/)\n[GUI for launchd](https://www.soma-zone.com/LaunchControl/)\n","n":0.218}}},{"i":510,"$":{"0":{"v":"Lsp","n":1},"1":{"v":"\n# Language Server Protocol (LSP)\n- the problem is that there is a matrix of code editors and languages to support. It doesn't make sense that every editor has extensive support for js, ruby, python, C... Why not just make dedicated servers that will handle all things for a specific language (syntax checking, autocomplete, go-to-file etc).\n- For instance, we have vim and a language server that handles everything for javascript. This same language server can be used for VSCode, Sublime, IntelliJ etc.\n![](/assets/images/2021-03-11-19-53-45.png)","n":0.111}}},{"i":511,"$":{"0":{"v":"Linux","n":1}}},{"i":512,"$":{"0":{"v":"Os","n":1},"1":{"v":"\n### Systemd\n- systemd is a collection of programs that provides system components for Linux.\n- Its purpose is to unify system configuration across different Linux distributions.\n- Systemd's primary component is a system and service manager, which is an init (boot) system used to manage user processes.\n\t- systemd also provides replacements for various daemons and utilities of the Linux system, including device management, login management, network connection management.\n- Effectively, systemd is used on most Linux systems, and has replaced distribution-specific init systems. \n- below is the systemd startup log.\n![74a24a20baffe9c81ed4a14a5d4c398a.png](:/3d7b66b1422e473096a6481247f59393)\n\n- `systemctl` is a utility used to control systemd \n\t- ex. we can issue a command to restart the `ssh` server\n","n":0.097}}},{"i":513,"$":{"0":{"v":"Linter","n":1},"1":{"v":"\n### Linters have two categories of rules:\n\n1. Formatting rules: eg: max-len, no-mixed-spaces-and-tabs, keyword-spacing, comma-style…\n- this is something that Prettier would handle\n\n2. Code-quality rules: eg no-unused-vars, no-extra-bind, no-implicit-globals, prefer-promise-reject-errors…\n","n":0.189}}},{"i":514,"$":{"0":{"v":"Lerna","n":1},"1":{"v":"\nLerna allows us to have multiple packages within our project. Complete with each package, will be a package.json and a corresponding node_modules directory\n\nWe can list one of our packages as a dependency of another one by including the package in the `dependencies` section of the dependent's `package.json` \n\nThe root of a project with Lerna will have a `lerna.json`, as well as a `packages/` directory\n- Each sub-package of our project must be within the `packages/` directory \n\n- To run `lerna bootstrap` is to install all dependencies in sub-modules\n- When we run `bootstrap`, lerna will call `yarn` in each module, then create symlinks between the packages that refer to each other in the dependent's `node_modules`\n\t- ex. If we have 3 modules: Addition, Subtraction, and Calc (which performs the add/subtract operations), then Calc's `node_modules` will contain symlinks to the Addition and Subtraction modules. \n- if we use hoisting, then symlinks don't play a part. Instead, if we have `react` in 2 different submodules, then hoisting will allow us to remove `react` from those submodules, install it at the root level, then allow the node recursive resolver to handle resolution for us. In other words, from within the submodule, when we require `react`, it will look for the package in its nearest `node_modules`, not find it, then continue upwards until it does, which will be at the root.\n\t- The reason why hoisting works is due to the resolve algorithm of node require\n\n### Duplication\n- naturally, having multiple sub-packages in a project will result in duplicate package listings. Lerna offers hoisting, which allows us to effectively list the same package in multiple places, but have the packages install at the root level\n\t- therefore, the duplicated package (ex. React) will be in the root directory's `node_modules`, even though it is listed in the sub-package's `package.json` \n- If we have a project with 2 sub-modules: A and B, and both have React as a dependency, we can run `lerna bootstrap --hoist` at the root, which will remove React (and all of its dependencies) from `A/node_modules` and `B/node_modules`, and move them to the root level `node_modules`. Because of the recursive nature of how `node_modules` are resolved, this will cause the sub-module's package.json to look upwards in the directory hierarchy until the react binary is found. \n\t- Lerna will warn us when running this command if we have version mismatches\n\n### Commands\n- `bootstrap` - generate `node_modules` for packages.\n- `clean` - remove all `node_modules` that are not in root directory\n- `create` - add a new sub-package to your project\n- `run` - run the script that is listed in each sub-`package.json`\n\t- Therefore, it will run only npm or yarn commands (with the script that is listed in package.json)\n- `exec` - run a command inside each package\n\t- similar to `run`, but is not restricted to running scripts in `package.json` \n\n### Misc\nYou don't actually need to run lerna bootstrap if you're using yarn workspaces.","n":0.046}}},{"i":515,"$":{"0":{"v":"Lan","n":1}}},{"i":516,"$":{"0":{"v":"Mac","n":1}}},{"i":517,"$":{"0":{"v":"Kubernetes","n":1}}},{"i":518,"$":{"0":{"v":"Objects","n":1},"1":{"v":"\nKubernetes objects should not be created imperatively (ie. by hand), just as we don't typically create Docker containers by hand.\n\nFurthermore, even though we can for instance create services by running `kubectl expose`, we should typically follow a documented approach by using YAML files.\n","n":0.152}}},{"i":519,"$":{"0":{"v":"Karabiner","n":1},"1":{"v":"\n# Complex Modifications (json)\n- found in `~/.config/karabiner/assets/complex_modifications`\n\nFrom\n- this field specifies the key combination that we want to press\n\nTo\n- this field specifies the key combination that we want the system to understand\n\n# Modifiers\nMandatory\n- this key must be pressed\n\nOptional\n- this key can be pressed and the mapping will be registered, though it is not necessary\n- ex. useful if say we have `<Shift-Ctrl-j>` mapped to \"switch workspace\", and we also don't care if we happen to also be holding down `cmd`.\n\n# Tools\n[Complex rules generator](https://genesy.github.io/karabiner-complex-rules-generator/)\n\npackage.json has a pre/post version of every script","n":0.107}}},{"i":520,"$":{"0":{"v":"Kafka","n":1},"1":{"v":"\n- Traditionally, we use only databases as sole data storage, and think of that data in terms of \"things\", along with their state.\n    - Kafka encourages us to think of events first, and things second.\n- Kafka stores data in a distributed log instead of a database.\n    - log is an ordered sequence of events, along with state and a description of what happened\n    - Kafka is a system for managing these logs (in Kafka, a log is called a `Topic`)\n- logs are easy to build at scale\n- Implementations of Kafka are declarative\n\n### Topic\nAn ordered collection of events stored in a durable way.\n- Durable - written to disk, and replicated\n- Can be thought of as a *real-time stream*\n- Topics can be thought of as a database table\n\nKafka Services can be used as a sort of glue between microservices of an application. A microservice can consume a message from the Kafka Topic, and produce an output which gets registered to another Topic.\n- Since it can act as glue between many services, we can produce an output from these Topics that can be consumed by a new service to perform real-time analysis of that data\n    - This is contrast to the old-school method of running a batch-process overnight\n\nThink of a topic like a db table\n\n### Kafka Connect\nImagine we have multiple databases, a legacy service, and a SaaS product, and we want a way to get the data that they produce into Kafka.\n- Kafka Connect helps us get that data into Kafka, and back out again.\n- Kafka Connect is a general term to refer to 100's of pluggable modules that handle the I/O of the whatever service we are connecting to.\n    - ex. There would be a connector to capture row-level changes in a Postgres database.\n\nConnectors are either Source or Sink Connectors, and are responsible for a some of the Task management, but not the actual data movement.\n\n### Kafka Streams\n- A Java API that performs helps us perform grouping, aggregating, filtering, enrichment (table joining)\n- the API would be used from within the services\n- This is available to us out of the box as a consequence of using Kafka.\n\n#### KSQL\n- a language that allows us to to use SQL-like syntax to query data from one Topic, and output it into another Topic.\n- Solves the problem statement: imagine we want to perform some analysis on data kept in Kafka, but we don't want to stand up a separate service to consume that data.\n\n### Confluent\n- a distribution of Kafka.\n- open source, but offers a paid managed service (similar to Docker)\n\n### Kafka vs Logstash\n[Kafka is a cluster; Logstash is a single instance](https://stackoverflow.com/questions/40864312/how-logstash-is-different-than-kafka)\n\n### Kafka vs RabbitMQ\nThese are different forms of communicating. When service A calls service B to exchange information, it's similar to a phone conversation. I ask you a question, and you respond. In the meantime we're both occupied with that conversation; you have to have time to talk with me, and so do I.\n\nRabbitMQ and Kafka (and similar tools) also support two more types of communication:\n1. *Topics*: a topic is a one-way one to many conversation. Basically it's like standing on a soapbox and shouting to a megaphone. Whoever is interested can listen; I don't care if that is zero people or a hundred. I send my message and the broker (kafka/rabbit) makes sure whoever is subscribed to the topic all get the message.\n2. *Queues*: a queue is similar to sending letters to a company. I send a letter complaining about their service, and within that company someone opens and reads this letter. I'm not involved anymore after I send  the letter. While there may be a hundred workers opening letters, only one of them is going to be handling mine. Copies don't magically appear.\n\nSo both queues and topics are one-way communication that are fire and forget (asynchronous). This as opposed to TCP (and HTTP on top of that) that are two-way synchronous communication. The broker guarantees that the other side gets the message. Depending on how you configure (for example) Kafka; you can either have a single service get a message (queue example), all of the services get the same message (topic example) or something in between.\n\nWhen working with microservices you want every unit to be as decoupled as possible.\n- Let's say you have a component that handles user information (`A`) and you have some other module that need to do something every time a user updates his info (`B`). The classic way to do it would be to make `A` send some signal (usually an http request) to `B` so it knows it has work to do. But, for that, `A` needs to know a lot about `B`. And what if you now have `C` that needs to be triggered to? Or `D` that needs to process just some of the users? `A` would need to keep track of all of this, meanings they are highly coupled.\n- With a broker, `A` just publishes that there's a change, and all other components can subscribe to this event and react accordingly. This way, components can be added or removed with minimal impact on the overall structure.\n\n## Do you need Kafka?\nFrom Reddit [thread](https://www.reddit.com/r/apachekafka/comments/hyxezo/kafka_when_to_use_and_when_not_to_use/):\n\nKafka is like an event bus for distributed messages. It can't solve the intractable problem of distributed systems, but it does provide a nice framework for handling messages at scale.\n\nI would say if you don't need Kafka, then that's a perfectly good reason not to use it. You might have a monolith, you might have a series of services that all own their data within a database, and they all communicate via REST APIs. If that works, then Kafka won't really serve a purpose.\n\nYou start to need Kafka when you run into the constraints of distributed scale. If you have more events than any single worker can handle, and you've decided you can't increase the available CPU and memory any further, then you start facing distributed messaging problems.\n\nAs soon as two computers located some distance from each other try to determine what is true, and what happened first, you'll run into problems. Your next goal is to try and figure out which constraints you're willing to bend.\n\nMaybe your pipeline doesn't actually care about order, but absolutely cares that you don't drop a single record. Maybe you only care about throughput. Maybe you care about order, but not about availability. Eventually somethings gotta give. Kafka just helps you manage that infrastructure, it doesn't solve the underlying issues.\n\nIf you run an app that collects logs and metrics, order might not matter. Just send everything down the pipe and you'll aggregate later. But you probably wouldn't run your banking transaction on it.\n\n### Using Kafka in a Logging system\n\"There are plenty of open-source tools available for logging. We decided to use Graylog—an excellent tool for logging—and Apache Kafka, a messaging system to collect and digest logs from our containers. The containers send logs to Kafka, and Kafka hands them off to Graylog for indexing. We chose to make the application components send logs to Kafka themselves so that we could stream logs in an easy-to-index format. Alternatively, there are tools that retrieve logs from outside the container and forward them to a logging solution.\"\n\n# UE Resources\n[The place to start learning Kafka](https://kafka-tutorials.confluent.io/)\n[Getting Started](https://www.confluent.io/blog/getting-started-with-kafkajs/)\n[Kafka with Azure Functions](https://github.com/Azure/azure-functions-kafka-extension)\n[Connect Architecture](https://medium.com/@Instaclustr/apache-kafka-connect-architecture-overview-842097d3eb96)","n":0.029}}},{"i":521,"$":{"0":{"v":"K8s","n":1},"1":{"v":"\nKubernetes is all about abstracting how, when and where containers are run.\n\nKubernetes is a [[docker orchestrator|docker.orchestrators]]. It can also be thought of as a container scheduler.\nAll containers in Kubernetes are scheduled as pods\n- We don't tell Kubernetes to run a container. Rather, we tell it to create a pod that wraps a container.\n\nKubernetes uses client-server architecture\n\nIt’s not enough to run containers. We need to be able to scale them, to make them fault tolerant, to provide transparent communication across a cluster\n- Containers are only a low-level piece of the puzzle. The real benefits are obtained with tools that sit on top of containers. Those tools are today known as container schedulers. They are our interface. We do not manage containers, they do.\n\nKubernetes follows the ethos that we should not run our containers directly, and should instead trust Kubernetes to handle container scheduling for us.\n- The chief reasoning for this is that containers by themselves don't provide fault tolerance. They cannot be deployed easily to the optimum spot in a cluster, and are not operator friendly\n\nspec: There are ~20 docker containers that Kubernetes depends on\n\nToday, modern infrastructure is created from immutable images. Any upgrade is performed by building new images and performing rolling updates that will replace VMs one by one. Infrastructure dependencies are never changed at runtime\n- One of the inherent benefits behind immutability is a clear division between infrastructure and deployments. Until not long ago, the two meshed together into an inseparable process. With infrastructure becoming a service, deployment processes can be clearly separated, thus allowing different teams, individuals, and expertise to take control.\n\nspec: Kubernetes offers us process isolation (via Docker), and an immutable way to deploy software\n\n### Benefits of Kubernetes\n- We can use it to deploy our services, to roll out new releases without downtime, and to scale (or de-scale) those services.\n- We can move a Kubernetes cluster from one hosting vendor to another without changing (almost) any of the deployment and management processes.\n\n### Kubernetes vs Docker Swarm mode\nIn Kubernetes, an application can be deployed using a combination of pods, deployments, and services (or micro-services).\n\nWhereas, in Docker Swarm, applications can be deployed as services (or micro-services) in a Swarm cluster. YAML files can be used to specify multi-container. Moreover, Docker Compose can deploy the app.\n\n# E Resources\n[Kubernetes Illustrated Children's Guide](https://chrisshort.net/kubernetes-illustrated-childrens-guide/)\n[1 year using Kubernetes in production](https://techbeacon.com/devops/one-year-using-kubernetes-production-lessons-learned)\n","n":0.051}}},{"i":522,"$":{"0":{"v":"Yaml","n":1},"1":{"v":"\n## YAML Fields\nIn most cases, Kubernetes has sensible defaults, but there are things we will generally want to tweak\n\n### Strategy\n##### RollingUpdate (default)\n- creates a new ReplicaSet with zero replicas and, depending on other parameters, increases the replicas of the new one, and decreases those from the old one. This process is done one at a time, and as a new ReplicaSet is added, and old one is removed.\n- The process is finished when the replicas of the new ReplicaSet entirely replace those from the old one.\n\nCan be fine-tuned with the `maxSurge` and `maxUnavailable` fields.\n- `maxSurge` defines the maximum number of Pods that can exceed the desired number (set using replicas). It can be set to an absolute number (e.g., 2) or a percentage (e.g., 35%). The total number of Pods will never exceed the desired number (set using replicas) and the maxSurge combined. The default value is 25%.\n- `maxUnavailable` defines the maximum number of Pods that are not operational. If, for example, the number of replicas is set to 15 and this field is set to 4, the minimum number of Pods that would run at any given moment would be 11. Just as the maxSurge field, this one also defaults to 25%. If this field is not specified, there will always be at least 75% of the desired Pods.\n\n##### Recreate\n- this will kill all the existing Pods before an update\n- this resembles the processes we used in the past when the typical strategy for deploying a new release was first to stop the existing one and then put a new one in its place.\n  - This approach inevitably leads to downtime\n- The only case when this strategy is useful is when applications are not designed for two releases to coexist.\n- ask yourself the following question: Would there be an adverse effect if two different versions of my application are running in parallel? If yes, then Recreate might be a good option\n- ex. Since most databases cannot have multiple instances writing to the same data files, killing the old release before creating a new one is a good strategy when replication is absent. This strategy is more suitable for singe-replica databases\n\n### Selector\nUsed to select which pods should be included in the ReplicaSet. It does not distinguish between the Pods created by a ReplicaSet or some other process. In other words, ReplicaSets and Pods are decoupled.\n\n- If Pods that match the selector exist, ReplicaSet will do nothing. If they don’t, it will create as many Pods to match the value of the replicas field.\n- Not only that ReplicaSet creates the Pods that are missing, but it also monitors the cluster and ensures that the desired number of replicas are (almost) always running. In case there are already more running Pods with the matching selector, some will be terminated to match the number set in replicas.\n","n":0.046}}},{"i":523,"$":{"0":{"v":"Probes","n":1},"1":{"v":"\nProbes allow us to do health checks on containers\n\nKubernetes is designed to start up containers automatically when the original fails, but what happens if the container didn't fail, but is serving content 3x slower because of a memory leak? We need to be able to check the statuses of containers, and we can do that with Probes.\n\n### Probe Types\n##### livenessProbe\nCan be used to confirm whether a container should be running\n- If the probe fails, Kubernetes will kill the container and apply restart policy which defaults to Always.\n\nThe below probe will make a get request, and will kill the container if it fails.\n```yaml\n# podfile\n...\nlivenessProbe:\n\thttpGet:\n\t\tpath: /this/path/does/not/exist\n\t\tport: 8080\n\tinitialDelaySeconds: 5\n\ttimeoutSeconds: 2 # Defaults to 1\n\tperiodSeconds: 5 # Defaults to 10\n\tfailureThreshold: 1 # Defaults to 3\n```\n\n##### readinessProbe\nThe readinessProbe should be used as an indication that the service is ready to serve requests.\n- When combined with Services construct, only containers with the readinessProbe state set to Success will receive requests.\n\nThe lack of the retry mechanism in Kubernetes is mitigated with readinessProbe, which we added to the ReplicaSet.\nThe readinessProbe has the same fields as the livenessProbe\n\nthe readinessProbe is used by the iptables\n","n":0.074}}},{"i":524,"$":{"0":{"v":"Objects","n":1}}},{"i":525,"$":{"0":{"v":"Volume","n":1},"1":{"v":"\nA volume can be backed by a variety of implementations, including files on the host machines, AWS Elastic Block Store (EBS), and Network File System (NFS).\n","n":0.196}}},{"i":526,"$":{"0":{"v":"Service","n":1},"1":{"v":"Without services, there is no way for pods to communicate with each other. The only communication that is available by default is between containers within a pod (via localhost)\n\nServices should not be used to enable external access to an application.\n\nServices are based on resources.\n\n- ex. we create a service based on Pods through a ReplicaSet. Put another way, a service is created by exposing the ReplicaSet. This is why we specify the resource type when we are running `kubectl expose`\n\nA service is an abstract way to expose an application (running on a set of Pods) as a network service.\n\n- this decouples work definitions from the pods\n- can be thought of as a logical set of pods and a policy by which to access them\n  - the front end shouldn't care which backend it uses. Services allow us to achieve this decoupling\n\nA service provides access to pods from inside the cluster ([[Kube Proxy|k8s.components.node.kube-proxy]]) or from outside the cluster ([[Kube DNS|k8s.components.node.kube-dns]])\n\nA service tells the rest of the Kubernetes environment (including other pods and replication controllers) what services your application provides. While pods come and go, the service IP addresses and ports remain the same.  Other applications can find your service through Kurbernetes service discovery\n\nSince pods of a single service can exist on different machines, it makes sense for us to be able to interact with the service itself, so that we can orchestrate activities between all containers that are part of a service(?)\n\nWhen a service is created, it inherits all of the labels of the resource type (eg. ReplicaSet) that the service is based on.\n- The service is not directly associated with any controller (eg. ReplicaSet), but rather it is associated with the underlying Pods via the matching labels.\n\nThe problem with services is that each application can be reached through a different port, and we cannot expect users to know the port of each service in our cluster.\n- it is a bad practice to publish fixed ports through Services, because it is likely to result in conflicts or, at the very least, create the additional burden of carefully keeping track of which port belongs to which Service.\n\n## Service discovery\n\nServices can be discovered through two principal modes:\n\n- Environment variables\n- DNS\n  - DNS is an easier approach\n\nKubernetes converts Service names into DNSes and adds them to the DNS server.\n\n- This is a cluster add-on that is already set up by Minikube.\n\n### Services and Env Variables\n\nEvery Pod gets environment variables for each of the active Services\n![[dendron://code/k8s.kubectl.cli#list-env-variables-in-a-pod,1:#*]]\n\nEnv provide a reference we can use to connect to a Service and, therefore to the related Pods.\n\nThrough the service IP (`kubectl describe svc <servicename>`), we can access the service externally. This IP matches the values of the environment variables `GO_DEMO_2_DB_*` and `GO_DEMO_2_DB_SERVICE_HOST`.\n\n### Service discovery breakdown\n\n1. When the api container go-demo-2 tries to connect with the go-demo-2-db Service, it looks at the nameserver configured in /etc/resolv.conf.\n   - kubelet configured the nameserver with the kube-dns Service IP (10.96.0.10) during the Pod scheduling process.\n2. The container queries the DNS server listening to port 53. go-demo-2-db DNS gets resolved to the service IP 10.0.0.19.\n   - This DNS record was added by kube-dns during the service creation process.\n3. The container uses the service IP which forwards requests through the iptables rules.\n   - They were added by kube-proxy during Service and Endpoint creation process.\n4. Since we only have one replica of the go-demo-2-db Pod, iptables forwards requests to just one endpoint.\n   - If we had multiple replicas, iptables would act as a load balancer and forward requests randomly among Endpoints of the Service\n\n![](/assets/images/2021-06-01-08-40-26.png)\n\n* * *\n\n### Motivation\n\nEach Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.\n\nThis leads to a problem: if some set of Pods (call them “backends”) provides functionality to other Pods (call them “frontends”) inside your cluster, how do the frontends find out and keep track of which IP address to connect to, so that the frontend can use the backend part of the workload?\n\n### Under the hood — Adding a service\n\n1. In running `kubectl expose rs -f rs/go-demo-2.yml`, Kubernetes client (kubectl) sent a request to the API server requesting the creation of the Service based on Pods created through the go-demo-2 ReplicaSet.\n2. Endpoint controller is watching the API server for new service events. It detected that there is a new Service object.\n3. Endpoint controller created endpoint objects with the same name as the Service, and it used Service selector to identify endpoints (in this case the IP and the port of go-demo-2 Pods).\n4. kube-proxy is watching for service and endpoint objects. It detected that there is a new Service and a new endpoint object.\n5. kube-proxy added iptables rules which capture traffic to the Service port and redirect it to endpoints. For each endpoint object, it adds iptables rule which selects a Pod.\n6. The kube-dns add-on is watching for Service. It detected that there is a new service.\n7. The kube-dns added db's record to the dns server (skydns).\n   ![](/assets/images/2021-05-31-10-06-52.png)\n   ![](/assets/images/2021-05-31-21-38-25.png)\n\n* * *\n\n### Selector\n\nA service contains selector labels which are used to establish communication with the Pods containing the matching labels. There is no relation between the service and the ReplicaSet; both reference Pods through labels.\n\nThe selector is used by the Service to know which Pods should receive requests. It works in the same way as ReplicaSet selectors. In this case, we defined that the service should forward requests to Pods with labels type set to backend and service set to go-demo. Those two labels are set in the Pods spec of the ReplicaSet.\n\n```yaml\nselector:\n  type: backend\n  service: go-demo-2\n```\n\n### Request forwarding\n\nEach Pod has a unique IP. Incoming requests will be forwarded on to Pods in a round-robin style, that is similar to load balancing.\n\n","n":0.032}}},{"i":527,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n#### ClusterIP (default)\nexposes the port only inside the cluster, making it inaccessible from the outside world.\n- Therefore, use this service when we want to enable communication between pods, while preventing external access.\n- With ClusterIP, all the Pods in the cluster can access the `TargetPort` (the port of the associated Pod that will receive all the requests)\n\n#### NodePort\nexposes ports to all the nodes\n- If the service is of type NodePort, ports will be available both within the cluster as well as from outside by sending requests to any of the nodes.\n- If NodePort is used, ClusterIP will be created automatically.\n- NodePort is the port which we can use to access the Service and, therefore, the Pods from outside the cluster\n  - in most cases the port should be randomly generated to avoid clashes\n\n#### LoadBalancer\nonly useful when combined with cloud provider’s load balancer.\n\n#### ExternalName\nmaps a service to an external address (e.g., kubernetes.io)\n- This service has limited usage.\n","n":0.08}}},{"i":528,"$":{"0":{"v":"Pod","n":1},"1":{"v":"A Pod is a way to represent a running process in a cluster.\n\n- Pod refers to a pod of whales or pea pod\n\nA pod is a collection of containers that share resources\n- Though realistically, we tend to only have a single container in a Pod. We might see more than 1, but it normally isn't more than 2 or 3.\n\nA pod is designed to run multiple cooperative processes that could be seen as a single cohesive piece of work. This is the level of abstraction that we live at in Kubernetes.\n\nPods are the building blocks of Kubernetes, just as containers are the building block of Docker.\n- In Docker, we think in terms of processes. In Kubernetes, we think in terms of multiple processes (co-existing to perform one task)\n\nAll the containers in a pod run on the same machine. \n- That is, a pod cannot be split across multiple nodes\n\nA pod provides a way to set `.env` variables, mount storage, and feed other information into a container\n\nA pod encapsulates one or more containers deployed together on one host, thereby sharing the same resources (of the host)\n- ex. if we have 5 containers of a mongodb service deployed, and 3 of them were on the same host (ex. same machine), those 3 together would be called a Pod\n\nPods are not long-lived services. Even though Kubernetes is doing its best to ensure that the containers in a Pod are (almost) always up-and-running, the same cannot be said for Pods. In Kubernetes, containers are fault-tolerant, but pods are not.\n\n- If a Pod fails, gets destroyed, or gets evicted from a Node, it will not be rescheduled.\n- Similarly, if a whole node is destroyed, all the Pods on it will cease to exist.\n\nEach pod gets its own IP address, though it is unreliable, since pods are designed to be short-lived; so the creation of a new pod would result in a new IP\n\nWhen the container inside a pod exits, the pod dies too.\n\nWhen a container inside a pod fails, Kubernetes will create a new container based off the same image:\n\n```sh\n$ kubectl exec -it db pkill mongod\n$ kubectl get pods\n```\n\nproduces (note how RESTARTS is 1):\n\n```\nNAME READY STATUS  RESTARTS AGE\ndb   1/1   Running 1        13m\n```\n\nEverything in a pod is tightly coupled.\n\nThe containers in a pod are not necessarily Docker containers, though it is the most common implementation.\n\nNormally pods are not created by hand. Instead, we depend on higher level constructs like Controllers to do that for us.\n\nPods...\n\n- are mortal. They are born and cannot be resurrected once they die.\n- are not intended to run multiple instances of the same application,\n\nContainers within a pod...\n\n- share an IP address and port space, and can find each other via localhost.\n- share storage space\n\nIn a pre-container world, being executed on the same physical or virtual machine would mean being executed on the same logical host.\n\n- logical host would contain relatively tightly coupled code\n\n### How many containers in a pod?\n\nEven though a Pod can contain any number of containers, the most common use case is to use the **single-container-in-a-Pod** model\n\n- Imagine we had your express api server image and a postgres image. If we put both of these in a single pod, we would no longer be able to have different numbers of containers. For instance, we could not have 2 api containers and 1 postgres container.\n\nThere are scenarios when having multiple containers in a Pod is a good idea. However, they are very specific and, in most cases, are based on one container that acts as the main service and the rest serving as side-cars.\n\nA frequent use case is multi-container Pods used for:\n\n- Continuous integration (CI)\n- Continious Delivery (CD)\n- Continuous Deployment processes (CDP)\n\n### PodTemplate\n\nWhen we create a pod, a hash of the PodTemplate is taken and it appended to the Pod name. This means that 2 pods created from identical PodTemplates on different machines will produce the same hash.\n\n- This is also how Git SHAs work.\n\nspec: PodTemplate is in the ReplicaSet\n\n## Pod Scheduling\n\n### Major components\n\nThere are 3 major components: API Server, Scheduler, Kubelet\n\n#### API Server\n\nCentral component of the K8s cluster\n\n- runs on the master node\n  - with Minikube, both master and worker nodes are baked into the same VM. Realistically, the K8s cluster should have the two separated on different hosts.\n\nMost of the coordination in Kubernetes consists of a component writing to the API Server resource that another component is watching. The second component will then react to changes almost immediately.\n\n#### Scheduler\n\nThe scheduler is also running on the master node.\n\n- Its job is to watch for unassigned pods and assign them to a node which has available resources (CPU and memory) matching Pod requirements.\n\n#### Kubelet\n\n[[reference|k8s.components.node.kubelet]]\n\n### Process of creating a pod\n\nie. when running `kubectl create -f pod/db.yml`\n\n1. kubectl (the K8s client) sends a request to the API Server, requesting the creation of a pod\n2. Since the scheduler is watching the API server for new events, it detected that there is an unassigned Pod.\n3. The scheduler decided which node to assign the Pod to and sent that information to the API server.\n4. Kubelet is also watching the API server. It detected that the Pod was assigned to the node it is running on.\n5. Kubelet sent a request to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines a single container based on the mongo image.\n6. Finally, Kubelet sent a request to the API server notifying it that the Pod was created successfully.\n\n![Pod Scheduling Sequence](/assets/images/2021-05-28-11-22-21.png)\n\n## Pod Definition File\n\n```yml\n# using v1 of the K8s Pods API\napiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    type: db\n    vendor: MongoLabs\nspec:\n  containers:\n  - name: db\n    image: mongo:3.3\n    command: [\"mongod\"] # the command that should be executed when the container starts\n    args: [\"--rest\", \"--httpinterface\"]\n\n```\n\n","n":0.032}}},{"i":529,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n`kubectl run db --image mongo`\n- create a pod with a mongo database\n\t- this is similar to how we can `docker run` to create a container.\n\t- running this command will pull the image from Docker Hub\n- this is not how we normally run pods\n","n":0.152}}},{"i":530,"$":{"0":{"v":"Namespace","n":1},"1":{"v":"\nA namespace is a virtual cluster. It allows Kubernetes to manage multiple clusters (which could be multiple different application within the same product) within the same physical cluster\n\nNamespaces provide us a way to group and segment pods, rcs, volumes, and secrets\n- A namespace functions as a grouping mechanism inside of Kubernetes. Services, pods, replication controllers,\nand volumes can easily cooperate within a namespace, and the namespace provides a degree of isolation from\nother parts of the cluster.\n","n":0.115}}},{"i":531,"$":{"0":{"v":"Ingress","n":1},"1":{"v":"\nIngress objects manage external access to the applications running inside a Kubernetes cluster.\n- it may seem that this use-case is already solved by [[services|k8s.objects.service]], but services don't make applications truly accessible, because we still need forwarding rules based on paths and domains, SSL termination and a number of other features.\n- in a more traditional setup, we’d probably use an external proxy and a load balancer. Ingress provides an API that allows us to accomplish these things, in addition to a few other features we expect from a dynamic cluster.\n\nIngresses act as traffic cop, routing traffic from outside the cluster into destination points within the cluster.\n- One single external ingress point can accept traffic destined to many different internal services.\n\nWith Ingress, we can configure an external load balancer directly from Kubernetes\n","n":0.088}}},{"i":532,"$":{"0":{"v":"Deployment","n":1},"1":{"v":"\n# Overview\nKubernetes Deployments are all about: how do we swap out the old ReplicaSets with the new ones, and not lose any downtime in the process? How do we gently replace them 1 by 1, without having to replace them all at once?\n- ex. Imagine that we want to start using the Docker image `Postgres:12.2` instead of `Postgres:11.0`. The deploy.yml file specifies that we want to have 3 ReplicaSets. The implication of this, is that if we run `kubectl set image -f deploy.yml <containername>=postgres:12.2`, we would expect the Kubernetes engine to drop one `11.0` RS, and add a `12.2`, then drop another `11.0` and add a `12.2`, and so on until the rollout is complete and we only have `12.2`.\n  - note: when we rollback (ie. undo a rollout), this process happens in reverse.\n\nImagine the systems of Kubernetes, Docker, and everything in between. If Docker were at the bottom of a physical chain, we could run containers ourselves with `docker run`. Moving up, we can create pods directly with `kubectl create pod\n\n### The Problem\nWith just Pods, ReplicaSets and Services, we can deploy, scale and enable communication for our application. However, this is all useless if we cannot update those applications with new releases. This is the problem that Deployments solve.\n\nThe desired state of our applications is changing all the time. The most common reasons for new states are new releases. The process is relatively simple. We make a change and commit it to a code repository. We build it, and we test it. Once we’re confident that it works as expected, we deploy it to a cluster.\n- It does not matter whether that deployment is to a development, test, staging, or production environment. We need to deploy a new release to a cluster, even when that is a single-node Kubernetes running on a laptop. No matter how many environments we have, the process should always be the same or, at least, as similar as possible. We need to do all of this with zero-downtime. Deployments allow us to do that.\n\n### What is it?\nA deployment is a higher-level abstraction that controlling and scaling a set of pods\n- Behind the scenes, it uses a ReplicaSet to keep the Pods running, but it offers sophisticated logic for deploying, updating, and scaling a set of Pods within a cluster.\n\nA Deployment provides declarative updates for Pods and ReplicaSets.\n- You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate\n\nThe service in front of the deployment has an IP address, but this address only exists within the Kubernetes cluster. This means the service isn’t available to the Internet at all.\n\n### Zero-Downtime Deployment\nDeployments support rolling updates and rollbacks\n\nSimilar to [[Graphile-Migrate's|graphile-migrate]] opinion about rolling forward only (ie. adding new migrations rather than removing them), we should follow this approach to deployments in Kubernetes (although the opinion is not as strong as Graphile-Migrate's).\n- We definitely do not want to rollback (ie. `kubectl rollout undo`) in the situation where there is a database change, which would result in backend services that depend on a different version of the database.\n\nNew deployments do not destroy ReplicaSets, but rather scale them to 0. Therefore, to undo a rollout all we need to do is scale the bad ReplicaSets to 0 and scale the good ReplicaSets to our desired number.\n- This fact is reflected when we run `kubectl rollout history`. When we rollback, what we will see is that the associated command with the new revision will be the initial command that was used to create it (eg. `kubectl create --filename=deploy/go-demo-2-api.yml`). It might be natural to think that the command would be `kubectl set image`, but this is not the case.\n\n* * *\n\nJust as we are not supposed to create Pods directly but using other controllers like ReplicaSet, we are not supposed to create ReplicaSets either. Kubernetes Deployments will create them for us.\n- Therefore, if we naively observe creating Pods via the 2 procedures, there is no difference:\n  - Deployment -> ReplicaSet -> Pods\n  - ReplicaSet -> Pods\n\nHowever, the advantage becomes evident if we try to change some of its aspects.\n- ex. we might choose to upgrade MongoDB to version 3.4.\n\nThe Deployment Controller is what creates the objects that are listed in the `deploy.yml` file. It watches the API server for new events, and creates the objects (eg. ReplicaSet) in response.\n- Once the ReplicaSet is created, the ReplicaSet Controller (which also watches for API events) notices that a new ReplicaSet was created, and in response, creates the Pods that it specified.\n- From there, the scheduler takes over, which watches for unassigned pods and assigns them to nodes.\n- Following that, Kubelet (which is watching for pods being assigned to nodes) causes the containers to be run (via Docker), then sends a signal to the API server indicating the updated status of the Pods.\n","n":0.035}}},{"i":533,"$":{"0":{"v":"Minikube","n":1},"1":{"v":"\nMinikube is a virtual machine that runs locally and has the necessary Kubernetes components deployed into it. The VM will get configured with Docker and Kubernetes via a single binary called localkube.\n\nIf we were using Docker Swarm (instead of Kubernetes), we'd be able to `docker swarm init` to create a local Docker Swarm Cluster. Minikube is a service that allows us to have this level of simplicity with Kubernetes\n\nIn Minikube, there's only 1 server that acts as both the master and the node\n\n### localkube\nProvides a single-node cluster running locally on our machine\nThe localkube library provides everything needed to run a Kubernetes cluster locally\n","n":0.099}}},{"i":534,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\nMinikube commands are almost exactly the same as those from Docker Machine which, on the other hand, are similar to those from Vagrant.\n\n`minikube start --vm-driver=virtualbox`\n- creates a new VM based on the Minikube image, and deploys the necessary Kubernetes components into it.\n- The VM will get configured with Docker and Kubernetes via a single binary called *localkube*.\n- This image includes several binaries, including Docker engine\n- after creating a new VM, we'll need to connect the docker client to the docker server with `eval $(minikube docker-env)`\n\n`minikube dashboard`\n- open a browser-based UI\n\n`minikube docker-env`\n- output the docker environment variables to console\n- these env variables are relevant for the docker engine running in the image\n\n`minikube stop`\n- stop the cluster, but preserve cluster state and data\n","n":0.091}}},{"i":535,"$":{"0":{"v":"Labels","n":1},"1":{"v":"\nLabels are key/value pairs that are attached to objects, such as pods\n- Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system\n- Labels can be used to organize and to select subsets of objects.\n\nLabels allow for efficient queries and watches and are ideal for use in UIs and CLIs\n\nLabels can be seen with `kubectl describe pod`\n\nLabels enable users to map their own organizational structures onto system objects in a loosely coupled fashion, without requiring clients to store these mappings.\n\n```\n\"metadata\": {\n  \"labels\": {\n    \"key1\" : \"value1\",\n    \"key2\" : \"value2\"\n  }\n}\n```\n\nIt is throught Labels that Pods and ReplicaSets are able to be loosely coupled.\n\nLabels are what a ReplicaSet uses to identify the pods that are in existence that satisfy the RepicaSet's requirement for pod count.\n- ex. we have a ReplicaSet that specifies 4 pods should exist. If we remove the `service` label from a pod, then ReplicaSet will no longer be aware of it, and will create a brand new pod in order to reach its quota again. At the end of the day, we will have one extra pod (5 total) in the cluster. Of course, this pod with the removed `service` label will be running freely and won't be controlled by the ReplicaSet, since the labels no longer match.\n  - Following this exercise, if we then add back the label that we removed, the ReplicaSet will be made aware of it, and will remove one of the other pods to reach its desired state of 4 pods.\n","n":0.061}}},{"i":536,"$":{"0":{"v":"Kubectl","n":1},"1":{"v":"\n`kubectl`, is used to manage a cluster and applications running inside it.\n- `kubectl` is a Kubernetes client that connects with the Kubernetes API, much like how psql is a client to a postgres server\n","n":0.171}}},{"i":537,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\nAll of these commands send requests to the Kubernetes API\n\n##### Apply\n`kubectl apply -f rs/rs.yml`\n- apply a configuration to a resource.\n- we can create resources with this command, by `apply`ing to a resource that doesn't exist.\n\t- we can also `create` with `--save-config`\n\n##### Config\n`kubectl config current-context`\n- see where kubectl is pointing\n\n##### Create\n`kubectl create -f pod/db.yml`\n- create a pod based on the db.yml file\n- note: we didn't need to specify that we were creating a pod, since the resource type is already defined in the yml file (`kind` field).\n- `--record` allows us to track each change to our resources such as a Deployments.\n\n##### Delete\n`kubectl delete pod <podname>`\n- delete a pod\n\n##### Describe\n- give information on a resource (aka Kubernetes object)\n`kubectl describe pod <podname>`\n`kubectl describe -f db.yml`\n\n##### Exec\n- execute a new process inside the first container within a pod\n\t- almost the same as `docker container exec`, except kubectl allows us to execute a process in a container running in any node within the cluster (`docker container exec` is limited to container running on a specific node)\n- pass `-c <containername>` to specify which container to run the process in.\n\n###### List processes in pod\n`kubectl exec db -- ps aux`\n\n###### Open bash shell in pod\n`kubectl exec -it db bash`\n\n###### List env variables in a pod\n`kubectl exec <POD_NAME> env`\n- list environment variables available in a Pod\n\n##### Expose\n- expose a resource as a new Kubernetes service\n\t- the resource can be a Deployment, another Service, a ReplicaSet, a ReplicationController, or a Pod\nex\n```sh\nkubectl expose rs go-demo-2 \\\n\t--name=go-demo-2-svc \\\n\t# this port will be exposed on every node of the cluster to the outside world,\n\t# and it will be routed to one of the Pods controlled by the ReplicaSet\n\t--target-port=28017 \\\n\t--type=NodePort # what type of service?\n```\n\n##### Get\n`kubectl get all --all-namespaces`\n- get all components that make up the cluster\n\n`kubectl get nodes`\n- get all nodes of a cluster\n\n`kubectl get pods`\n- get all pods of a cluster\n- pass `-o wide` or `-o yaml` for more info\n- `--show-labels` to see the labels on each pod\n\n`kubectl get rs`\n- get all ReplicaSets\n\n`kubectl get ep`\n- get endpoints of a cluster\n\n`kubectl get -f pod/go-demo-2.yml -o jsonpath=\"{.spec.containers[*].name}\"`\n- format output as only including the `name` property, within `spec.containers`\n\n##### Logs\n- get logs from a pod\n`kubectl logs <podname>`\n\n`-f` to follow the logs in real-time\n\n##### Label\n- We can remove a label from a pod:\n\t- `kubectl label pod/pod.yml service-` will remove the service label (the `-` at the end is the syntax for removing labels)\n\n##### Rollout\n- allows us to manage the rollout of a resource\n`kubectl rollout status -w -f deploy/go-demo-2-api.yml`\n\n`kubectl rollout history -f deploy/go-demo-2-api.yml`\n- This lets us see how many revisions of the software there have been, and which command created each revision.\n`kubectl rollout undo -f deploy/go-demo-2-api.yml`\n`kubectl rollout undo -f deploy/go-demo-2-api.yml --to-revision=2`\n\n##### Set\n- set configuration properties for resources\n\t- ex. update docker image of a pod template, update env variables of a pod template etc.\n- note: the output of this command only indicates that the definition (ex. of the image used in the Deployment) was successfully updated. This means that if we used an image that doesn't exist, we would still get a \"success\" output from this command.\n","n":0.044}}},{"i":538,"$":{"0":{"v":"Controllers","n":1},"1":{"v":"\nA controller watches the API server for new events. When it detects that there is a new object (eg. a new ReplicaSet), it acts in accordance with the type of controller it is.\n- ex. In the case of a ReplicaSet, the controller would create pods equal to the number found in the replica-set yaml file\n","n":0.135}}},{"i":539,"$":{"0":{"v":"Replication Controller","n":0.707},"1":{"v":"\nThrough the replication controller, Kubernetes will manage your pods’ lifecycle, including scaling up and down, rolling deployments, and monitoring\n![](/assets/images/2021-05-28-11-33-47.png)\n","n":0.229}}},{"i":540,"$":{"0":{"v":"ReplicaSet","n":1},"1":{"v":"A ReplicaSet ensures that a set of identically configured Pods are running at the desired replica count. If a Pod drops off, the ReplicaSet brings a new one online as a replacement.\n\nReplicaSets are considered a low-level type in Kubernetes\n\n- Higher level abstractions can be used instead, like [[deployments|k8s.objects.deployment]] and [[daemon sets|kubernetes.daemon-set]]\n\nIf we run pods without a controller, than a failing pod will remain dead (and will not be restarted by Kubernetes). Pods run in this manner have no fault-tolerance.\n\n- ReplicaSet serves as a self-healing mechanism\n- Pods associated with a ReplicaSet are guaranteed to run. They provide fault-tolerance and high availability.\n\nA ReplicaSet operates at the cluster level, and therefore has control over all pods in the cluster\n\nReplicaSets are rarely used independently. You will almost never create a ReplicaSet directly just as you’re not going to create Pods.\n\n- Instead, we tend to create ReplicaSets through [[Deployments|k8s.objects.deployment]]. In other words, we use ReplicaSets to create and control Pods, and Deployments to create ReplicaSets (and a few other things).\n\nReplicaSet `MatchLabels` are used to identify pods and must be the same as the pod labels\n\nSince ReplicaSets and Pods are loosely coupled objects with matching labels, we can remove one without deleting the other.\n\n- We can, however, pass `--cascade=true` if we want to delete pods that are associated with a RS\n\nReplicaSet uses labels to decide whether the desired number of Pods is already running in the cluster\n\n- this fact means that if we are to delete a RS and then re-create it, it will use the same pods that were running in the cluster. This shows how a RS knows what the pods it defines look like, and if there is a pod (or pods) already in existence that satisfy that definition, then it will simply consider them as part of its pod # count.\n\n## Process of Creating a ReplicaSet\n\n1. Kubernetes client (kubectl) sent a request to the API server requesting the creation of a ReplicaSet defined in the rs yaml file.\n2. The controller is watching the API server for new events, and it detected that there is a new ReplicaSet object.\n3. The controller creates two new pod definitions because we have configured replica value as 2 in the rs yaml file.\n4. Since the scheduler is watching the API server for new events, it detected that there are two unassigned Pods.\n5. The scheduler decided which node to assign the Pod and sent that information to the API server.\n6. Kubelet is also watching the API server. It detected that the two Pods were assigned to the node it is running on.\n7. Kubelet sent requests to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines two containers based on the mongo and api image. So in total four containers are created.\n8. Finally, Kubelet sent a request to the API server notifying it that the Pods were created successfully.\n\n![](/assets/images/2021-05-30-18-19-36.png)\n\n","n":0.046}}},{"i":541,"$":{"0":{"v":"Endpoint Controller","n":0.707},"1":{"v":"\nThe endpoint controller watches the API server for new service events. When it detects that there is a new Service object, it creates endpoint objects with the same name as the Service, and it uses Service selector to identify endpoints (eg. the IP and port of the pods)\n\nThe endpoint controller exists in the Kubernetes master node, and interfaces with the k8s API server\n","n":0.126}}},{"i":542,"$":{"0":{"v":"DaemonSet","n":1},"1":{"v":"\nDaemonSets have many uses\n- One common pattern is to use a DaemonSet to install/configure software on each host node.\n\nDaemonSets provide a way to ensure that a copy of a pod is running on every node in the cluster. As a cluster grows and shrinks, the DaemonSet spreads these specially labeled Pods across all of the nodes.\n","n":0.134}}},{"i":543,"$":{"0":{"v":"Components","n":1}}},{"i":544,"$":{"0":{"v":"Node","n":1}}},{"i":545,"$":{"0":{"v":"Kubelet","n":1},"1":{"v":"\nKubelet runs on each node. Its primary function is to make sure that assigned pods are running on the node.\n- It watches for any new Pod assignments for the node. If a Pod is assigned to the node Kubelet is running on, it will pull the Pod definition and use it to create containers through Docker or any other supported container engine.\n","n":0.127}}},{"i":546,"$":{"0":{"v":"Kube Proxy","n":0.707},"1":{"v":"\nkube-proxy is a network proxy that runs on each node in your cluster\n\nkube-proxy maintains network rules on nodes.\n- These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.\n\nkube-proxy watches and detects when there are new services, and new endpoint objects.\n- it also manages iptable rules which capture traffic to the Service port and redirect it to endpoints. For each endpoint object, it adds an iptables rule which selects a Pod.\n\nkube-proxy interfaces with the k8s API server and iptables\n","n":0.108}}},{"i":547,"$":{"0":{"v":"Kube DNS","n":0.707},"1":{"v":"\nWhen using a service, requests to one of the exposed ports will be picked up by Kube DNS and forwarded to one of the pods.\n- ex. as a user, I can access the exposed port (ie. **27017**:34172) through any node of the cluster. Kube DNS is the component that picks up those requests and forwards them on.\n","n":0.132}}},{"i":548,"$":{"0":{"v":"Cluster","n":1},"1":{"v":"\nA Kubernetes cluster is a set of node machines for running containerized applications\n\nAt a minimum, a cluster contains a control plane and one or more compute machines (nodes).\n- The control plane is responsible for maintaining the desired state of the cluster, such as which applications are running and which container images they use.\n- Nodes actually run the applications and workloads.\n\nThe cluster enables Kubernetes to schedule and run containers across a group of machines.\n- note: this could mean multiple VMs on the same machine\n- The heart of this whole idea is that Kubernetes containers aren’t tied to individual machines. Rather, they’re abstracted across the cluster.\n\nA Kubernetes cluster has a desired state, which defines which applications or other workloads should be running, along with which images they use, which resources should be made available for them, and other such configuration details.\n- This desired state is described in a YAML file\n","n":0.082}}},{"i":549,"$":{"0":{"v":"Custom Resource Definition","n":0.577},"1":{"v":"\nA CRD defines a new resource type, and tells Kubernetes about it\n- Once a new resource type is added, new instances of that resource may be created.\n\nHandling CRD changes is up to us as the developer, though a common pattern is to create a custom controller that watches for new CRD instances, and responds accordingly.\n","n":0.135}}},{"i":550,"$":{"0":{"v":"Javascript","n":1},"1":{"v":"\n# Terms\n## expression\n- def - a piece of code that will be evaluated to produce a value, or a\npiece of code that is already at its furthest level of evaluation (string,\nnumber etc) \n    - ex. `5`\n    - ex. `\"hello\"`\n    - ex. `1 / 2` evaluates to `0.5`\n    - ex. `i++`\n    - ex. `'A' + 'string'`\n    - ex. `a && b`\n    - ex. `a ? b : c`\n    - ex. `function() {}`\n    - ex. `window.resize()`\n\n## Assignment\n- any time there is a LHS and RHS separated by `=`\n\n`__proto` is the connection between object instances and the prototypes that they \"inherit\" from\n\n* * *\n\n## Modules\nThe general pattern of a module is a function that defines private variables and functions; creates privileged functions which, through closure, will have access to the private variables and functions; and that returns the privileged functions or stores them in an accessible place.\n\n# UE Resources\n[Dan Abramov JS Fundamentals course: highly recommended](https://justjavascript.com/)","n":0.081}}},{"i":551,"$":{"0":{"v":"Lang","n":1}}},{"i":552,"$":{"0":{"v":"Type","n":1}}},{"i":553,"$":{"0":{"v":"Symbol","n":1},"1":{"v":"\nan instance of `Symbol` is guaranteed to be unique.\n\nSymbols can be used to add unique property keys to an object that won't collide with other code that might be added to the object.\n- this effectively enables a weak form of encapsulation.\n\nSymbols are specifically created to be used as Object keys where you fear that key might be overwritten by someone else and lead to trouble (as it can happen with String keys for example).\n- Since Symbols are all unique, the only way to access a property with a Symbol key is to already have that Symbol. This means you either have to be the one who wrote the code, or that Symbol was defined by the standard (e.g. `Symbol.iterator`) and they did that to avoid breaking older code that might have used `obj[\"iterator\"]` for example.\n\n```js\nlet sym1 = Symbol()\nlet sym2 = Symbol('foo')\n// since symbols are guaranteed to be unique, we can create multiple symbols with the same description\nlet sym3 = Symbol('foo')\n\nconsole.log(sym2 === sym3) // false\n```\n\nThe `description` is the value passed to `Symbol()`.\n- can be used for debugging purposes, but not to access the `symbol` itself.\n\n# UE Resources\n[Recommended MDN article on Symbols](https://hacks.mozilla.org/2015/06/es6-in-depth-symbols/)\n","n":0.073}}},{"i":554,"$":{"0":{"v":"Set","n":1},"1":{"v":"\nA `Set` is similar to an array, in that it is a linear non-keyed data structure. \n- Also, like an array the order of data is maintained. \n- Unlike an array, each value in the set may only occur once, making it more like an enum in that sense.\n\nIts main methods are:\n- new Set(iterable) – creates the set, and if an iterable object is provided (usually an array), copies values from it into the set.\n- set.add(value) – adds a value, returns the set itself.\n- set.delete(value) – removes the value, returns true if value existed at the moment of the call, otherwise false.\n- set.has(value) – returns true if the value exists in the set, otherwise false.\n    - `set.has` is on average faster than the `Array.prototype.includes` method when an Array object has a length equal to a Set object’s size.\n- set.clear() – removes everything from the set.\n- set.size – is the elements count.\n\nA key feature is that repeated calls of `set.add(value)` with the same value don’t do anything. That’s the reason why each value appears in a Set only once.\n\nFor example, we have visitors coming, and we’d like to remember everyone. But repeated visits should not lead to duplicates. A visitor must be “counted” only once.\n\nA set can be iterated over with `forEach` or `for..of`\n\n```js\nconst mySet = new Set()\nmySet.add(2)\nmySet.add(5)\nmySet.add(1)\n\n// [2, 5, 1]\n```\n","n":0.067}}},{"i":555,"$":{"0":{"v":"Numbers","n":1},"1":{"v":"\nyou should not do javascript math on non-integers. If you want to perform math on decimal numbers, you should first convert all values to an integer, do the math, then convert back\n- reason because javascript does math in binary, there are rounding errors when doing math.\n","n":0.147}}},{"i":556,"$":{"0":{"v":"Map","n":1},"1":{"v":"\nMap is a collection of keyed data items, just like an Object. But the main difference is that Map allows keys of any type, whereas object only allows strings and numbers.\n- ex. with a Map, `true`/`false` can be boolean keys. Even an object can be used as a key\n    - Using objects as keys is one of the most notable and important Map features.\n\nMethods and properties are:\n- new Map() – creates the map.\n- map.set(key, value) – stores the value by the key.\n- map.get(key) – returns the value by the key, undefined if key doesn’t exist in map.\n- map.has(key) – returns true if the key exists, false otherwise.\n- map.delete(key) – removes the value by the key.\n- map.clear() – removes everything from the map.\n- map.size – returns the current element count.\n\n```js\nconst myMap = newMap()\n\nmyMap.set('a', 1)\nmyMap.set('b', 2)\nmyMap.get('a') // 1\n\n/*\n{\n    a: 1,\n    b: 2\n}\n*/\n```\n","n":0.085}}},{"i":557,"$":{"0":{"v":"Scope","n":1},"1":{"v":"\n# Lexical scope\n- def - scope that is determined by the JS engine before any code has even been executed (ie. it only looks at the source code— known as [lexing time](https://en.wikipedia.org/wiki/Lexical_analysis))\n- when you see lexical, think \"static\"\n- lexical scoping is analogous to prototypical inheritance, in a sense that the engine will walk up the chain to find a variable, if one is not found in the local scope.\n\nblocks within if statements, else statements, while statements, and so on should ideally be one line long. If that is unachievable, then it should be striven for to have all code at a single level of indentation.\n","n":0.098}}},{"i":558,"$":{"0":{"v":"Promises","n":1},"1":{"v":"\n# Overview\nA promise accepts a callback function as a parameter. The callback function accepts 2 parameters, resolve and reject. If the task is successfully performed, then it returns resolve. Else it returns the reject.\n\nAn asynchronous function returns a promise immediately, so the client isn’t blocked. \n\nWith the callback method of achieving asynchrony, the idea is to pass a callback into a function and call that function after some event has occurred. \nPromises work a bit differently. Essentially, we create this object called a Promise (which if we recall, returns immediately when invoked, albeit with `[Pending]` status). To this object, we attach 2 callback functions: `resolve` and `reject`.\n- we call `resolve` if the async action succeeds\n- we call `reject` if the async action fails\n  - most commonly the `resolve` and `reject` callbacks are asynchronous functions that return a promise. Naturally, they get called upon completion/failure of the async operation (or in the case of a promise chain, they get called when the previous promise resolves.)\n\nPromises make some guarantees about their use:\n- Callbacks added with `then()` will never be invoked before the completion of the current run of the event loop. These callbacks will be invoked even if they were added after the success or failure of the asynchronous operation (e.g. fetch for data) that the promise represents.  Multiple callbacks may be added by calling `then()` several times. They will be invoked one after another in a synchronous way.\n\nusing promises effectively abstracts time out of the picture.\n- abstracting away time allows us to better handle race conditions\n\n# Breakdown\nThere are 2 sides to promises: the `executor` (the one doing the actions) and the `consumer` (the one waiting for the actions to be done so it can consume the result.\nExecutor ex. `(resolve, reject) =>`\nConsumer ex. `.then, .catch, .finally`\n\n```js\n// Executor\nvar p1 = new Promise((resolve, reject) => {\n  resolve('Success!');\n  // or\n  // reject(new Error(\"Error!\"));\n});\n\n// Consumer\np1.then(value => {\n  console.log(value); // Success!\n}, reason => {\n  console.error(reason); // Error!\n});\n```\n\n## Executor\nExecutor is usually defined as part of a library, so it's not often that we write this code\nA executor sends the data with `resolve(stuff)`\n\nExecutor gets run as soon as it's defined. Therefore, its state will either be resolved or rejected\n\n`Promise.resolve()` and `Promise.reject()` are shortcuts to manually create an already resolved or rejected promise respectively. This can be useful at times.\n\n## Consumer\nThink of `.then`, `.catch` and `.finally` as the way that consumers subscribe to the executor\nA consumer receives the data (`stuff`) sent via the executor (ie. `resolve(stuff)`) with `.then(stuff)`\n\n```js\ndoSomething()\n  .then(result => doSomethingElse(result))\n  .then(newResult => doThirdThing(newResult))\n  .then(finalResult => {\n    console.log(`Got the final result: ${finalResult}`);\n  })\n  .catch(onRejected);\n```\n\n#### `.then()`\n`.then` lets us chain (ie. compose) 2+ asynchronous operations.\n\nSignature:\n- note: most often the onRejected is omitted, with responsibility delegated to a final `.catch`\n```js\nconst then = (onFulfilled, onRejected) => Promise\n```\n\nTo avoid surprises, functions passed to `then()` will never be called synchronously, even with an already-resolved promise:\n- Instead of running immediately, the passed-in function is put on a microtask queue, which means it runs later (only after the function which created it exits, and when the JavaScript execution stack is empty), just before control is returned to the event loop\n```js\nPromise.resolve().then(() => console.log(2));\nconsole.log(1); // 1, 2\n```\n\n`.then()` is a method available on a Promise. It doesn't even matter if the Promise is `pending`, `fulfilled`, or `rejected`.\n- if we call `.then` on an already resolved/rejected promise, the `.then` block will be triggered instantly; however, the handler functions will be triggered asynchronously. To illustrate:\n```js\nconst resolvedProm = Promise.resolve(33);\n\nlet thenProm = resolvedProm.then(value => {\n    console.log(\"this gets called after the end of the main stack. the value received and returned is: \" + value);\n    return value;\n});\n// instantly logging the value of thenProm\nconsole.log(thenProm);\n\n// using setTimeout we can postpone the execution of a function to the moment the stack is empty\nsetTimeout(() => {\n    console.log(thenProm);\n});\n\n// logs, in order:\n// Promise {[[PromiseStatus]]: \"pending\", [[PromiseValue]]: undefined}\n// \"this gets called after the end of the main stack. the value received and returned is: 33\"\n// Promise {[[PromiseStatus]]: \"resolved\", [[PromiseValue]]: 33}\n```\n\n\n##### Handler functions (`onFulfilled`/`onRejected`)\nif the promise is resolved (ie. when the underlying async operation is completed), `onFulfilled` is called asynchronously (to be scheduled in the current thread loop).\n\nSignature:\n```js\nconst onFulfilled = (fulfillmentValue) => valueOfResolveFunction\n```\n\nHandler functions have some behaviours. If it...\n- returns a value, the promise returned by then gets resolved with the returned value as its value.\n- doesn't return anything, the promise returned by then gets resolved with an undefined value.\n- throws an error, the promise returned by then gets rejected with the thrown error as its value.\n- returns an already fulfilled promise, the promise returned by then gets fulfilled with that promise's value as its value.\n- returns an already rejected promise, the promise returned by then gets rejected with that promise's value as its value.\n- returns another pending promise object, the resolution/rejection of the promise returned by then will be subsequent to the resolution/rejection of the promise returned by the handler. Also, the resolved value of the promise returned by then will be the same as the resolved value of the promise returned by the handler.\n\nThink of async actions atomically, and make each step a `.then()` that returns the input for the next `.then()`\nEx. First .then() returns the JSON data as an obj, second returns a subset of that data, third acts on it\n\n`.then` is analogous to adding another subscriber to the mailing list\n\n#### Rejecting Promises\n`.catch(onRejected)` is short for `.then(null, onRejected)`\n- If there's an exception, the browser will look down the chain for `.catch()` handlers or `onRejected`.\n\nWhenever a promise is rejected, a [rejectionhandled event](https://developer.mozilla.org/en-US/docs/Web/API/Window/rejectionhandled_event) is sent to the global scope (probably `window`).\n\nJust like `.then`, `.catch` returns a promise, meaning we can chain another `.then` onto `.catch` if we want to \n- this is effectively saying \"do this thing, even if some previous async operation failed\"\n\nspec:when we wrap a function in `Promise(..)`, we are promisifying it\n- The new function that returns this promisified function now returns a Promise that resolves to its original return value\n\n(Inside promises) if a promise is returned (eg. return Promise.resolve('stuff')), the next .then() will execute only when that promise has resolved (with 'stuff').\nIf the return value is anything else besides a promise, then it will be passed immediately to the next .then()\n\n.resolve(value)\nIf the value passed to it is a promise itself, this will automatically \"follow\" that promise chain and wait to pass back the final resolved value.\n- Good to use if you are unsure if a value is a promise or not\n\n`Promise.all` is a server at a restaurant waiting to bring everyone's food at the same time, even though one meal may be ready before the others\n\n* * *\n\n## Wrapping Callbacks in Promises\nIn an ideal world, all asynchronous functions would already return promises. Unfortunately, some APIs still expect success and/or failure callbacks, like `setTimeout()`.\n\nLuckily we can wrap `setTimeout` in a promise. \n- Best practice is to wrap problematic functions at the lowest possible level, and then never call them directly again:\n```js\n// Basically, the promise constructor takes an executor function that lets us resolve or reject a promise manually. \n// Since setTimeout() doesn't really fail, we left out reject in this case.\nconst wait = ms => new Promise(resolve => setTimeout(resolve, ms));\n\nwait(10*1000).then(() => saySomething(\"10 seconds\")).catch(failureCallback);\n```\n\n## Promises are only ever resolved once per creation\n- In the following code it seems that we would connect to the database 2 times, but Promises don't work like that.\n- since `databasePromise` is only defined one time, it can by definition only resolve one time.\n```js\nconst databasePromise = connectDatabase();\n\nconst booksPromise = databasePromise\n  .then(findAllBooks);\n\nconst userPromise = databasePromise\n  .then(getCurrentUser);\n\nPromise.all([\n  booksPromise,\n  userPromise\n])\n  .then((values) => {\n    const books = values[0];\n    const user = values[1];\n    return pickTopRecommentations(books, user);\n  });\n```\n\n## Sleep example\n- We can use promises make a function who's purpose is to simply wait, before executing further code. We define it as such:\n```js\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n```\n- as soon as we call `sleep(1000)`, a promise is returned to us. This means that javascript will say \"ok, since we're waiting on that promise to resolve (or reject), I'm going to go do some other stuff, and once your promise resolves, I'll be back to execute the `.then()` code\". Once\n\t- This is precicely why promises are said to \"handle asynchronous things synchronously\". It is because promises help us manage 2 different lines of execution at a time\n","n":0.027}}},{"i":559,"$":{"0":{"v":"Async Await","n":0.707},"1":{"v":"\nevery function annotated with async returns an implicit promise\n- *\"The async function declaration defines an asynchronous function, which returns an AsyncFunction object. An asynchronous function is a function which operates asynchronously via the event loop, using an implicit Promise to return its result.\"*\n\nevery time we see `await`, it means the promise must resolve before moving on. If it is rejected, then an error is thrown, and it's up to use to `catch` it.\n\n`await` must appear directly inside an `async` function. Therefore, we cannot do this:\n```js\nconst myFunc = async () => {\n  const mapResult = dataFromA.map(item => {\n    const dataFromB = await doApiCallHere(item.id)\n\n    return {\n      ...dataFromA,\n      dataFromB,\n    }\n  })\n}\n```\nHere, we are trying to use `await` within a `map` function. spec: since `map` is not asynchronous, this wouldn't make sense.\n\nIf we wanted to do this, we would need to pass an `async` function to map:\n```js\nconst mapResult = dataFromA.map(async item => {\n...\n```\n\n`map` now returns an array of promises, which we can then `Promise.all` over:\n```js\nconst myFunc = async () => {\n  const promises = dataFromA.map(async item => {\n    const dataFromB = await doApiCallHere(item.id)\n\n    return {\n      ...dataFromA,\n      dataFromB,\n    }\n  })\n\n  Promise.all(promises)\n}\n```\n\n\n### Top-level Await\n- Top-level await allows us to use await outside of a function marked `async`\n\t- It acts like a big async function causing other modules who import them to wait before they start evaluating their body.\n- In the latest ECMAScript, we can run `await` at the top level\n","n":0.065}}},{"i":560,"$":{"0":{"v":"Op","n":1}}},{"i":561,"$":{"0":{"v":"Pipeline","n":1},"1":{"v":"\nUseful any time we want to do something like this:\n```js\nconst result = double(double(increment(double(2))))\n```\n\nWith pipeline operator, we can do this:\n```js\n2 |> double |> increment |> double |> double\n```\n\nNote: as of 06/2020, this is still only stage one, so polyfills are needed\n","n":0.158}}},{"i":562,"$":{"0":{"v":"Optional Chaining","n":0.707},"1":{"v":"\n# `?.`\nJust as `??` doesn't abide by JS falsy (instead, only `undefined`/`null`), so too does the `?.` operator\n\nExample:\n```js\nconst eligibleRegion = publishingDetails?.salesRights?.[0]?.countriesIncluded?.map(\n```\n","n":0.218}}},{"i":563,"$":{"0":{"v":"Coalesce","n":1},"1":{"v":"\nIn Javascript we often reach for `||` when we want to get the RH value if the LH value is *any* falsy value\n- ex.\n```js\nconst valA = nullValue ?? \"default for A\";\n```\n\nThis way of doing it works most of the time, but if our business logic considers falsy values like `0` or `''` as legitimate values, then our program will have unexpected behavior. Using `??` gives us more predictable results, and should be used most of the time instead of `||`\n\n### Assigning a default value to a variable\nIn the past, when one wanted to assign a default value to a variable, a common pattern was to use the logical OR operator (`||`):\n```js\nlet foo;\n\n//  foo is never assigned any value so it is still undefined\nlet someDummyText = foo || 'Hello!';\n```\n\nHowever, this is kind of hacky when we consider what we are doing. `||` is a boolean logical opertor (ie. it performs \"math\" on booleans). In order to do boolean math on 2 variables, it has to make sure both values are booleans. If they are not, they have to be coerced (Of course, JS is notorious for its sometimes unpredictable coercion behaviour).\n- With boolean logical operators, the only two values that are considered are truthy values and falsy values. This might work most of the time, but sometimes our business logic dictates that we would expect `0` to be a truthy value. If this is the case, then\n\nExample:\n\nBelow, `quantityOfApples` will not be `0` as we want. Rather, the default value of `10` is being used, because `0` is considered falsy in the eyes of the boolean logical operator.\n```js\nconst count = 0\n\nconst quantityOfApples = count || 10 // 10 — unexpected!\nconst actualQuantityOfApples = count ?? 10 // 0 — totally expected\n```\n\n### Misc\n- It is not possible to combine both the AND (`&&`) and OR operators (`||`) directly with `??`\n\t- To accomplish this, we need to use parentheses (`(null || undefined) ?? \"foo\";`)\n","n":0.056}}},{"i":564,"$":{"0":{"v":"Objects","n":1},"1":{"v":"\nThe lineage of an object is irrelevant. What matters about an object is what it can do, not what it is descended from.\n\nJavaScript object keys are always coerced to a string, so obj[0] is always the same as obj[\"0\"].\n\nAs of ES6, objects have a predictable order, determined in one of 2 ways:\n1. *Keys are numbers* - The key-value pair being inserted will obey numerical order\n\t- ex. if we have existing keys `4`, `8`, and `33`, then inserting a key-value pair with the key as `1` will put it in first position of the object\n2. *Keys are strings* - The key-value pair being inserted will be appended to the end of the object\n\n### Polymorphism with objects\nImagine we had the idea of a session in our code. Instead of being one type however, we want it to either evaluate to an object, or the id of the object. Therefore, either object or string. This enables us to achieve polymorphism in functions, since we can take in a `session` argument, and depending on its type, we can act accordingly.\n```js\n// code from graphile/starter\nexport const becomeUser = async (\n  client: PoolClient,\n  userOrUserId: User | string | null,\n) => {\n  await becomeRoot(client)\n  const session = userOrUserId\n    ? await createSession(\n        client,\n        typeof userOrUserId === 'object' ? userOrUserId.id : userOrUserId,\n      )\n    : null\n  await client.query(\n    `select set_config('role', $1::text, true), set_config('jwt.claims.session_id', $2::text, true)`,\n    [process.env.DATABASE_VISITOR, session ? session.uuid : ''],\n  )\n}\n```\n","n":0.066}}},{"i":565,"$":{"0":{"v":"Proxy","n":1},"1":{"v":"\n## Proxy Object\n- def - an object that wraps another object. The proxy object intercepts the fundamental operations of the wrapped object, including property lookup, assignment, function invocations etc\n\t- In other words, we can have a regular object that we interact with normally. We can also wrap that regular object with a proxy object, that will intercept those interactions that we have with the object.\n- Think of a proxy object like an enhanced (wrapped) component in React. It can do everything that the original can, but has some added functionality. This added functionality might be as simple as logging to the console every time a property is read (this would be implemented using a **get trap**. \n- The wrapped object is called the *target*\n- The custom functionality that is added is called the *handler*\n- *trap* - a method defined on the proxy that will intercept some interaction we have with the object (ie. read, write)\n","n":0.08}}},{"i":566,"$":{"0":{"v":"Cloning","n":1},"1":{"v":"\n# UE Resources\n[Cloning objects in Javascript](http://www.zsoltnagy.eu/cloning-objects-in-javascript/)\n","n":0.408}}},{"i":567,"$":{"0":{"v":"Methods","n":1},"1":{"v":"\n## Split\n","n":0.707}}},{"i":568,"$":{"0":{"v":"Iterator","n":1},"1":{"v":"\nThere are two iteration protocols: *iterable protocol* and *iterator protocol*.\n\n### Iterator protocol\nAn object is an iterator when it implements an interface that answers two questions:\n- Is there any element left?\n- If there is, what is the element?\n\nTechnically speaking, an object is qualified as an iterator when it has a `next()` method that returns an object with two properties:\n- `done`: a boolean value indicating whether or not  there are any more elements that could be iterated upon.\n- `value`: the current element.\n\nIf we call `next()` after the last value has returned, we are returned:\n```js\n{done: true: value: undefined}\n```\n\nArray, Maps, and Sets have built-in iterators (ie. the protocol is already implemented)\n- If you have a custom type and want to make it iterable so that you can use the `for...of` loop construct, you must implement the iteration protocols by hand.\n\n### Iterable protocol\nAn object is iterable when it contains a method called `[Symbol.iterator]` that takes no argument and returns an object which conforms to the iterator protocol (ie. has `next()`, and an object `{ value: x, done: false }`).\n- The `[Symbol.iterator]` is one of the built-in well-known symbols in ES6.\n\n# E Resources\n[Roll your own iterable object](https://www.javascripttutorial.net/es6/javascript-iterator/)\n","n":0.072}}},{"i":569,"$":{"0":{"v":"Imports","n":1},"1":{"v":"\nModules import one another using a module loader. At runtime the module loader is responsible for locating and executing all dependencies of a module before executing it. Well-known module loaders used in JavaScript are Node.js’s loader for CommonJS modules and the RequireJS loader for AMD modules in Web applications.\n\nIn TypeScript, just as in ECMAScript 2015, any file containing a top-level import or export is considered a module. Conversely, a file without any top-level import or export declarations is treated as a script whose contents are available in the global scope (and therefore to modules as well).\n\n# ES Modules (ES6 Import)\nSince imports aren't a part of the Javascript standard yet, imports are just a specification— it is up to the implementation (Babel, Typescript, Metro Bundler(?)) to carry out the operation of linking modules.\n\nESM are required as they are needed, rather than there being a bundle created beforehand (as with CommonJS)\n\nwhen you tell the JS engine to run a module, it has to behave as though these four steps are happening:\n1. Parsing: The implementation reads the source code of the module and checks for syntax errors.\n2. Loading: The implementation loads all imported modules (recursively). (This is the part that isn’t standardized yet)\n3. Linking: For each newly loaded module, the implementation creates a module scope and fills it with all the bindings declared in that module, including things imported from other modules.\n\t- This is the part where if you try to import {cake} from \"paleo\", but the “paleo” module doesn’t actually export anything named cake, you’ll get an error. And that’s too bad, because you were so close to actually running some JS code. And having cake!\n4. Run time: Finally, the implementation runs the statements in the body of each newly-loaded module. By this time, import processing is already finished, so when execution reaches a line of code where there’s an import declaration… nothing happens!\n- when we import a module like `import 'firebase/storage'` (ie. without declaring a variable), it means we are executing the module `firebase/storage`, but not bothering to assign the default export to a variable.\n\t- This implies we are doing side-effects.\n\n- When you `import *`, what’s imported is a *module namespace object*. The properties of this object are the module’s exports:\n```\n[Module] {\n  default: '[Function]', // the default export\n  first: 'Kyle' // named export `first`\n  last: 'Tycholiz', // named export `last`\n}\n```\n- if we wanted to import the named exports, we could `import { first } from _____`. spec: also, we could import the default by `import default from _____` (or `import { default }`?)\n\n- `import _ from \"lodash\"` is an alias for `import { default as _ } from \"lodash\"`\n* * *\nImported ES6 modules are executed either asynchronously or synchronously, depending on the module loader (ie. the implementation) we use. Therefore, to be safe we must assume async. However, all imports are executed prior to the script doing the importing. This makes ES6 modules different from Node.js modules or `<script>` tags without the `async` attribute\n\n### Importing without name\nex. `import './bootstrap'`\n- this will execute the target module (ie. run the module's code), without importing anything. It will not affect the scope of the active module\n\t- There may be side-effects, such as declaring global variables.\n- This method of importing is described as \"importing a module for its side-effects only\"\n\n### Aggregating modules (Re-exporting)\n- We can import modules and immediately export them again by aggregating the import and export commands:\n```\nexport * from './atoms'\n```\n- If any name exported by “atoms” happened to collide with the other exports, that would be an error, so use export * with care.\n\nUnlike a real import, this doesn’t add the re-exported bindings to your scope, meaning we can't use the exports from \"atoms\" within that file.\n\n# CommonJS Imports\n## module.exports\n- `module.exports` is an object that is included in every `.js` file in a Node application.\n\t- `module` represents the current module\n\t- `exports` is an objects that will be exposed as a module\n\t- Therefore whatever we assign to `module.exports` is exposed as a module.\n- like `exports` below, `module.exports` can also be extended by including more properties/methods on the object.\n\nbefore a module's code is actually executed, Node will wrap it in a function that looks something like this:\n```\n(function(exports, require, module, __filename, __dirname) {\n// Module code actually lives in here\n});\n```\nThis gives us the benefit of:\n- scoped variables, rather than global variables.\n- ability to use `module` and `exports` objects.\n- ability to reference the module's absolute filename and directory path with `__filename` and `__dirname`\n\n## exports\n- `exports` is an object that we can attach properties and methods to.\n- when we import a module, we must then call the same property/method:\n```\n// dependency\nexports.name = 'Kyle'\nexports.phone = '7788713377'\n\n// dependent\nconst person = require('./information')\n\nconsole.log(person.name) // Kyle\n```\n\n# CommonJS vs ES6 Modules\n- Under the hood, we need something like Babel to convert from ES6 modules to CommonJS.\n\n[explanation](https://stackoverflow.com/questions/40294870/module-exports-vs-export-default-in-node-js-and-es6)\n\n# Circular Dependencies\n- Not always a problem, but they introduce tight coupling.\n\t- These kinds of modules are harder to understand and reuse, as doing so might cause a ripple effect where a local change to one module has global effects.\n\t- As such, it might indicate lack of a larger context or proper architecture, since a good architecture imposes uni-directional flow between modules and entire layers.\n\nusing `const` over `function` while defining functions prevents function hoisting within a single module and ensures the absence of circular dependencies within a single module.\n\nCircular dependencies with Function calls would not cause problems when the cycle is asynchronous, meaning that directly referenced functions are not called immediately.\n- ex. Cycle of function calls when one continues chain through a DOM event listener being async, i.e. waiting for user click.\n\n# Dynamic Imports\n`import(module)` loads the module and returns a promise that resolves into a module object that contains all its exports. `import` can be called from anywhere in the code.\n\nexample\n```\nimport(modulePath)\n  .then(obj => <module object>)\n  .catch(err => <loading error, e.g. if no such module>)\n```\nor\n```\n// 📁 say.js\nexport function hi() {\n  alert(`Hello`);\n}\n\nexport function bye() {\n  alert(`Bye`);\n}\n```\nthen\n```\nlet {hi, bye} = await import('./say.js');\n\nhi();\nbye();\n```\nNote: Although `import()` looks like a function call, it is specified as syntax that just happens to use parentheses (similar to super()). That means that import doesn’t inherit from Function.prototype so you cannot call or apply it.\n\nDynamic means code that can be executed at runtime. Static means it occurs before compilation and before runtime. An example is dynamic imports. in this case, dynamic means that we can decide the path at runtime, or we can decide which module to import at runtime\n\n# UE Resources\nhttps://v8.dev/features/modules\n","n":0.031}}},{"i":570,"$":{"0":{"v":"Functions","n":1},"1":{"v":"\n## Tagged Template Literals\n```js\n// These are equivalent:\nfn`some string here`\nfn(['some string here'])\n```\n- - -\nThe rest of the arguments will be the interpolations, in order.\n```js\nconst aVar = 'good'\n\n// These are equivalent:\nfn`this is a ${aVar} day`\nfn(['this is a ', ' day'], aVar)\n```\n\n## Identity function\n```\nfetchBook()\n  .then((book) => formatBook(book))\n  .then((postscript) => print(postscript))\n```\nis equivalent to (verify this)\n```\nfetchBook()\n  .then(formatBook)\n  .then(print);\n```\n","n":0.137}}},{"i":571,"$":{"0":{"v":"Ttl","n":1},"1":{"v":"\n[explanation using styled-components](https://mxstbr.blog/2016/11/styled-components-magic-explained/)\n","n":0.577}}},{"i":572,"$":{"0":{"v":"Generator Functions","n":0.707},"1":{"v":"\n- can be exited and later re-entered\n- like closures, variables inside the generator function maintain state.\n- when calling a generator function, an iterator object is returned. When we call `next()` on that object, all the code up until the first `yield` will be executed. Calling `next()` again will then execute all the code up until the second `yield`, and so on.\n\t- The function that calls the generator function is the **iterator**\n- the generator function can pass values to the iterator object (`yield`). Anything that occurs after `yield` gets stored in the iterator's `next()` value\n\t- The generator function can also retrieve values from the iterator object (`next(___)`)\n- `yield` returns execution to outside the generator function (ie. the context from which the gen fn was called), it's possible to use `while(true)`, as long as there is a yield inside\n\t- This way, `next()` can keep getting called\n- spec: `next` is like async/await in the sense that it will execute code up until a point (`yield`), then stop and wait for the availability of that data before continuing on\n\n# UE Resources\n[Observable Async flow control (Eric Elliott)](https://medium.com/javascript-scene/the-hidden-power-of-es6-generators-observable-async-flow-control-cfa4c7f31435)\n","n":0.074}}},{"i":573,"$":{"0":{"v":"IIFE","n":1},"1":{"v":"\nWhen async/await was first introduced, attempting to use an await outside of an async function resulted in a SyntaxError. Many developers utilized immediately-invoked async function expressions as a way to get access to the feature.\n\n```js\nawait Promise.resolve(console.log('🎉'))\n// → SyntaxError: await is only valid in async function\n\n;(async function() {\n  await Promise.resolve(console.log('🎉'))\n  // → 🎉\n}())\n```\n- note: this IIFE here could be considered to be analogous to the `main` function of a C program (ie. it is the entrypoint— it is what gets called when you run `node filename.js`). Otherwise, we would be declaring the function, then calling it\n\nWithout top-level await, JavaScript developers often used async immediately-invoked function expressions just to get access to await. Unfortunately, this pattern results in less determinism of graph execution and static analyzability of applications. For these reasons, the lack of top-level await was viewed as a higher risk than the hazards introduced with the feature.\n","n":0.082}}},{"i":574,"$":{"0":{"v":"Feat","n":1}}},{"i":575,"$":{"0":{"v":"While","n":1},"1":{"v":"\n### Use while(true) and break\n- we can make a while loop that will continuously execute code until `break` is reached.\n```js\nwhile(true) {\n\ttry {\n\t\tawait client.query('select true as \"Connection test\";')\n\t\tbreak //if the promise above resolves, then break will be run and we will exit the while-block \n\t} catch(e) {\n\t\tawait sleep(1000)\n\t}\n}\n```\n","n":0.146}}},{"i":576,"$":{"0":{"v":"This","n":1},"1":{"v":"\n`this` only cares about execution context (where the fn was called). it doesn't care about the scope chain\n\n`this` is essentially an implicit input to a function, thereby negatively impacting function purity. Think of `this` as an implicit parameter that gets passed into the function.\n- ie. because of this impure nature, `this` shouldn't be used in FP\n\nThe scope chain encompasses the prototype chain\n- Expl. Imagine 2 functions: inner() and outer(). If a variable is used within inner() and it does not exist within that function's context, it will look at its prototype to see if it exists (prototype chain). If it goes all the way up the chain and still doesn't exist, then the scope of outer() will be considered.\n- Expl. If this were illustrated as 2 for loops, the scope chain would be the `i` iterator, while prototype would be `j`\n\n`return` is a keyword which returns us to the immediate outer execution context (to continue parsing at the point directly after where the function was called)\n\n## Classes\nThe behavior of this in classes and functions is similar, since classes are functions under the hood. There are some important caveats:\n- methods of a class can be accessed on `this`. We might assume that this means that the method is a property on the `this` object. This is a false assumption. The reality is that the method is a property on the class itself (which is the prototype of `this`). This means that when we are accessing a method like `this.moveUnit`, the  `moveUnit` is being accessed by traversing this prototype chain, up from `this` and reaching the class itself, upon which the method is found.\n- note: if a method is marked `static`, then it is not added to the prototype of `this`.\n\n## Arrow functions\nIn arrow functions, `this` retains the value of the enclosing lexical context's `this`. \n- In global code, it will be set to the global object. Here, the enclosing lexical context (ie. the context of `foo`) is the global context:\n```js\nvar globalObject = this;\nvar foo = (() => this);\nconsole.log(foo() === globalObject); // true\n// `true` returned because the function `foo` is in the global scope\n```\n\nThis logic follows that if we had an arrow function within another function, the arrow functions `this` value would retain the value of the outer functions `this` object.\n```js\nfunction outer() {\n\tconst outerFunctionContext = this\n\tconst inner = (() => this)\n\tconsole.log(outerFunctionContext === inner())\n}\n```\n","n":0.051}}},{"i":577,"$":{"0":{"v":"Spread","n":1},"1":{"v":"\n### Spread Syntax\nSpread syntax is like zooming. When you see '...', you should think \"expand\"\n- what's important is the receiver (what we are spreading it out into (eg. [], {}, () )\n- this gives the ability for us to spread it out into an array, object, or function (as args). Notice these all accept iterables\n\n## UE Resources\n- the copying process is shallow\nhttps://zhenyong.github.io/react/docs/jsx-spread.html\nhttps://stackoverflow.com/questions/31048953/what-do-these-three-dots-in-react-do\n","n":0.127}}},{"i":578,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Remove a property from an object\nThis will remove id from the obj:\n```js\nConst { id, ...rest } = obj\n```\n","n":0.229}}},{"i":579,"$":{"0":{"v":"Math","n":1}}},{"i":580,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Pick random element in array\n```js\nfunction randomPick(array) {\n  let choice = Math.floor(Math.random() * array.length);\n  return array[choice];\n}\n```\n\n#### Generate random number between 1-10\n```js\nMath.floor(Math.random() * 10) + 1\n```\n","n":0.2}}},{"i":581,"$":{"0":{"v":"Hoist","n":1},"1":{"v":"\nOnly declarations are hoisted, not initializations\nEx. 'Const name' is hoisted, 'name = \"Kyle\"' is not.\n\n","n":0.258}}},{"i":582,"$":{"0":{"v":"For","n":1},"1":{"v":"\n### Run loop indefinitely, and stop when a certain condition is met\n```js\nfor (let turn = 0;; turn++) {\n  if (state.parcels.length == 0) {\n    console.log(`Done in ${turn} turns`);\n    break;\n  }\n  // do your stuff here\n}\n```\n\n## `for..of` vs `for..in`\n`for..of` should be used with arrays, and other iterable objects, like strings\n- an object is considered iterable if it adheres to the [[iterator protocols|js.lang.iterator]]\n`for..in` should be used with objects, and is used to iterate over the properties of an object\n\n### For..of\nGet index in `for..of`\n```js\nfor (const [i, value] of arr.entries()) {\n  indexMap[i] = value\n}\n```\n\n","n":0.106}}},{"i":583,"$":{"0":{"v":"Do While","n":0.707},"1":{"v":"\nBenefit of a `do...while` over a regular `while` is that `do...while` will always run at least once. A `while` will only run as long as the condition is satisfied. \n\nTherefore,\n```js\ndo {\n    do_work();  \n} while (condition);\n\n// is equivalent to:\ndo_work();\n\nwhile (condition) {\n    do_work();\n}\n```\n\n### Easy retry process with do-while\n```js\n// each time the loop runs, it checks to see if the place equals the address. If did, then there is an automatic retry done, accomplished with the do-while\nVillageState.random = function(parcelCount = 5) {\n  let parcels = [];\n  for (let i = 0; i < parcelCount; i++) {\n    let address = randomPick(Object.keys(roadGraph));\n    let place;\n    do {\n      place = randomPick(Object.keys(roadGraph));\n    } while (place == address);\n    parcels.push({place, address});\n  }\n  return new VillageState(\"Post Office\", parcels);\n};\n```\n","n":0.092}}},{"i":584,"$":{"0":{"v":"Destructuring","n":1},"1":{"v":"\n### Destructuring\n- With this technique we can pull out values from an array or object declaratively\n```js\nconst arr = [1, 2, 3, 4]\nconst [ a, , b ] = arr\n// a = 1\n// b = 4\n```\n- when spreading, we are unpacking a variable one level deeper (into an array, object etc)\n    - the `...` is unpacking, the `{}` is putting it into an object\n    - ***ex.*** - if we had an array of objects, the unpacking by itself would result in us having an invalid javascript, since it would just be a group of objects with no \"bucket\" to house them (such as an array or object). This is why `{ ...arrayOfObjects }` will result in those \"stray\" objects having found a home in the surrounding `{}`\n```js\nnewObj = {\n    0: {},\n    1: {},\n    ...\n}\n```\n```js\n<Component a={obj.a}, b={obj.b} c={obj.c} /> === <Component {...obj} />\n```\nwe can rename the object keys of a destructured object like so:\n- an object is returned from calling `useQuery` with fields `loading` and `error`, which we rename.\n```js\nconst { loading: queryLoading, error: queryError, data } = useQuery()\n```\n\nDestructuring creates a shallow copy\n\n# UE Resources\n[advanced destructuring](https://dmitripavlutin.com/javascript-object-destructuring/)\n","n":0.074}}},{"i":585,"$":{"0":{"v":"Coercion","n":1},"1":{"v":"\n# Object::String\nThe default conversion from an object to string is `[object Object]`\n- when we see `[object Object]` somewhere, it would seem to imply that the object is getting coerced into a string somewhere along the line, thus resulting in us seeing `[object Object]` \n- check to see if you didn't properly stringify json\n- output from `console.log` is captured at the application level, meaning that we won't get access to the logs if we are using multiple applications\n\t- ex. if we are using Azure Functions in our app, we are using multiple applications, since our codebase exists in a different application than our Azure Functions. \n","n":0.098}}},{"i":586,"$":{"0":{"v":"Async Iterator","n":0.707},"1":{"v":"\n### Async iterator\nAsynchronous iteration allow us to iterate over data that comes asynchronously, on-demand.\n- ex. download something chunk-by-chunk over a network.\n- in other words, it allows you to access asynchronous data sequentially.\n\nAsync iterators can be used when we don’t know the values and the end state we iterate over. Instead, we get promises.\n\n`Symbol.asyncIterator` is a built-in symbol that enables an object to be async iterable.\n- This enables us to use a `for-await-of` loop with async iterators, which allows us to loop over the async iterator.\n    - Imagine having an array of promises, and each promise represents some data that will arrive in the next chunk. We can loop over each promise, and do something with that data when it arrives. Only once the current promise has resolved will it move onto the next.\n- `Symbol.asyncIterator` is a method that returns the default AsyncIterator for an object. If the `asyncIterator` property is on an object, it is an Async Iterable, and therefore we can use `for-await-of`.\n\nAn async iterator is like an iterator except that its `next()` method returns a promise that resolves to the `{value, done}` object.\n\n\nThere are currently no built-in JavaScript objects that have the `[Symbol.asyncIterator]` key set by default.\n- Streams do, but they are from Node.js. In fact, async iterators are very useful when dealing with streams\n\nA great use case for async iteration is when we need to fetch data from a remote paginated dataset.\n- https://www.nodejsdesignpatterns.com/blog/javascript-async-iterators/\n\nSince the ability of an iterator to be async is determined by the presence of a `[Symbol.asyncIterator]()` property on an object, we can define our own:\n```js\nconst myAsyncIterable = {\n    async* [Symbol.asyncIterator]() {\n        yield \"hello\";\n        yield \"async\";\n        yield \"iteration!\";\n    }\n};\n\n(async () => {\n    for await (const x of myAsyncIterable) {\n        console.log(x);\n        // expected output:\n        //    \"hello\"\n        //    \"async\"\n        //    \"iteration!\"\n    }\n})();\n```\n\nAnother example:\n```js\nconst asyncIterable = [1, 2, 3];\nasyncIterable[Symbol.asyncIterator] = async function*() {\n    for (let i = 0; i < asyncIterable.length; i++) {\n        yield { value: asyncIterable[i], done: false }\n    }\n    yield { done: true };\n};\n\n(async function() { \n    // The for-await-of loop will wait for every promise it receives to resolve before moving on to the next one\n    for await (const part of asyncIterable) {\n        console.log(part);\n    }\n})();\n```\n\n#### Consuming paginated APIs\nhttps://blog.risingstack.com/async-iterators-in-node-js/\n","n":0.052}}},{"i":587,"$":{"0":{"v":"Console Methods","n":0.707},"1":{"v":"\n`console.X()` methods return undefined\n\n### console.table\nIf we have an array of identically formatted objects, we can print out a nice table to the console, instead of getting a dump of a JSON-like structure, as we would have with a simple console.log\n\n### console.group\n","n":0.156}}},{"i":588,"$":{"0":{"v":"Closures","n":1},"1":{"v":"\nNormally, when you exit a function, all its variables “disappear”. This is because nothing needs them anymore. But what if you declare a function inside a function? Then the inner function could still be called later, and read the variables of the outer function. In practice, this is very useful! But for this to work, the outer function’s variables need to “stick around” somewhere. So in this case, JavaScript takes care of “keeping the variables alive” instead of “forgetting” them as it would usually do.\n\n## Closures and Classes\nWhen a closure returns an object, it can function as an alternative to a class.\n- the key-value pairs of the closure correspond to the properties and methods of the class\n\nNotice that the following closure can be implemented as a class:\n```js\nconst UserClosure = ({ firstName, lastName, age, occupation }) => {\n\treturn ({\n\t\tdescribeSelf: () => {\n\t\t\tconst msg = `My name is ${firstName} ${lastName}, I am ${age} years old and I work as a ${occupation}`\n\t\t\treturn msg\n\t\t},\n\t\tgetAge: () => {\n\t\t\treturn age;\n\t\t},\n\t\tshowStrength: () => {\n\t\t\tlet howOld = age;\n\t\t\tlet output = 'I am';\n\t\t\twhile (howOld-- > 0) {\n\t\t\t\toutput += ' very';\n\t\t\t}\n\t\t\treturn output + ' Strong';\n\t\t}\n\t})\n}\n```\n\nClosures and classes behave differently in JavaScript with a fundamental difference: closures support [[encapsulation|paradigm.oop.encapsulation]], while JavaScript classes don’t support it.\n- in other words, we can create a closure where individual members of the closure are invisible to the outside world.\n\nWhen opting for closures over classes, closures offer simplicity, since we don’t have to worry about the context that this is referring to.\n- If we are creating multiple instances of an object, classes will best suit our needs. Meanwhile, if we don’t plan to create multiple instances, the simplicity of closures may be a better fit for our project.\n\n* * *\n\n### Simple state-management with closure\n```ts\nfunction makeState<S>() {\n  let state: S\n  function getState() {\n    return state\n  }\n  function setState(x: S) {\n    state = x\n  }\n  return { getState, setState }\n}\n\nconst { getState, setState } = makeState()\nsetState(1)\nconsole.log(getState()); // 1\nsetState(2)\nconsole.log(getState()); // 2\n```\n\nObserve that you can pass a type like so:\n```ts\nmakeState<number>()\n```\n\nThen, when you go to use `getState` and `setState`, the generic `S` will become `number`\n\n* * *\n\n\n### Inner/Outer function illustration\nFrom the context of an inner scope, there is a: local scope, any number of closure scopes, and a global scope. The closure scopes represent the different scopes of the surrounding code. If our current scope is nested 3 levels deep then there are 2 closure scopes. Within these scopes, there may exist variables.\n\nbecause of how lexical scope works, when we call a function that accesses a variable from outside its scope, it will capture it at the very time the function is created. This means even if that value changes in the future, the value it had *at the time* it was captured will be used.\n\n- A closure is a function that encloses its surrounding state by referencing fields external to its body. The enclosed state remains across invocations of the closure.\n- A closure occurs when the function we are calling returns another function, so the second function lives on, while the initial function dies (because it has already returned). Put another way, a closure is formed any time an inner function is made available from outside of the scope it was defined within.\n\n```js\nvar outer = () => () => {}\nvar innerFunc = outer()\n```\n- above, `innerFunc` causes `outer()` to execute, returning a function and setting its value to it. `innerFunc` has access to the local variables of its containing object (normally a containing function). Therefore, these \"sibling\" local variables are changeable from outside the function.\n- Think of a closure as the lifeline that an inner function extends to the variables (that the inner function has used) defined in the outer function. They continue to exist because the closure exists. In other words, the inner function closes over (ie. captures/remembers) the variables defined in the outer function.\n- Conceptually (but not actually), the closed over function (`outer`) has all of its variables put into an object. That is how `inner` is able to access those values. Something like this is happening:\n```js\nfunction outer() {\n    var x = 1;\n\n    return function inner(){\n        return x;\n    };\n}\n```\nmakes 2 objects like this:\n```js\nscopeOfOuter = {\n    x: 1\n}\n\nscopeOfInner = {};\n```\nthen `scopeOfOuter` is set as the prototype of `scopeOfInner`, so when we try to access the value of x with `return scopeOfInner.x`, we see that `scopeOfInner` doesn't have an `x` property, so it goes up the prototype chain and finds an `x` property on `scopeOfOuter`\n\n```js\nObject.setPrototypeOf( scopeOfInner, scopeOfOuter );\n```\n\n- Conceptually, the structure of a closure is not mutable. In other words, you can never add to or remove state from a closure\n- closures are a subset of lambdas\n\n### How scope enables closures to happen\nIn JS, a scope is created by a function or code block.\n- When we have 2 separate functions at the same level of the code, both can use the same variable names and not have collisions. But what happens when one fn (`inner`) goes within another (`outer`)?\n\nIn the following example, `myInnerFunc` is an instance of `innerFunc`, with the enhanced benefit of having access to `outerVar`\n- The reason it has access is because of lexical scope, which (importantly) is defined before any javascript code has run (ie. analyzed just by the source code)\n![dad83091392736b4fc218299b2073d6d.png](:/8e496538fa28463a9e908e9164c39882)\n- Therefore, a closure is a function that has access to its lexical scope, *even though* that function was executed from outside of that lexical scope.\n\t- Simpler, the closure is a function that remembers the variables from the place where it is defined (and not where it was executed)\n- A rule of thumb to identify a closure: if you see in a function an alien variable (not defined inside the function), most likely that function is a closure because the alien variable is captured.\n\n### Analogy\nImagine a magical paintbrush with an interesting property. If you paint with it some objects from real life, then the painting becomes a window you can interact with.\n\nThrough this window, you can move the painted objects with your hands.\n\nMoreover, you can carry the magical painting anywhere, even far from the place where you’ve painted the objects. From there, through the magical painting as a window, you can still move the objects with your hands.\n\nThe magical painting is a closure, while the painted objects are the lexical scope.\n![c40e3c1034d769d6bc5aa8f2100a83e7.png](:/cee3345f41c44ceca614faea5e5cc400)\n\n### Stale closures\n- stale closures capture variables that have outdated values.\n```js\nfunction createIncrement(i) {\n  let value = 0;\n  function increment() {\n    value += i;\n    console.log(value);\n    const message = `Current value is ${value}`;\n    return function logValue() {\n      console.log(message);\n    };\n  }\n\n  return increment;\n}\n\nconst inc = createIncrement(1);\nconst log1 = inc(); // logs 1\nconst log2 = inc();             // logs 2\nconst log3 = inc();             // logs 3\nlog1();             // logs \"Current value is 1\"\nlog2();             // logs \"Current value is 2\"\nlog3();             // logs \"Current value is 3\"\n```\n- `log{1,2,3}()` are stale closures, because it has already captured the value *at the time* that `inc()` was called. What's important to note here is that `inc()` is called 3 times. Every time it is called, it runs through the `increment` function that was closed over. It then returns that value, and holds it (within a function called `logValue` that prints out the held value). In other words, it does not get updated with each subsequent call of `inc()`. It has already held onto that value, and there is nothing it can do to change that fact.\n- Therefore, if we want to capture the freshest value, we have to figure out which closure it is that has those freshest variables.\n\t- Here, that variable would be the *latest* call of `inc()`.\n\n### Closures vs Objects\nclosure offers granular change control and automatic privacy.\nobject offers easier cloning of state\n\nClosures are made every time we create an event handler, a promise, setTimeout, and even within `useEffect` in React.\n\n### Node Debugger\nclosure scope is outside of local scope\nthere are multiple layers of closure state\n\n![](/assets/images/2021-10-26-09-37-54.png)","n":0.028}}},{"i":589,"$":{"0":{"v":"Callbacks","n":1},"1":{"v":"\nA callback is a function you provide as an argument to another function. When that other function has completed its task, it will execute the provided function. In the meantime, however, the code coming after the request will be executed regularly.\n- spec: we see that `bcrypt.genSalt` takes in salt as its argument. given its position, this tells us that the function it is a part of will ultimately return that item. looking at the inner one, `hash` is now in that position. that function `bcrypt.hash` returns a hash.\n```js\nbcrypt.genSalt(10, (err, salt) => {\n    if (err) return next(err)\n    bcrypt.hash(user.password, salt, (err, hash) => {\n      if (err) return next(err)\n      user.password = hash\n      next()\n    })\n  })\n```\n\nThis is also the same value that will be used as the parameter taken in with `.then` when working with promises\n```js\n  bcrypt.genSalt(10)\n    .then(salt => {\n      return bcrypt.hash(user.password, salt)\n    })\n    .then(hash => {\n      user.password = hash\n      next()\n    })\n    .catch(err => {\n      console.error(err)\n      return next(err)\n    })\n```\n","n":0.08}}},{"i":590,"$":{"0":{"v":"Array","n":1},"1":{"v":"\nArrays technically aren't a type in Javascript.\n\nSince everything in Javascript is an object, `Array` doesn't follow the same limitations as an actual [[array|general.lang.types.array]], such as being of fixed length.\n- for example, the `length` property returns the size of the internal storage area for indexed items of the array. \n\nin JavaScript, array indexes are coerced to strings by an implicit `toString()` call.\n\n## When to use Arrays for lists of data\nIf you have a list of data that you are going to be transforming somehow, or mapping over it somehow, you should try to structure that data as an array\n- expl: if you had an object of keys, and each key was a list item, then to operate on that might get a little messy and imperative. Imagine if we stored taxes in an object like this:\n```js\nconst taxes = {\n    GST: 0.05,\n    PST: 0.07\n}\n```\nWe can loop over this with `Object.entries` without problem, but now imagine that we want to map over each entry, and display it in the UI in React. That might look something like this:\n```jsx\nObject.entries(taxes).map(tax =>\n    <>\n        <p>Tax type: {tax[0]}</p>\n        <p>Amount: {tax[1]}</p>\n    </>\n)\n```\n\nAlternatively, imagine we structured taxes like this:\n```js\nconst taxes = [\n    {\n        jurisdiction: 'PST',\n        percentage: 0.05,\n    },\n    {\n        jurisdiction: 'GST',\n        percentage: 0.05,\n    },\n]\n```\n\nAlthough this is less succinct, there is a benefit to doing it this way. This is how we would handle that React task:\n```js\ntaxes.map(tax => {\n    <>\n        <p>Tax type: {tax.jurisdiction}</p>\n        <p>Amount: {tax.percentage}</p>\n    </>\n})\n```\n\nThis is yet another example of why you should go from a data-consumption perspective (in other words, consider how it will be used; think of the end-user of that code.)\n\n","n":0.062}}},{"i":591,"$":{"0":{"v":"Method","n":1},"1":{"v":"\nOperating on arrays offers `O(1)` search speed if the index is known, but adding/removing an element is slow since size of array cannot change once it's created\n- note: is this relevant for Javascript?\n\n`forEach` is almost always used for side-effects at the end of a chain\n","n":0.149}}},{"i":592,"$":{"0":{"v":"Sort","n":1},"1":{"v":"\nThe ES6 `sorted` method takes in a `comparator` callback.\n\n### Comparator callback\neach array element is sorted according to the return value of the callback.\n- `undefined` values are just popped onto the end\n\nIf we omit the comparator callback, then the sort order is ascending/alphabetical (ie. as a string). Therefore, an array of numbers becomes an array of strings, then the strings are compared alphabetically, making `[1, 10, 100, 3, 5, 7, 9]` perfectly sorted.\n\nIf possible, prefer to use Lodash's `sortBy`, as it provides more sensible defaults\n","n":0.109}}},{"i":593,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Sort array of numbers\n```js\nconst sorted = [...positions].sort((a, b) => a - b)\n```\n\n#### Sort array of strings\n```js\nlist.sort(function (a, b) {\n    return a > b ? 1: -1;\n})\n```\n","n":0.192}}},{"i":594,"$":{"0":{"v":"Slice","n":1},"1":{"v":"\n- splice() changes the original array \n- slice() doesn't change the original array.\n\n- splice() returns the items that were removed from the array (often, this is simply discarded in favor of the effect of changing the original array via mutation)\n- slice() returns a new subarray that is constructed by using the original as a reference (therefore immutable)\n","n":0.132}}},{"i":595,"$":{"0":{"v":"Reduce","n":1},"1":{"v":"\nWhen calling reduce you specify a function that describes how to calculate the result for the next array element, given you already know the result for the previous array elements. \n- It’s like a functional version of a foreach loop with an additional variable that you change in the loop and return at the end.\n\nThe `reduce()` method executes a user-supplied “reducer” callback function on each element of the array, \n- the return value of the calculation on the previous element gets passed as input. Therefore, each calculation feeds the next element during the next iteration.\n\n`reduce` could just as easily have been named `transform`, for its ability to transform one data structure into another.\n\nAnytime you need to do filter and map on same array, use reduce\n\nThe final result of running the reducer across all elements of the array is a single value.\n\nThe accumulator is the hero of `.reduce()`. It is a story about how the accumulator changes at each step of the way (ie. at each array element). The way that the accumulator changes is described by the reducer function. The value returned from that reducer function describes what that accumulator looks like at that particular iteration.\n- ex. Take for instance a reducer function that adds up all elements in the array. The hero of the story (ie. the Accumulator) starts out as the array's first element (assuming no `defaultValue` passed). As we iterate through the array, the Accumulator is modified in a way that is described by the return value of the reducer function. Whatever value this function returns becomes the value of the Accumulator for that iteration.\n\nA reducer function takes the `accumulator` and `currentValue` arguments.\n- *accumulator* - the value resulting from the previous call to the callback.\n- *currentValue* - the value of the current element\n\n### `initialValue`\nIf initialValue is specified, that also causes `currentValue` to be initialized to the first value in the array. \nIf initialValue is not specified, `previousValue` is initialized to the first value in the array, and currentValue is initialized to the second value in the array.\n\n### Signature\nSignature\n```ts\nfunction reduce<Element, ReturnType>(arr: Element[], initialValue?: ReturnType) {\n    (acc: ReturnType, val: Element) => ReturnType\n}\n\n```\n","n":0.053}}},{"i":596,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Convert array of objects into Object\n```js\n// before\nconst data = [\n  { key: 1, name: \"A\", condition: true },\n  { key: 4, name: \"B\", condition: false },\n]\n\n// after\nconst data = {\n  1 : { key: 1, name: \"A\", condition: true },\n  4 : { key: 4, name: \"B\", condition: false },\n]\n```\n\n```js\nconst arrayToObject2 = (array, key) =>\n    array.reduce((obj, item) => ({\n            ...obj, [item[key]]: item\n        }), {}\n    )\n```\n\n### Return an object\n```js\nconst group = (collection, grouper) => {\n    return collection.reduce((acc, val) => {\n        const objectIndex = grouper(val)\n        if (acc[objectIndex]) {\n            acc[objectIndex].push(val)\n        } else {\n            acc[objectIndex] = [val]\n        }\n        return acc\n    }, {});\n}\n\nconsole.log(group([6.5, 4.2, 6.3, 6.8, 4, 3, 1], Math.floor))\n// { '4': [4.2], '6': [6.5, 6.3] }\n```\n\n### Remove duplicates from array\nThis shows how we can introspect on the array/object as it's being built up\n```js\nconst arrayWithNoDuplicates = myArray.reduce((acc, val) => {\n  if (acc.indexOf(val) === -1) {\n    acc.push(val)\n  }\n  return acc\n}, [])\n```\n\n### `.map()` implemented in `.reduce()`\n```js\nif (!Array.prototype.mapUsingReduce) {\n  Array.prototype.mapUsingReduce = function(callback, initialValue) {\n    return this.reduce(function(mappedArray, currentValue, currentIndex, array) {\n      mappedArray[currentIndex] = callback.call(initialValue, currentValue, currentIndex, array)\n      return mappedArray\n    }, [])\n  }\n}\n```\n","n":0.076}}},{"i":597,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Execute array of callbacks\n```js\nconst iterate = (count, callback) =>\n  [...Array(count)].map((_, i) => callback(i))\n```\n\n#### Generate empty array\n```js\nconst emptyArray = Array(5).fill(null)\n```\n\n#### Surgically remove one item from an array\n```js\n    return [\n        ...bigArray.slice(0, itemToRemoveIndex),\n        ...bigArray.slice(itemToRemoveIndex + 1)\n    ]\n```\n\n#### Surgically replace one item in an array\n```js\n    return [\n    ...bigArray.slice(0, itemToReplaceIndex),\n        itemToInsert //if item is not object\n        { ...array[itemToReplaceIndex], ...itemToInsert } //if item is object\n        ...bigArray.slice(itemToReplaceIndex + 1)\n    ]\n```\n\n#### Move elements within array\n```js\nfunction swapInArrayByIndex(arr, i1, i2){\n    let t = arr[i1];\n    arr[i1] = arr[i2];\n    arr[i2] = t;\n}\n\nfunction moveBefore(arr, el){\n    let ind = arr.indexOf(el);\n    if(ind !== -1 && ind !== 0){\n        swapInArray(arr, ind, ind - 1);\n    }\n}\n\nfunction moveAfter(arr, el){\n    let ind = arr.indexOf(el);\n    if(ind !== -1 && ind !== arr.length - 1){\n        swapInArray(arr, ind + 1, ind);\n    }\n}\n```","n":0.091}}},{"i":598,"$":{"0":{"v":"Event Loop","n":0.707},"1":{"v":"\n## Event loop\nWhen you create a new click handler on an HTML element, that click handler is being registered with the event loop, meaning that the event loop is now listening for that handler to be called. Every time it witnesses the event to happen, it causes a certain snippet of code to be executed.\n\nAn event loop must exist because of the fact Javascript is an asynchronous language. What happens is that we put what we need to do in the queue (eg. fetch data), and tell it \"when you're finished, do this\". The result is non-blocking code.\n\n# UE Resources\n[High quality recommended resource for understanding Event Loop](https://www.youtube.com/watch?v=8aGhZQkoFbQ)\n","n":0.097}}},{"i":599,"$":{"0":{"v":"Event Bubbling","n":0.707},"1":{"v":"\nWhen you click on a button, you are inadvertently clicking on things that inside the button, as well as things that contain the button\n- ex. when you click on a strong tag, the event happens on that strong tag. But if nothing happens with that event, then it will bubble up to the surrounding element (ex. a Button). If the button wasn't listening for that specific event, then it will keep bubbling up onto the next surrounding element\n\nEvent capturing\n- the process of figuring out what got clicked. Happens after the bubbling phase\n- ex. when you click on a `span`, the browser starts at the top: \"user clicked on the `window` > `html` > `body` > `div` > `button` > `span`\"\n\nWhen we add an event listener, we can specify that it triggers on the capture phase or bubble phase\n- bubble phase is default, and is what we want 99% of the time.\n\n### currentTarget vs target\n`currentTarget`\n- what actually got clicked, prior to any bubbling happening\n- ex. we have a `span` inside a `button`. If the user clicks the `span`, then `span` is the `currentTarget` that got clicked, and the `button` is the `target`\n\n`target`\n- what you listened for a click on (what the bubbling ended up at)\n","n":0.07}}},{"i":600,"$":{"0":{"v":"Bundler","n":1},"1":{"v":"\nBefore ES modules were available in browsers, developers had no native mechanism for authoring JavaScript in a modularized fashion. This is why we are all familiar with the concept of \"bundling\": using tools that crawl, process and concatenate our source modules into files that can run in the browser.\n","n":0.143}}},{"i":601,"$":{"0":{"v":"Jest","n":1},"1":{"v":"\nJest sets `NODE_ENV=test` by default\n\nThe data entering a component via props is the data, whose stability we want to test during unit tests. For instance, if we have a `<Table />` component, we are chiefly concerned with the data coming in and how it impacts `<Table />`. In this case, it would be: How long is the array of data that comes in (which ultimately translates to amount of rows)? What is the 'title' prop that we are expecting (which renders the title on the Table), etc. Given that the data is what we are interested in, begin to think of ways that data can fail on us. What happens if we get no data? Well, then I guess we expect the amount of `<Row />` components to be 0. This would mean a passing test, meaning that paticular piece of logic is guarded, and will remain robust unless the test breaks.\n\nKeep in mind we are forming our tests around: what is expected given this circumstance? In tests, an error case and happy-path are both equivalent in terms of value. They are simply scenarios that may happen, and we want to just clarify it for the record that \"this is what should happen, given this circumstance.\"\n\n- ex. in `const wrapper = shallow(<FeedbackSummary />);`, imagine we are the parent, and `FeedbackSummary` is the child we are testing. In other words, `FeedbackSummary` is the input, and `wrapper` is the output. In jest, we are basically saying \"hey FeedbackSummary, I'm going to give you some props and render you. The output will be called wrapper, and I will take my magnifying glass (aka enzyme library) and take a close look to ensure that the everything looks like it should\"","n":0.059}}},{"i":602,"$":{"0":{"v":"Jest and TypeScript","n":0.577},"1":{"v":"\n### Problem with Jest mocks and Typescript\nMocking functions happens at runtime. At compile-time however, the Typescript compiler doesn't know that you have mocked those functions. During compilation, the TSC will do a type-check and it will complain that `mockReturnValueOnce` does not exist on that function.\n- To solve this, we have to extend our function so it both have the original types and the type of a Jest Mock function:\n```ts\njest.mock(\"../models/events\");\nimport { getEvents } from \"../models/events\";\nconst mockedGetEvents = getEvents as jest.MockedFunction<typeof getEvents>\n```\n","n":0.112}}},{"i":603,"$":{"0":{"v":"Transformation","n":1},"1":{"v":"\nJest runs our code as plain Javscript. That means that any part of our code that isn't plain js (e.g. jsx, TypeScript types) needs to be transformed into plain js.\n- A transformer is just a synchronous function that transforms source files into plain js.\nsee [transform configuration option](https://jestjs.io/docs/configuration#transform-objectstring-pathtotransformer--pathtotransformer-object)\n\n## Implementations\n[ts-jest: a Jest transformer with source map support allowing us to use Jest to test TypeScript code](https://kulshekhar.github.io/ts-jest/)","n":0.125}}},{"i":604,"$":{"0":{"v":"CLI","n":1}}},{"i":605,"$":{"0":{"v":"jest.spyOn","n":1},"1":{"v":"\n### `jest.spyOn(object, methodName)`\nCreates a mock function similar to jest.fn but also tracks calls to the `object[methodName]`","n":0.25}}},{"i":606,"$":{"0":{"v":"jest.mock","n":1},"1":{"v":"\n`jest.mock` allows us to mock an entire module\n\nIf we call `jest.mock` with only 1 arg,\n- the module we pass will be replaced simply with `jest.fn`\n\nIf we call `jest.mock` with 2 args,\n- the module we pass will be replaced with the return value of the function provided (arg2).\n    - in other words, if there is 2 args, `jest.mock` says \"replace all occurrences of arg1 with arg2\"\n\nThis code tells jest, \"any time `calculateAge` is called, stick in 42 as its return value\"\n- we say \"any time you import `calculateAge` into some other module, replace its value with `jest.fn(() => 42)`\n```js\njest.mock('../calculateAge', () => {\n  return jest.fn(() => 42);\n});\n\n// This runs the function specified as second argument to `jest.mock`.\nconst calculateAge = require('../calculateAge');\ncalculateAge(); // Will return '42';\n```\n\n- ***ex.*** `jest.mock('7g-components/box/box.jsx', () => 'Box');`\n    - this is saying \"for the component we are testing, take all instances of `Box` and replace them with the text `Box`\"\n    - what's happening underlying is that since components are just functions, we are saying \"replace our component function with the function `() => 'Box'`\"\n    - we do this when we don't really care about all the props that come with box, but are just interested in the structure. `<Box />` doesn't really matter, since that isn't what we are testing.\n    - we do this basically when our proptypes fails, because we aren't supplying data in the way proptypes would expect (ex. passing to `Box` margin as `25px` rather than the expected `25`)\n\nWhen we do `jest.mock('./my-file.js')`, it will turn all functions into Jest mock functions and all objects will have more properties on them like, `mockReturnValueOnce`\n\nspec: Mocks get hoisted, so if there is a variable you define *after* the mock, you probably won't be able to use it\n\n```js\njest.mock('./isValidCoupon', () => {\n  return {\n    // isValidCoupon: async () => true,\n    isValidCoupon: jest.fn().mockImplementation(async () => true),\n  }\n})\n```","n":0.057}}},{"i":607,"$":{"0":{"v":"jest.fn","n":1},"1":{"v":"\nAllows us to mock a function\n\nUsed we want to mock a fn, and don't really care about its original implementation (often just mocking the return value)\n- returns a `spy` (which remembers each call that was made on it)\n- creates a stub\n- useful in removing dependencies to some backend (eg. server, API)\n\nIf we pass an argument, then it means we are passing a mock implementation:\n\n```js\nconst queryMock = jest.fn(() => Promise.resolve({ data: 1 })\nconsole.log(queryMock()) // Promise { pending }\n``\n","n":0.114}}},{"i":608,"$":{"0":{"v":"Async","n":1},"1":{"v":"\nBy default, Jest tests complete once they reach the end of their execution. Therefore, async tests will not work by default\n\n## Callbacks\nThis code doesn't work, because `fetchData` returns before `callback`\n```js\n// Don't do this!\ntest('the data is peanut butter', () => {\n  function callback(data) {\n    expect(data).toBe('peanut butter')\n  }\n\n  fetchData(callback);\n})\n```\n\nWe can solve this with the `done` arg\n```js\ntest('the data is peanut butter', done => {\n  function callback(data) {\n    try {\n      expect(data).toBe('peanut butter')\n      done()\n    } catch (error) {\n      done(error)\n    }\n  }\n\n  fetchData(callback)\n})\n```\nNow, Jest will wait until the `done` callback is called before finishing the test.\n- if never called, it will fail with a timeout error, which is the desired way of handling this.\n-  If we want to see in the test log why it failed, we have to wrap expect in a try block (as shown above)","n":0.087}}},{"i":609,"$":{"0":{"v":"Java","n":1},"1":{"v":"\n### javac\n- the main compiler included within the JDK\n- converts source code into Java bytecode\n\n# Commands\n### List Java versions installed\n- `/usr/libexec/java_home -V`\n","n":0.213}}},{"i":610,"$":{"0":{"v":"Ios","n":1},"1":{"v":"\n### Podfile\n- holds our ios-written dependencies.","n":0.408}}},{"i":611,"$":{"0":{"v":"iOS Cmds","n":0.707},"1":{"v":"\n# Simulator\n- open developer menu - `cmd+d`\n- enable on-screen keyboard - `cmd+shift+k`","n":0.289}}},{"i":612,"$":{"0":{"v":"Intersect","n":1}}},{"i":613,"$":{"0":{"v":"HTTPS","n":1}}},{"i":614,"$":{"0":{"v":"San Certificate","n":0.707},"1":{"v":"\n### SAN Certificate (Subject Alternate Name)\n- allows multiple hostnames to be protected by a single certificate.\n\n- SSL Termination - The process of decrypting SSL-encrypted data. In other words, it is the final step in the SSL process, where the receiving end can actually read what has been sent to it.\n","n":0.141}}},{"i":615,"$":{"0":{"v":"Lets Encrypt","n":0.707},"1":{"v":"\n# Let's Encrypt\n## Plugins\nCertbot supports 2 types of plugins for obtaining and installing certificates: authenticators and installers\n- some plugins can do both, such as the Apache and Nginx plugins\n\n### Authenticator\n- Authenticators are plugins used with the `certonly` command to obtain a certificate, validating that we own the domain we are requesting a certificate for. It then obtains the certificate for that domain, and places the certificate in the `/etc/letsencrypt` directory on your machine\n\t- The authenticator does not install the certificate (it does not edit any of your server’s configuration files to serve the obtained certificate)\n- If we list multiple domains to authenticate, they will all be included in a single certificate by default.\n\n### Installer\n- Installers are Plugins used with the `install` command to install a certificate.\n- These plugins modify the webserver's configuration in order to server the site over HTTPS \n\n## Certificates\n- All generated keys and certificates can be found on the host that serves the application. \n\t- found in `/etc/letsencrypt/live/$domain` if using Let's Encrypt\n- note: `pem` is a type of encoding\n\n### privkey.pem\nThis is the private key for the certificate \n- This is what Apache needs for `SSLCertificateKeyFile`, and Nginx for `ssl_certificate_key`\n\n### fullchain.pem\nThis is the full list of certificates, including the server certificate (a.k.a Leaf Certificate or End-Entity Certificate)\n- the server certificate is the first one listed. It is followed by intermediary certificates. \n- This is what Apache needs for `SSLCertificateFile`, and what Nginx needs for `ssl_certificate`.\n\n## Concepts\n### ACME\n- ACME is a communications protocol for automating interactions between CAs and their users' webservers.\n\t- This allows automated deployment of public key infrastructure.\n- Certbot is an example of an ACME client\n\n### Challenge\n- Challenges are a way for the Let's Encrypt servers to validate that you own the domain.\n- ex. HTTP-01 Challenge, DNS-01 Challenge\n- We only need one.\n\nHTTP-01\n- The webserver proves it controls the domain by provisioning resources on its filesystem. The ACME server then challenges the webserver to provision a file at a specific path. If the webserver is able to do that, it is proof that the domain is under the webserver's control.\n- When our webserver gets a token from Let's Encrypt, the webserver creates a file at `http://<YOUR_DOMAIN>/.well-known/acme-challenge/<TOKEN>`\n\t- this file also includes a thumbprint of your account key\n- Once our webserver tells Let’s Encrypt that the file is ready, Let’s Encrypt tries retrieving it. If successful in doing so, then we are able to issue the certificate.\n- This is the most common type of challenge.\n","n":0.05}}},{"i":616,"$":{"0":{"v":"TLS","n":1},"1":{"v":"\nTLS consists of two phases: secure connection establishment and the use of that encrypted channel for further communication.\n- A TCP handshake and connection must be established before messages to create a TLS connection are exchanged.\n\t- the handshake is done in one round trip. \n\t- after the first two steps, all messages are encrypted.\n\nTLS involves public-key cryptography to establish a shared secret that is then used to encrypt future communication\n\nTLS (Transport Layer Security) replaced SSL (Secure Sockets Layer), which is deprecated\n\nspec:TLS is an agreement (protocol) between 2 IP addresses (your own and the web server you are connecting to).\n\n### SSL Termination\n- the act of data reaching the end of the SSL chain and getting decrypted (or offloaded) so the recipient can read the data.\n\t- happens at the server end of an SSL connection\n- SSL termination helps speed the decryption process and reduces the processing burden on backend servers.\n![](/assets/images/2021-03-09-09-46-03.png)\n\n### Wildcard SSl Certificate\na single ssl certificate that let's us have SSL on any `*.mydomain.com`\n\n### UE Resources\n[self-signed certificates](https://medium.com/@jonatascastro12/understanding-self-signed-certificate-in-chain-issues-on-node-js-npm-git-and-other-applications-ad88547e7028)\n[cloudflare guide on SSL](https://developers.cloudflare.com/ssl/)\n\n### E Resources\n[setting up a private CA](https://www.digitalocean.com/community/tutorials/how-to-set-up-and-configure-a-certificate-authority-ca-on-ubuntu-20-04)","n":0.076}}},{"i":617,"$":{"0":{"v":"certificates","n":1},"1":{"v":"\n## Server Certificate\nAlso known as SSL/TLS certificates\n\nUsed to authenticate the identity of a server to the client that is trying to connect to it.\n\nWhen a server certificate gets installed on a website, HTTPS gets enabled, and the certificate chain gets created, the result of which vouches for the authenticity of the website.\n- When we hop on to our computers and type in a website URL, the server certificate ensures that the data flow between our client browser and the domain we’re trying to reach stays secure\n\nBased on PKI\n\n## Client Certificate\nUsed to validate the identity of a client.\n- In this way, it serves as a password, as in theory, no one should be able to produce that certificate but the true client.\n- Client certs don't encrypt any data.\n\nClient certificates exist because passwords aren't that secure.\n\nClient certificates use the Public Key Infrastructure (PKI) for authentication (just as server certificates do).\n- Difference is that client certificates don't encrypt any data; they are just for validation purposes.\n\nContain \"Issued to\" and \"Issued by\" fields\n\nBased on PKI","n":0.076}}},{"i":618,"$":{"0":{"v":"ALPN Protocol","n":0.707},"1":{"v":"\nALPN is a TLS extension that allows the application layer to negotiate which protocol should be performed over a secure connection\n- A benefit of this is that it inherently avoids additional round trips\n\nALPN is sent on the initial TLS handshake (Client Hello), and it shows which protocols the client (eg. browser) supports","n":0.139}}},{"i":619,"$":{"0":{"v":"Ca","n":1},"1":{"v":"\n## Certificate Authority (CA)\n- the CA is an entity responsible for issuing digital certificates that verify identities on the internet.\n- a CA is an organization that stores public keys and their owners, and every party in a communication trusts this organization (and knows its public key)\n\t- When we navigate to a trusted site, the browser will have already had the public key of the CA, which it can then use to verify the signature, implying that the certificate and its contained public key are trustworthy\n- CA's are used to sign certificates that enable HHTPS\n- In essence, the CA is responsible for saying \"yes, this person is who they say they are, and we, the CA, certify that\"\n- CA servers can manage certificate enrollment requests from customers, and are able to issue and revoke digital certificates\n- CA servers support various certificate templates, such as SSL (both server and client), email signing/encryption, etc\n- to authenticate the recipient of the certificate, HTTPS servers typically use a technique called \"domain validation\"\n\t- This is where the domain name of the applicant is validated by proving that it owns and has control over a DNS domain. After this validation, a Domain Validated Certificate (DV) is issued.\n- A CA issues digital certificates that contain a public key and the identity of the owner\n\t- the certificate being issued is confirmation (by the CA) that the public key within the certificate belongs to the entity noted in the certificate.\n\t- spec: the CA has the private key\n- the CA market is very fragmented, and government-related activities (govt forms that verify identity) will have their own CA. For commercial use, the CA market is dominated by a few players (Let's Encrypt, GoDaddy, VeriSign are two of them)\n- ex. when using DigitalOcean managed Postgres, we are given a `ca-certificate.crt` from DigitalOcean. This is the certificate we must pass from our application server. The reason this works, is because the certificate is signed by one of the CAs that is trusted by the DigitalOcean server.\n\n### Chain of Trust\n- example chain:\nGlobalSign → Google CA → neverforgetapp.co\n\n- The idea of Chain of Trust is that the Root CA can \"trust\" other Intermediate CAs (ICA) to issue certificates (which are signed by the Root CA). Though the ICAs themselves are not trustworthy, they are trusted by a trustworthy entity (the Root CA), so the certificates they issue are also considered trustworthy.\n- The Root CA is kept behind many layers of security\n\t- if private keys are compromised, then all certificates based on the Root CA are compromised as well. For this reason, we use an Intermediate CA\n- In the browser, if we click on the lock to the left of the URL, we can see the certificate chain \n\t- The one at the bottom (leaf node) of the tree is the SSL certificate (`*.neverforgetapp.co`), which was issued by the ICA directly above it (`Google CA`)\n\t\t- The SSL certificate is signed with the private key of the ICA (`Google CA`). Once this is done, it is sent back to `neverforgetapp.co`.\n\t\t- All certificates contain the public key of the entity that the certificate is for, and the private key of the entity that signed the certificate.\n\t\t\t- ex. the SSL certificate (the end of the chain) contains the public key of `neverforgetapp.co`, and is signed by Google CA; Google's CA certificate contains Google's public key and is signed by GlobalSign.\n\t- In the browser's Certificate Manager, we can see that GTS CA 101 is not listed as a trusted CA. Therefore, this Intermediate CA needs to be verified by a Root CA, which it is (GlobalSign). In the Certificate Manager, we can see that this is indeed trusted. This is the chain of trust. \n- In our example chain, there are 2 certificates issued: the Google ICA certificate, and the SSL certificate (to `neverforgetapp.co`). These both get sent to `neverforgetapp.co` and are installed on the webserver.\n\t- When someone visits `neverforgetapp.co` with HTTPS, both certificates are sent back to the browser. It starts by verifying the certificate that is higher up in the chain (closer to the root), which is the Google ICA certificate. It sees that GlobalSign has signed this certificate, whom it recognizes as a trusted Root CA.\n\t\t- The browser has the public keys (and the certificates, of which they are a part) of all Root CAs, so it can unencrypt the Google ICA certificate. This proves that this ICA is trustworthy.\n\t\t- the browser trusts all Root CAs in its list. The fact that a Root CA in that list has signed the Google ICA certificate means that the browser automatically trusts that ICA. \n\t\t\t- As a consequence of unecrypting the certificate, the browser gets the public key from Google ICA. \n\t\t- Now that Google ICA has been considered trustworthy, the browser then looks to the `neverforgetapp.co` SSL certificate. It uses the public key issued by Google ICA to decrypt the certificate. It sees that Google ICA has signed this certificate, making it trustworthy.\n\t\t- Now, the browser gets the public key from `neverforgetapp.co`, generates a symmetric key, encrypts it (using the public key), and sends it to the `neverforgetapp.co` webserver.\n\t\t\t- since the `neverforgetapp.co` webserver has the private key, it is able to decrypt it, which yields the same symmetric key. Now, both the browser and the `neverforgetapp.co` webserver have the same symmetric key that is used to exchange information securely.\n\n### Private CA\n- There are 2 versions, public and private CA\n\t- public are for verifying the identity of websites and other services that are provided to the general public\n\t- private are for closed groups and private services.\n- spec:While in production we will use a public CA, we may want to configure a private CA in development and staging environments so those environments match the production environment, and also have HTTPS connections. \n- we need to build a private CA when our programs require encrypted connections between client and server.\n\t- with the private CA, we can issue certificates to users, servers, or individual programs and services within your infrastructure.\n\t\t- ex. OpenVPN would use its own private CA\n\t\t- ex. we can configure our web server to use certificates issued by a private CA to make development and staging environments match production servers that use TLS to encrypt connections.\n\n### Bank Example\nWhen a user logs into an HTTPS enabled bank, they receive a public key. The public key is used to create a temporary shared symmetric encryption key, which is used to encrypt all messages sent from the client to the server. These messages are enciphered with the bank's public key in such a way that only the bank has the private key required to decrypt them. For the rest of the session, the (disposable) symmetric key is used to encrypt messages from client to server.\n","n":0.03}}},{"i":620,"$":{"0":{"v":"HTML","n":1},"1":{"v":"\nHTML has what’s called complex elements. This is like a drop down, i.e. something that is made up of other elements. If you are to inspect the drop-down element invert browser, you will see the shadowDOM, Which is in accessible to users\n- ex. Input, Select, checkbox, radio button","n":0.144}}},{"i":621,"$":{"0":{"v":"Tags","n":1},"1":{"v":"\n### Script\nJavascript loaded through script tags should be located before the closing `</body>` tag.\n- This is to stop JavaScript loading from blocking content loading. So, the content is loaded and the scripts are loaded next.\n\nAn alternative to this is to use async in the script tag, like so:\n```html\n<script src=”main.js” async></script> \n```\nthis way it will not block the main thread and hence will not block the content from loading.\n\n### Head\nput links to CSS style sheets in `<head></head>`\n- this way, the browser styles the HTML as it loads. If style sheets are put at the bottom of the HTML, the browser will have to restyle and render the whole page from the top, which can cause a performance bottleneck.\n\n### `<nav>`\nThe `<nav>` element represents a section of a page whose purpose is to provide navigation links, either within the current document or to other documents.\nex. menus, tables of contents","n":0.083}}},{"i":622,"$":{"0":{"v":"Homebrew","n":1},"1":{"v":"\n### Services\nHomebrew has a services aspect to it. We can run `brew services list` to see the services that are currently running from Homebrew. We can run `brew services start` to start a service.\n- ex. Redis, Postgres, vault, caddyserver \n","n":0.158}}},{"i":623,"$":{"0":{"v":"Hardware","n":1},"1":{"v":"\n### What happens when we run an executable?\n1. as we type the characters `./hello` into the shell program, each letter is read into a register (in the CPU), and then stores it in memory\n2. When we hit `<enter>`, the shell loads the executable `hello` file by executing a sequence of instructions that copies the code and data in the `hello` object file from disk to main memory (included is the string of characters `\"hello, world\\n\"`)\n3. the data then travels directly from disk to main memory, without passing through the processor (a technique known as *direct memory access*)\n5. Once the code and data of the `hello` file are loaded into memory, the processor begins executing machine-language instructions in the `hello` program's `main` function.\n\t- These instructions copy the bytes in the `\"hello, world\\n\"` string from memory to the register file (CPU), and from there to the display device (monitor).","n":0.082}}},{"i":624,"$":{"0":{"v":"Interfaces","n":1}}},{"i":625,"$":{"0":{"v":"Serial Port","n":0.707},"1":{"v":"\nWith this type of port, information transfers in or out sequentially one bit at a time\n![](/assets/images/2021-03-20-19-15-42.png)\n\nThroughout most of the history of personal computers, data has been transferred through serial ports to devices such as modems, terminals, various peripherals, and directly between computers.\n- Modern consumer PCs have largely replaced serial ports with higher-speed standards, primarily USB\n- serial ports are still frequently used in applications demanding simple, low-speed interfaces, such as industrial automation systems, scientific instruments, point of sale systems and some industrial and consumer products.\n- Server computers may use a serial port as a control console for diagnostics\n\n## UE Resources\n[Connect Pi to Arduino with Serial Ports (and using Node's serialport.js)](https://medium.com/@machadogj/arduino-and-node-js-via-serial-port-bcf9691fab6a)","n":0.096}}},{"i":626,"$":{"0":{"v":"Parallel Port","n":0.707},"1":{"v":"\nmultiple bits transfer in or out simultaneously\n\nnetworking hardware (such as routers and switches) commonly use serial console ports for configuration, diagnostics, and emergency maintenance access\n- Today, the parallel port interface is virtually non-existent because of the rise of Universal Serial Bus (USB) devices\n\nContrasted with [[Serial Ports|hardware.interfaces.serial-port]]","n":0.147}}},{"i":627,"$":{"0":{"v":"Circuit Board","n":0.707}}},{"i":628,"$":{"0":{"v":"Transistor","n":1},"1":{"v":"\n### Transistor\n- amplifies charge.\n- transistors have 3 terminals\n- transistors can regulate current or voltage flow while also acting as a switch for signals of the electronic variety\n- Semiconductors make up three layers of a transistor.\n\t- Therefore, each layer can carry current\n","n":0.156}}},{"i":629,"$":{"0":{"v":"Transformer","n":1},"1":{"v":"\n### Transformer\n- converts high AC voltage like 230VAC to 24VAC low voltage to be used by bridge rectifier (aka diode bridge)","n":0.218}}},{"i":630,"$":{"0":{"v":"Resistor","n":1},"1":{"v":"\n### Resistor\n- control the electric current as it passes through them. \n\t- The colour-coded lines are used to determine their value.\n- resistors have many uses including: \n\t1. reduce current flow\n\t2. adjust signal levels\n\t3. divide voltages. This happens when resistors are lined up in series with each other. \n- resistors dissipate electrical power as heat\n\t- For bigger applications like a motor, this is exactly what they are used for.\n![](/assets/images/2021-03-22-22-24-31.png)","n":0.121}}},{"i":631,"$":{"0":{"v":"Microchip","n":1},"1":{"v":"\n### Microchip (Integrated Circuit)\n- IC is contrasted with Discrete Circuit (such as a PCB)\n- an IC is a set of electronic circuits on one small flat piece (or \"chip\") of semiconductor material (normally silicon)\n- The integration of large numbers of tiny MOS transistors into a small chip results in circuits that are orders of magnitude smaller, faster, and less expensive than those constructed of discrete electronic components.\n- ICs can be seen as a building block approach to integrated circuit design \n\t- This has resulted in discrete transistors being far more rare on the PCB itself\n- a modern chip (year 2020) may have many billions of MOS transistors in an area the size of a human fingernail\n","n":0.093}}},{"i":632,"$":{"0":{"v":"Diode","n":1},"1":{"v":"\n### Diode\n- Diodes only allow current to flow in a single direction\n\t- This is done by having zero resistance in one direction, and very high resistance in the other.\n- Diodes have 2 terminals, each of which is an electrode: an *anode* and a *cathode*\n\t- anodes allow current to flow in from outside a circuit\n\t- cathodes allow current to flow out of a polarized device.\n![](/assets/images/2021-03-22-22-26-11.png)","n":0.125}}},{"i":633,"$":{"0":{"v":"Capacitor","n":1},"1":{"v":"\n### Capacitor\n- basically a tiny battery\n- potentially carries just enough energy to keep something like memory chip running for a few second\n\t- This is why when we restart devices to try and fix an issue, we need to leave it off for about 10 seconds. That timespan will ensure that the capacitors are fully drained and the memory chips are all wiped. \n![e0122960d37020f2ce639a7495f0bb15.png](:/f350c9ead44241b5b40df7a983279d93)\n\n","n":0.126}}},{"i":634,"$":{"0":{"v":"Bus","n":1},"1":{"v":"\nA bus carries byes of information back and forth between the hardware components of a computer system (or even between computers, if connected physically)\n- Buses are normally designed to transfer fixed-sized chunks of bytes known as *words*. The number of bytes in a word (word size) is a fundamental system parameter that varies across different systems.\n\t- ex. word size could be 4 bytes long, while others could be 8. Typically smaller systems (such as embedded controllers) have word sizes of 1 or 2.\n\n### I/O Device\n- An I/O device is the system's connection to the outside world\n- ex. keyboard and mouse for input, monitor for output, and disk drive for long-term data storage.\n- Each I/O device is connected to the I/O bus by either a controller or adapter.","n":0.089}}},{"i":635,"$":{"0":{"v":"RAM","n":1},"1":{"v":"\nRAM works by means of fast-switching pointers. This is analogous to how information retrieval from a database is so efficient. The engine doesn't really care what the data is it's retrieving— it only cares about getting it out.\n\n### Main memory (RAM)\n- the temporary storage device that holds both a program and the data it manipulates while the processor is executing the program.\n- physically, main memory consists of a collection of DRAM chips.\n- logically, memory is organized as a linear array of bytes, each with its own array index starting at 0.\n- ex. Imagine every spot of RAM having its own address.\n\t- If your computer is 32 bit (4,294,967,296), then our computer can't talk to any address that is higher than that.\n\t\t- This much memory is commonly known as 4gb.\n\n### Virtual RAM\nSometimes, the physical RAM runs short for actively running programs. When this happens, a block of space on the hard drive can be configured by the OS to pretend to be memory.\n\nOf course, virtual RAM is a lot slower, so the more your computer is forced to rely on it, the less performant it will be","n":0.073}}},{"i":636,"$":{"0":{"v":"GPU","n":1},"1":{"v":"\nspec: The GPU's principle job is to render triangles onto the screen\n- It is more performant than the CPU, but also more specific, so cannot do the wide-range of jobs that can be performed by a CPU","n":0.164}}},{"i":637,"$":{"0":{"v":"CPU","n":1},"1":{"v":"\nCPU is the engine that interprets (or executes) instructions that are stored in main memory.\n- at its core, there is a word-sized storage device (or register) called the program counter (PC).\n\t- At any given point in time, the PC points at (contains the address of) some machine-language instruction in main memory.\n- From the time the system is powered on until it is powered off, the processor blindly and repeatedly performs the same basic task over and over again:\n\t1. reads the instruction from memory pointed at by the PC\n\t2. interprets the bits in teh instruction\n\t3. performs some simple operation, as per the instruction\n\t4. updates the PC to point to the next instruction (which may or may not be in contiguous memory) \n- Different CPU architectures understand different instruction types, which is why we can't take a program from one architecture (ex. x86) and run it on another (ex. ARM)\n\n- A socket is the physical socket where the physical CPU capsules are placed\n- Cores are the number of CPU-cores per CPU capsule\n- some CPUs can run more than one parallel thread per CPU-core\n- If you multiply the number of socket, cores and threads, then you get the number of \"CPUs\": 24.\n\t- These aren't real CPUs, but the number of possible parallel threads of execution your system can do.\n\n\nThe CPU reads instructions in, does the instruction, and then reads the next instruction. These instructions need addresses (RAM address) to know where to go to get data, or put data. But it also needs to know what to do with that data. These two parts compose a basic \"instruction\", usually divided into op-codes(operation code) and addresses.\n- So a 32 bit CPU has 32 bits for an instruction, while a 64 bit CPU has 64 bits to hold an instruction. This means we can have many more opcodes, and also have many more places in memory we can address.\n- ex. Let's say numbers 1 through 20 are addresses in RAM. If we have, for example, a 3 bit processor, an instruction would look like so: S-1-2 (something like subtract 1 from 2, assuming S is subtract op-code). Now what if we want to subtract something from memory address 17? We can't, we don't have enough room, since we only have 3 bits(1 for opcode, one for first address, one for second address). There are ways to work around this but for simplicity, it's impossible.\n\n### Hardware Acceleration","n":0.05}}},{"i":638,"$":{"0":{"v":"Register","n":1},"1":{"v":"\nA register is quickly accessible storage available to the computer's processors\nmost computers load data from a larger memory into registers where it is used for arithmetic operations\n- manipulated data is then stored as RAM\nregisters are at the top of the memory hierarchy, meaning they provide the fastest access to data\nregisters measure how much data they can hold in terms of bits (ex. 64-bit register)\n- ARM instructions operate only on registers with a few instructions for loading and saving data from / to memory while x86 can operate directly on memory as well.\n- when discussing 32bit or 64-bit architectures, we are referring to the size of the register\n","n":0.097}}},{"i":639,"$":{"0":{"v":"CPU Clock","n":0.707},"1":{"v":"\n# Clock Generator\nCPUs have electronic clocks in them, in fact it’s a fundamental part of how they operate as the electronics within a CPU have to operate in a synchronised way.\n\nThe clock is a crystal that oscillates a predictable number of times each second when electricity is passed through it. Counting these oscillations allows the computer to measure the passing of time.\n","n":0.127}}},{"i":640,"$":{"0":{"v":"Architecture","n":1},"1":{"v":"\n# CPU Architecture\n## 32 bit vs 64 bit architecture\n- a memory address that's 32 bits long can only refer to 4.2 billion unique locations (i.e. 4 GB).\n- anal: we can count on our fingers up to 10 in the decimal system. Since computers are binary, a bit is the number of \"digital fingers\" that the computer can count on.\n\t- ex. If I said a computer has 4 bits, the biggest number I could put in it is 4 places long, or 1111.\n- the biggest number a 32 bit system could handle is `11111111111111111111111111111111` (4,294,967,296)\n\t- this would be the theoretical highest number that our computer could do calculations with (though there are workaround tricks that are slower)\n\t- The practical use of this \"largest number\" is in determining how much memory we can have.\n- At a high level, a higher bit CPU allows the machine to run faster by having advanced operations supported by opcodes(rather than using simple opcodes to build up to a complex algorithm) and utilize more memory by having access to more address spaces(it can save stuff in RAM instead of hard-disk, which is slower than RAM, even if it is flash memory also).\n- ex. In legacy versions of Excel, the max number of rows is 65,537, which corresponds to 16 bits (`1111111111111111`)\n- ex. in the original zelda, the max number of rupees was 255, which corresponds to 8 bits (`11111111`)\n- anal: imagine a paper letter. More bits means more possible addresses, which would correspond to a bigger envelope. The other side of the equation is how much RAM we have. Going from a 32-bit system to a 64-bit system would be like going from a standard sized envelope to an envelope that wraps around the world 3 times (since: 2<sup>64</sup> is more than 4 billion times larger than 2<sup>32</sup>).\n- a 32-bit system would only be able to understand 4GB worth of RAM, making any RAM beyond that amount redundant on a 32-bit system.\n\t- This is not entirely true because of the existence of PAE (Physical Address Extension), which allows a 32-bit system access more than 4GB of RAM. However it is still true that individual programs are limited to the 4GB.\n\n## ARM vs. x86\nARM and x86 are 2 different families of CPU architecture\n- ARM is more efficient than regular x86 processors, which is made possible by a simplified instruction set, and having a stripped down hardware.\n- unlike x86, ARM wasn't made with backward compatability in mind, so it doesn't repeat a lot of the inefficient designs that were made by its antecedents.\n- known as *reduced instruction set computing* (RISC), while x86 uses *complex instruction set* (CISC)\n\nanal: imagine that we have 2 general-purpose factories that can make a wide array of things: the x86 Factory, and the ARM Factory\n\t- **x86 Factory** - There is a receptionist at the front that receives mail (requests) one at a time. Each time he gets a piece of mail, he passes it on to a group of people who open (decode) the messages to figure out what action should be taken. Each person in this group can figure out his own request, and figure out where it should go next. Also, these requests can vary wildly in size and purpose. For instance, one request might be \"take data from this warehouse and put it in bucket X (load to register)\". Another might be \"take data from this warehouse and put it on the conveyor to this trucking station (bus to I/O)\". Another might be \"take 3 different pieces of data from the warehouse, add them together, then put them back in a different spot of the warehouse\". Since each request varies in the amount of time is takes to process, the factory gets more efficient if more people are hired to handle these requests. This is why this type of factory can be so expensive: to get more efficient, more decoders must be hired.\n\t- **ARM Factory** - There is a receptionist at the front that receives mail (requests) **and** opens them. These requests differ from the ones accepted by the x86 Factory, in that they are more atomic and simpler to understand. The implication of this is that the receptionist knows exactly which department to send the request to, without needing another department of decoders to figure everything out. One type of request the receptionist might receive is \"load this one piece of data from the warehouse into this bucket\". Another might be \"add these two buckets together and store the result in the first bucket\". However, this means that the compiler (which converts the program to machine code) needs to write out all of these smaller instructions. Instead of the x86 method of decoding large instructions into smaller ones on the fly, the ARM method is to do it all ahead of time.\n\nSince the ARM architecture has less hardware doing things, it is a lot cheaper to design and make, and is more energy efficient.\nOriginally, ARM was meant for small embedded systems. This meant that there was little incentive to improve performance. Since ARM chips have expanded to phones, performance is a more recent focus of ARM chips.","n":0.034}}},{"i":641,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get info about CPU, including architecture\n`less /proc/cpuinfo`","n":0.354}}},{"i":642,"$":{"0":{"v":"ALU","n":1},"1":{"v":"\n# Arithmetic Logic Unit (ALU)\n- made up of the Arithmetic Unit and the Logic Unit \n\t- Arithmetic handles numeric operations, like add/subtract, increment\n- the numbers operated on are in binary format ","n":0.18}}},{"i":643,"$":{"0":{"v":"Graphql","n":1},"1":{"v":"\nGraphQL is useful when data is relational. Related data can be modeled as a graph and can thus power a GraphQL API. - This is precisely what the GraphQL Engine does.\n\n### UE Resources\n[Graphql concepts visualized](https://www.apollographql.com/blog/the-concepts-of-graphql-bc68bd819be3/#.hfczgtdsj)\n[security (preventing DDOS)](https://www.apollographql.com/blog/securing-your-graphql-api-from-malicious-queries-16130a324a6b/)\n","n":0.164}}},{"i":644,"$":{"0":{"v":"Variables","n":1},"1":{"v":"\nTechnically, we could do string interpolation to make our queries dynamic. However, this wouldn't be a good idea because our client would be tasked with manipulating that string at runtime. Instead, Graphql provides us with first-class variables that allow us to factor-out the dynamic parts of a query (eg. userId, first: 10)\n\nVariables must be either scalars, enums or input object types.\n- Input Object Type: Passing in an object might be a more sensible solution than strings, if there is related data. For instance, imagine we are running a `createUser` mutation. Instead of passing `first_name`, `last_name`, as so on to the mutation, why not just define a type, and pass an object?:\n```\nmutation createUser($userInfo: UserInfoInput!) {\n\tcreateUser(userInfo: $userInfo)\t{\n\t\tfirst_name\n\t\tlast_last\n\t}\n}\n```\n","n":0.093}}},{"i":645,"$":{"0":{"v":"Directives","n":1},"1":{"v":"\n# Directives(`@`)\n- Using variables, Directives let us dynamically change the structure and shape of our queries.\n- This allows us to either `@include` or `@skip` over fields.\n- ex. In Never Forget, consider the BrowseNugget screen. Our list component allows us to switch between just showing the nugget name, and showing a summary of the nugget. To determine whether or not we want the summary, we can use a variable:\n```\nquery Nugget($details: details) {\n\tnugget {\n\t\ttitle\n\t\tmediaItems @include(if:  $details) {\n\t\t\ttitle\n\t\t}\n\t}\n}\n```\nDirectives are useful when we find ourselves needing to do string manipulation in order to modify (ie. add/remove fields from) our query structure.\n\nTo be Graphql spec-compliant, a server must implement only the `@include` and `@skip` directives. This leaves each server implementation open to extending their own directives.\n\n### Use with Inline Fragments\nDirectives can also be applied with inline fragments like so:\n```\nquery inlineFragmentNoType($expandedInfo: Boolean) {\n  user(handle: \"zuck\") {\n    id\n    name\n    ... @include(if: $expandedInfo) {\n      firstName\n      lastName\n      birthday\n    }\n  }\n}\n```\n\n### Deprecation\nFields in an object may be marked with the `@deprecated` directive like so:\n```\ntype ExampleType {\n  oldField: String @deprecated\n}\n```\n\nThese fields can still be queried (to prevent breaking changes).","n":0.075}}},{"i":646,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n# Types\nIn the same way a schema defines the shape of the total response, the type of an individual field defines the shape of that field's value.\n- The shape of the data we return in our resolver must likewise match this expected shape. When it doesn't, we frequently end up with unexpected nulls in our response.\nGraphQL is based on its type system, and depending on what type the GraphQL server receives, it will determine what it should do next.\n- If we are trying to execute a query to get all nuggets, then the GraphQL server will see we are querying an object (nuggets). Since it is an object (and therefore not a scalar, which would end the execution cycle), GraphQL server knows that it needs to \"fetch\" those fields on the object. It then checks the type of each field. If they are non-scalar, then it checks for more data. This continues until every field is a scalar type. \n\n# User-defined Types\nIn GraphQL we deal with 8 different types:\n- scalars, enums, lists, objects, input objects, interfaces, unions, non-null\n\n## Scalar\nScalar types are:\n1. Int\n2. Float\n3. String\n4. Boolean\n5. ID\n\nA GraphQL object type has a name and fields, but at some point those fields have to resolve to some concrete data. These basic types of data are called scalars, and they are the \"atomic type\" of GraphQL\n- At each type, graphql will keep resolving until it gets to the scalar\n\nScalar types resolve to a scalar object. By definition, they cannot have sub-selections (fields) in the query.\n\nAll Graphql scalars are representable as strings\n\nGraphql provides built-in scalars, but type systems can add additional scalars\n- ex. Imagine making a scalar called `Time`, which we define as conforming to ISO-8601. When we query a field of type `Time`, we can rely on the ability to parse the result with an ISO-8601 parser. On the client, we can use a primitive (such as Date) to represent the value.\n- ex. Imagine a scalar called `Url`. It would still be serialized as a string, but would be guaranteed by the server to be a valid URL.\n\n### ID\nID is a wrapped type of kind `NON_NULL`\n\n## List\nRepresents are sequence of values in Graphql.\n- A list type is a *type modifier*, meaning it wraps another type, found in the `ofType` field. This field defines the type of each item in the list.\n\nWe can request paginated data from list fields with the `first` argument\n\n## Object\nWhile scalars represents the leaf values of a query hierarchy, objects represent the intermediate levels.\n\nRepresent concrete instantiations of sets of fields. the introspection types (e.g. `__type`, `__field`, etc) are examples of objects.\n\n## Input Object\nA composite type used for inputs into queries, defined as a list of named input values.\n- often, an update and create operation on a db record will take in the same inputs (ex. title, mediaitems...).\n- an input type is simply a type that includes all of these same inputs so we can reuse it in multiple places.\n- input types can't have fields that are other objects, only basic scalar types, list types, and other input types.\n```\ntype nugget {\n\tid: id!\n\ttitle: string\n\tmediaitems: json\n}\n\ninput nuggetinput {\n  title: string\n  mediaitems: json (?)\n}\n\n// then...\n\ntype mutation {\n  createnugget(input: nuggetinput): nugget\n  updatenugget(id: id!, input: nuggetinput): nugget\n}\n\n```\n\n## Null\na null result on a Non-Null type bubbles up to the next nullable parent. If this bubbling never stops because everything is of Non-Null type, then the root data field is null.\n- in other words, if we are trying to query a field that is `NONNullable` and the result happens to be `null`, that `null` will bubble, and the parent \"object\" will result in null. If however the field we are querying is `nullable` and the result happens to be `null`, it will remain at that field and not bubble\n- When fields are nullable, you still get partial data: even if you don't have the name, you can still show the age and other fields. When you mark fields as Non-Null, as in the second example, you forfeit your right to partial data.\n[source](http://spec.graphql.org/June2018/#sec-Errors)\n\n## Interface\nSimilar to how fragments allow us to DRY our queries, interfaces can DRY our type definitions.\n\nInterfaces represent a list of named fields and their arguments. These interfaces can then be implemented by objects.\n\n## Union\nUnions represent an object that could be one of many specified types. However, there is no guarantee that any of the fields on each of those types will be provided (whereas interfaces guarantee that a field will be available)\n\nUnlike interfaces, Unions do not implement fields of their own\n\nThey differ from interfaces in that Object types declare what interfaces they implement, but they are not aware of what unions contain them.\n\n```\nunion SearchResult = Photo | Person\n\ntype Person {\n  name: String\n  age: Int\n}\n\ntype Photo {\n  height: Int\n  width: Int\n}\n\ntype SearchQuery {\n  firstSearchResult: SearchResult\n}\n```\n\n## Non-null\nGraphql fields are nullable, making `null` a valid response for field type.\n- Like lists, a non-null type is a type modifier (it wraps another type instance in the `ofType` field).","n":0.035}}},{"i":647,"$":{"0":{"v":"Tools","n":1},"1":{"v":"\n# Client-side\n[Periqles: Generate a React form from a Graphql mutation](https://github.com/oslabs-beta/periqles)\n\n# Server-side\n## The Guild Libraries\n[Graphql-Tools: Graphql-first mentality to building Graphql Schema](https://www.graphql-tools.com/)\n- graph-tools provides a thin convenience layer on top of graphql.\n- we can get schema validation\n- biggest benefit of using graphql-tools is its nice API for connecting your declarative schema with resolvers\n\n[Graphql-Inspector: Maintain and improve Graphql API thru validation (+receive notifications) (runs in CI/CD pipeline)](https://graphql-inspector.com/)\n\n[Graphql-Scalars: A library of custom scalar types, ready to be imported](https://github.com/Urigo/graphql-scalars)","n":0.116}}},{"i":648,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\n### UE Resources\n[mocking graphql](https://graphql.org/blog/mocking-with-graphql/)","n":0.5}}},{"i":649,"$":{"0":{"v":"Structures","n":1}}},{"i":650,"$":{"0":{"v":"Node","n":1},"1":{"v":"\n# Node\nA node represents the actual object you were looking for.","n":0.302}}},{"i":651,"$":{"0":{"v":"Edge","n":1},"1":{"v":"\n# Edge\nAn edge describes the relationship between a parent object and the target node. In our graph above, we have a central node (the parent) that points to other objects. The relationship (edge) is that of friendship. The entity on the other end of the line is the node.\n- An edge has metadata about one object in the paginated list, and includes a cursor to allow pagination starting from that object.\n- Each edge has a node and a cursor \n\nAn edge type may look like so:\n```\ntype UserFriendsEdge {\n  cursor: String!\n  node: User\n}\n```\nEdges are not just valuable for pagination purposes.\nIn graph theory an edge can have properties of its own, which act effectively as metadata.\nIt has become common to think of the `edges` field as boilerplate, and simply a container for the cursor, but it actually can be quite powerful. When we consider that an edge is a field that represents the relationship between 2 nodes, we can start to realize there is a lot of potentially appropriate fields to describe that connection.\n- ex. In the friend graph above, we can keep metadata about the friendship, such as when it started.\n- ex. Relevancy score when searching (e.g the relevancy of this result to the input search query is 0.7).\n- ex. Distance when searching/sorting by distance from a certain location.\n\nIn the above cases, the field doesn't belong on the node because its value changes depending on the connection parameters. Remember, you should be able to get to the same record (e.g. node) via multiple routes within a single query and the values for its fields should be the same, so contextual data has to go somewhere else -- the edge types.\n\n## Pagination\nEdges enable us to perform cursor-based pagination. Since the cursor is (usually) just an `id`, we can use the `after` argument on a list, passing that `id`. This allows us to specify the starting point that data should be retrieved from\n","n":0.056}}},{"i":652,"$":{"0":{"v":"Connection","n":1},"1":{"v":"\n# Connections\n- a connection is a collection of objects with metadata such as `edges`, `pageInfo`...\n\t- It is effectively the edges, and the metadata associated with their environment.\n- `pageInfo` will contain `hasNextPage`, `hasPreviousPage`, `startCursor`, `endCursor`\n\t- `hasNextPage` will tell us if there are more edges available, or if we’ve reached the end of this connection.\n\n- A connection is a paginated field on an object — for example, the friends field on a user or the comments field on a blog post.\n- very similar to cursor-based pagination\n\nConnections are made up of edges, but a connection is more than just that, since it also contains metadata about the group of edges, such as `pageInfo`, which gives us pagination info (ex. what the most recent cursor was, if there is another page or not, etc)\n![](/assets/images/2021-03-09-21-57-34.png)\nIn the above image, the connections would be all of the grey lines together as one. \n\nWe might define our `User` type like this:\n```\ntype User {\n  id: ID!\n  name: String\n  friendsConnection(\n    first: Int,\n    after: String,\n    last: Int,\n    before: String\n  ): UserFriendsConnection\n}\n```\n\nThis might lead to a query like so:\n```\n{\n  user(id: \"ZW5jaG9kZSBIZWxsb1dvcmxk\") {\n    id\n    name\n    friendsConnection(first: 3) {\n      edges {\n        cursor\n        node {\n          id\n          name\n        }\n      }\n    }\n  }\n}\n```\nA connection is a way to get all of the nodes that are connected to another node in a specific way\n- In this case we want to get all of the nodes connected to our users that are friends. Another connection might be between a user node to all of the posts that they liked.\n\nA connection is by nature an abstract concept, and it is difficult to think about. An edge makes sense, because we can think of a user having a friendship with another user, or we think of a user authoring a post.\n\nA connection type may look like so:\n```\ntype UserFriendsConnection {\n  pageInfo: PageInfo!\n  edges: [UserFriendsEdge]\n}\n```\n","n":0.058}}},{"i":653,"$":{"0":{"v":"Server","n":1},"1":{"v":"\nA GraphQL server essentially takes in your API and exposes your GraphQL API via an endpoint. It has two core parts:\n- A Schema, which includes type definitions.\n- Resolve functions, which hold all functions that define how to get the data\n\n# Type Coercion\nwhen preparing a field of a given scalar type, a GraphQL server must uphold the contract the scalar type describes, either by: \n- coercing the value\n- producing a field error if a value cannot be coerced or if coercion may result in data loss.\n\nA GraphQL server may decide to allow coercing different internal types to the expected return type. Unless the coercion is sensical (ie. no information is lost), the Graphql server will raise a field error.\n- For example when coercing a field of type `Int`, a boolean `true` value may produce `1`. Alternatively, a string value \"123\" may be parsed as base‐10 123.\n\nThe Graphql server will also coerce input values it receives as arguments to fields, as long as those arguments are a scalar type.\n- For example, if we pass a string `\"4\"` or int `4` to an ID input type, the value should be coerced to an ID format (as expected by the Graphql server)\n\n# Examples\n`express-graphql`, `apollo-server`, `graphql-yoga`\n\t- In Express, these are nothing but middleware functions that act as glue between the request and GraphQL engine provided by graphql.js (the tool that provides functionality for resolving queries)\n- express-graphql has 2 responsibilities:\n\t1. Ensure that the GraphQL query (or mutation) contained in the body of an incoming POST request can be executed by GraphQL.js.\n\t2. Attach the result of the execution to the response object so it can be returned to the client.\n- apollo-server is more generic than express-graphql, so it works in other frameworks, in addition to express.\n\t- it can also be configured to work as FaaS, like AWS Lambda\n- graphql-yoga\n\t- like create-react-app for GraphQL servers","n":0.057}}},{"i":654,"$":{"0":{"v":"Schema","n":1},"1":{"v":"\n# Schema\n- specifies the capabilities of the API and defines how clients can request the data. It is often seen as a contract between the source of data (ie. server) and its destination (ie. client), and it defines what data can be queried and how it ought to be queried\n- the schema gives the app's backend the ability to say \"here's the data that I want to make available to the client\"\n- when queries come in, they are validated and executed against the schema\n- Our schema, along with the requested query, defines the \"shape\" of the data object in the response returned by our endpoint\n\t- By shape, we mean what properties objects have, and whether those properties' values' are scalar values, other objects, or arrays of objects or scalars\n- Prefer building a GraphQL schema that describes how clients use the data\n\nThe schema is a mode of the data that can be retrieved through the Graphql server. It specifies:\n- It specifies what queries clients are allowed to make\n- what types (scalar, object, query, mutation) of data can be fetched from the server\n- what the relationships between these types are.\n\nThe Schema says \"Here's the data you can look at\". If we were to imagine our database as a complex graph, then the schema would be the graph. The schema defines how we can query and alter the data of the underlying database.\n\n- ex. a simple user and post app `schema.gql`:\n```\n// schema may be implicitly defined\nschema {\n\tquery: Query\n\tmutation: Mutation\n}\n\ntype Query {\n\tgetUser(uuid: String!): User\n}\n\ntype Mutation {\n\tcreateUser(input: UserInput!): User\n}\n\ntype User {\n\tUUID: String\n\tName: String\n\tPosts: [Post]\n}\n\ntype Post {\n\tUUID: String\n\tText: String\n}\n\ninput UserInput {\n\tName: String\n\tPosts: [PostInput]\n}\n\ninput PostInput{\n\tText: String\n}\n\n```\n\nA field does not become queryable until it exists on the `Query` type:\n```\ntype Query {\n\tmyName: String\n}\n```\n\n## Descriptions\nDescriptions are a first-class way of documentation in Graphql. To implement, we only need to include comments before the definition we are describing. There are 2 different ways to add documentation: Comment blocks for constructs, and inline comments for fields:\n```graphql\n\"\"\"\nA simple GraphQL schema which is well described.\n\"\"\"\ntype Query {\n  \"\"\"\n  Translates a string from a given language into a different language.\n  \"\"\"\n  translate(\n    \"The original language that `text` is provided in.\"\n    fromLanguage: Language\n\n    \"The translated language to be returned.\"\n    toLanguage: Language\n\n    \"The text to be translated.\"\n    text: String\n  ): String\n}\n\n\"\"\"\nThe set of languages supported by `translate`.\n\"\"\"\nenum Language {\n  \"English\"\n  EN\n\n  \"French\"\n  FR\n\n  \"Chinese\"\n  CH\n}\n```\n\n\n","n":0.051}}},{"i":655,"$":{"0":{"v":"Resolver","n":1},"1":{"v":"\n# Resolver\nResolvers specify how the types and fields in the schema are connected to various backends. Through these functions, you are able to answer questions such as “How do I get the data regarding Course Authors?” and “Which backend do I need to call with what arguments to get the data regarding Courses?”.\n\n- A Resolver is a collection of function whose responsibility is fetching the data for a particular field. They are responsible for generating a response to the Graphql query.\n\t- the function is mapped to a schema. In other words, each type in the schema has a corresponding resolver\n\t- The schema (made up of queries and mutations) says \"here's what you can look at\", and the resolver says \"here's how you get it\"\n- therefore, the resolver is the part that initiates the SQL query\n- The resolvers map to your schema and are what GraphQL actually executes to retrieve each piece of data. The resolvers are like controllers in a regular REST API.\n- a resolver receives 4 args:\n\t1. obj\n\t2. args\n\t3. context\n\t4. info\n\n**Obj**\nthe previous object in the graphql \"tree\" (with the root being `query` or `mutate`). It contains the result returned from the resolver on the parent field.\n- sometimes aka `root`\n- when we are making a resolver function on the root Query type, we probably won't need to use `obj`.\n- All Graphql has to do in order to resolve a query is call the resolvers on the query's fields. This is being done level by level (in other words, from most outdented to most indented; ltr).\n- `obj` argument in each resolver call is simply the result of the previous call\n\t- ex. we are querying `getNuggetById`. `obj` is the `query` type at this point. When the backend receives that request, the resolver executes a db query and returns to us `{ id: 1, title: 'first nugget' }`. With the first field resolved, the value of `obj` on the second iteration is the `nugget` type, since that is what was returned by the first iteration. This is precicely the reason why we don't have to explicitly write resolvers for every single field.\n\t- In fact, if we were to console.log `obj`, on the second iteration we'd have the nugget object.\n``\nquery {\n\tnugget(id: $id) {\n\t\tid\n\t\ttitle\n\t}\n}\n```\n\n**args**\nThe arguments provided to the field in the GraphQL query\n\n**context**\nan object that gets passed through the resolver chain that each resolver can read from and write to (basically a means for resolvers to communicate and share information).\n- holds info like currently logged in user, current access to the database (which includes postgres user) etc.\n- therefore, we can use the context to provide access to the database\n\n**info**\nholds field-specific information relevant to the current query as well as the schema details\n- The way you form relationships is by defining custom types in your resolvers\n- In its most basic form, a GraphQL server will have one resolver function per field in its schema\n\t- Each resolver knows how to fetch the data for its field\n\n## How GraphQL resolves fields\nWhile you certainly can write a resolver for every field in your schema, it's often not necessary because GraphQL.js uses a default resolver when you don't provide one.\n- in most cases, the GraphQL library will just omit simple resolvers and will just assume that if a resolver isn't provided for a field, that a property of the same name should be read and returned.\n- what the default resolver does is simple: it looks at the value the parent field resolved to and if that value is a JavaScript object, it looks for a property on that Object with the same name as the field being resolved. If it finds that property, it resolves to the value of that property. Otherwise, it resolves to null.\n\t- This process is the reason why deeply nested queries are more computationally expensive.\n\n# E Resources\n[Good breakdown of how schema/resolvers work](https://www.prisma.io/blog/graphql-server-basics-the-schema-ac5e2950214e)","n":0.04}}},{"i":656,"$":{"0":{"v":"Pagination","n":1},"1":{"v":"\n# Pagination\nAt a high level, cursor-based pagination works like this:\n1. User requests some data (either through initial page load, or clicking some UI pagination button).\n2. Along with the list of data that to display, we also want to get pagination information `pageInfo` that is defined on the connection\n```\nquery OrganizationForLearningReact {\n  organization(login: \"the-road-to-learn-react\") {\n    name\n    url\n    repository(name: \"the-road-to-learn-react\") {\n      issues(first: 10, after: \"Y3Vyc29yOnYyOpHODAESqg==\") {\n        pageInfo {\n          endCursor\n        }\n        edges {\n          node {\n            author {\n              login\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n3. Get the `id` of the last item in the most previously-fetched data list (known as the `cursor`) \n  - stored in `endCursor` variable\n4. Store the cursor in local state (e.g. React, Apollo Client)\n5. pass as variable on subsequent fetches of data (ex. when user clicks \"Load more\", or scrolls more on infinite page)\n6. use that variable as input to the `after` parameter defined on the Connection (ex. in looking through a list of Github issues, the cursor is defined on the `IssuesConnection`)","n":0.079}}},{"i":657,"$":{"0":{"v":"Overview","n":1},"1":{"v":"\n# Overview\nGraphql is a protocol that gets implemented on a server, which gets communicated with from a client. The Graphql protocol defines a language for the client to use. The Graphql server is an expert at locating data. Through its resolvers, it knows exactly where every piece of data resides, and it knows how to get it through the most effective means. But in order to be able to know exactly which data you want, you have to speak to it in the graphql language.\n\nSince Graphql is just a spec, we need to have an implementation to use it (similar to how SQL is a spec for Postgres, MySQL etc). The 2 components of a GraphQL implementation are: the GraphQL server and the GraphQL client.\n\nGraphql has resolvers, each understanding where a certain resource of data is and how to get it. We might have data in a mongo database or a Postgres database, and as long as the resolver knows how to get it, Graphql doesn't really care. All it cares is that the data is retrievable.\n\nTo teach our Graphql server to be able to retrieve the data, we need to define a type system for it.\n\nGraphql uses `POST` requests for queries and mutations under the hood, and websockets for subscriptions\n\n![2bc1e31d73c0da1e8598d766424dd751.png](:/a10e5211520a457fb318c12aec62d1ea)\n\n### Arguments\nIn REST, there is one place to pass arguments, which is in query params or in the request body. With Graphql, every field and nested object can get its own set of arguments.\n\n\n- declarative data fetching where a client can specify exactly what data it needs from an API. Instead of multiple endpoints that return fixed data structures, a GraphQL server only exposes a single endpoint and responds with precisely the data a client asked for.\n- The `GraphQLSchema` object is the core of a GraphQL server\n\t- `GraphQLSchema` consists of 2 parts:\n\t\t1. schema definition (ie. the structure)\n\t\t2. resolver functions that implement the API (ie. the behaviour)\n\t- Query and Mutate are the rootTypes\n- Graphql is completely agnostic to the network layer (usually HTTP) and payload format (normally JSON). In fact, Graphql is not opinionated about the application architecture in general.\n- Graphql allows us to make relational queries that allow us to get all the data needed in one trip, instead of having to make multiple calls.\n\n## Between the client and app server\nA key thing to understand about GraphQL is that it’s actually agnostic to the way how data is transferred over the network\n- ie. it can work on protocols other than HTTP, like websockets\n\n* * *\n## Selection Sets\nA selection set in Graphql is similar to a result set in SQL, except the selection set is where we specify what we'd like to get back, while a result set in SQL is what we *actually* got back. \n\nAs the name suggests, a selection set is a list of Selections (type Selection)\n- a Selection has the following signature:\n```\ntype Selection {\n\tField\n\tFragmentSpread\n\tInlineFragment\n}\n```\n\nA selection set might look like:\n```\n{\n\tid\n\tfirstName\n\tlastName\n}\n```\nSome fields may describe more complex data or relationships to other data. To allow us to explore this data, a field itself may contain a selection set.\n- To remain unambiguous, the most nested fields must be scalars","n":0.044}}},{"i":658,"$":{"0":{"v":"Introspection","n":1},"1":{"v":"\nIntrospection is about the links between types and scalars\nThe Graphql server is what supports introspection over its schema.\nTypes and fields required by the Graphql introspection systema are prefixed with `__`\n\nDetermining which types are available\n```\n{\n  __schema {\n    types {\n      name\n    }\n  }\n}\n```\n\n## Introspection Types\n`__Type` is at the core of the type introspection system. It represents scalars, interfaces, object types, unions, enums in the system.\n- All types in the introspection system provide a `description` field to allow type designers (ie. us, the developer) to publish documentation about a type.\n\n### __Field\nRepresents each field in an Object or Interface type.\n\n\n### __InputValue\nRepresents field and directive arguments as well as the inputFields of an input object.\n\nHas fields:\n- `name`— returns string\n- `description`— returns string or null\n- `type`— returns `__Type`, representing the type this input value expects\n- `defaultValue`\n\n## Introspection Queries\nThe schema introspection system is accessible from the meta‐fields `__schema` and `__type` which are accessible from the type of the root of a query operation.\n\nCan be of 3 types:\n```\n__schema: __Schema!\n__type(name: String!): __Type\n__typename: String!\n```\n\nThese fields are implicit and do not appear in the fields list in the root type of the query operation.\n\nThe schema of the GraphQL schema introspection system:\n\n```\ntype __Schema {\n  types: [__Type!]!\n  queryType: __Type!\n  mutationType: __Type\n  subscriptionType: __Type\n  directives: [__Directive!]!\n}\n\ntype __Type {\n  kind: __TypeKind!\n  name: String\n  description: String\n\n  # OBJECT and INTERFACE only\n  fields(includeDeprecated: Boolean = false): [__Field!]\n\n  # OBJECT only\n  interfaces: [__Type!]\n\n  # INTERFACE and UNION only\n  possibleTypes: [__Type!]\n\n  # ENUM only\n  enumValues(includeDeprecated: Boolean = false): [__EnumValue!]\n\n  # INPUT_OBJECT only\n  inputFields: [__InputValue!]\n\n  # NON_NULL and LIST only\n  ofType: __Type\n}\n\ntype __Field {\n  name: String!\n  description: String\n  args: [__InputValue!]!\n  type: __Type!\n  isDeprecated: Boolean!\n  deprecationReason: String\n}\n\ntype __InputValue {\n  name: String!\n  description: String\n  type: __Type!\n  defaultValue: String\n}\n\ntype __EnumValue {\n  name: String!\n  description: String\n  isDeprecated: Boolean!\n  deprecationReason: String\n}\n\nenum __TypeKind {\n  SCALAR\n  OBJECT\n  INTERFACE\n  UNION\n  ENUM\n  INPUT_OBJECT\n  LIST\n  NON_NULL\n}\n\ntype __Directive {\n  name: String!\n  description: String\n  locations: [__DirectiveLocation!]!\n  args: [__InputValue!]!\n}\n\nenum __DirectiveLocation {\n  QUERY\n  MUTATION\n  SUBSCRIPTION\n  FIELD\n  FRAGMENT_DEFINITION\n  FRAGMENT_SPREAD\n  INLINE_FRAGMENT\n  SCHEMA\n  SCALAR\n  OBJECT\n  FIELD_DEFINITION\n  ARGUMENT_DEFINITION\n  INTERFACE\n  UNION\n  ENUM\n  ENUM_VALUE\n  INPUT_OBJECT\n  INPUT_FIELD_DEFINITION\n}\n```\n\n### __schema\nThe most important field (query) in graphql, as it allows us to fetch the whole schema. It is also the primary source for Graphiql. The name for this query is `__schema`, and its Schema Definition Language (SDL) is:\n```\ntype __Schema {\n  types: [__Type!]!\n  queryType: __Type!\n  mutationType: __Type\n  subscriptionType: __Type\n  directives: [__Directive!]!\n}\n```\n\nAs we can see, when we query `__schema`, we get some information about the whole schema, including which operation types we have, and which types and directives we have. The result of that query might look like this:\n```\n{\n  __schema {\n    directives {\n      name\n      description\n    }\n    subscriptionType {\n      name\n      description\n    }\n    types {\n      name\n      description\n    }\n    queryType {\n      name\n      description\n    }\n    mutationType {\n      name\n      description\n    }\n    queryType {\n      name\n      description\n    }\n  }\n}\n```\n\n### __type\nAllows us to query for information about the exact type we are interested in. All we need to do is pass in the type as an argument like so:\n```\nquery introspectionUserType {\n  __type(name: \"User\") {\n\tname\n\tkind\n\tfields {\n\t\tname\n\t\ttype {\n\t\t\tname\n\t\t}\n\t}\n  }\n}\n```\nThis might return:\n```\n{\n  \"__type\": {\n    \"name\": \"User\",\n\t\"kind\": \"OBJECT\"\n    \"fields\": [\n      {\n        \"name\": \"id\",\n        \"type\": { \"name\": \"String\" }\n      },\n      {\n        \"name\": \"name\",\n        \"type\": { \"name\": \"String\" }\n      },\n      {\n        \"name\": \"birthday\",\n        \"type\": { \"name\": \"Date\" }\n      },\n    ]\n  }\n}\n```\n\nWhen the `name` field is `null`, it is because it is a wrapper type (ID, List). If we query for the `ofType` on these fields, we can see what the \"wrapped\" type is\n- ex. a list is a wrapper type, because each item in the list has its own type. The List is just a type that wraps them altogether. The same can be said for Non-null and ID.\n\n### __typename\nOffers type name introspection. `__typename` is a meta-field that allows us to get the type name at any point within a query. It is available through all types when querying.\n\nApollo client makes use of `__typename` to construct the apollo cache. It uniquely identifies an item by `id + __typename`","n":0.04}}},{"i":659,"$":{"0":{"v":"Operations","n":1},"1":{"v":"\n# Operation Types (ie. Queries and Mutations)\n- `query` types and `mutation` types are the entry point to a graphql query\n\t- It's important to remember that other than the special status of being the \"entry point\" into the schema, the Query and Mutation types are the same as any other GraphQL object type, and their fields work exactly the same way.\n\nScalars are primitive values in GraphQL. If we consider a graphql query as a hierarchical graph, the leaves would be the primitives\n\nGraphQL requires that you construct your queries in a way that only returns concrete data\n- Each field has to ultimately resolve to one or more scalars (or enums). That means you cannot just request a field that resolves to a type without also indicating which fields of that type you want to get back (hence why `graphiql` will auto-complete for us).\n\ntwo operations that occur in GraphQL servers are:\n1. **result coercion**: upholding the contract of a type which we receive from the server (basically upholding the primitive values or object type)\n\t- The type system knows what to expect and will convert the values returned by a resolver function into something that upholds the API contract\n2. **input coercion**: upholding the contract of a type for input arguments that we pass into the GraphQL query or mutation\n\t- if we pass in `5` for the `id` field, the type will be parsed into a string as `\"5\"`","n":0.066}}},{"i":660,"$":{"0":{"v":"Subscriptions","n":1},"1":{"v":"\n## Subscriptions\nreal-time connection from the client to the server that allows the client to get immediately informed about events happening server-side\n- when a client subscribes to an event, it will hold a steady connection to the server. when this event happens, the server will push that corresponding data to the client\nTherefore, subscriptions are event-based, acting in response to what just happened.\n- We can see this in the subscription nomenclature: `commentAdded`, `paymentMethodAdded`\n- Unlike queries and mutations that follow a typical “request-response-cycle”, subscriptions represent a *stream* of data sent over to the client.\n\nYou should use subscriptions for the following:\n- Small, incremental changes to large objects.\n  - Repeatedly polling for a large object is expensive, especially when most of the object's fields rarely change. Instead, you can fetch the object's initial state with a query, and your server can proactively push updates to individual fields as they occur.\n- Low-latency, real-time updates.\n  - For example, a chat application's client wants to receive new messages as soon as they're available.\n\n```\nsubscription {\n  newPerson {\n    name\n    age\n  }\n}\n```\nWhenever a newer mutation is performed that creates a new `Person`, the server sends the information about this person to the client\n","n":0.072}}},{"i":661,"$":{"0":{"v":"N1","n":1},"1":{"v":"\n# GraphQL suffers from the N+1 problem\n- The number of queries grows exponentially with the depth of the query\n- N1 problem occurs when you have to retrieve the same information multiple times. In graphql, the resolver for that particular field would have to be hit every single time, instead of the system understanding that it's the same result, and being more efficient about it.\n\t- ex. We have a query shaped as below. imagine that the article has 5 `comments` and all are written by the same person. In Graphql, that would be 5 times that the resolver got called, instead of once.\n```js\nquery {\n  user(id: \"abc\") {\n\tname\n\tarticle(title: \"GraphQL is great\") {\n\t  comments {\n\t\ttext\n\t\twrittenBy {\n\t\t  name\n\t\t}\n\t  }\n\t}\n  }\n}\n```\n- N1 problem can be solved by DataLoader library\n[possibly redundant info](https://stackoverflow.com/questions/97197/what-is-the-n1-selects-problem-in-orm-object-relational-mapping)\n[using a batchloader (like joinMonster) to solve N+1](http://www.petecorey.com/blog/2017/08/14/batching-graphql-queries-with-dataloader/?from=east5th.co)\n","n":0.087}}},{"i":662,"$":{"0":{"v":"Fields","n":1},"1":{"v":"\n# Fields\nEvery field on a GraphQL object type can have zero or more arguments:\n```\ntype Starship {\n  id: ID!\n  name: String!\n  length(unit: LengthUnit = METER): Float\n}\n```\n- REST endpoints are similar to GraphQL fields, as they are entry points into the data that call functions on the server.\n- sometimes you can have an entity that is both a field and a type. When I query for one nugget, I am querying on a field `nugget`, and getting back a Nugget type.\n\t- in this case, `nugget` is known as the root field\n- Each field on each type is backed by a *resolver*\n\t- When a field is executed, the corresponding resolver is called to produce the next value. If a field produces a scalar value like a string or number, then the execution completes. However if a field produces an object value then the query will contain another selection of fields which apply to that object. \n\t\t- This continues until scalar values are reached. GraphQL queries always end at scalar values.\n\nFields are conceptually functions which return values, and occasionally accept arguments which alter their behavior.\n- These arguments often map directly to function arguments within a GraphQL server’s implementation.\n- you can think of each field as a function of the previous type which returns the next type\n\t- therefore, fields can exist at different levels. Imagine we had a `User` type, and that `user` had an `id` and `name`. We would be looking at 3 fields in total here, since `user` is just a field defined on the `Query` type. \n\nSince fields are conceptually just functions, that means we may be able to pass arguments to them. \n- Arguments can also be passed into scalar values, which would allow the server to do data transformations before sending it to the client (as opposed to the client having to handle that). All we need to do is define an enumeration type:\n```\ntype Person {\n\tname: String\n\tpicture(size: Int): Url\n}\n\n{\n  person(id: \"1000\") {\n    name\n    picture(size: 600)\n  }\n}\n```\n\n## Alias\nBy default, the response of our query is an object with a key matching the field name(s) we queried\n\nSometimes we want to request data about 2 identical objects. (ex. querying the same field twice, but with different arguments). In cases like this, we need to use aliases, which changes the key of the object we get as our query result.\n\n","n":0.051}}},{"i":663,"$":{"0":{"v":"Structures","n":1}}},{"i":664,"$":{"0":{"v":"Queries","n":1},"1":{"v":"\n## Queries\nEvery GraphQL service has a query type and may or may not have a mutation type\n\n### Understanding how a query works\n```\nquery {\n  hero {\n    name\n    appearsIn\n  }\n}\n```\n\n1. We query the special root Query type, and get back an object\n2. We select the hero field on that object\n3. For the object returned by hero, we select the name and appearsIn fields\n","n":0.128}}},{"i":665,"$":{"0":{"v":"Mutations","n":1},"1":{"v":"\n## Mutations\n- While query fields are executed in parallel, mutation fields run in series, one after the other.\n\t- This means that if we send two `updateNugget` mutations in one request, the first is guaranteed to finish before the second begins\n### Input types\noften we will be using the same inputs to different mutations (ex. createNugget, updateNugget). Input types offers us a way to decalare these inputs once, and alias it as a type. \n","n":0.117}}},{"i":666,"$":{"0":{"v":"Fragments","n":1},"1":{"v":"\nFragments are the primary unit of composition in Graphql. They...\n- must specify the type they apply to\n- cannot be specified on any input value (scalar, enumeration, or input object).\n- can be specified on object types, interfaces, and unions.\n\nBelow, the part after `on` is the type we are selecting from\n- so `people` is of type `Person`, and we want the `firstName` and `lastName` fields from `people(id: \"7\")`\n```\nfragment NameParts on Person {\n  firstName\n  lastName\n}\n\nquery GetPerson {\n  people(id: \"7\") {\n    ...NameParts\n    avatar(size: LARGE)\n  }\n}\n```\n## Conditional Fragments\n### Type Conditions \n- allow us to conditionally include fields based on their runtime type.\n\nOn Facebook, imagine we have 2 different fields that both specify a `count`: User Friends, and Page Likes. We have set up our schema in such a way that we define a `Profile` type, which can be either a `User` or `Page`. \n\nNow imagine that we want to write a query that gets back both the friends count of Kyle Tycholiz, and it gets back the like count of the Never Forget page. If we want to write this in a single query, we are presented with a problem: while both have a field `count`, the parent field is different, namely `friends` and `likes`. To solve this problem, we must use fragments, which will be applied *depending on* the type of profile that is returned in the query.\n\nWhen we query for this, the `profiles` root field returns a list where each element could be a `Page` or a `User`.\n\n```\n{\n\tprofiles(name: [\"kyletycholiz\", \"neverforget\"]) {\n\t\tname\n\t\t...userFragment\n\t\t...pageFragment\n\t}\n\t\nfragment userFragment on User {\n\tfriends {\n\t\tcount\n\t}\n}\n\nfragment pageFragment on Page {\n\tlikers {\n\t\tcount\n\t}\n}\n}\n```\n\n### Inline Fragments\nif querying a field that returns an interface or union type, we need to use inline fragments to access data on the underlying concrete type\n- These can be used to compose fields in a type-dependent way\n    \nThe above Facebook example can be accomplished using inline fragments. \n\nThe Graphql server will determine whether to return `homeAddress` or `address` at runtime, depending on whether the requested object is a `User` or `Business`\n\n```\nquery Foo {\n  profile(id: $id) {\n    url\n    ... on User {\n      homeAddress\n    }\n    ... on Business {\n      address\n    }\n  }\n}\n```\n\n#### Practical usage\n- Imagine we had an interface `Character` that represents a character from Star Wars:\n```\ninterface Character {\n  id: ID!\n  name: String!\n  friends: [Character]\n  appearsIn: [Episode]!\n}\n```\n- Imagine now we create two types that *implement* this interface:\n```\ntype Human implements Character {\n  id: ID!\n  name: String!\n  friends: [Character]\n  appearsIn: [Episode]!\n  starships: [Starship]\n  totalCredits: Int\n}\n\ntype Droid implements Character {\n  id: ID!\n  name: String!\n  friends: [Character]\n  appearsIn: [Episode]!\n  primaryFunction: String\n}\n```\n- As we can see, **Droid** has the field `primaryFunction`, while **Human** does not\n    - This means that if we were to make a query that wanted that field back, we would get an error:\n```\nquery HeroForEpisode($ep: Episode!) {\n  hero(episode: $ep) {\n    name\n    primaryFunction\n  }\n}\n// PRODUCES ERROR: cannot query field 'primaryFunction' on type Character\n```\n- To get around this, we need to use an inline fragment:\n```\nquery HeroForEpisode($ep: Episode!) {\n  hero(episode: $ep) {\n    name\n    ... on Droid {\n      primaryFunction\n    }\n  }\n}\n```\n","n":0.045}}},{"i":667,"$":{"0":{"v":"Client","n":1},"1":{"v":"\n# Graphql Client\nThe Graphql API has more underlying structure than a REST API. This means there are more things to handle and keep track of, such as batching, caching, and other features. ","n":0.177}}},{"i":668,"$":{"0":{"v":"Auth","n":1},"1":{"v":"\n### Authorization\n- all authorization should be handled by the business logic layer in the application, not in graphql\n\t- [source](https://graphql.org/learn/authorization/)\n- In a REST API, authentication is often handled with a header, that contains an auth token which proves what user is making this request. Express middleware processes these headers and puts authentication data on the Express request object. \n\nThere are two broad ways of handling authentication in GraphQL APIs:\n1. **Authentication via the GraphQL server**: All users have to be logged in by the GraphQL server before they can query the endpoint—purely GraphQL workflows. Authentication is implemented in the GraphQL Schema\n\t- It involves first getting a JWT token, then passing that token in subsequent requests\n\t- this is the method proposed by Postgraphile (a JWT-based approach). \n2. **Authentication via a Web Server** (ex. Express and Passport): Users can make queries to the GraphQL endpoint once they are logged in. Authentication is implemented with middleware.\n","n":0.081}}},{"i":669,"$":{"0":{"v":"AST","n":1},"1":{"v":"\nIn order to interpret and understand these unique and complex requests, GraphQL employs the use of an Abstract Syntax Tree (AST) to structure the incoming requests.\n- This makes it easier for the back-end gurus & frameworks to parse and construct the required response.\n\nAs a consequence of Graphql being data-layer-independent, libraries such as graphql-js and graphql-tools exist to abstract away the heavy lifting, removing our need to worry about having to interact with the underlying AST.\n- Sometimes however we must be able to perform our own custom operations based on the specifics of the user's request.\n\nWhen a user makes a request, GraphQL combines the query document that the user requested with the schema definition that we defined for our resolver in the form of an AST. This AST is used to determine which fields were requested, what arguments were included and much more.\n\n### Rugby Player Example\nSuppose we wanted to expose a list of rugby players on our Graphql backend. We make a schema definition which models a new Graphql type `RugbyPlayer`, which has 2 fields: `full_name` and `club`:\n```js\nexport const RugbyPlayer = new gql.GraphQLObjectType({\n    name: 'RugbyPlayer',\n    fields: {\n        full_name: {\n            type: gql.GraphQLString,\n        },\n        club: {\n            type: new gql.GraphQLObjectType({\n                name: 'Club',\n                fields: {\n                    name: {\n                        type: gql.GraphQLString,\n                    },\n                },\n            }),\n        },\n    },\n})\n```\nImagine now that a client makes a request for that list of players. From the Graphql server's perspective, the schema knows nothing about the query document (ie. the request). Therefore, it has to parse the query document into AST format (ie. heavily nested objects) before it can traverse it. Now, it can perform any necessary validation, like throwing errors for having the wrong types for fields or arguments.\n- Following this validation, schema types are mapped onto the respective branches of the AST to provide a richer set of metadata.\n\n### What we can do with the AST\nBy understanding the AST, we can craft custom directives and optimise user requests\n- We do this by breaking the traditional Graphql lifecycle and inercepting a request before it gets passed onto another library to generate a response.\n\nBy traversing and augmenting the AST we can implement:\n- Schema stitching\n- Custom directives\n- Enriched queries\n- Layered Abstraction\n- More backend magic!\n\nIn a step to give more control to the front-end client, we often find it useful to implement a range of directives that can transform or filter out fields specified in the AST.\n- ex. In the Rugby example, imagine we want to solve this problem: \"Rugby player's names are being mispronounced too often\".\n\t- What we can do is implement a Graphql directive called `@pronounce`, which can be added onto the `full_name` field when we are querying our `RugbyPlayers` type:\n```\nquery {\n  RugbyPlayers {\n    full_name @pronounce\n  }\n}\n```\nlibraries like `graphql-js` can provide methods such as `gql.GraphQLDirective` that allow us to add these directives.\n\nWhenever we see code like this:\n```\nconst Type = gql`\n  type ${nameOfType} {\n    str: String\n    int: Int\n  }\n`;\n```\n\nWe can understand `gql` as a function that transforms our human-readable graphql schema language into an AST that the application can understand.","n":0.045}}},{"i":670,"$":{"0":{"v":"Graphile Worker","n":0.707},"1":{"v":"\n## Tasks\nWhen we run a task in Node (ex. with `run()`, we are returned a `Runner` object, which has the following methods/properties:\n- stop(): Promise<void> - stops the runner from accepting new jobs, and returns a promise that resolves when all the in progress tasks (if any) are complete.\n- addJob: AddJobFunction - see addJob.\n- promise: Promise<void> - a promise that resolves once the runner has completed.\n- events: WorkerEvents - a Node.js EventEmitter that exposes certain events within the runner (see WorkerEvents).\n\nex. we can add a job to the queue in response to another job being added to the queue:\n```js\nawait runner.addJob(\"testTask\", {\n  thisIsThePayload: true,\n});\n```\n\n### Events\nWe can listen to events like so:\n```js\nrunner.events.on(\"job:success\", ({ worker, job }) => {\n  console.log(`Hooray! Worker ${worker.workerId} completed job ${job.id}`);\n});\n```\n\nTo realize the importance and place of Workers, we have to break asynchronous tasks down into 2 types:\n- async tasks, where the process of execution is dependent on the response to continue on to execute the next lines of code.\n  - ex. When the Express server is making database queries, the it has to wait for the data to return before it can continue on with its operation. This is why we use `async`/`await`, because we need that data\n- async tasks, where the 2 processes are realistically distinct from one another, and can be completely decoupled.\n  - ex. When we want to send registration success emails to users, we don't want to tie up the Express server thread doing this task. It is something that can be handed off to some other service to handle for us.","n":0.062}}},{"i":671,"$":{"0":{"v":"Graphile Migrate","n":0.707},"1":{"v":"\n## Tracking Changes\n- GM has an .gmrc file, which allows us to define hooks. We can therefore define a hook that will run `pg_dump` on the shadow db after every migration. If we track this file in git, then we can see a clear history of what each migration has done.\n\n## Development\n- In Development, aside from the main database, there is a shadow db which is used internally by graphile-migrate and is mainly for testing consistency of the migrations, among other minor tasks\n- the shadow database gets reset frequently\n- `commit`, `uncommit`, `watch` and `reset` are development-only commands.\n\n## Flow\n1. We write our new idempotent migration in `current.sql`\n\t- any seed data we have can be placed at the bottom, to be removed prior to committing.\n2. when ready, we run `commit`\n\t- this should only be done immediately before the branch is merged into master (see README###Collaboration). This it to ensure commits are linear.\n\t- all sql in `current.sql` is removed and catalogued into `committed/`\n\t- the shadow database is dropped, and recreated by running all migrations. This is to ensure that the migration works without a hitch.\n\n* * *\n\n### Pitfalls\n- when dropping tables, schemas, and functions, we should use CASCADE\n- use idempotent commands whenever possible.\n\t- ex. DROP ____ IF EXISTS\n\t- ex. CREATE OR REPLACE FUNCTION\n\t- The initial migrations don't need to be idempotent if this migration starts off by dropping all schemas (which implicitly drops all tables attached to those schemas)\n","n":0.065}}},{"i":672,"$":{"0":{"v":"Graphics","n":1},"1":{"v":"\n### Rendering Pipeline\nThe steps taken to transform and render a 3D model to 2D (the screen).\nThere are APIs that can be used to control the underlying hardware and keep the programmer away from writing code to manipulate the graphics hardware accelerators .\n- some examples are Direct3D and OpenGL. The underlying hardware would be AMD/Intel/NVIDIA etc.\n\nspec:(one of?) The most common  type of rendering pipeline implementations is done with a shader.","n":0.12}}},{"i":673,"$":{"0":{"v":"Rendering","n":1},"1":{"v":"\n# UE Resources\n[Catlikecoding: high quality tutorials and guides on how this stuff works at low level](https://catlikecoding.com/unity/tutorials/rendering/)\n","n":0.25}}},{"i":674,"$":{"0":{"v":"Shader","n":1},"1":{"v":"\nA shader's role is to convert 3D assets into 2D pixels onto the screen.\n\n\nConsider that a triangle (in 3D space) is made up of three 3D points (vertices), for example:\n```\n(-1.03, 000,   0.00)\n(0.69,  0.00,  1.00)\n(0.69,  0.00, -1.00)\n```\nTo show a triangle on the screen, we need to convert these 3D points into 2D points onto a virtual screen (called a Viewport)\n- To figure this out, we need to consider the position/rotation of the triangle, as well as the position/rotation of the camera itself.\n- All transformations required to convert a 3D point into a 2D point in the viewport can be encoded in a single 4x4 matrix:\n```\n[0, 0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0]\n```\n\nWhen you write a Shader, you are primarily responsible for writing a *vertex function* and a *fragment function*. The graphics system makes use of these functions, and handles the rest.\n- vertex function is responsible for conversion from 3D to 2D (most of the complicated math can be handled for us by utilizing APIs)\n- fragment function is responsible for determining which color each pixel within the triangle will be (ie. The fragment function returns a color)\n    - this occurs right after rasterization.\n\nVertex function is also responsible for telling the interpolation system what information to interpolate.\n- whatever data is returned from the vertex function will be interpolated for each pixel and sent to the fragment function:\n\nVertex function runs once for each vertex in the triangle (ie. 3 times per triangle)\nFragment function runs once per pixel that is covered by the triangle.\nThe rasterization and interpolation step ties the vertex function and fragment function steps\n\nVertex Function -> position, UV etc -> Interpolation system -> Fragment function -> pixel color to be written to the screen\n\n### Rasterization\nWith a 2D triangle on the screen, rasterization is the process of figuring out which pixels in the screen belong within the triangle.\n\n## Lighting\n","n":0.057}}},{"i":675,"$":{"0":{"v":"3D Modeling","n":0.707},"1":{"v":"\nWith a 3D modeling software like Blender, artists model the shape of a 3D object, **rig** it so that it can be animated, **animate** it to move in a particular way, and **texture** it to give it color and shading. An example of this is shown below. \n![](/assets/images/2021-08-15-16-54-09.png)\nUnity has a quick n' dirty implementation of a 3D modeler called [ProBuilder](https://unity.com/features/probuilder) \n\n# Resources\n[Turbosquid: pre-made 3D modeled assets](https://www.turbosquid.com/)\n[CG Trader: pre-made 3D modeled assets](https://www.cgtrader.com/)","n":0.119}}},{"i":676,"$":{"0":{"v":"Go","n":1},"1":{"v":"\nRunning code concurrently is part of the language.\nIt’s faster than scripting languages like PHP, Python, or Javascript.\nThe final application is a binary compiled statically without any dependency.\n- You just need to drop the executable on your server to deploy the application.\n\n# UE Resources\n[Go tutorials from Flavio Copes](https://flaviocopes.com/tags/go/)\n","n":0.146}}},{"i":677,"$":{"0":{"v":"GitHub","n":1}}},{"i":678,"$":{"0":{"v":"Pr","n":1}}},{"i":679,"$":{"0":{"v":"Strategies","n":1},"1":{"v":"\n### Push new changes to PR branch that has diverged.\n1. Before pushing any new changes, rebase `master` onto the feature branch, and resolve conflicts:\n`git rebase master`\n2. on feature branch, run `git push --force`\n\t- If we don't force push, we will get a warning that branches have diverged and we must first pull. This is a bad idea, spec: because we would be getting rewritten history, and would in effect be introducing many more commits than we are actually intending.\n","n":0.113}}},{"i":680,"$":{"0":{"v":"Pages","n":1},"1":{"v":"\n[Subdirectory of master branch at root of gh-pages branch](https://gist.github.com/cobyism/4730490)","n":0.333}}},{"i":681,"$":{"0":{"v":"Hub CLI","n":0.707},"1":{"v":"\n##### Open a PR template with Vim\n`hub pull-request`","n":0.354}}},{"i":682,"$":{"0":{"v":"Code Review","n":0.707},"1":{"v":"\n# E Resources\n[Good guidelines for CR etiquette](https://github.com/thoughtbot/guides/tree/main/code-review)\n[Showing empathy during CR](https://thoughtbot.com/blog/empathy-online)\n\n# UE Resources\n[A recommended video on good CR culture](http://confreaks.tv/videos/railsconf2015-implementing-a-strong-code-review-culture)","n":0.236}}},{"i":683,"$":{"0":{"v":"Cmds","n":1}}},{"i":684,"$":{"0":{"v":"Search","n":1},"1":{"v":"\n# Searching\nsearch all code bases with `@json`\n- `\"json;\"`\nexclude results from repository WatermelonDB\n- `-repo:Nozbe/WatermelonDB`\n\n- checkout a branch from github (ex. for CR)\n    - `hub pr checkout <issue number>`\n\n\ndrop role if exists google_data_studio;\ncreate role google_data_studio with login password 'sk338fjw0g74nf92nf%34d8' noinherit;\n\nrevoke usage on schema stats from google_data_studio;\ngrant usage on schema stats to google_data_studio;\n\nrevoke select on all tables in schema stats from google_data_studio cascade;\ngrant select on all tables in schema stats to google_data_studio;\n\n\n\n//after all tables\n\nrevoke select on stats.daily_registration_historical from google_data_studio;\ngrant select on stats.daily_registration_historical to google_data_studio;\n\nrevoke select on stats.current_active_users_historical from google_data_studio;\ngrant select on stats.current_active_users_historical to google_data_studio;\n\nrevoke select on stats.daily_book_sales_historical from google_data_studio;\ngrant select on stats.daily_book_sales_historical to google_data_studio;\n\nrevoke select on stats.daily_readership_historical from google_data_studio;\ngrant select on stats.daily_readership_historical to google_data_studio;\n\nrevoke select on stats.daily_reading_activity_by_publisher_historical from google_data_studio;\ngrant select on stats.daily_reading_activity_by_publisher_historical to google_data_studio;\n","n":0.091}}},{"i":685,"$":{"0":{"v":"Actions","n":1},"1":{"v":"\nA Github Action is installed as soon as we have a `.github/workflows` directory committed to our repo on Github.\n\n![](/assets/images/2021-04-15-21-56-11.png)\n\n## Terminology\n\n### Workflow\n![[github.actions.workflow]]\n\n### Job\n![[github.actions.job]]\n\n### Step\n![[github.actions.step]]\n\n### Action\n![[github.actions.action]]\n\n### Runner\n![[github.actions.runner]]\n\n* * *\n\n### Secrets\n`GITHUB_TOKEN` is passed to the runner when a workflow is triggered.\n- Aside from this, no other secrets are passed by default, and must be configured manually.\n\nTo provide an action with a secret (either as input or as an env variable), use the `secrets` context.\n- This allows us to access secrets we've created in our repo.\n- [more info](https://docs.github.com/en/actions/reference/context-and-expression-syntax-for-github-actions)\n\nSecrets should not be passed between processes via the command line, as these commands might be visible to other users with the `ps` command.\n- This is why we use `STDIN` or env variables to pass secrets.","n":0.092}}},{"i":686,"$":{"0":{"v":"Workflow","n":1},"1":{"v":"\n### Workflow\nA workflow is an automated procedure that exists as a part of our repo.\n- Workflows are made up of a collection of jobs which run is response to some event (on PR, on push etc)\n\nA Github Workflow can be used to build, test, package, release, or deploy a project on GitHub.\n\nWorkflows can be configured to run in response to a webhook call, so we can trigger workflows in response to events happening outside Github.\n","n":0.115}}},{"i":687,"$":{"0":{"v":"Step","n":1},"1":{"v":"\n### Step\nA step is an individual task that can run commands in a job\n\nA step can either be:\n- a shell command\n- an action\n\nEach step in a job executes on the same runner, allowing the actions in that job to share data with each other.\n\nChanging directories in one step does not carry through to the next step:\n```yml\n# workflow.actions.yml\n    - name: cd into publishable\n      run: |\n        cd publishable \n        pwd\n\n    - name: does the cd carry thru?\n      run: |\n        pwd\n```\n\nlog:\n```\nRun cd publishable\n/home/runner/work/Digital-Garden/Digital-Garden/publishable\n\nRun pwd\n/home/runner/work/Digital-Garden/Digital-Garden\n```","n":0.111}}},{"i":688,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Run shell command\n```yml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - name: run a script\n      run: |\n        echo \"here it is boys!\"\n        ls main\n```\n\n","n":0.213}}},{"i":689,"$":{"0":{"v":"Runner","n":1},"1":{"v":"\n### Runner\nA runner is a server that has the GitHub Actions runner application installed\n- meaning we could use the managed service from Github, or self-host the Runner.\n\nA runner listens for available jobs, runs one job at a time, and reports the progress, logs, and results back to GitHub\n\nGitHub-hosted runners are based on Ubuntu Linux, Microsoft Windows, and macOS\n","n":0.131}}},{"i":690,"$":{"0":{"v":"Job","n":1},"1":{"v":"\n### Job\nA job is a set of steps that execute on a single runner\nA workflow with multiple jobs will run those jobs in parallel by default (configurable)\nEach job in a workflow runs in a fresh virtual environment\n","n":0.164}}},{"i":691,"$":{"0":{"v":"Env Vars","n":0.707},"1":{"v":"\nThe runner automatically creates a `GITHUB_TOKEN` secret to authenticate in your workflow\n\nhttps://docs.github.com/en/actions/learn-github-actions/environment-variables\n","n":0.289}}},{"i":692,"$":{"0":{"v":"Action","n":1},"1":{"v":"\n### Action\nActions are standalone commands that are combined into steps to create a job\nActions are the smallest portable building block of a workflow. You can create your own actions, or use actions created by the GitHub community\n\nFor instance, we have the `checkout@v2` action, which allows the runner to checkout a repo. In fact, we can checkout any SHA from any repo.","n":0.128}}},{"i":693,"$":{"0":{"v":"Git","n":1},"1":{"v":"\n# Resources\n[SQL queries for Git](https://askgit.com/)\n","n":0.447}}},{"i":694,"$":{"0":{"v":"Rewrite History","n":0.707}}},{"i":695,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Remove a file permenantly from the history\ntip: check the resulting SHAs and compare the commit messages of before and after. Do the SHAs differ from what you would have expected? Between the two, are SHAs of \"same\" commits different BEFORE (ie. earlier in git history) the file you are removing (e.g. here `.env`)\n`git filter-repo --force --invert-paths --path .env`","n":0.13}}},{"i":696,"$":{"0":{"v":"Refspec","n":1},"1":{"v":"\n### Refspec\n- Map a branch in local repo to branch in remote repo.\n    - Allows us to manage remote branches using local Git commands\n- Specified as `<src>:<dst>`\n","n":0.192}}},{"i":697,"$":{"0":{"v":"Ref","n":1},"1":{"v":"\n### Relative Refs\n- we can use relative refs to move around.\n","n":0.302}}},{"i":698,"$":{"0":{"v":"Refs","n":1},"1":{"v":"\n### Ref\n- In a world where all we have is [[blobs|git.inner.objects.blob]], [[trees|git.inner.objects.tree]] and [[commit objects|git.inner.objects.commit]], we would have to remember the commit SHAs so we have a starting point for our history. Instead it would be easier if we had a file where we could store that SHA under a simple name, so we could use the name instead of the SHA. These simple names are called *refs*.\n- A ref is anything pointing to a commit.\n    - ie. branches (local/remote) and tags, mostly.\n    - unlike objects, they are mutable and constantly change.\n\t\t- In Git, the immutable parts are the expensive ones (ex. an entire blob), while the mutable parts are just references, and therefore cheap (ex. branch, remote, HEAD)\n\t- Think of it like a user-friendly alias for commit SHA\n\t\t- ex. `my-feature-branch` is actually an alias for the branch. In reality, the branch name is a SHA\n- `git show-ref`\n\t- ex. if we look at `.git/refs/heads/master`, we will see a commit SHA, which is the latest commit on this branch. If this wasn't done automatically for us, we could manually create this file with the SHA, which would effectively be saying \"hey Git, when we are on master branch, the latest commit will be this SHA\"\n\t\t- Therein lies basically what a branch is: a simple pointer to the latest commit\n- stores pointers to commit objects that we would consider to be significant\n\t- ie. the ref is the location of the commit \n    - The head commit of each branch that exists (`refs/heads` directory)\n    - The head commit of each branch on the remote repos (mostly just `origin`)\n    - stash commits\n    - tags\n- if commits point to trees, and trees point to blobs, then refs conceptually point to commits","n":0.059}}},{"i":699,"$":{"0":{"v":"Packfiles","n":1},"1":{"v":"\n### Packfiles\n- since different versions of the same file create multiple blobs, it can get inefficient if we only difference between these versions were only a single line. What would be great is if we could store one version of the file, and just store the delta of the other with the first.\n\t- Git does exactly this using packfiles.\n- The initial format in which Git saves objects on disk is called a “loose” object format. Occasionally Git packs up several of these objects into a single binary file called a *packfile* for the purpose of being more efficient.\n- Packfiles are created under 3 circumstances:\n\t1. there are too many loose objects around (~7000)\n\t2. `git gc` is run manually\n\t3. we push to a remote repo \n- When we run `git gc`, we will notice that many of the objects in our object database disappear and are replaced by packfiles. The objects that remain are those blobs that aren't pointed to by any commit (in other words, the blobs were never added to any commits). Because of this, those blobs are considered dangling and as a result were never packed up into the new packfile.\n\t- The other files are the packfile and an index. The packfile contains the contents of all the objects that were removed from the filesystem. The index can be thought of as the index of a textbook, which helps us locate specific objects quickly.\n\t\t- if we inspect this packfile index with `git verify-pack`, we can see that 2 versions of the same file will have one version referencing the other, showing that we use deltas to be efficient. \n\t\t","n":0.061}}},{"i":700,"$":{"0":{"v":"Inner","n":1},"1":{"v":"\n# Inner Working of Git\n- Git is a database of references. Essentially, it is just one giant acyclic graph\n\t- here, acyclic means that we can pick one commit and traverse its parents in one direction and there will never be a chain that begins and ends with the same commit\n- Each time we create a file and begin tracking it, git compresses it and stores it into its own structure\n\n### How git stores the data\nCommits are snapshots, not diffs\n- Git takes snapshots (via commits), and stores it in something of a mini-filesystem. When Git stores a new version of a project, it stores a new tree (a bunch of blobs and a collection of pointers) that can be used to expand back out into the whole directory of files and directories.\n- ex. if we change a single line in a single file, that is technically a new version of the project. However, Git is smart and it realizes that there were no changes in 99.9% of the files in the codebase. This \"new version\" of the project does in fact have its own tree, but because all (except one) of the blobs in this tree are just references to other blobs, this version of the project is extremely lightweight (in fact the only \"weight\" is the content of the new file). This way of handling snapshots is a core reason why Git is so efficient, yet so powerful.\n\nGit stores content similar to how a UNIX filesystem does. All content is stored as tree and blob objects\n- trees would correspond to directories\n\t- Like UNIX filesystem, all a directory is is a file with its contents listed. (The relationship between parent-child is made via reference, not the child actually being inside parent)\n- blobs would correspond to file contents or inodes\n\nA single tree object contains one or more entries, which is a SHA of a sub-tree or blob (ie. directory or file)\n- ex. imagine we had the following project structure:\n```\n.\n|-- README\n`-- lib\n    |-- inc\n    |   `-- tricks.rb\n    `-- mylib.rb\n```\n- if we made a commit, we would create a tree object for each directory, and a blob object for each file. It would be represented like this:\n![](/assets/images/2021-03-06-16-14-27.png)\n\nThe reason we can represent the history of the codebase as a tree is because each snapshot (commit) specifies its parent\n\nThe object database is content-addressable, meaning that we can retrieve the data based on its content, rather than its location.\n- This is a major reason why Git is so performant\n\n* * *\n\n- At the core of Git is a simple key-value store, with the value being the content, and the key being a SHA representing that content.\n- With a fresh git repo, upon adding `foo.txt` to the index, a new object (in `.git/objects`) is created. If we run `git cat-file` on that object, we will see the contents of the file.\n- Upon committing, 2 more objects will be created:\n\t1. a tree object, which makes reference to `foo.txt`, which points to a blob object SHA\n\t\t- if we `git cat-file` that SHA, we will see the contents of `foo.txt`.\n\t\t\t- What this demonstrates is that we have a tree object which references a filename, which is associated with the content in the file. None of these things are glued together, but they merely make references to each other.\n\t2. a commit object, with reference to the tree (the same tree that was just created)\n\t\t- when we run `git commit`, the SHA of that commit object is output into the console\n\t- We have just demonstrated that a commit points to a tree, and a tree points to a blob. First a blob is created (when adding to index), then committing will create a commit and tree object.\n- Now with a clean tree, we create a new file `bar.txt`, and edit the contents of `foo.txt`. We add both those files to the index.\n\t- two more blob objects are created\n- With those 2 files in the staging area, we commit, and 2 more objects are created: a tree object and a commit object.\n\t- Since this is the second commit, it also references its parent commit\n\t- The tree object will reference both `foo.txt` and `bar.txt`\n- Again with a clean tree, we create a new file `baz.txt`, add it and commit it.\n\t- Interestingly, the tree object that just got created will reference all `foo`, `bar` and `baz`, even though only `baz` was changed in this commit.\n\t\t- This is because the commit object maintains the entire state of every file (and every version of the file) by SHA at that point in time.\n\t\t\t- This is precisely why we can cherry pick a commit and put it on disk\n\n### How Git handles renaming files\nThere is no data structure in Git that stores a record that a file was renamed during a commit. Instead, Git attempts to detect renames during the dynamic diff calculation. \n- There are 2 stages to this rename detection\n\t1. exact renames— here, OIDs (of the blobs) are identical, meaning that the contents of the files are the same; just with a different name.\n\t2. edit-rename— here, Git must iterate through each added file and compare it against each deleted file. From there, Git calculates a **similarity score** as a percentage of lines in common (anything >50% counts as a potential edit-rename).\n\nAfter computing the diff, Git inspects it to discover which paths were added/deleted\n- naturally, a file moved from directory `A/` to directory `B/` would appear as a file being deleted in `A/` and a file being added in `B/`. Git attempts to match these deletions/additions to create a set of *inferred* renames.\n- The first","n":0.033}}},{"i":701,"$":{"0":{"v":"Object","n":1},"1":{"v":"\n# Objects\n- Git is built around manipulating the structure of the 4 object types (blob, tree, commit, tag). In a way, it is like a mini-filesystem that sits upon the machine's filesystem\n- Nearly all git commands manipulate either the working tree, the staging area, or the commits.\n- objects are immutable\n- A git repo is a collection of objects, with each having its own hash\n    - A commit is a hashed object\n- Each object consists of 3 things:\n    - *type* - what type of object it is (blob, tree, commit, tag)\n    - *size* - the size of the contents\n    - *content*\n\n## SHA of Objects\n- The SHA of the object is generated by the content of the files, not the names\n\t- Therefore if we have 2 files `foo.txt` and `bar.txt` with identical content, then the SHA will be the same. But if we introduce an extra character in one of the files, then the SHA will be different. \n\t- the object is created as soon as it is known to git (ie. when added to index)\n- inside `.git/objects`, we see many hexadecimally named directories. The names of these are actually the first 2 digits of the SHA\n\t- ex. `.git/objects/77/a54737` is the commit with SHA `77a54737...`\n- The commits are encrypted, but we can see the contents of the object with `git cat-file -p <SHA>`\n\t- use `-t` to see what type the object is \n- The below image is a representation of our workflow. Here we have 3 commits (with the first commit on the left). It's important to note how the same blob is referred by different trees. This is because that file didn't change between commits, therefore the blob is identical \n![](/assets/images/2021-03-06-16-16-37.png)\n![](/assets/images/2021-03-06-16-17-02.png)","n":0.06}}},{"i":702,"$":{"0":{"v":"Tree","n":1},"1":{"v":"\nA tree is Git's representation of snapshots, meaning they store the state of a directory at a given point (without notion of time/author). To tie trees together into a coherent history, Git wraps each one in a commit object and specifies a parent commit. By following the parent of each commit, we can walk through the entire history of the project.\n- Each commit refers to only one tree object.\n\nThe Tree holds pointers to filenames and other trees, effectively allowing us to group files together (which is essentially what a directory is)\n- A tree object contains 1+ entries. Each of which is either a blob or subtree hash.\n\nThe tree object is what associates the filename (or directory name) with its content.\n- We can confirm this by running `git cat-file` on the tree object. It will give us back a list of blobs and their associated filenames\n- We can use a plumbing command `update-index`, which effectively allows us to associate an existing blob with a filename:\n\t- `git update-index --add --cacheinfo 100644 83baae61804e65cc73a7201a7252750c76066a30 test.txt`\n\t\t- add a file to the index (`--add`), get it from the object database (`--cacheinfo`)\n\t\t- upon executing this command, we have `test.txt` added to the staging area.\n\t\t- `100644` represents the file permissions on disk\n\nThe tree is normally made by examining the state of the staging area\n\nTrees also include information such as Unix file permissions for each entry\n\nThe tree itself doesn’t know where it exists within the repository, that is the role of the objects pointing to the tree. The tree referenced by `<ref>^{tree}` is a special tree: the root tree. This designation is based on a special link from your commits.\n\nThe fact that git doesn't really about folder names is precicely the reason why you cannot commit an empty directory. Often on Github you will notice that people put a blank `.keep` file. This naming is meaningless to Git, but it is just a convention, as if to say \"hey, I just put a placeholder here so we can still get the empty directory to be part of our tree.\"","n":0.054}}},{"i":703,"$":{"0":{"v":"Tag","n":1},"1":{"v":"\nSimply a way to mark specific commits as special in some way (ex. tag a specific commit as a specific release, or something along those lines)\n- tags and branches are similar, in that they both point to a specific commit (ie. they are pointers to snapshots). Theoretically, branches could perform the role of tags. We could just keep around a branch called *Release 2.0*. Instead, Git allows us to separate concerns.\n    - A branch is a moveable pointer to a snapshot, while tags point at a single snapshot and never move. Also, tags are actually stored as objects, while branches are not.\n- if we are doing something a little risqué (like undoing a rebase), it's good to make a backup `git tag BACKUP`. Then if we ever need to go back to it, run `git reset --hard BACKUP`\n\n### Annotated vs. Lightweight Tags\nThere are 2 types of tag, lightweight and annotated\n- *lightweight* - a reference to a commit that never moves\n\t- this type of tag does not create a tag object\n- *annotated* - when we create an annotated tag, Git creates a tag object and then writes a reference point to it (rather than directly to the commit).\n\t- we usually want this one.\n\n### Inner\n- The fourth (and less integral) object functions similar to a commit object, in that it contains a tagger, a message, a date, and a pointer.\n\t- The main difference is that a tag normally points to a commit rather than a tree. In this sense it is similar to a branch reference, but it never moves (ie. it always points to the same commit).","n":0.061}}},{"i":704,"$":{"0":{"v":"Commits","n":1},"1":{"v":"\n# Commit Object\n- stores metadata about the commit, like who authored it, when it was made, what the previous commit is etc.\n- a commit points to a single tree, marking it as what the project looked like a certain point in time\n\t- Therefore, the commit object is what gives a project its sense of history in Git\n- While the tree and blob objects are the content of the Git, the commit objects allow us to use the data in a user-friendly way. Technically, Git could be used without commit objects. However, we would have to remember every SHA to recall the snapshots. Also, we wouldn't have important information like who saved the snapshots, when they were saved, and who saved them. These benefits are what commit objects bring to us.\n- When we run `git commit`, a commit object is created, and the parent commit is specified as the commit that the HEAD file points to (recall that a branch is just a pointer to a commit)\n","n":0.078}}},{"i":705,"$":{"0":{"v":"Merge Commit","n":0.707},"1":{"v":"\nWhen git creates a merge commit it will also by default append a list of files that had conflicts to the commit message:\n```\nConflicts:\n    src/foo-service.c\n    src/bar-client.c\n```\nThis is a piece of useful information for us when poring over the history to find out what went wrong\n- Even if your merge conflict was trivial, there is always a non-zero chance of introducing a bug when resolving a conflict, and seeing those lines in the merge commit message could be valuable information. They are basically a hint saying “Still confused? Maybe you should be extra careful when reviewing the changes in these files”.\n","n":0.101}}},{"i":706,"$":{"0":{"v":"Boundary","n":1},"1":{"v":"\n# Boundary Commit\nA boundary commit is the commit that limits a revision range but does not belong to that range. For example the revision range `HEAD~3..HEAD` consists of 3 commits (HEAD~2, HEAD~1, and HEAD), and the commit HEAD~3 serves as a boundary commit for it.\n- This is related to the concept of inclusive/exclusive ranges. In this comparison, the boundary commit would be an exclusive boundary.\n\nMore formally, git processes a revision range by starting at the specified commit and getting at other commits through the parent links. It stops at commits that don't meet the selection criteria (and therefore should be excluded) - those are the boundary commits.\n[Source](https://stackoverflow.com/questions/42437590/what-is-a-git-boundary-commit)","n":0.097}}},{"i":707,"$":{"0":{"v":"Blob","n":1},"1":{"v":"\n### Blob\n- A particular version of one file\n\t- the only part of a file that is relevant to a blob is the file's contents. The object database does not care about the filename.\n\t\t- ex. if we write 2 different files with the same content to the object database, only one SHA will get recorded, since the same content hashed will always produce the same result.\n\t\t\t- the same can be said for a tree object, but not for a commit object, since the author and time of commit makes their content always unique.\n- the blob itself doesn't have a name, but it is referred internally by the hash of its content\n\t- the \"hash of its content\" part of this last point means that as long as 2 different pieces of content are identical (ie. file contents are identical), the resulting hash will also be identical. This also illustrates the fact that blobs are decoupled from a (file)name, as it has no bearing on the produced hash.","n":0.078}}},{"i":708,"$":{"0":{"v":"Head","n":1},"1":{"v":"\n### HEAD\nThe pointer that points to the latest commit on the currently checked out branch.\n- Therefore the HEAD is what lets Git know which commit will be the parent for the next commit.\n\n- spec: While a branch simply points to a commit, a HEAD simply points to a branch (or a commit). If it points to a commit, it is because the commit is not at the tip of its branch, resulting in a detached HEAD.\n\t- Recall that when HEAD does not coincide with the tip of any branch, the result is a detached HEAD. From Git's point of view, this simply means that HEAD doesn't contain a local branch.\n- under normal circumstances, HEAD points to the branch we currently have checked out. However, we can also checkout a commit. Interestingly, we can checkout the same commit that our branch points to (which would be a detached head). In this case, both working trees would be identical. \n- normally, the HEAD file is a symbolic reference to the currently checked out branch. We can see this by logging out the contents of `.git/HEAD`\n\t- here, symbolic reference means that it contains a pointer to another reference.\n","n":0.072}}},{"i":709,"$":{"0":{"v":"Gitignore","n":1},"1":{"v":"\n# Gitignore\n- When git comes across a file in your repo that is untracked, it will report it to you (in `git status`). However, if that file has been added to `.gitignore`, it will suppress it.\n- once a file is known to git (ie. it has been in the index), adding the file to `.gitignore` has no effect (ie. the file will continue to be tracked)\n- if you need to stop tracking a file (for example, we've already added the file to the index, but now want to start ignoring it), we need to run `git rm --cached <file>`\n\t- for folders, `git rm -r --cached <folder>`\n","n":0.097}}},{"i":710,"$":{"0":{"v":"Git Extras","n":0.707},"1":{"v":"\ngit standup\ngit obliterate\ngit missing\ngit reset-file\ngit delta\ngit browse\ngit rename-branch\n\nhttps://github.com/tj/git-extras/blob/master/Commands.md#git-standup\n","n":0.354}}},{"i":711,"$":{"0":{"v":"Formulas","n":1},"1":{"v":"\n<!-- This file should probably be refactored so that we include each recipe along with whichever submodule it belongs to (ex. deleting commits should probably be in git.cli.rebase.cook) -->\n<!-- Is this the most logical place that we'd expect to find this information? -->\n\n### Rewording/deleting commits\n- imagine we want to change a commit message from 2 commits ago, or imagine we want to delete it altogether\n\t1. We can run `git rebase -i HEAD~2`\n\t\t- meaning \"I want to operate on the last 2 commits\"\n\t2. vim will open and will list all of the commits that we have asked to change.\n\t3. change the command from `pick` to `reword` (or `drop` to delete) of the relevant commit, then save and close the file.\n\t4. change the message, save and close.\n\n### Changing contents of commit/Splitting commits\n1. run `git rebase -i HEAD~n`, where `n` is the commit immediately preceding the commit we want to edit.\n3. change the rebase command of the commit we want to edit from `pick` to `edit`\n4. run `git reset --mixed HEAD^`, which leaves the working directory unchanged, but reverses the commit\n\t- `HEAD^` is the parent of our current commit\n6. do our normal operations, adding files, deleting files, or modifying them as we please. `git add` them, `commit` them as per normal workflow.\n7. run `git rebase --continue`\n\n### Reverting commits\n- think of `git revert` as an inverse operation to `git commit`. Effectively, this command creates a new commit that undoes all of the changes introduced by a certain commit.\n- Imagine we made a commit that simply changed which port our app connects to. Later on down the line, imagine that we want to \"undo\" that, and go back to the original port. We could change that port in the code and commit it, or we could simply run `git revert <SHA>` to make a new commit, known as a `revert commit` whose sole purpose is to reverse the changes that that particular commit actually made.\n\n### Squashing commits\n- Imagine we made 3 commits that should logically only be one.\n\t1. run `git rebase -i HEAD~3`\n\t2. leave the commit that everything will get squished into alone, but change the commits that will be thrown away from `pick` to `fixup`\n\t\t- listed by oldest to newest\n\t\t- fixup and squish are similar, but fixup will discard the commit messages of the discarded commits.\n\t\t- if we want to retain the commit messages, then use `squash` instead of `fixup`\n- If we compare the git logs before and after, we will notice that the SHA of the commit with the same message will be different, showing that we are in fact rewriting history\n\n### Splitting commits\n- Imagine we have made a commit that realistically should actually be split into 2 commits (\"add navbar and fix bug\")\n\t1. run `git rebase -i HEAD~2`\n\t2. change the commit we want to split from rebase operation `pick` to `edit`\n\t3. after saving the file, git will put us onto a special rebasing branch\n\t4. unstage all files that were added during that commit by running `git reset HEAD^`\n\t5. perform normal workflow, by adding commit1 changes and committing, then adding commit2 changes and committing\n\t6. run `git rebase --continue`\n\n## Undoing Work\n- this will discard everything permanently\n\n### Restoring to the state of the last commit\n- if just a single file, we can simply run `git checkout HEAD <file>` to blow away all changes, and restore the file to what it was in the most recent commit.\n- if we want to blow away all changes and get the exact state of the last commit, we can run `git reset --hard HEAD`\n\t- This tells Git to replace the files in your working copy with the \"HEAD\" revision\n\t- we can replace HEAD with any SHA to go back to that previous version.\n\t- note: this will not produce any new commits (like revert), nor will it delete any old ones. Instead, it works by resetting your current HEAD branch to an older revision (also called \"rolling back\" to that older revision)\n\t- note2: since this doesn't rewrite history, the commits we \"erased\" are still available","n":0.039}}},{"i":712,"$":{"0":{"v":"Flow","n":1},"1":{"v":"\nWith an interactive rebase workflow, the idea is to create small commits as you go along, atomic enough that you can describe it in one line. For a given PR you might end up with 30 commits. If along the way you make mistakes, for instance you forget to rename something during an earlier commit when you renamed a variable in multiple places, you should just make a `fix` commit. Later on when you are ready to put your code up for PR, you can do an interactive rebase and move commits around, such as squashing the missed rename into the first commit that was supposed to do that.","n":0.096}}},{"i":713,"$":{"0":{"v":"Extras","n":1},"1":{"v":"\ngit standup\ngit obliterate\ngit missing\ngit reset-file\ngit delta\ngit browse\ngit rename-branch","n":0.354}}},{"i":714,"$":{"0":{"v":"Deploy","n":1},"1":{"v":"\n## Centralized vs Distributed Git\n- we can run Git in a centralized system, where we have a git repo on a central server. All contributors must push and pull to the same repo for progress on the project to be made.\n\t- A concern with this is that for anyone to be able to contribute, they must have access to the entire project. Therefore you cannot maintain control with a centralized workflow.\n\t\t- This may not be a concern, if there are 2 or 3 people working on a project.\n- With an integrator workflow, everyone has their own private repo and public repo. When a contributor wants to add their changes, they push to their own public repo. From there, we can pull their changes into our local repo to ensure everything works as expected. If everything is good, we merge them into our local branch, then push that branch to the main repo. From there, everyone can pull those changes.\n\t- with this workflow, everyone pulls from a single official repo, but pushes to their own public repo.\n\nCentralized:\n![](/assets/images/2021-03-11-15-39-35.png)\nDistributed:\n![](/assets/images/2021-03-11-15-39-48.png)","n":0.075}}},{"i":715,"$":{"0":{"v":"Conventional Commits","n":0.707},"1":{"v":"\n## About\nConventional Commits are designed to dovetail with SemVer\n- the CC `fix` corresponds to SemVer `PATCH`\n- the CC `feat` corresponds to SemVer `MINOR`\n- a `!` after the CC type/scope corresponds to SemVer `MAJOR` (ie. breaking change)\n\n## Types\n- fix\n- feat\n- build - changes that affect build components like build tool, ci pipeline, dependencies, project version etc.\n- ops - changes that affect operational components like infrastructure, deployment, backup, recovery\n- chore\n- ci\n- docs\n- style - changes to the code to do with formatting (white-space, semi-colons etc). Consistent with changes that a linter makes\n- refactor - a rewrite/restructure to the code that does not change any behaviour\n\t- perf - a subtype of `refactor` that improves performance.\n- revert\n- test\n\n[ref](https://github.com/commitizen/conventional-commit-types/blob/master/index.json)\n[explanation](https://news.ycombinator.com/item?id=19706037)\n\n## Structure:\n```\n<type>[optional scope]: <description>\n\n[optional body]\n\n[optional footer(s)]\n```\n\n### Subject (required)\nThe subject contains a succinct description of the change.\n- Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\n- Don't capitalize the first letter\n- No dot (.) at the end\n\n### Body (optional)\nThe body should include the motivation for the change and contrast this with previous behavior.\n- Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\n- This is the place to mention issue identifiers and their relations\n\n### Footer (optional)\nThe footer should contain any information about Breaking Changes and is also the place to reference Issues that this commit refers to.\n- optionally reference an issue by its id.\n- Breaking Changes should start with the word `BREAKING CHANGES:` followed by space or two newlines. The rest of the commit message is then used for this.\n\n## Example:\n```\nfeat(stripe)!: grant the ability to users to determine a primary payment method\n\nThis feature builds on database work that was previously done, to allow users to select their primary card from the payment-methods page.\nreference: JIRA-1337\n\nBREAKING CHANGES:\n```\n\ncommmit\nfeat(stripe): create `book_order` when user signals intent to buy.\n\nThis commit introduced a bit of a functionality change to how payments are processed through our backend and Stripe. Previously, rows in the invoices, invoice_items, and book_orders tables were only created after Stripe had verified our purchase (ie. by calling our webhook). Now, a row in the book_orders table will be inserted at the point that the user signals their intent to buy (ie. by entering their address info and hitting the 'Continue' button). At this point, our Express server makes an API call to get taxes (future implementation, though a placeholder solution included in this commit), and it is included on the Stripe PaymentIntent object. We retrieve this metadata in our webhook to insert it into the invoice table.\n","n":0.05}}},{"i":716,"$":{"0":{"v":"Semantic Release","n":0.707},"1":{"v":"\nGenerate release notes using your conventional commits:\nhttps://github.com/semantic-release/semantic-release\n","n":0.378}}},{"i":717,"$":{"0":{"v":"Commit","n":1},"1":{"v":"\n# Overview\nA commit is the state of a folder structure at a given point in time. It is a snapshot— not a diff\n- There is a misconception that Git stores diffs, since we see diffs when using github, or using common commands like `git diff`. However, these diffs are dynamically generated, given 2 different commits. In fact, we can see a diff of any 2 commits. If Git stored diffs, we wouldn't be able to see a diff between 2 commits that are far apart from each other.\n    - Under the hood, Git is comparing the root trees of each commit, and taking note of the paths that have different Object Ids\n        - ex. if we have 2 commits and the only different is an edit made in the README, Git is able to get that diff by introspecting on the trees, and noticing that the objects representing that README file have different OIDs. When the OIDs do not match, it means they have different contents\n\nYou tell Git you want to save a snapshot of your project with `git commit` and it basically records a manifest of what all of the files in your project look like at that point\n    - Most of the git commands interact with those manifests in some way\nWe can think about Git as a tool for storing, comparing and merging snapshots of our project\n- We can get a snapshot of an individual file with `git checkout <Commit SHA> <filename>`\n- A snapshot is basically a commit\n- Snapshot is to a repository as screenshot is to a video, since it represents one moment of time in that video\n\n## Ahead/Behind\n- If your current branch is 3 commits *ahead* of master, it means the current branch has 3 commits that don't exist on master\n- If the current branch is 3 commits *behind* master, it means there are 3 commits on master that don't exit on the current branch\n- The ahead number tells you roughly how much impact the current branch will have on the base branch should it be merged.\n- The behind number tells you how much work has happened on the base branch since this branch was started.\n    - If the number is high, it's an indication that there will not be a clean merge. This would be a good time to merge master (or other base branch) into the current branch, which would bring the \"behind\" number to 0\n- In the follow diagram:\n    - A is 2 commits behind and 0 commits ahead of B\n    - B is 0 commits behind and 2 commits ahead of A\n    - C is 1 commit behind and 2 commits ahead of A\n    - C is 3 commits behind and 2 commits ahead of B\n\n![](/assets/images/2021-03-06-16-45-56.png)\n\n## Detached State\n- Occurs when we check out a commit or a remote branch, as opposed to a local branch.\n    - Put another way, any ref that does not originate from your line of commits (and would thus be unable to trace any sort of history with the code you'd been working on)\n- If we were to develop in detached mode then try to merge it into master, git would complain to us, because by definition, detached mode means there is no path to get back into master. (ie. there is no way to reference that feature \"branch\")\n\n## Fast-Forward commit\n- Occurs when we tell git to merge our feature branch into master, but git realizes that there are no 2 branches to merge, but instead the master just needs to be brought up to speed to the current feature branch. This is known as *fast-forwarding*\n- effectively what is happening is Git engine is moving the current branch tip up to the target branch tip.\n- If the tip of master hasn't moved since we branched off, no merge commit will be made. The HEAD will simply move to the most recent commit of the feature branch.\n\t- In a ff merge scenario, since the master has not changed since we branched, a \"merging/melding\" of branches is not relevant here.\n\n\n### Forcing a FF Merge\nThis ff may not be desirable. Imagine we want to merge in a branch called *oauth*. This is a pretty significant feature, and we'd like to keep the branch in our history. This is a scenario where we can merge with `--no-ff`.\n\n![240d716cd6708b64cbefb47f30773132.png](:/5edfa6a1c2a4442faff1e60705f87d32)\n\n- ***non fast-forwarding*** (a.k.a *3 way merge*) ex. - we create a feature branch from master. We do some work on feature, then decide to merge into master. Since we had branched off from master, other feature branches have been merged into master. This means the git engine has to figure out how to *merge* the master branch (which is now different from how our feature remembers it) and our feature branch.\n    - If this were a *fast-forward merge*, then there would have been no merges into master during the time that our feature branch existed.\n","n":0.035}}},{"i":718,"$":{"0":{"v":"CLI","n":1}}},{"i":719,"$":{"0":{"v":"Tag","n":1}}},{"i":720,"$":{"0":{"v":"Tag","n":1},"1":{"v":"\n###### List tags\n`git tag`\n\n###### List tags given a pattern\n`git tag -l \"v1.8.5*\"`\n\n###### Create annotated tag\n`git tag -a v1.4 -m \"my version 1.4\"`\n\n###### Create lightweight tag\n`git tag v1.4-lw`\n\n###### See tag data\n`git show v1.4`","n":0.177}}},{"i":721,"$":{"0":{"v":"Switch","n":1},"1":{"v":"\nDesigned to replace the branch-switching functionality of `git checkout`","n":0.333}}},{"i":722,"$":{"0":{"v":"Subtree","n":1},"1":{"v":"\n## Subtree Push\n- `git subtree push` allows us to cause a sub-directory of the current branch to be the root-level directory of another branch\n- ex. `git subtree push --prefix docs origin gh-pages`\n\t- this pushes just the `docs` directory to the `gh-pages` branch.\n\n## UE Resources\nhttps://www.atlassian.com/git/tutorials/git-subtree","n":0.151}}},{"i":723,"$":{"0":{"v":"Stash","n":1},"1":{"v":"\n# Stash\n- A stash is simply a list of patches, that you can apply wherever you want.\n- Take all modified tracked files (that are unstaged) and staged changes, and save them onto a stack of unfinished changes\n\t- When you run git stash (alias of `git stash save`), git makes two commits that are not on any branch. One commit holds the state of the index, the second commit holds state of the work tree.\n- `git stash` takes uncommitted changes, stores them internally, then runs `git reset --hard` to give us a clean working directory.\n\t- This means that stashes can be applied to any branch, useful if we ever discover that we were developing on the wrong branch.\n\n### Apply vs Pop\n- pop will delete the stash after it is applied, while apply keeps it around for future use\n\t- this is why the below trick to revert the stash does not work if `pop` is used\n","n":0.081}}},{"i":724,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\nDiff between stash and HEAD (ie. view the changes tied to the stash)\n- `git stash show -p stash@{1}`\n\nRetain staged work of stashes\n- run `git stash pop --index` so that staged files return as staged when you pop the stash\n\nReverting `stash apply`\n- `git reset --hard`\n\t- assuming you had everything in a clean state when you started\n\nPopping stash onto a new branch\n- `git stash branch <branch-name> stash@{0}`\n\nAdding a message\n- `git stash save \"<message>\"`\n\nIncluding untracked files with stash\n- `git stash --include-untacked`\n\nDeleting a stash\n- `git stash drop stash@{1}`","n":0.109}}},{"i":725,"$":{"0":{"v":"Show","n":1}}},{"i":726,"$":{"0":{"v":"Show","n":1},"1":{"v":"\n##### Show differences introduced by a commit\n`git show <SHA>`\n\n##### List the files changed in a commit\n`git show <SHA> --name-only`","n":0.229}}},{"i":727,"$":{"0":{"v":"Rm","n":1},"1":{"v":"\n# rm\n- remove a file from the index as well as working directory so they will stop being tracked.\n- with `--cached`, only remove it from index.","n":0.196}}},{"i":728,"$":{"0":{"v":"Restore","n":1},"1":{"v":"\nRefreshes the current working version to be the HEAD version. No modifications are made to the staging area.\n- We can also pass `-s <branch-name>` to restore in the version on the specified branch\n\nDesigned to replace version-checking out functionality of the `checkout` command\n","n":0.154}}},{"i":729,"$":{"0":{"v":"Reset","n":1},"1":{"v":"\n# Reset\n- `reset` is the opposite of `add`\n- In general, git reset is used to move branch tips around (likely to another commit)\n- If we pass a filename to `git reset`, then the staging area will be updated to match the given commit instead of the working directory (the branch pointer does not move).\n\t- ex. if we have 3 files staged, we can remove one of them with `git reset HEAD index.js`, making `index.js` match the version found in HEAD. The working directory and current branch are left alone. The result is a staging area that matches the most recent commit and a working directory that contains the modified `index.js` file.\n\t\t- Put another way, we are unstaging the file.\n![8f6b8c503feeeceab0f2175850b7acbd.png](:/78999c9edf5c4f129b389d40acc423cc)\n\n## Soft\n- moves the HEAD to the provided `<SHA>`, while keeping working tree and staging area intact.\n- *soft* means the commit is canceled and moved before the `HEAD`\n- `git reset --soft HEAD^` - undo last commit\n\n## Mixed\n- moves the HEAD to the provided `<SHA>`, while keeping working tree intact.\n\t- Move the HEAD backward `<n>` commits, but don’t change the working directory.\n- reset the staging area\n- `git reset <file>` is the opposite of `git add <file>`\n- ex. if we are on c3 and do `git reset c1`, we will go back to c1, and the working directory and index will remain unchanged\n    - This is the default type of reset\n\n## Hard\n- moves the HEAD to the provided `<SHA>`, keeping neither the working tree nor staging area intact.\n\t- Move the HEAD backward `<n>` commits, and change the working directory to match\n- the only version of `reset` that actually results in a changed working directory file.\n- ex. if we are on c3 and do `git reset --HARD c1`, we will go back to c1 (i.e. our head will point to c1) and c2 and c3 will be \"destroyed\", and working directory wiped.","n":0.057}}},{"i":730,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Reset all files in a directory\n`git restore --source=HEAD --staged --worktree -- modules/client-reader/colibrio`\n- restore all files in the `modules/client-reader/colibrio` to their HEAD version (in other words, unstage them)","n":0.189}}},{"i":731,"$":{"0":{"v":"Remote","n":1},"1":{"v":"\n# Remote\n- a pointer to a branch on a copy of the same repository.\n\t- remote simply means a copy of the repo on someone else's machine.\n- *origin/master* means \"the master branch of the origin remote\"\n- When you clone a repository, Git automatically adds an origin remote pointing to the original repository, under the assumption that you’ll probably want to interact with it down the road\n\t- run `git remote -v` to see what origin is\n- we can run `git branch -r` to see the remote branches available to us. If there are none, then we can run `git fetch <remote-name>` to copy them over.\n- Checking out a remote branch takes our HEAD off the tip of a local branch, resulting in a detached HEAD:\n![efafe6e1a14641006f80ee5a895572b2.png](:/287b1c5127484a3b99356dfdfa60acbc)\n\n## Upstream\n- Imagine we forked a repo remotely, then forked it locally. `Upstream` would be the original repo that we forked, and `origin` would be the remote repo of our forked version\n- upstream means \"towards the trunk\" (ie. towards the single source of truth)\n- By default, `origin/master` is set as the upstream branch of `master`, so `git pull/push` will default there.\n- `git branch -vv` <-- show upstream branch of local version.\n\n## Tracking Branch\n- The local branch that is connected to a remote branch.\n- ***ex.*** `master` **==>** `origin/master`\n- checking out a remote branch from the local repo will create that branch.\n- `git branch --remotes`\n- `git remote -v` - list all remote repos you've connected to\n\n#### Get URL of remote\n`git remote get-url origin`","n":0.064}}},{"i":732,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Set your local branch to track a remote branch\n`git branch -u origin/dev`\n\n#### push to new upstream\ntip: use with `--force` to overwrite everything remotely\n`git push -u origin dev`\n\n#### Get origin URL\n`git config --get remote.origin.url`\n\n#### Add remote URL\n`git remote add origin git@github.com:USERNAME/REPOSITORY.git`\n\n#### Remove remote\n`git remote rm origin`","n":0.147}}},{"i":733,"$":{"0":{"v":"Reflog","n":1},"1":{"v":"\n### Reflog\nThe Reflog gives a chronological account of how HEAD has moved. Each time it moves, there is a new entry in the Reflog.\n\nThe reflog is a list of chronological hashes, which represent where you have been during commits, all without regard to branches.\n- Since branches don't matter, we are able to recover dangling commits.\n- Each time a branch is updated to point to a new reference (ie. HEAD changes), an entry is written in the reflog to say where you were.\n- Since the branch is updated whenever you commit, the git reflog has a nice effect of storing your local developer's history.\n\nIt serves as Git's safety net, recording every change made in the repo (regardless of whether it was committed or not)\n- The listed commit hash represents the HEAD after that action.\n\nWhile `git log` shows a history of the commits, we can think of `git reflog` as showing us a history of everything (what branch we checked out, what commands we ran etc.)\n\nBecuase it keeps a full history, even if we were to `git reset`, we can still access the commit SHA\n- ex. we made some commits, then reset. If we decide that we want to *undo* the reset, all we need to do is checkout the commit we want with `git checkout HEAD@{1}`\n```\nad8621a HEAD@{0}: reset: moving to HEAD~3\n298eb9f HEAD@{1}: commit: Some other commit message\nbbe9012 HEAD@{2}: commit: Continue the feature\n9cb79fa HEAD@{3}: commit: Start a new feature\n```\n- This puts us in detached head state. All we need to do is create a new branch, and continue working on our feature.\n\n#### Data Recovery\n- Most of the time, the reflog is our friend in the circumstance where we want to recover data that has been \"lost\"\n\t- we can either run `git reflog`, or use `git log -g` which gives us normal log output for the reflog\n- Imagine we hard reset a number of commits, effectively \"erasing\" them from the git log. All we need to do is find that commit with `git log -g`, create a new branch with its SHA (`git branch recover-branch <SHA>`)\n- If the data we are looking for is not in the reflog, we can try using `git fsck --full`, which will list all objects that aren't pointed to by another object.\n","n":0.052}}},{"i":734,"$":{"0":{"v":"Rebase","n":1},"1":{"v":"\n# Overview\nFrom a feature branch (FB), if we rebase master, conceptually we are rewinding back to the point where the FB split from master and updating that point with master and then “replaying” the FB commits on top of that\n- Therefore, rebase is an alternative way to get commits from a feature branch onto a master branch\n\t- with `git merge`, we take all of the commits on the feature branch and try and jam them together with the commits of the master branch, in order to make a new commit (the merge commit).\n\t- with `git rebase`, we take all of the commits from the current branch and move them on top of another. This involves rebasing first, then doing a regular merge (which will be fast-forward, since the base commit will now be a direct ancestor)\n\t\t- When we run `git rebase master` from our feature branch, we say \"hey master, I want to take all the work I've done so far and make it look like it was built directly on top of all of the work that *you* have\"\n\nIf you're `rebase`ing, you will always have to `--force` when you push, because rebase rewrites history— That's simply how it works. Rewritten history isn't going to fast-forward on top of that same history by definition.\n- only the commits that are being replayed are rewritten (and thus, new SHAs). If we are on FB and rebasing on top of master, the commits from master will remain unchanged (ie. history isn't being written here). However, the commits that only existed on FB will be rewritten, and thus will have new SHAs.\n- While collaborating with others using the \"rebase workflow\", you should always `git pull --rebase` to avoid the circumstance where a merge commit is made. If one person rebases, and then pulls that code with merge commits, it will be a difficult rebase (spec:) due to the rewritten history.\n\nAs the name suggests, rebase exists to change the base of a branch (ie. the origin commit). We do this by replaying a series of commits on top of a new base.\n- This is mostly needed when a series of local commits is deemed to start from an obsolete base (put another way, our local master is very out of sync with origin/master)\n\n- behind the scenes, git is duplicating the commits of the feature branch, putting them on top of the master branch, and then blowing away those original feature branch's commits (hence why they are greyed out in the following image).\n\t- therefore, in a sense it is rewriting history, as evidenced by the fact that the duplicated commits have a different SHA than the originals\n\n- Because rebase rewrites history, it's important that we pull all remote changes to our local master branch before rebasing, so that we are reanchoring our feature branch's commits to the current version of the code.\n- ex. we are on branch `about`, which has diverged from `master`. We want to incorporate changes from master into `about`. From `about`, we run `git rebase master`:\n\n## Process\nGit always squashes a newer commit into an older commit or “upward” as viewed on the interactive rebase todo list, that is into a commit on a previous line.\n- This means if commit1 is a `WIP` commit, and commit2 is the one we want to keep (along with changes from commit1), then we must actually `squash` commit2 into commit1. Doing so will allow us modify the commit message (now a combination of the messages from commit1 and commit2) before rewriting the history.\n\n\n### Behind the Scenes\n1. Git will checkout the upstream branch and copy all the changes you've done since you last merged, placing them on the tip of the upstream branch.\n\t- ex. in the above image, to an outside observer it would seem that you had checked out the upstream branch from ***a***b and then done your changes.\n\t- note: here upstream most likely is origin/master or simply master, but it could be any branch we are \"merging\" into.\n2. Git produces a series of patch files of your work and starts applying them to the upstream branch.\n\t- consider that these commits are actual copies with different commit SHAs\n\n### Process\n1. when finished with feature branch, pull all remote changes onto master\n\t- if local master === origin/master, step 2 can be skipped, since it would have no effect anyway\n2. from feature branch, run `git rebase master`, which will cause our feature branch commits to be anchored against the updated master branch\n\t- consider that when we checkout a new branch, we have a common base with the branch which we checked out from. Rebasing master here ensures that the remote changes that happened and got merged into master (remotely) are included as part of that anchor.\n3. from master branch, run `git rebase feature-branch`, copying and placing the commits of the feature branch onto the main branch.\n\n#### Conflicts\n- say we are rebasing 8 commits onto the new branch — each one could cause a conflict, and we can resolve the conflicts introduced by each commit one by one.\n\t- fix the file, run `git add`, then run `git rebase --continue` (which moves us on to the next patch, until all are completed)\n\n### When to use rebase\n1. Any time we are working on a long term branch that needs to stay somewhat up to date with master. It is better to keep it as up to date as possible, rather than staying diverged for a long time.\n\t- ex. Imagine working on an experimental branch and getting blocked at some point. This is a scenario that would cause your branch to diverge from master more and more over time. When it does come time to pick up work again, it would be a good time to rebase to the master branch. The result is that it would look just like you started from there.\n2. Consider that in a perfect world, my coworker and I would have a linear commit history (even though we are developing asynchronously, it makes more sense looking back if we have a straight line of commits). In this ideal world, I would be developing my work off the base of my coworker's work, and vice versa.\n3. Imagine we have a *quick-fix* branch that we don't want muddying up the history. If master has not been touched since we branched, the ff merge is automatic. However, if master has indeed changed, then we need a way to tweak *quick-fix* so it becomes a direct descendent of *master* again.\n\t- In this scenario, we want our local master to have the same tip as origin/master. This would allow us to do a ff merge, thus avoiding muddying up history.\n\n### Drawbacks to rebase\n- doesn't play too well with open source projects, since it becomes hard to trace changes introduced to a codebase.\n- doesn't work well when working on a shared branch, since commits are rewritten.\n- only rebase when working on a local branch prior to pushing, or on remote repos where you are the only contributor (ex. for backup purposes).\n\t- In the second scenario, we'll need to force push (since we replaced its commit history with a fresh one).\n\t- Issues arise when other people pull in objects that were orphaned by the rebase process.\n\n#### Shared branches\nRebase is not a great candidate for shared branches. Because `git push --force` is a fact of life to the \"rebase-way\" of Git workflow, we would have to be careful to check if someone else has pushed to the remote branch first. This is why we should use `--force-with-lease`, so that we cannot overwrite commits that have been pushed already to that remote branch. If there have been, we will get errors, and we can `git pull --rebase` to incorporate those changes, before force pushing again.\n\n## Long-lived Feature Branches (LLFB)\nas the LLFB gets periodically rebased off master its commits get rewritten, so we end up with different SHAs for \"the same\" content of the commit.\n\n# E Resources\n[VS Code tip: Interactive rebase editor from the GitLens extension](https://m.youtube.com/watch?v=P5p71fguFNI)","n":0.027}}},{"i":735,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### See the patch of the current commit\n`git rebase --show-current-patch`\n\n##### Status of rebase\nNumber of commits in this rebase\n`cat .git/rebase-apply/last`\n\nOut of the number of commits in this rebase, which are we on?\n`cat .git/rebase-apply/next`\n\nWhich commit is currently being applied?\n`cat .git/rebase-apply/original-commit`\n\n#### Cherry-Pit: Remove a specific commit\n`git rebase -p --onto SHA^ SHA`\n\n##### alias cherry-pit\nusage: `git cherry-pit <SHA-to-remove>`\n\n#### Include some changes as part of a previous commit\nImagine we realized that we should have included a change (perhaps deleting some old comments) as part of a previous \"cleaning commit\"\nOf course, we will have to change history to do so, with `rebase`:\n\n##### Stash method\nStrategy: stash the desired changes, reset back to a commit, pop those changes and amend commit, then complete rebase\n\n1. Use `git stash` to store the changes you want to add.\n2. Use `git rebase -i HEAD~10` (or however many commits back you want to see).\n3. Mark the commit in question (a0865...) for edit by changing the word pick at the start of the line into edit. Don't delete the other lines as that would delete the commits.\n4. Save the rebase file, and git will drop back to the shell and wait for you to fix that commit.\n5. Pop the stash by using `git stash pop`\n6. Add your file with `git add <file>`.\n7. Amend the commit with `git commit --amend --no-edit`.\n8. Do a `git rebase --continue` which will rewrite the rest of your commits against the new one.\n9. Repeat from step 2 onwards if you have marked more than one commit for edit.\n\n##### Autosquash method\n1. Stage the changes that we want to include in a previous commit\n2. Create a new commit with an identical message of the original commit with the following command\n`git commit -c <ORIGINAL-COMMIT-SHA>`\n\t- pro-tip: use `HEAD~2` syntax for relative\n3. With the editor open, prepend the name of the new commit with `fixup! ` (or `squash! ` if we want to edit the commit message)\n`fixup! refactor: clean up payment methods`\n4. run `git rebase -i --autosquash <SHA>`, where `<SHA>` is the commit immediately before the original commit we want to amend.\n5. Git will detect the `fixup! ` directive, and will look for the commit with the same message. With this information, Git will perform the squash directive (combining the commits), and then continue to rebase the rest of the commits\n\n##### fixup (manual)\n1. make the commit `git commit -m fixup`\n2. run `git rebase -i`\n3. move the commit, and squash it into an earlier one\n\n##### git fixup alias\n```\nfixup = \"!fn() { git commit --fixup ${1} && GIT_EDITOR=true git rebase --autosquash -i ${1}^; }; fn\"\n\n// then add staged changes to commit 3 before HEAD:\ngit fixup HEAD~3\n\n```","n":0.048}}},{"i":736,"$":{"0":{"v":"Push","n":1},"1":{"v":"\n- `git push origin master` moves the HEAD of the central repo:\n\t- This has the exact same result as if we went on the central repo and ran a fetch and fast-forward merge.\n![cbbedd3aff8b9bae62000af2a996468e.png](:/1f2474a7a067490f91335052cb37b7cf)\n\n## Avoiding race conditions with force push\nWhat if we want to force push, but don't want to run into problems if someone else has pushed to that branch in the meantime?\nTo get a warning when trying to force push to a branch that has been committed to in the meantime, run:\n`git push --force-with-lease`\n\n## Force push\n- Force pushing to feature branches is a fact of life. Force pushing to a `master` branch should be considered with extreme care.\n\t- Force pushing to feature branches allows us to have a clean history of commits on that feature branch. If we embrace `rebase`-`merge` instead of just `merge`, then we will encounter lots of scenarios where we must force push to that feature branch.\n\t\t- Note: this strategy should NOT be taken if multiple developers are working on the same branch. Rebase is not a good candidate for shared branches.","n":0.075}}},{"i":737,"$":{"0":{"v":"Prune","n":1},"1":{"v":"\n# Prune\n- delete lost or \"orphaned\" objects (ie. those that are unaccessable by any ref). Any commit that cannot be accessed through a branch or tag is unreachable.\n- prune is a garbage collection command, and considered a child command of `git gc` (in other words, `gc` runs `prune`)\n","n":0.144}}},{"i":738,"$":{"0":{"v":"Merge","n":1},"1":{"v":"\n# Merge\n- to undo a merge, simply run `git reset --hard HEAD`\n\t- by default, only the index will be reset, meaning the partially merged files will remain in the working directory.\n\t- If we've merged and committed already but want to revert the merge, we can discard the commit with `git reset --hard ORIG_HEAD`\n- From a DX standpoint, the big difference between rebase and merge is that `merge` will preserve the \"railroad\" switch that was our branch when we look at the history, whereas `rebase` will show it linear.\n\t- \"Should this branch remain visible in the graph?\"\n\n[git merge with multiple parents](https://softwareengineering.stackexchange.com/questions/314215/can-a-git-commit-have-more-than-2-parents)\n\n## When to use Merge\n1. If master remained untouched while we were working on our feature branch (in which case a fast-forward merge would be automatic).\n2. When we have work that is related to an agile ticket or bugfix (which would allow us to look back in history and clearly see it)\n\n## Terminology\nThe `ours` `theirs` terminology is from the perspective of the \"already existing branch\". Put another way, if we are on main and merging in a feature branch, main is `ours` and feature is `theirs`. Also in the same way (and using a different paradigm), `current change` would refer to main, and `incoming change` to feature.\n\nThis is also the same as with rebase. if we are rebasing main onto feature, then `ours` will refer to main (`git rebase main` from feature branch)\n","n":0.066}}},{"i":739,"$":{"0":{"v":"Merge Strategies","n":0.707},"1":{"v":"\nGit has several different methods to find a base commit, these methods are called \"merge strategies\".\n- Once Git finds a common base commit it will create a new \"merge commit\" that combines the changes of the specified merge commits.\n- Technically, a merge commit is a regular commit which just happens to have two parent commits.\n\nwhen merging (and pulling), we can pass the `-s` flag to specify which merge strategy we want to use.\n- When not specified, Git will select the most appropriate based on the provided branches.\nmerges can be either explicit or implicit\n- *explicit* means a new merge commit is created.\n- *implicit* means no new merge commit is made, and any evidence of a branch having existed is erased from history\n\t- This is accomplished either by fast-forward merges or rebases.\n\n## Strategy types\nSpecified with `-s` flag\n- ex. `git merge -s ours feature-branch`\n\n### Recursive\nOperates on 2 heads. Therefore, it is the default when pulling/merging one branch.\n\nHas additional sub-operation options, which allow us to auto-resolve conflicts, by accepting all changes in one HEAD version\n- *ours* - accept all \"ours\" versions of the HEADS\n- *theirs* - accept all \"theirs\" versions of the HEADS\n- *patience* - This option spends extra time to avoid mis-merges on unimportant matching lines. This options is best used when branches to be merged have extremely diverged.\n\n### Resolve\noperates on 2 heads using 3-way merge algorithm.\n\n### Octopus\ndefault when there are more than 2 heads.\n\n### Ours\nThe output merge result is always that of the current branch HEAD. The \"ours\" term implies the preference effectively ignoring all changes from all other branches. It is intended to be used to combine history of similar feature branches.\n- note: this should not be confused with the `ours` option of the Recursive Merge Strategy\n","n":0.059}}},{"i":740,"$":{"0":{"v":"Log","n":1},"1":{"v":"\n### Log\n- we can pass a filename to `git log` to only show the history of a given file\n- also, we can pass a commit SHA to see the revision history up until that point\n\t- This fact shows how by default, `git log` will run with `HEAD`, showing the revision history from the current branch's tip\n- We can run `git log -n 4` to show only the last 4 commits from HEAD\n\t- equivalent to `git log HEAD~4..HEAD`\n- running `git log master..origin/master` checks what exists on origin that doesn't on our local master. The opposite, `git log origin/master..master` checks what exists on our local master that doesn't on origin. If the output is empty, it means that our branches have not diverged.\n","n":0.091}}},{"i":741,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### See commits that exist on a given branch\n`git log feature-branch`\n\n##### See commits that exist on a branch, while excluding ones that live on another\n`git log feature-branch ^main`\n- equivalent to `git log main..feature-branch`\n\n##### See the history of a file across this rename\n`git log --follow -- /path/to/file`\n\n##### See only commits where files were deleted\n`git log --diff-filter=D --summary`","n":0.134}}},{"i":742,"$":{"0":{"v":"Fetch","n":1},"1":{"v":"\n# Fetch\n- running fetch will pull all refs and objects that we don't already have from the remote repo, and will put them into the object database. \n- fetch is opposite of push, in the sense that fetch will import branches, while push will export them\n\t- when we run `git push origin master`, we are exporting our master branch to origin. If we were to run `git branch` on origin, we would see our local branch that we just pushed, listed as a local branch.\n\t\t- ex. Imagine we had a repo called `foo`, then cloned in locally into a different directory on our machine and called it `bar`. On `bar`, origin would default to `foo` (since it was copied from `foo`). `foo` would not have an origin by default, since it did not come into existence by being copied. We could manually add `bar` as a remote. Now, if we made a branch on `foo` and pushed it, that branch would be available locally on `bar`.\n","n":0.078}}},{"i":743,"$":{"0":{"v":"Diff","n":1},"1":{"v":"\n### Patch file\nThe default output of `git diff` is a valid patch file, meaning we can pipe its output into a file, give that file to someone else, and they can apply it with the `patch` command.\n- ex. `git diff master..experiment > experiment.patch`. The recipient can then run `patch -p1 < ~/experiment.patch`\n\nEach patch represents a full commit, complete with metadata like author and date.\n- ex. If we have made 2 commits since master, then running `git format-patch master`\n\n### range-diff\n`git range-diff` allows us to inspect how two commit ranges are different\n\n## Under the hood\nto compare 2 commits, Git starts by looking at the root trees of each commit (which are almost always different). Then, Git initiates a depth-first-search on the subtrees by following pairs when paths for the current tree have different OIDs.\n\n* * *\n\n- `git diff` - show changes between index and working tree\n- `git diff --staged` - show changes between index and HEAD (ie. last commit)\n- `git diff master..feature-branch` - show changes between master and feature-branch \n\t- `git diff f733ed..` - show changes between a commit and HEAD\n- `git diff -- package.json` - show only changes in a specific file.\n\n","n":0.072}}},{"i":744,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Match a pattern only in UNSTAGED hunks\n`git diff -U1 | grepdiff 'TODO' --output-matching=hunk`\n\n#### Match a pattern only in STAGED hunks\n`git diff -U1 --staged | grepdiff 'TODO' --output-matching=hunk`\n\n#### List the files that match a pattern in a given git range\nHere, look for the pattern `TODO` in all commits from `main..HEAD`\n`git grep TODO -- $(git diff --name-only main..HEAD)`","n":0.132}}},{"i":745,"$":{"0":{"v":"Commit","n":1},"1":{"v":"\n# Amend\n\n* * *\n\n## Operators\n\n### ~ Operator\n- allows us to reach parent commits\n\t- ex. `git show HEAD~2` shows the grandparents of HEAD\n- if the commit has more than 1 parent (ie. merge commit), then the first parent of the commit will be used by default.\n\t- Need to use `^` to specify a different commit\n\n### ^ Operator\n- allows us to specify which parent we want to refer to\n\t- spec: therefore, if there is only one parent, then `HEAD~1` refers to the same commit as `HEAD^1`\n![](/assets/images/2021-03-07-22-45-05.png)","n":0.109}}},{"i":746,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Find out which branch a commit took place on\nIf within last 90 days (ie. before gc), check [[reflog|git.reflog]]\n`git reflog show --all | grep d7f32e`\n\n`git name-rev --name-only d7f32e`","n":0.189}}},{"i":747,"$":{"0":{"v":"Amend","n":1},"1":{"v":"\nAmend the most previous commit, allowing us to add/remove files to that commit, or even to simply change the commit message.\n\nif we forgot to stage some changes as part of the last commit, simply add them to the working tree (should be clean, considering we just committed), then run `git commit --amend` ","n":0.139}}},{"i":748,"$":{"0":{"v":"Clean","n":1},"1":{"v":"\n# clean\n- remove untracked files and any patchfiles.","n":0.354}}},{"i":749,"$":{"0":{"v":"Cherrypick","n":1},"1":{"v":"\n# Overview\nCherry-pick works by copying a commit diff (between it and its parent) onto the current branch\n\ncherry-picking results in the creation of a new commit that has an identical diff to the diff between the specified commit and its parent. When we run cherry-pick, the changes that were introduced in a single commit will be applied on top of HEAD.\n\nInstead of working bottom-up (or past to the present) like rebase, cherry-pick works top-down (or present back to the past)\n\nWhen you cherry-pick a commit, you also specify which parent commit to consider, with the `-m` parent-number argument. The cherry-pick command then generates a diff against that parent, so that the resulting diff can be applied now.\n\nShould you choose to cherry-pick a non-merge commit, there is only one parent, so you don't actually pass `-m` and the command uses the (single) parent to generate the diff. But the commit itself is still a snapshot, and it's the cherry-pick command that finds the diff of commit^1 (the first and only parent) vs commit and applies that.\n\nIf you want something else done aside from applying the patch between a commit and it’s parent, then you don't want the cherry-pick command. For instance, if you just want a particular snapshot's version of some file, use `git checkout <revspec> -- <path>`, or `git show <revspec>:<path>`\n\nIt's crucial to recognize that cherry picking does not involve moving a commit on top of HEAD. Rather, it involves copying the diff produced by 2 commits, and applying that patch on top of HEAD. This process results in a new commit SHA.\n\nuse cherry-pick when you want to rebase, but have more power over what exactly you want to bring over from one branch to another\n\n### Resetting and Cherrypicking\nThis lends itself to a strategy that is similar conceptually to `rebase`\n\nImagine we have 2 branches that have a common base, but have diverged. We want to get a series of commits to be placed onto the tip of an existing branch (much like rebase)\n1. from feature-branch, `git reset main --hard` to the HEAD of the master branch. These branches are now identical.\n- Before doing this, we need to know which commits we want. It's probably even a good idea to duplicate the branch as a backup just in case.\n2. cherry pick the range of commits on top of feature-branch.\n- as a result, the only conflicts will be in the feature-branch commits.\n","n":0.05}}},{"i":750,"$":{"0":{"v":"Cherrypick","n":1},"1":{"v":"\n### Take commits from 6f3e5a (inclusively) until fe85ab, and plop it on top of `HEAD`\n`git cherry-pick 6f3e5a^..fe85ab`\n- `^` gets the parent, to make the range inclusive\n","n":0.196}}},{"i":751,"$":{"0":{"v":"Checkout","n":1},"1":{"v":"\n# Checkout\n- The act of switching between different versions of a file, commit or branch\n- Think of it as switching between snapshots\n- When checking out a different branch, Git makes your working directory look like that branch. Any checked in content that is in your working directory but is not in the new tree will be removed. This is why Git only lets us checkout another branch if everything is checked in (ie. no uncommitted modified files).\n\t- The reason for this is that Git will remove files that are not necessary in the branch we are checking out.\n- if we add a file path to `git checkout`, only the specified file will be checked out, and the branch pointer will not be updated.\n\t- ex. `git checkout HEAD index.js` will check out the most recent (HEAD) version of `index.js`\n![0a0ed0120ebc145ec651db96de7b73c4.png](:/a7a7bd8f7ee646d4a308c17366095fad)\n\nChecking out a file\n- running `git checkout <file>` is similar to running `git reset <file>`, except checkout updates the working directory, while reset updates the staging area.\n\t- This has a similar effect to `git revert`, with an important difference: `revert` only undoes changes introduced the commit, while `checkout` undoes all changes *since* that commit.\n\t- ex. what if we want to change the file in the working tree to what it was 2 commits ago. We can run `git checkout HEAD~2 <file>`.\n- while running `checkout` on a branch/commit will move the HEAD reference, running `checkout` on a file will not, meaning we don't change branches.","n":0.064}}},{"i":752,"$":{"0":{"v":"Strategies","n":1},"1":{"v":"\n### Pull files from one branch to current branch\n- If we have a base branch that we want to keep, but several files from another branch that we want to pull over, from the base branch we can run:\n    - `git checkout <other-branch-name> -- file1.js file2.js`\n","n":0.147}}},{"i":753,"$":{"0":{"v":"Branch","n":1},"1":{"v":"\n# Branch\n- Simply a pointer to a specific commit\n\t- Therefore, creating a branch is nothing more than writing 40 characters (a SHA) to a file.\n- The key difference is that it is moveable\n- It’s important to realize that Git uses the tip of a branch to represent the entire branch. That is to say, a branch is actually a pointer to a single commit—not a container for a series of commits\n\t- this is why Git diagrams show commits pointing to other commits.\n- If we make a new branch from master, that branch will point to the same commit as master. Once we commit, this new branch will then point to the new commit. Therefore, the branches will have diverged.\n- When we create a new branch, Git knows which commit to use because of the HEAD file.\n\n## Slashes in branch names\nImagine we had a branch named `stripe` already, and we wanted to make a new branch called `stripe/saved-methods`. This would cause an error, because as far as git knows, we have a file named `stripe`, and we are now trying to create a directory named `stripe`.\n- Because they are just directories, deleting `stripe/saved-methods` branch will result in the `stripe` directory still existing.\n\nIt's probably a good convention to stick to at most one level. Consider if we had a branch `wip/stripe`, and we then wanted to create `wip/stripe/saved-methods`, we would get an error because we are trying to create a directory `stripe`, when there is already a file called `stripe`\n\nWhen we have a branch with slashes in it, it gets stored as a directory hierarchy under `.git/refs/heads`.\n","n":0.061}}},{"i":754,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### Create new branch from given commit\n`git checkout -b new-branch a67de0`\n- Equivalent to checking out a commit, then creating a new branch\n\n`git checkout -b new-branch HEAD~3`\n\n##### Stop tracking a remote branch\n`git branch --unset-upstream`","n":0.174}}},{"i":755,"$":{"0":{"v":"Bisect","n":1},"1":{"v":"\n### Flow\nFrom the commit that is not working,\n1. run `git bisect`\n2. run `git bisect bad` to indicate the current commit is \"bad\"\n3. run `git bisect good <SHA>` on a commit we know works\n4. git will checkout a commit in the middle of the good-bad range. See if this version works, and run `git bisect good`/`git bisect bad`\n5. repeat until we find the commit that introduced the issue\n6. run `git bisect reset` to clean up bisection state, returning to original HEAD\n\n* * *\n\nyou can pass git-bisect a script which programmatically tests a tree for the presence of a bug and avoid false positives","n":0.099}}},{"i":756,"$":{"0":{"v":"General","n":1}}},{"i":757,"$":{"0":{"v":"Time","n":1},"1":{"v":"\n# Overview\nwe need to express both a time and a location whenever we want to place some event\n- The basic problem is that we assume spatial coordinates more than we assume temporal coordinates\n\t- Consider how timezones really are. Everyone has a concept of 3:00 in the afternoon. It doesn't matter which timezone you are in, because 3:00 will have a certain feel. We ignore the timezone (spatial coordinate) when it comes to conceptualizing the time. It falls secondary to the time (temporal coordinate).\n- This problem results in people having a poor conception of how their timezone relates to others. This is why `-07:00` is a pretty abstract concept, and thus meaningless to most people.\n- When talking about time, people almost always leave out some crucial information, leaning on the context of what they are saying to help communicate.\n\t- ex. to denote a year people often shorten to 2 digits ('04).\n\n### Other issues\n- Scientific time naturally has origin 0, as usual with scientific measures, even though the rest of human time notations tend to have origin 1\n\n#### Unix time\nThe Unix-time way of solving time-related problems is to set time 0 equal to 1970-01-01 00:00:00 UTC. This approach works well as long as the rules for converting between relative and absolute time are stable. As it turns out, they are not.\n- Some have used local time as the point of reference, some use decoded local time as the reference, and some use hardware clocks that try to maintain time suitable for direct human consumption. Without consistency, problems arise.\n\n#### Political Time\nOn the surface, answering the question \"how long does it take for a clock to show the same value?\" is an easy one to answer. The answer is 24 hours, right? Well, this question gets complex when we consider daylight savings, since 1/365 days will have 23 hours, and 1/365 days will have 25 hours.\n\nThere is no way you can know from your location alone which time zone applies at some particular point on the face of the earth: you have to ask the people who live there what they have decided.\n\nUnfortunately, political time is what people want their computers to produce when asking for time of day.\n\n# UE Resources\n[Essential reading before working with Timezones](https://zachholman.com/talk/utc-is-enough-for-everyone-right)\n- Jensen recommends using https://date-fns.org/\n\n# E Resources\n[Scientific paper on time and its complexities](http://naggum.no/lugm-time.html)","n":0.051}}},{"i":758,"$":{"0":{"v":"Terms","n":1},"1":{"v":"\n### Resource\nA resource is any computer component of limited supply\n- a [[file descriptor|dendron://tech/unix.streams#file-descriptor-fd]] is a resource because a system can only handle so many\n- a network connection is a resource because there are only so many network sockets\n- memory is a resource because a computer only has so much memory\n","n":0.141}}},{"i":759,"$":{"0":{"v":"Side Effects","n":0.707},"1":{"v":"\nFunctions that have side effects are often convenient and easy to write. Their effects can in principle be encoded in their names and in the comments. A function called `SetPassword` or `WriteFile` is obviously mutating some state and generating side effects, and we are used to dealing with that. It’s only when we start composing functions that have side effects on top of other functions that have side effects, and so on, that things start getting hairy. It’s not that side effects are inherently bad — it’s the fact that they are hidden from view that makes them impossible to manage at larger scales. Side effects don’t scale, and imperative programming is all about side effects.\n","n":0.093}}},{"i":760,"$":{"0":{"v":"Indirection","n":1},"1":{"v":"\n# Indirection \n- Indirection is the ability to refer to something through use of a *reference* (id). Normally, this involves manipulating the variable in question by using a pointer. Pointers exist to provide reference to an object (called indirection node)\n\t- *\"All problems in computer science can be solved by another level of indirection\"*\n\t- When the complexity of information becomes too much at one level, we simplify it with a layer of indirection that pushes the bulk of the complexity to a new level.","n":0.11}}},{"i":761,"$":{"0":{"v":"Idempotent","n":1},"1":{"v":"\n## Idempotency\nIn mathematics, the an idempotent function is one that will yield the same output, even as its output feeds back as input indefinitely, such as:\n`Math.abs(Math.abs(Math.abs(Math.abs(-2))))`\n`String( String( x ) )`\na way to test mathematical idempotency is to check if the output of a single function is equal to multiple calls of that function:\n```js\ncurrency( -3.1 ) == currency( currency( -3.1 ) )\n// true\n```\nWherever possible, restricting side effects to idempotent operations is much better than unrestricted updates.\n\nThe programming-oriented definition for idempotence is similar, but less formal. Instead of requiring `f(x) === f(f(x))`, this view of idempotence is just that `f(x);` results in the same program behavior (in other words, not just the output. *All* impacts of the function) as `f(x);` `f(x);`. In other words, the result of calling `f(x)` subsequent times after the first call doesn't change anything.\n- Pure functions are idempotent in the programming sense\n\nIf you need to do side-effects, try to make it idempotent\n\nPure functions can reference `free variables` (ie. those that are not defined within the function scope), as long as those variables are sure to not change during the execution of the program (ie. they are constant)\n\nAn example of idempotency is in Postgres, when we say `delete function if exists`, rather than just `delete function`\n- It is idempotent because running the command more than once has no additional effect than simply running it once.","n":0.067}}},{"i":762,"$":{"0":{"v":"Refactoring","n":1},"1":{"v":"\nwhen you are refactoring some piece of code, all you need to ask is \"did the signature of what I just refactored change?\" put another way, were the inputs and outputs of the changed code modified, so that code that depends on it would no longer work properly? \n- Any time you are changing code, always be mindful of the code that depends on it. Think of how that depending code consumes the code that you are refactoring\n","n":0.113}}},{"i":763,"$":{"0":{"v":"Principles","n":1},"1":{"v":"\nwhen coding a problem, you must first understand what you are coding before you understand how you are coding\n- Put another way, you need to understand the underlying logic/business case that you are implementing with code. Understand the issue, understand how a resolution of that issue feels. The what is the important part. Once that's done, the how will flow naturally","n":0.128}}},{"i":764,"$":{"0":{"v":"Code Maintainability","n":0.707},"1":{"v":"\n### Splitting out a data structure into 2 forms: one for maintainability, and one for functionality\n- [source: Robot Delivery game `buildGraph` function.](https://eloquentjavascript.net/07_robot.html)\nIn this project, it makes sense to have an object of key-value pairs \n- where the key is the name of a node, and the value is an array of adjacent nodes that can be travelled directly to (ie. without having to go through another node to get there):\n```js\nconst graph = {\n    'Alice-House': ['Bob-House', 'Post-office', 'Cabin']\n    // ...\n}\n```\nWe could do this by hand, but then we are looking at a maintainability nightmare. Consider that if we add a new node that connects to 3 other existing nodes, then we need to modify the existing object in 4 places: modify 3 values (ie. the arrays), and add a new key. \n- When building out this code, we can take a strong focus to future-proofing it. Always consider likely maintenance needs, and find a way to make it simpler.\n\nThe way the project goes about future proofing this is by using a more primary way of representing the nodes, which is done through using an array where each element is an edge (ie. the link between 2 nodes):\n```js\nconst roads = [\n  \"Alice's House-Bob's House\",   \"Alice's House-Cabin\",\n  // ...\n]\n```\n\na `buildGraph` function is then made, which takes in the array and outputs the `graph` object. Now, whenever a new node is added to the town, only a single element needs to be added to the `roads` array.\n\n### Immutability in data structures\nImmutable data structures help you to understand your programs, making it a solution to the problem of complexity management. When objects in a system are known to be fixed, stable things, we can consider operations in isolation (in the Robot game above, this might be moving from one location to another; going from location A to location B always produces the same new state). A simpler program lets us build it more ambitiously.","n":0.056}}},{"i":765,"$":{"0":{"v":"Separation of Concerns","n":0.577},"1":{"v":"\nApplying SoC well results in a program that is highly modular\n\n### Room Analogy\nConsider that we separate a space into rooms, so that an activity in one room does not affect people in other rooms, and keeping the stove on one circuit and the lights on another, so that overload by the stove does not turn the lights off. The example with rooms shows encapsulation, where information inside one room, such as how messy it is, is not available to the other rooms, except through the interface, which is the door. The example with circuits demonstrates that activity inside one module, which is a circuit with consumers of electricity attached, does not affect activity in a different module, so each module is not concerned with what happens in the other.","n":0.088}}},{"i":766,"$":{"0":{"v":"Domain-Driven Design","n":0.707},"1":{"v":"\nDomain Driven Design is the concept that we should design our application code with heavy consideration into the business logic.\n- ex. we don't know if it will always be the case that you must be a user to purchase a book. Therefore, we shouldn't design the system in a way where we can't accomplish that easily.\n\nDDD is not about writing code in a certain way. Rather, it's about understanding the underlying logic behind the problems you need to solve. It's the thinking about domains that drives the design of the code. In a way, DDD is the opposite of jumping right in with code and trying to solve something without thinking about it beforehand. \n- \"The novice jumps in right away with code. The veteran ponders the problem they are solving, and thinks (in code) about the logic of that problem\"\n\n### Bounded Context\nOne of the core ideas of DDD is Bounded Context: the subsystems of a business (business used loosely here). Consider that a business has many different contexts, and that there is interaction between them at certain interface points, like the sales context and the customer support context \n![](/assets/images/2021-06-05-21-26-21.png)\n\nBecause Bounded Context is a fundamental idea, the whole software project needs to be on board with the idea of DDD.\n\nDDD is all about unifying all of the code in a way that is highly comprehensible to those uninvolved with the code's creation.","n":0.066}}},{"i":767,"$":{"0":{"v":"Design Pattern","n":0.707},"1":{"v":"\nA design pattern can be one of four general types:\n1. Creational design patterns\n2. Structural design patterns\n3. Behavioral design patterns\n4. Architectural design patterns\n\n### Creational\nThese patterns are used to provide a mechanism for creating objects in a specific situation without revealing the creation method. In absence of the pattern, the approach for creating an object might lead to complexities in the design of a project.\nUsing creational design patterns allow flexibility in deciding which objects need to be created for a specific use case by providing control over the creation process.\n![](/assets/images/2021-10-08-13-42-21.png)\n\n### Structural\nThese patterns concern class/object composition and relationships between objects. They let you add new functionalities to objects so that restructuring some parts of the system does not affect the rest. Hence, when some parts of structure change, the entire system does not need to change.\n![](/assets/images/2021-10-08-13-43-17.png)\n\n### Behavioural\nThese patterns are concerned with communication between dissimilar objects in a system. They streamline the communication and make sure the information is synchronized between such objects.\n![](/assets/images/2021-10-08-13-43-47.png)\n\n### Architectural\nThese patterns are used for solving architectural problems within a given context in software architecture.\n![](/assets/images/2021-10-08-13-44-15.png)\n","n":0.076}}},{"i":768,"$":{"0":{"v":"State Machine","n":0.707},"1":{"v":"\nA state machine is a directed graph where nodes represent all possible states of a view or of the whole app, and where edges represent possible transitions between the states.\n\nState machines are particularly useful when the state is very complex and involves multiple transitions.\n\nTurnstile:\n![](/assets/images/2021-10-27-09-53-05.png)\n\nAn arrow between two nodes means that it’s possible to go through one state to another via some action. All non-listed transitions are not possible. There can be a meaningful transition from a state to the same state that is marked by a circular arrow.\n\nHere is a state machine for the storage of fetched data that is to be stored in the browser\n![](/assets/images/2021-10-27-09-56-33.png)\n\nThere are four states:\n- empty - fetching has not started yet or has been canceled\n- loading - fetching is in progress\n- withData - fetching was successful\n- error - fetching failed","n":0.086}}},{"i":769,"$":{"0":{"v":"Observer","n":1},"1":{"v":"\nThe idea with the observer pattern is that an object (the `subject`) maintains a list of all its dependents. Whenever a piece of state in the subject changes, all of the observers are automatically notified. This is normally accomplished by the subject calling the observer's `update()` method.\n- The sole responsibility of a *subject* is to maintain a list of observers and to notify them of state changes by calling their update() operation. \n- The responsibility of *observers* is to register (and unregister) themselves on a subject (to get notified of state changes) and to update their state (synchronize their state with the subject's state) when they are notified. This makes subject and observers loosely coupled.\n    - Observers can be added and removed independently at run-time. This is very similar to [[pub-sub|general.architecture.pubsub]]\n\nObserver pattern is used in event handling systems that are generally distributed. In this case, the subject is usually a \"stream of events\", while the observers are \"sinks of events\". The observers themselves are physically separated and have no control over their emitted events from the stream-source (ie. subject).\n\nThe observer pattern is well-suited to a process where data arrives from some input that is not available to the CPU at startup, but instead arrives \"at random\" (HTTP requests, GPIO data, user input from keyboard/mouse/..., distributed databases and blockchains, ...).\n\nThe Observer pattern addresses the following problems:\n\n- A one-to-many dependency between objects should be defined without making the objects tightly coupled.\n- It should be ensured that when one object changes state, an open-ended number of dependent objects are updated automatically.\n- It should be possible that one object can notify an open-ended number of other objects.","n":0.061}}},{"i":770,"$":{"0":{"v":"Object Pool","n":0.707}}},{"i":771,"$":{"0":{"v":"Factory Functions","n":0.707},"1":{"v":"\nFactory functions allow us to create objects without having to specify the exact class of the object that will be created.\n- This is accomplished by calling a function, rather than calling a constructor.\n\nThe idea is to define an interface for creating an object, but let subclasses decide which class to instantiate. The Factory method lets a class defer instantiation it uses to subclasses.\n\nIn the absence of factory functions, we may have some added problems, and an object's creation...\n- may lead to a significant duplication of code,\n- may require information not accessible to the composing object,\n- may not provide a sufficient level of abstraction,\n- may otherwise not be part of the composing object's concerns.\n\nThe factory method pattern relies on inheritance, as object creation is delegated to subclasses that implement the factory method to create objects.\n\nThe problem with constructors is that they look just like regular functions, but do not behave like regular functions at all.\n\nThe factory function pattern is similar to constructors, but instead of using new to create an object, factory functions simply set up and return the new object when you call the function.\n\nWith a factory\n```js\nconst personFactory = (name, age) => {\n  const sayHello = () => console.log('hello!');\n  return { name, age, sayHello };\n};\n\nconst jeff = personFactory('jeff', 27);\n\nconsole.log(jeff.name); // 'jeff'\n\njeff.sayHello(); // calls the function and logs 'hello!'\n```\n\nWith a constructor\n```js\nconst Person = function(name, age) {\n  this.sayHello = () => console.log('hello!');\n  this.name = name;\n  this.age = age;\n};\n\nconst jeff = new Person('jeff', 27);\n```\n\nES6 modules are actually very similar to factory functions. The main difference is how they’re created.\n- The concepts are exactly the same as the factory function. The nuance is that instead of creating a factory that we can use over and over again to create multiple objects, the module pattern wraps the factory in an IIFE (Immediately Invoked Function Expression).\n```js\nconst calculator = (() => {\n  const add = (a, b) => a + b;\n  const sub = (a, b) => a - b;\n  const mul = (a, b) => a * b;\n  const div = (a, b) => a / b;\n  return {\n    add,\n    sub,\n    mul,\n    div,\n  };\n})();\n\ncalculator.add(3,5) // 8\ncalculator.sub(6,2) // 4\ncalculator.mul(14,5534) // 77476\n```\n\nabove, the function inside the IIFE is a simple factory function, but we can just go ahead and assign the object to the variable calculator since we aren’t going to need to be making lots of calculators, we only need one.","n":0.05}}},{"i":772,"$":{"0":{"v":"Event Sourcing","n":0.707},"1":{"v":"\nA system that allows us to query a database tells us the current state of everything. A system that allows Event Sourcing also tells us how we got here. In other words, the system keeps records of all changes that have happened.\n- Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.\n\nevery change to the state of an application is captured in an event object, and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.\n- To be clear, we are persisting two different things: an application state (ex. DB) and an event log.\n\nThe key to Event Sourcing is that we guarantee that all changes to the domain objects (ex. what is stored in the DB) are initiated by the event objects\n- This shows that when we want to change something in our database, our event system is first updated with the new log. Only once that log has been entered can our database be updated as well.\n\t- An implication of this is that we could dump the whole application state, and rebuild it from the log.\n\t- Another implication is that we can see the state of the application as a timeline, analogous to Git branching.\n\nA common example of an application that uses Event Sourcing is a version control system\n\n[[CQRS|general.patterns.CQRS]] and event sourcing often go hand-in-hand\n\n# UE Resources\nhttps://martinfowler.com/eaaDev/EventSourcing.html","n":0.061}}},{"i":773,"$":{"0":{"v":"Decoupling","n":1},"1":{"v":"\n## Should this code be decoupled?\nWhen considering whether or not code should be decoupled, you have to consider both modules and ask if there is a reasonable likelihood that one could exist without the other. It is easy to fall into the trap of only considering if one module should ever need to exist on its own, while completely ignoring the other one.\n\n### Illustration of this thought\nconsider the case where we had considered adding the is_subscribed argument to the register_user function.\n\nMy first point of view was that \"we should not include this as an argument in the function, because it couples the logic of registering a new account with the logic of subscribing that user to a mailing list. What if we want to register a user without having to decide about subscribing? Why is this logic coupled?\"\n\nThis is how I first reasoned about it, but realistically, I should have looked at the other side of it too: \"is there any place in the code that we'd want to set a user as subscribed, without registering them also?\".\n\nThe lesson is: when considering questions of if code should be decoupled, look at all modules that are being decoupled together, and ask \"will any of these modules need to be used without the other?\"\n\n* * *\n\nCoupled code is not necessarily an anti-pattern. You have to ask if these modules should live together under all circumstances. Would it make sense for one to exist without the other? (again, thinking about it from both angles)\n- ex. webpage data. We can introduce 3 separate hooks to the page, each which gets a different piece of data, or we can wrap these 3 hooks into a separate hook, which couples the 3 hooks together, and indirectly to the page (since this is a very specific set of data, we can now consider coupled to the concept of this particular page, which requires this particular set of data).\n    - anal. Think of how a Cable Management Sleeve is used. We wrap it around multiple cables to \"abstract away\" the need to concern ourselves with each cable. We would only ever wrap cables that we are pretty sure would stay together. If we had only one charging cable for our laptop and had to carry it around wherever we go, then we wouldn't wrap this cable in the sleeve. This analogy helps shows that there are costs and benefits to each strategy.","n":0.05}}},{"i":774,"$":{"0":{"v":"SSR","n":1},"1":{"v":"\nThe idea for SSR is that there is a large chunk of our website (if not all) that is static. It is expensive to ship a bundle of javascript to the client, and expect the client to do all the work of parsing it and converting it to html. The question becomes: \"what if we could just pre-render this react into html beforehand on the server, and give that content to the client when it is requested? That way, the content can be served as fast as if we were using plain HTML/CSS/Javascript.\n\nThe most common use case for server-side rendering is to handle the initial render when a user (or search engine crawler) first requests our app. When the server receives the request, it renders the required component(s) into an HTML string, and then sends it as a response to the client. From that point on, the client takes over rendering duties.\n\n## CSR vs SSR\n### CSR Approach\nUsing CSR, the server will send a mostly empty HTML file to begin with. Through links, it will build up the page through a series of further HTTP requests.\n\n1. Browser requests `example.com/index.html`\n2. Server responds with `<script/>` tag at end of `<body/>`\n3. Browser makes an API request for the content in those script tags\n4. Server responds with that content.\n5. Browser makes async requests for Javascript bundles.\n    - note: these bundles can be cached to speed things up in the future.\n\nCons of CSR:\n- Loading content may take a while because requests have to travel all the way to the server, which can be very far away.\n- SEO takes a hit because all search engines see is an empty HTML file.\n\n### SSR Approach\nIn SSR, the whole web page is compiled on the server. The HTML is completely populated with the content before it is sent to the client.\n\n1. Browser requests `example.com/index.html`\n2. Server makes co-located API requests to populate the HTML \"template\" with the data it needs\n3. Server sends that populated HTML back to the client.\n\nCons of SSR:\n- A page will have to be rendered on the server and reloaded every time a new page on the site is visited, which will lead to full page reloads.\n- The server will receive frequent requests, which can easily lead to the server getting flooded with requests and slowing down.\n\n## Implementations\n\n### with React\nWith simple client-side rendering, we have a root node `<div id=\"root\">` to which the entire React app gets attached to. This node is by default empty.\nWith SSR, this node has some content by default. But, instead of replacing that content, we want React to attach a running application to it.\n\nThis is where hydration comes in. During hydration, React works quickly in a virtual DOM to match up the existing content with what the application renders, saving time from manipulating the DOM unnecessarily. It is this hydration that makes SSR worthwhile.\n\nThere are two big rules to hydrating an application in React.\n1. The initial render cycle of the application must result in the same markup, whether run on the client or the server.\n2. We must call ReactDOM.hydrate instead of ReactDOM.render in order to instruct React to hydrate from our SSR result.\n\n### with Redux\n- When using Redux with server rendering, we must also send the state of our app along in our response, so the client can use it as the initial state. This is important because, if we preload any data before generating the HTML, we want the client to also have access to this data. Otherwise, the markup generated on the client won't match the server markup, and the client would have to load the data again.\n- To send the data down to the client, we need to:\n\n1. create a fresh, new Redux store instance on every request;\n2. optionally dispatch some actions;\n3. pull the state out of store;\n4. and then pass the state along to the client.\nRedux's only job on the server side is to provide the initial state of our app.\n\nOn the server, you will want each request to have its own store, so that different users get different preloaded data.\n\n### with Gatsby\nSometimes Gatsby is referred to as doing SSR. More realistic is that its doing SPG (Static site generation)\n\n# UE Resource\n- [Good breakdown of SSR](https://www.freecodecamp.org/news/react-server-components/)\n","n":0.038}}},{"i":775,"$":{"0":{"v":"Inversion of Control","n":0.577},"1":{"v":"\nIoC means letting other code call you rather than insisting on doing the calling.\n- ex. An example of IoC without dependency injection is the template method pattern. Here, polymorphism is achieved through subclassing, that is, inheritance.\n\nIf you follow these simple two steps, you have done inversion of control:\n1. Separate what-to-do part from when-to-do part.\n2. Ensure that when part knows as little as possible about what part; and vice versa.\n\n## Examples\n\n### Email Example\n#### The Scenario\nImagine we had a module called `FortuneCookieEmailer`, which sends email. It uses some kind of service to do the work of actually sending the emails, `BillsBulkEmailService`. Here, the `FortuneCookieEmailer` code now contains a direct reference to the `BillsBulkEmailService`.\n\nThis works, but imagine our program expands and now `FortuneCookieEmailer` has to sometimes use `KylesSpammingService`. We would have to go into the `FortuneCookieEmailer` module and introduce logic that would make it conditionally use either `BillsBulkEmailService` or `KylesSpammingService`. Now we need to have some kind of config file somewhere, or some kind of extra \"switch\" for our `FortuneCookieEmailer` module. The bad thing is that we are adding the job of \"deciding which to use\" to a module whose sole responsibility should be fortune cookie emails.\n\n#### The solution\nInstead of `FortuneCookieEmailer` going out and getting a reference to the `BillsBulkEmailService` or `KylesSpammingService` itself and then using that, `FortuneCookieEmailer` should be provided with a reference to the desired service by something outside of `FortuneCookieEmailer`.\n\n#### Summary\n- First situation: FortuneCookieEmailer actively goes out and gets the service that it needs.\n- Second (IoC) situation: Someone else gives FortuneCookieEmailer the service that it should use.\n\nThe benefit of the second approach is that we can configure our application to use different email services without having to even touch `FortuneCookieEmailer`\n\n### Text Editor Examples\nsay your application has a text editor component and you want to provide spell checking. Your standard code would look something like this:\n```cs\npublic class TextEditor {\n\n\tprivate SpellChecker checker;\n\n\tpublic TextEditor() {\n\t\t\tthis.checker = new SpellChecker();\n\t}\n}\n```\nWhat we've done here creates a dependency between the TextEditor and the SpellChecker. In an IoC scenario we would instead do something like this:\n\n```cs\npublic class TextEditor {\n\n\tprivate IocSpellChecker checker;\n\n\tpublic TextEditor(IocSpellChecker checker) {\n\t\t\tthis.checker = checker;\n\t}\n}\n```\n\nIn the first code example we are instantiating `SpellChecker` (`this.checker = new SpellChecker();`), which means the `TextEditor` class directly depends on the `SpellChecker` class.\n\nIn the second code example we are creating an abstraction by having the `SpellChecker` dependency class in `TextEditor`'s constructor signature (not initializing dependency in class). This allows us to call the dependency then pass it to the `TextEditor` class like so:\n\n```cs\nSpellChecker sc = new SpellChecker(); // dependency\nTextEditor textEditor = new TextEditor(sc);\n```\n\nNow the client creating the TextEditor class has control over which SpellChecker implementation to use because we're injecting the dependency into the TextEditor signature.\n\n### Analogy to understand IoC\nInversion of Control, (or IoC), is about getting freedom (You get married, you lost freedom and you are being controlled. You divorced, you have just implemented Inversion of Control. That's what we called, \"decoupled\". Good computer system discourages some very close relationship.) more flexibility (The kitchen in your office only serves clean tap water, that is your only choice when you want to drink. Your boss implemented Inversion of Control by setting up a new coffee machine. Now you get the flexibility of choosing either tap water or coffee.) and less dependency (Your partner has a job, you don't have a job, you financially depend on your partner, so you are controlled. You find a job, you have implemented Inversion of Control. Good computer system encourages in-dependency.)\n\nWith the above ideas in mind. We still miss a key part of IoC. In the scenario of IoC, the software/object consumer is a sophisticated framework. That means the code you created is not called by yourself. Now let's explain why this way works better for a web application.\n\nA traditional software framework will be like a garage with many tools. So the workers need to make a plan themselves and use the tools to build the car. Building a car is not an easy business, it will be really hard for the workers to plan and cooperate properly. A modern software framework will be like a modern car factory with all the facilities and managers in place. The workers do not have to make any plan, the managers (part of the framework, they are the smartest people and made the most sophisticated plan) will help coordinate so that the workers know when to do their job (framework calls your code). The workers just need to be flexible enough to use any tools the managers give to them (by using Dependency Injection).\n\nAlthough the workers give the control of managing the project on the top level to the managers (the framework). But it is good to have some professionals help out. This is the concept of IoC truly come from.\n\nDependency Injection and Inversion of Control are related. Dependency Injection is at the micro level and Inversion of Control is at the macro level. You have to eat every bite (implement DI) in order to finish a meal (implement IoC).\n\n\n# UE Resources\n[Martin Fowler on IoC](https://martinfowler.com/bliki/InversionOfControl.html)","n":0.035}}},{"i":776,"$":{"0":{"v":"Dependency Injection","n":0.707},"1":{"v":"\n# Overview\nDependency Injection is one form of implementing [[Inversion of Control|general.patterns.IOC]]\n- Here, IoC would be achieved through composition\n\nA big reason to use dependency injection is to make your code more testable.\n\n### Goal\nThe goal is to achieve [[separation of concerns|general.principles.SoC]]\n\nThe client should have no concrete knowledge of the specific implementation of its dependencies. It should only know the interface's name and API. As a result, the client will not need to change even if what is behind the interface changes. Because we achieve this by using interfaces, if the interface itself changes, then the client will have to change as well.\n- ex. if the interface is refactored from being a class to an interface type (or vice versa) the client will need to be reworked.\n- This last point creates a potential issue if the client and services are published separately.\n\n### Motivation\nDependency injection separates the creation of a client's dependencies from the client's behavior, which allows program designs to be loosely coupled and to follow the dependency inversion and single responsibility principles.\n\nA client who wants to call some services should not have to know how to construct those services. Instead, the client delegates the responsibility of providing its services to external code (the injector; see below)\n- The client is not allowed to call the injector code; it is the injector that constructs the services\n\nThe injector then injects (passes) the services into the client which might already exist or may also be constructed by the injector. The client then uses the services. This means the client does not need to know about the injector, how to construct the services, or even which actual services it is using. The client only needs to know about the intrinsic interfaces of the services because these define how the client may use the services. This separates the responsibility of \"use\" from the responsibility of \"construction\".\n\n### Components\nDependency injection involves four roles:\n1. the service object(s) to be used\n2. the client object that is depending on the service(s) it uses\n3. the interfaces that define how the client may use the services\n4. the injector, which is responsible for constructing the services and injecting them into the client\n    - The injector may be referred to by other names such as: assembler, provider, container, factory, builder, spring, construction code, or main\n\nAs an analogy,\n1. `service` - an electric, gas, hybrid, or diesel car\n2. `client` - a driver who uses the car the same way regardless of the engine\n3. `interface` - automatic, ensures driver does not have to understand details of shifting gears\n4. `injector` - the parent who bought the car for the driver and decided which kind\n\nAny object that may be used can be considered a service. Any object that uses other objects can be considered a client. The names have nothing to do with what the objects are for and everything to do with the role the objects play in any one injection.\n\n## Pros and Cons\n### Pros\n- The client becomes an entity whose behavior is fixed, yet configurable. The client may act on anything that supports the intrinsic interface the client expects\n- Because clients are more independent, they are easier to unit test. They are more like proper modules, that we can test in isolation, using stubs and mocks to simulate that module interacting with other modules.\n- DI can be an effective way to refactor legacy code, because it does not require any change in code behavior.\n\n### Explanation of terminology\nAn \"Injection\" is the basic unit of dependency injection, and it works in the same way that \"parameter passing\" works\n- Referring to \"parameter passing\" as an injection carries the added implication that it is being done to isolate the client from details\n- An injection doesn't care about how the passing is accomplished. For instance, it doesn't care if it is passed by value or by reference.\n- An injection is about what is in control of the passing (never the client)\n\nThe DI pattern involves an object receiving other objects that it depends on.\n- These \"other objects\" are called dependencies\n\nConsider the client-server relationship.\n- Here, the client is the object that receives, and the server is the object that is passed (ie. \"injected\")\n- The code that passes the service to the client is called the *Injector*.\n- Instead of the client specifying which service it will use, the injector tells the client what service to use. The \"injection\" refers to the passing of a dependency (a service) into the object (a client) that would use it.\n\n## Examples\n\n### Desktop/Laptop Analogy\nWithout DI: You have a laptop computer and you accidentally break the screen. And darn, you find the same model laptop screen is nowhere in the market. So you're stuck.\n\nWith DI: You have a desktop computer and you accidentally break the screen. You find you can just grab almost any desktop monitor from the market, and it works well with your desktop.\n\nYour desktop successfully implements DI in this case. It accepts a variety type of monitors, while the laptop does not, it needs a specific screen to get fixed.\n\n# UE Resources\n[Expalantion with Nodejs](https://stackoverflow.com/questions/9250851/do-i-need-dependency-injection-in-nodejs-or-how-to-deal-with)\n[DI with Node](https://khalilstemmler.com/articles/tutorials/dependency-injection-inversion-explained/)\n[DI in Javascript with Sam](https://sammeechward.com/dependency-injection-in-javascript/)","n":0.035}}},{"i":777,"$":{"0":{"v":"Command-Query-Responsibility-Segregation","n":1},"1":{"v":"\nCommand Query Responsibility Segregation (CQRS) is an architectural pattern that separates reading and writing into two different models. This means that every method should either be a Command that performs an action or a Query that returns data. A Command cannot return data and a Query cannot change the data\n- ex. Redux attempts to follow this pattern. Consider that to write to state we must dispatch an action, and to read state we must use selectors. The implementations of reading and writing to state are decoupled.\n\n# UE Resources\nhttps://martinfowler.com/bliki/CQRS.html","n":0.107}}},{"i":778,"$":{"0":{"v":"Learning","n":1},"1":{"v":"\nAs a skilled guitarist, you might watch a 10 minute video and walk away with,\n> “Ah, so one major pentatonic scale cannot be applied universally to a chord progression. To sound good I have to match the scale to the specific chord being played.\n> I can accomplish that by using CAGED patterns to match up with the chords in a progression.”\n\nWhat if you jump into learning about AWS before learning the periphery concepts first? You will never learn this way, because knowledge builds on top of other knowledge.\n\nTo understand... \n- PassportJS - you need to know how auth works, how sessions works, or how/what OAuth is\n- Docker - you need to know what a virtual machine was or why it is useful, otherwise you won't understand containers.\n- AWS - you need to understand what “cloud architecture” so, otherwise you can't understand what it meant to deploy code in the cloud.\n- HTML template engines - you need to understand browsers, http, servers, databases, and client-side vs server-side rendering\n\n# Resources\n[RealWorld Example Apps: The same app (same API) built using different technologies](https://codebase.show/projects/realworld)","n":0.075}}},{"i":779,"$":{"0":{"v":"New Codebase","n":0.707},"1":{"v":"\n1. Understand at a high level what the structure looks like. How is the folder structure laid out? What have the designers of the structure seem to have emphasized in its architecture?\n- This must be understood in the context of the codebase you are analzing. A Java codebase that uses Springboot is fundamentally going to look different than a React+Express app. Understand the main philosophies of the framework/language that forms the context of the codebase.\n- ex. Sveltekit and Next.js apps use filesystem-based routing. This gives a sort of pseudo-sitemap of the site, and gives you a strong starting point to understanding it all at a high level.\n\nCheck out what network requests are sent when you do certain actions","n":0.092}}},{"i":780,"$":{"0":{"v":"Lang","n":1},"1":{"v":"\n## Compiled vs Interpreted Languages\n- A compiler figures out everything a program will do, turns it into “machine code” (a format the computer can run really fast), then saves that to be executed later.\n- An interpreter steps through the source code line by line, figuring out what it’s doing as it goes.\n\nTechnically any language could be compiled or interpreted, but one or the other usually makes more sense for a specific language. \n- Generally, interpreting tends to be more flexible, while compiling tends to have higher performance.\n- a lot of language design decisions are affected by the decision to make a language interpreted or compiled \n    - for example, static typing is a big benefit to compiled languages, but not so much for interpreted ones.\n\nInterpreted languages are generally easier design, build and learn\n\n### Writing your own language\nIf you are writing an interpreted language, it makes a lot of sense to write it in a compiled one (like C, C++ or Swift) because the performance lost in the language of your interpreter and the interpreter that is interpreting your interpreter will compound.\n\nIf you plan to compile, a slower language (like Python or JavaScript) is more acceptable. Compile time may be bad, but that isn’t nearly as big a deal as bad run time.\n\n#### From source file to execution\nA programming language is generally structured as a pipeline. The first stage is a string containing the entire input source file. The final stage is something that can be run. This will all become clear as we go through the Pinecone pipeline step by step.\n\n##### Lexer\nThe first stage. The lexer is supposed to take in a string containing an entire files worth of source code and spit out a list containing every token (a token is a unit of the language, like a function name or an operator).\n\nThe lexer provides all details of the source file that are needed by future stages.\n\n##### Parser\nThe parser turns a list of tokens into a tree of nodes. \n- A tree used for storing this type of data is known as an [[AST|general.lang.AST]].\n","n":0.054}}},{"i":781,"$":{"0":{"v":"Types","n":1}}},{"i":782,"$":{"0":{"v":"String","n":1},"1":{"v":"\n- String is collection of only char datatype.\n- string size can be changed if it is a char pointer.\n- The last character of a string is a null – ‘\\0’ character.\n\nWhen manipulating a string (ex. Algorithm) remember that a string is nothing but a character array\nEx. `['c', 'h'...]`\n","n":0.144}}},{"i":783,"$":{"0":{"v":"Array","n":1},"1":{"v":"\nAn array is a type of data structure that stores elements of the same type in a contiguous block of memory\n- An array size can not be changed.\n- The last element of an array is an element of the specific type.\n","n":0.156}}},{"i":784,"$":{"0":{"v":"Loops","n":1},"1":{"v":"\n### Nested for-loops\nGenerally, they are a bad idea, but not always. They are perfect if they describe the correct algorithm e.g. if we need to access every value in a matrix\n\n#### Tips for avoiding\nDoes sorting the array first help you?\n","n":0.158}}},{"i":785,"$":{"0":{"v":"Generator","n":1},"1":{"v":"\n\nA generator can be used to control the iteration behaviour of a loop\nA generator is also an iterator, meaning it can be looped over\n\nA generator is very similar to a function that returns an array. A generator: \n- has parameters \n- can be called \n- generates a sequence of values\n\nHowever, instead of building an array and returning all values at once, a generator yields the values one at a time, requiring less memory, and has the added benefit of allowing the caller to access the first few values immediately, instead of having to wait for the whole array to return.\n- In short, a generator looks like a function but behaves like an iterator.\n","n":0.094}}},{"i":786,"$":{"0":{"v":"Feat","n":1}}},{"i":787,"$":{"0":{"v":"Interface","n":1},"1":{"v":"\nAn interface fulfills the principle of \"duck typing\"\n\n```ts\ninterface inputObjectI = {\n\tlabel: string\n\toptionalVal: string?\n\treadonly readOnlyVal: number // value is immutable post-declaration\n}\n```\nOnce we define the interface, it can then be used as a type.\n\nFunction interfaces can also be created\n- ex. we have a fn with 2 params of type `String` and `String`, and the function returns `Boolean`\n```ts\ninterface search {\n\t(source: string, substring: string): boolean\n}\n```\n\n* * *\n\n## Different perspectives of Interface\nThe purpose of an interface changes depending on if we are looking at it from an Object-Oriented perspective or a Function perspective.\n\n### Interfaces in Object-Oriented Programming\nIn OOP context, an interface defines a list of methods that a class needs to implement in order to conform to the interface.\n- often used to fulfill the Dependency Inversion Principle from SOLID design principles.\n- The dependency inversion principles says that it’s much better to have interfaces than concrete classes as dependencies.\n\nIn general, the OOP part of interfaces is their ability to express some *required behavior*, as opposed to *data* in the functional paradigm.\n\n### Interfaces in Functional Programming\nIn functional programming context, interfaces are used not to enumerate methods of an object, but to describe the shape of the data contained by the object.\n- In this context, generic interfaces are used to describe a data shape when you don’t know or care about the exact type of some properties of the interface. It often makes sense when the data types contain some value.","n":0.066}}},{"i":788,"$":{"0":{"v":"Generic","n":1},"1":{"v":"\nThink of a generic as a \"type which takes in a parameter\"\n- contrast this with the more familiar \"function which takes in a parameter\"\n\nThink of a generically typed function as a prototype. When you define a generic `map()` function, it is basically saying \"I don't care if you pass in an array of strings, or integers, or objects... just pass it in, and I will make sure the function works with that type\". Conceptually, this is like creating many different versions of a function with different type parameters.\n\nAs exemplified by `map`, functions that take arrays as parameters are often generic functions. If you look at the typings for the lodash library, you will see that nearly all of them are typed as generic functions. Such functions are only interested in the fact that the argument is an array, they don’t care about the type of its elements.\n\nWhen you call a function, you pass it arguments. You can think of a generic as an additional thing you pass to the function. But instead of passing a value, you pass a type to it.\n- This type is inferred through whatever the type actually is during execution.\n\nConsider the `identity` function, which most fundamentally explains the value of generics\n```ts\nfunction identity<T>(arg: T): T {\n    return arg\n}\n```\n\nWe can describe the above function as \"the generic function `identity` takes a type parameter `T`, and an argument `arg` (which is of type T), and returns a value of type `T`.\n\nTo write an algorithm generically is to think of the algorithm in terms of *types-to-be-specified-later*. When we want to use these algorithms, the arguments that we pass *inform* the generic function, and the algorithm's behavior will be tailored.\n- this approach permits writing common functions or types that differ only in the set of types on which they operate when used.\n\nWhat they are - When you declare a generic, E, for example. You are basically saying *\"I don't know what this will be, it can be anything, so I shall simply refer to it as \"the thing\" and your type is a generic type.\"*\n\nsituation: you have some chunk of code that is identical between 2 functions, yet the function returns 2 different types. for instance, we have a createpizza and createhamburger. there is naturally a lot of overlap between these, but we can’t simply use one function between them because one function returns an object of type pizza and the other returns an object of type hamburger. to solve this, we can use a generic, and define a function called makefood:\n```ts\nfunction makefood<typeoffood>(): typeoffood {\n    // do all stuff to make food\n    const madefood = {}\n    return madefood\n}\n```\n\na generic is kind of like a function that returns a type, and that type that is returned is modified by whatever type was “passed in”\n\nEx imagine we have a wrapper around fetch(). In the context of our application, fetch might return Order, Customer, Product etc. ","n":0.046}}},{"i":789,"$":{"0":{"v":"Functions","n":1},"1":{"v":"\nA function is a binary relation between 2 sets (ie. collection of values). The input set is mapped to a return value.\n\nA function's type consists of the types of its argument and its return type.\n- ie. function type === function signature\n\nthe indent level of a function should not be greater than one or two.\n- If a function does only those steps that are one level below the stated name of the function, then the function is doing one thing.\n\nFunctions can be either total or partial.\n- A total function is one that will succeed for all possible inputs, whereas a partial function may not.\n\n```js\n/* No matter what numbers you specify, this function will always return a value. You can be confident that under no circumstances\n * will your program crash because of this function (Reality is slightly more complex, but this is close enough)\n * As such, this function is a total function.\n */\nfunction add( x, y ){ return x + y };\n\n// All numbers can be converted to strings, so this is a total function.\nfunction numberToString( num ) { return num.toString() };\n```\n\nWhen a function is called, its memory is allocated on the stack.\n- When a function executes, it adds its state data to the top of the stack; when the function exits this data is removed from the stack.\n    - All data, such as local variables, parameters, return address etc. are included in this single unit that is added to the stack.\n\n### Memory allocation of recursive functions\nA recursive function calls itself, therefore, the memory for a called function is allocated on top of memory allocated for calling function.\n- a different copy of local variables is created for each function call. When the base case is reached, the child function returns its value to the function from which it was called. Then, this child function’s stack frame is removed. This process continues until the parent function is returned.\n\nIf the memory stack was a stack of plates, then each execution of the function would be a plate added to the stack. Plates would continue to be added until the base case was reached. At that point, the top plate would be removed, and its return value would go to the next plate. This would continue until the return value reaches the final plate in the stack, which would return the final value to the initial calling site.\n\n### Function composition\nIntuitively, composing functions is a chaining process in which the output of function f feeds the input of function g.\n\n### Higher-order function\nA higher-order function is such a function that:\n- Has a parameter that has a function type, OR\n- it’s return type is a function type","n":0.048}}},{"i":790,"$":{"0":{"v":"Data Structures","n":0.707}}},{"i":791,"$":{"0":{"v":"Tree","n":1},"1":{"v":"\nTrees are relation-based data structure, which specialize in representing hierarchical structures.\n\nLike a linked list, nodes contain both elements of data and pointers marking its relation to immediate nodes.\n- This means that a BST is a linked data structure.\n\nFor a graph to be a tree, we must be able to take any 2 nodes and be able to draw exactly *one* path between them.\n\nalmost every operation you'll do with trees is implemented most easily with recursion\n\ntrees usually work in `O(log n)`\n","n":0.112}}},{"i":792,"$":{"0":{"v":"Binary","n":1},"1":{"v":"\nEach node has at most 2 children (referred to ask *left child* and *right child*).\n\nWe can break a binary tree down into the left subtree and the right subtree.\n\n# Binary Search Tree (BST)\nA binary tree is a BST if the key of the node is greater than all the nodes in its left subtree and is smaller than all the nodes in its right subtree.\nA binary tree is a BST if:\n- the key of each node in the left subtree is *less than* all the nodes in the right subtree, and\n- the key of each node in the right subtree is *more than* all the nodes in the left subtree\n- the root key is between the largest key in the left subtree and the smallest key in the right subtree\n    - in other works, the root key serves as the dividing point.\n\nThe following is a BST, because there are no nodes in the left subtree that are greater than any node in the right subtree.\n![](/assets/images/2021-10-07-06-54-40.png)\n\nThe following is *not* a BST, since there is a node with key 51 in the left subtree of 50 which breaks our invariant.\n![](/assets/images/2021-10-07-06-57-26.png)\n\n## Logic of a BST\n### Finding a node by key\n1. Compare current node's key with X. If it's equal, we've found the key. All done.\n2. If X is less than node's key, we start looking at node's left subtree. It's because we know that right subtree cannot contain anything greater than X.\n3. If X is greater than node's key, we start looking in the right subtree.\n4. We repeat this process until we find the key or we reach the leaf node. If we reach the leaf node and haven't found the key as yet, we return not found.\n\n### Inserting into tree\nNew nodes are *always* leaf nodes. Following the logic of left children being smaller and right children being larger, we start searching a key from the root until we hit a leaf node. Once a leaf node is found, the new node is added as a child of the leaf node.\n- Therefore, there is only one correct location for the node to be, which is dependent on the key.\n\nAssume we want to insert a node with key `K`. Starting at the root,\n1. Compare current node's key with `k`.\n2. If `k` is less than the current node,\n    - If left child of current node is `null`, we insert `k` as the left child of current node and return.\n    - If the left child is not `null`, the left child becomes the new current node, and we repeat the process from step 1.\n3. If `k` is greater than the current node,\n    - If right child of current node is `null`, we insert `k` as the right child of the current node and return.\n    - If the right child is not `null`, the right child becomes the new current node, and we repeat the process from step 1.\n\n#### Advantages\nBinary trees are useful for accessing nodes based on their key or value. When labelled like this, we can implement a Binary Search Tree (for efficient searching/sorting/min/maxing).\n\nBinary search trees allow binary search for fast lookup, addition and removal of data items, and can be used to implement dynamic sets and lookup tables.\n- with the way that the nodes of a BST are arranged, each comparison skips ~half the remaining tree.\n    - this results in a [[time complexity|general.algorithms.space-and-time]] that is logarithmic (`O(log n)`), making it much faster than the linear time required to find items in an unsorted array, but slower than finding items in a hash table.\n\n### Traversal types\nIn-order traversals are performed on each node by processing the left subtree, then the node itself, then the right subtree.\nPre-order traversal\nPost-order traversal\n\n### \"Storing\" a BST\nstore both Inorder and Preorder traversals. This solution requires space twice the size of Binary Tree.\n","n":0.04}}},{"i":793,"$":{"0":{"v":"B-tree","n":1},"1":{"v":"\nUnlike Binary Tree, B-tree allows more than 2 children per node.\n\nthe B-tree is well suited for storage systems that read and write relatively large blocks of data, such as disks. \n- Therefore, it is commonly used in databases and file systems.\n","n":0.156}}},{"i":794,"$":{"0":{"v":"Stack","n":1},"1":{"v":"\nStacks are linear data structures in a LIFO (last-in, first-out) order. Now, what does that mean? Imagine a stack of plates. The last plate that you put on top of the stack is the first one you take out. Stacks work that way: the last value you add is the first one you remove.\n- The most common functions in a stack are `push`, `pop`, `isEmpty`, and `peek`.\n\nAlong with queues, stacks are the most the basic list types in computing\n\nA stack has a number of useful applications:\n- Backtracking to a previous state\n- Expression evaluation and conversion\n","n":0.103}}},{"i":795,"$":{"0":{"v":"Queue","n":1},"1":{"v":"\nQueues are very similar to stacks in that they both are linear data structures with dynamic size. However, queues are FIFO (first-in, first-out) data structures, like a line to a rollercoaster. That is, first come, first serve.\n\nAlong with stacks, queues are the most the basic list types in computing\n\nA queue has a number of useful applications:\n- When a resource is shared by multiple consumers\n- Create directories\n- When data is transferring asynchronously between two resources\n\n","n":0.116}}},{"i":796,"$":{"0":{"v":"Linked List","n":0.707},"1":{"v":"\ninstead of using indexes or positions, Linked lists use a referencing system: elements are stored in nodes that contain a pointer to the next node, repeating until all nodes are linked.\n- Therefore, each node in a linked list contains a reference to the next node in the chain, in addition to its own data.\n\nThis system allows efficient insertion and removal of items without the need for reorganization.\n\nBest used when data must be added and removed in quick succession from unknown locations\n\nLinked lists do not use physical placement of data in memory, unlike arrays.\n\neven if you have millions of elements inside a list, the operation of adding a new element in the head or in the tail of the list is performed in constant time.\n\nThe `head` is a reference to the first node in the linked list. The last node on the list points to `null`.\n- Therefore, if the linked list is empty, the `head` points to `null`.\n\n### vs Arrays\nSimplistically, \"Use an Array for storing and accessing data, and LinkedList to manipulate data.\"\n\nNodes can easily be removed or added from a linked list without reorganizing the entire data structure. This is not the case with arrays, and adding/removing an element to any position of the array that is not the head or tail is non-trivial, and the whole array needs to be destroyed and created again. In circumstances such as this, linked lists add a lot of value.\n\nHowever, accessing nodes by position from a linked list is less efficient than with an array. If we consider that an array is just a contiguous row of memory blocks, then if we want to access `Array[5]`, then we just need to count up from the first block allocated to the array. With a linked list however, we need to manually traverse the chain until we reach the 5th node.\n\nAlso, search operations are slow in linked lists. Unlike arrays, random access of data elements is not allowed. Nodes are accessed sequentially starting from the first node.\n\n#### Deleting nodes\nIf we want to remove a non head/tail node from a linked list, the node *behind* the deleted node must update its pointer value to point to the node that the deleted node was previously pointing to:\n\nfirst -> second -> third\nfirst -> <s>second</s> -> third\nfirst -> third\n\nTo delete `second`, we must update `first` to now point to `third`.\n\n### Code\nThis is what the head node (ie. first node) of a 3-element linked list looks like:\n```js\nListNode {\n  data: 'First data',\n  next: ListNode {\n    data: 'Second data',\n    next: ListNode { data: 'Third data', next: null }\n  }\n}\n```\n\nIf a linked node were a class, it would look like this:\n```js\nclass ListNode {\n    constructor(data) {\n        this.data = data\n        this.next = null                \n    }\n}\n```\n\nAnd the LinkedList itself would look like:\n```js\nclass LinkedList {\n    constructor(head = null) {\n        this.head = head\n    }\n}\n```\n\nImplementing it would look like this:\n```js\nconst node1 = new ListNode('First data')\nconst node2 = new ListNode('Second data')\nconst node3 = new ListNode('Third data')\nnode1.next = node2\nnode2.next = node3\n\nconst list = new LinkedList(node1)\nconsole.log(list.head.next.next.data); // 'Third data'\n// notice how we repeat `next` to get to the subsequent node. If we only had it\n// once, it would have returned the second node.\n```\n","n":0.044}}},{"i":797,"$":{"0":{"v":"Heap","n":1},"1":{"v":"\nHeaps can thought of as regular binary trees with two special characteristics:\n1. Heaps must be Complete Binary Trees.\n2. The nodes must be ordered according to the Heap order property. Two heap order properties are as follows:\n    1. Max Heap Property: All the parent node keys must be greater than or equal to their child node keys in max-heaps. So the root node will always contain the largest element in the Heap. If Node A has a child node B, then, `key(A) >=key(B)key(A)>=key(B)`\n    2. Min Heap Property: In Min-Heaps, all the parent node keys are less than or equal to their child node keys. So the root node, in this case, will always contain the smallest element present in the Heap. If Node A has a child node B, then: `key(A) <=key(B)key(A)<=key(B)`\n\nHeaps are useful for sorting and implementing priority queues.\n\n\n","n":0.085}}},{"i":798,"$":{"0":{"v":"Hash Table","n":0.707},"1":{"v":"\nA hash table is an alternative to storing the type of data that we might find in an array. In a hash table, a large pool of memory is allocated, and a hash function is chosen that always returns values lying somewhere within the available memory. In order to store a value, the key is hashed, and the value is placed at the location given by the hash. To look a value up given a key, you just hash the key and get back the location of the corresponding value.\n\nOne complication of this method is that if two keys hash to the same value, the previous value would be overwritten. This is called a hash collision, and any hash table implementation must have a method of dealing with them.\n\nA hash table is an implementation of an associative array (also called a dictionary, map,symbol table or key-value store).\n\n## Purpose\nA hash table is one particular way of storing items that are associated with a key in such a way that the item can be looked up quickly using the key.\n- The key might be a person's name, an ID number, a directory path or anything else.\n\nLet's say you want to write a program, for example, that calculates how frequently each word is used in a book. You could use the word as the key, and the value associated with the key would be the number of times that word has been seen. If we wanted to access this information fast, we could store the results in a hash table, and make that data available for consumers. This could especially be useful if we need multiple clients need to access the same data.\n\nLet's say you want to store video game information in hash tables (like weapon stats). You could use hash tables to store the data for easy querying.\n- To give an idea of the scope of what could be included in a hash table, we might include weapon stats, the enemy being hit, your buffs, your accessories, etc. You formulaicly concatenate them together, like:\n```\n[weapon][buff1][buff2][accessory1][accessory2][accessory3][enemy][random]\n```\n\nand then you take the hash of that, and look up the resulting damage in a pre-computed table, indexed by that hash. That way, you're not spending in-game computation time adding, multiplying, subtracting, dividing, and etcetera for potentially dozens or hundreds of damage incidents each second, each of which has to be calculated instantly\n\n\nhash tables have been called \"the backbone of most programming\".\n\nHashing and trees perform similar jobs, but have different usefulness:\n- Trees are more useful when an application needs to order data in a specific sequence. \n- Hash tables are the smarter choice for randomly sorted data due to its key-value pair organization.\n\n## Composition\n\nHash tables are made up of two parts:\n1. *Object*: An object with the table where the data is stored. The array holds all the key-value entries in the table. The size of the array should be set according to the amount of data expected.\n2. *Hash function* (or mapping function): This function determines the index of our key-value pair. It should be a one-way function and produce the a different hash for each key.\n    - takes an item’s key as an input, assigns a specific index to that key and returns the index whenever the key is looked up.\n\n*Hashing* is the process of assigning an object into a unique index, known as a key. Each object is identified using a key-value pair, and the collection of objects is known as a dictionary.\n- Hash functions help to limit the range of the keys to the boundaries of the array\n\n## Implementation\n\nA hash table is implemented by storing elements in an array and identifying them through a key. A hash function takes in a key and returns an index, giving us the location of the data in the array.\n- whenever you input the key into the hash function, it will always return the same index, which will identify the associated element.\n\nIn JavaScript, hash tables are generally implemented using arrays as they provide access to elements in constant time.\n\nA hash table has a number of useful applications:\n- When a resource is shared by multiple consumers\n- Password verification\n- Linking file name and path\n\nSome common uses of hash tables are:\n- Database indexing\n- Caches\n- Unique data representation\n- Lookup in an unsorted array\n- Lookup in sorted array using binary search\n\nHash tables provide access to elements in constant time, so they are highly recommended for algorithms that prioritize search and data retrieval operations. \nHashing is ideal for large amounts of data, as they take a constant amount of time to perform insertion, deletion, and search.\n\ntime complexity is `0(1)`\n\nTo properly understand the difference between a map and an object (in JavaScript), we have to understand the difference between a concept and its implementation. For instance, we most think of a POJO when needing to implement a hash map in JavaScript. However, we can also use a Map object, and we can also just use an array. The implementation in all 3 cases are differ, but in the end of each, we end up with a hash map.\n","n":0.034}}},{"i":799,"$":{"0":{"v":"Deque","n":1},"1":{"v":"\nA Deque (pronounced \"Deck\") is a Double-ended queue, meaning elements can be added to or removed from either the front (head) or back (tail).\n\nThe basic operations on a deque are enqueue (add) and dequeue (remove) on either end.\n\nDeques are normally implemented with:\n- A modified dynamic array that can grow from both ends\n    - which has time-complexity of `O(1)` for all operations except insertion/deletion in middle, which is `O(n)`\n- A doubly linked list\n    - which has time-complexity of `O(1)`\n","n":0.113}}},{"i":800,"$":{"0":{"v":"Array","n":1},"1":{"v":"\nAn array is the most basic of all data structures. The implementation of its data storage is low-level, since adjacent memory blocks are used to store the elements. Therefore, to find elements we just need to count by memory blocks.\n\nAn array has a fixed number of cells that are allocated upon creation. This makes it expensive to \"insert\" or \"delete\" values, as the original array (at its original memory location) cannot easily be reused.\n\n### List vs Tuple\nArrays commonly are used to implement Lists and Tuples.\n- A list is where all elements have the same shape and the length is often unknown\n- A tuple has a fixed length and elements can be of different types","n":0.094}}},{"i":801,"$":{"0":{"v":"AST","n":1},"1":{"v":"\n## What is an AST?\nThink of an AST as a big nested object that holds metadata about the thing we are describing.\n- ex. we could generate an AST of a single Javascript file. The AST would have a field called `body`, which is an array of parts: \n  - variable declaration \n  - expression statement \n  \nImagine we have an extremely short file `file.js`:\n```js\nconst myname = 'jason'\nconsole.log(myname)\n```\n\nThe resulting AST of this file would be:\n```json\n{\n  \"type\": \"Program\",\n  \"start\": 0,\n  \"end\": 43,\n  \"body\": [\n    {\n      \"type\": \"VariableDeclaration\",\n      \"start\": 0,\n      \"end\": 22,\n      \"declarations\": [\n        {\n          \"type\": \"VariableDeclarator\",\n          \"start\": 6,\n          \"end\": 22,\n          \"id\": {\n            \"type\": \"Identifier\",\n            \"start\": 6,\n            \"end\": 12,\n            \"name\": \"myname\"\n          },\n          \"init\": {\n            \"type\": \"Literal\",\n            \"start\": 15,\n            \"end\": 22,\n            \"value\": \"jason\",\n            \"raw\": \"'jason'\"\n          }\n        }\n      ],\n      \"kind\": \"const\"\n    },\n    {\n      \"type\": \"ExpressionStatement\",\n      \"start\": 23,\n      \"end\": 42,\n      \"expression\": {\n        \"type\": \"CallExpression\",\n        \"start\": 23,\n        \"end\": 42,\n        \"callee\": {\n          \"type\": \"MemberExpression\",\n          \"start\": 23,\n          \"end\": 34,\n          \"object\": {\n            \"type\": \"Identifier\",\n            \"start\": 23,\n            \"end\": 30,\n            \"name\": \"console\"\n          },\n          \"property\": {\n            \"type\": \"Identifier\",\n            \"start\": 31,\n            \"end\": 34,\n            \"name\": \"log\"\n          },\n          \"computed\": false,\n          \"optional\": false\n        },\n        \"arguments\": [\n          {\n            \"type\": \"Identifier\",\n            \"start\": 35,\n            \"end\": 41,\n            \"name\": \"myname\"\n          }\n        ],\n        \"optional\": false\n      }\n    }\n  ],\n  \"sourceType\": \"module\"\n}\n```\n\nASTs are commonly used in compilers to parse the code we write, converting it into a tree structure that we can traverse programmatically.\n\nCreating lint rules is another area we can make use of ASTs. The AST enables us to recognize certain parts of the code and act accordingly with our rules. \n- ex. The `no-unused-vars` looks at all `VariableDeclaration` types and ensures that the variable names are used at least once elsewhere in the code.\n\nSyntax highlighting is made possible by ASTs\n\n# Tools\n[AST Explorer tool](https://astexplorer.net/) —multiple language support\n","n":0.059}}},{"i":802,"$":{"0":{"v":"Implementations","n":1}}},{"i":803,"$":{"0":{"v":"State Management","n":0.707},"1":{"v":"\n### Using a class to manage state\nConsider the [Robot delivery game](https://eloquentjavascript.net/07_robot.html#p_md/LJiyP4s), where we have a robot going around delivering parcels to different villages. For state, we need to track:\n1. Robot's current location\n2. The array of parcels (inc. current location, and destination)\n\nTo accomplish this, a class `VillageState` was used, which looks like this:\n```js\nVillageState {\n  place: 'Post Office',\n  parcels: [ { place: 'Post Office', address: \"Alice's House\" } ]\n}\n```\n\nIn taking this approach to state management, the next question to ask is: \"what methods should exist that will result in a new state to be computed?\". The answer here is the action of the robot moving from location to location, so we add a `move` method. The signature of this function is:\n- taking in the desired destination, and returns the new `VillageState`.\n\nTaking this approach lets us think of our program in terms of \"turns\". If two people were competing to see who could deliver all of the parcels in less turns, then conceptually we can think of each new computation of state (ie. an instance of `VillageState`) as being a turn.\n```js\nclass VillageState {\n  constructor(place, parcels) {\n    this.place = place;\n    this.parcels = parcels;\n  }\n\n  move(destination) {\n    if (!roadGraph[this.place].includes(destination)) {\n      return this;\n    } else {\n      let parcels = this.parcels.map(p => {\n        if (p.place != this.place) return p;\n        return {place: destination, address: p.address};\n      }).filter(p => p.place != p.address);\n      return new VillageState(destination, parcels);\n    }\n  }\n}\n```","n":0.066}}},{"i":804,"$":{"0":{"v":"Feature Flags","n":0.707},"1":{"v":"\n### Using Feature Flags while undergoing a DBMS transition\n\"What if we want to go from Mongo to Postgres, without having to make the switch all at once?\"\n- https://featureflags.io/feature-flags-database-migrations","n":0.189}}},{"i":805,"$":{"0":{"v":"Dark Mode","n":0.707},"1":{"v":"\n# UE Resources\n[Dark mode with Next.js](https://www.joshwcomeau.com/react/dark-mode/)\n","n":0.408}}},{"i":806,"$":{"0":{"v":"CSS","n":1}}},{"i":807,"$":{"0":{"v":"Drawer","n":1},"1":{"v":"\nHave your state management system keep track of whether the drawer is open or not. Use that state to determine the transform position of the drawer:\n```\ntransform: ${({ isOpen }) => isOpen ? 'translateX(0)' : 'translateX(-100%)'};\n```","n":0.169}}},{"i":808,"$":{"0":{"v":"Concurrency","n":1},"1":{"v":"\nConcurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or at the same time simultaneously partial order, without affecting the final outcome.\n\nOne thing you *never* want in computing is for 2 different programs to be accessing/writing to the same memory location at the same time. This results in file corruption.\n- This could result if 2 different programs are trying to write the same file at the same time.\n\nspec: a concurrent system is one where the processes know how to wait their turn to access resources. If a system was non-concurrent, it would mean that multiple processes would try and access the same data at the same time, and we would likely result in data corruption.\n\n### Resource Starvation\nResource starvation occurs in concurrent computing where a process is denied access to [[resources|dendron://tech/general.terms#resource]] that it needs to process its work\n\nWhen starvation is impossible in a concurrent algorithm, the algorithm is called starvation-free\n- aka *lockout-freed*, or said to have *finite bypass*\n\n### Liveness\nLiveness is the quality of a concurrent algorithm where multiple processes may have to \"take turns\" during the critical sections in order to continue to make progress. In this way, the program does not stall, and progress can still be made.\n\nLiveness guarantees are important properties in operating systems and distributed systems\n\n### Critical Section\nThe parts of a program where a shared resource is being accessed needs to be protected in a way that will prevent concurrent access from being possible. This protected section is called the critical section, and it cannot be executed by more than one process at a time.\n- think of the critical section as the traffic cop at the sensitive areas. It allows each process to know when it's their turn to access the data.\n\nTypically, the critical section accesses a shared resource, such as a data structure, a peripheral device, or a network connection, that would not operate correctly in the context of multiple concurrent accesses.\n\nA critical section is typically used when a multi-threaded program must update multiple related variables without a separate thread making conflicting changes to that data. In other words, each thread knows to wait its turn before manipulating and accessing the data.\n\nWhoever holds *the lock* is the process that is allowed to access the resource (ie. it's their turn)\n\n## Dining Philosophers Problem\nThe problem was designed to illustrate the challenges of avoiding deadlock, a system state in which no progress is possible\n![](/assets/images/2021-07-30-09-00-51.png)\n### rules:\nThe philosopher is either in thinking or eating state.\n- eating -> the philosopher has both forks in his hand\n- thinking -> everything else\n\nThe dining philosophers is an analogy for a program where the processses must alternate between one task and another (here, eating and thinking)\n\nThe goal is to create a program that would result in no philosopher having starved. This would be the case if the program were to deadlock because the line of execution was stalled.\n- if we are able to achieve this, then we will have created a **concurrent algorithm**\n\nA philosopher having starved represents **resource starvation**\n\nOnce the philosopher has finished their meal, they place down both forks so that they are available to others.\n\nThere are different approaches to solving the dining philosopher's problem\n1. Introduce a waiter who enforces that only one philosopher at a time may pick up 1 or 2 forks, meaning that the philosophers have to request and receive permission from the waiter before picking up the fork. This results in a system where only one philosopher at a time can pick up a fork.","n":0.041}}},{"i":809,"$":{"0":{"v":"Best Practices","n":0.707}}},{"i":810,"$":{"0":{"v":"Architecture","n":1}}},{"i":811,"$":{"0":{"v":"Workers","n":1},"1":{"v":"\nTask Queues/Worker Queues are used as a mechanism to distribute work across threads or machines.\nA task queue’s input is a unit of work called a task. Dedicated worker processes constantly monitor task queues for new work to perform.\n\nOften, a task queue will use a message broker to mediate between clients and workers\n- To initiate a task the client adds a message to the queue, the broker then delivers that message to a worker.\n\nTask Queues like Celery require a configured message broker like Redis broker transports or RabbitMQ to be functional\n- Realistically, you could roll your own using SQLite if you wanted to. You just need a service that will take care of the message queue for you.\n- Graphile-worker has this functionality built-in, as it manages its own queue in your Postgres db.","n":0.087}}},{"i":812,"$":{"0":{"v":"Webhooks","n":1},"1":{"v":"\n# Webhook\nA webhook is a callback living on a 3rd party server that sends an HTTP request upon some event.\n- ex. Twilio providing an Express server with SMS as they arrive to the Twilio server\nWebhooks are a way for an app to provide other applications with real-time information\n- ex. with Twilio, whenever their servers receieve an SMS, they will call the webhook endpoint that we provide to them. their webhook is a POST request, that will match up with a POST route that we define on our app server.\n- The key difference with a webhook is the use of callbacks. In traditional APIs, we manually have to poll for data in order to get a real-time experience. \n- Webhooks can be thought of as \"reverse APIs\", as they give you the API spec, and you must design the API for them to use.\n\t- Put another way, the webhook makes an API request to your app, which you must handle.\n- To make the connection, the webhook provider (ex. Twilio) must be provided with the address that it will send the requests to.\n\t- Therefore, your app must have a publicly accessible url for the webhook to work (ie. localhost will not work)\n\nMetaphorically, webhooks are like a phone number that Stripe calls to notify you of activity in your Stripe account\n- The webhook endpoint is the person answering that call who takes actions based upon the specific information it receives.\n\nA webhook on our server says \"hey, I want you to call this number (`/webhook` url) when *this* event happens\"","n":0.063}}},{"i":813,"$":{"0":{"v":"Pubsub","n":1},"1":{"v":"\n## Overview\nsenders (publishers) are not programmed to send their messages to specific receivers (subscribers). Rather, published messages are characterized into channels, without knowledge of what (if any) subscribers there may be. Subscribers express interest in one or more channels, and only receive messages that are of interest, without knowledge of what (if any) publishers there are. This decoupling of publishers and subscribers can allow for greater scalability and a more dynamic network topology.\n\nPub-Sub is a paradigm for getting real-time updates to a client. It is unlike webhooks, which would require the publisher of information to explicitly call someone who has \"signed up\" to receive it. Instead, with Pub-Sub the publisher determines that there is a certain set of information that it wants to send out, and it specifies a container (the topic) that it will be sent to. This leaves the topic open to be subscribed to, which a subscriber (a client) may do. The subscriber specifies that it is interested in receiving any new messages that arrive in a specific Topic (by name), and when new messages arrive, they are sent to the client.\n- This topic name would be scoped for a specific user, for instance we might call it `graphql:user:${userId}`\n\nA common example of pub-sub are push notifications on mobile devices, since information preferences are expressed in advance by the user.\n- A client \"subscribes\" to various information \"channels\" provided by a server; whenever new content is available on one of those channels, the server pushes that information out to the client.\n\nA subscriber receives messages either by Pub/Sub pushing them to the subscriber's chosen endpoint, or by the subscriber pulling them from the service. Upon successful receiving of the message, the subscriber informs that it was received successfully.\n\nPub-sub is a messaging pattern whereby senders of messages (publishers) do not program the messages to be sent to the receivers (subscribers). Instead, the publishers categorize published messages into classes, without knowing who the subscribers are that will ultimately consume those messages\n- Similarly, subscribers express interest in one or more classes and only receive messages that are of interest to them (without any knowledge about who the publishers are)\n- This above explanation shows that the Pub-Sub paradigm results in the publisher service and the subscriber service being decoupled from one another.\n\nAn RSS feed might be a good analogy for the Pub-Sub paradigm, if we consider that we don't really care where each post/article came from. We just get them all lumped together, and consume them there, without any real consideration to where they came from. The process is decoupled.\n\nCommunication may be:\n1. 1:many (fan-out)\n\t- or, many clients subscribing to one topic (ie. multiple subscriptions to 1 topic)\n2. many:1 (fan-in)\n\t- or, one client subscribing to many topics (ie. multiple subscriptions to multiple topics)\n3. many:many\n![PubSub many:1 vs 1:many](/assets/images/2021-03-24-10-42-17.png)\n\nThe pub-sub pattern provides greater network scalability and a more dynamic network topology, with a resulting decreased flexibility to modify the publisher and the structure of the published data.\n\n## Core concepts\n### Topic\n- messages are sent to resources, which we call Topics.\n- topics have names that uniquely identify them\n\t- ex. if we are subscribing to the event \"user clicks this button\", then the topic might be named `\n- the topics live on a server controlled by the publisher\n\t- ex. Postgraphile server, Google Pub/Sub\n\n### Subscription\n- A subscription represents the stream of messages from a single specific topic.\n\n### Relation to Message Queue paradigm\nthe Pub-Sub paradigm is a sibling of the \"Message Queue\" paradigm.\n- Most messaging systems support both the Sub-Pub and Messaging Queue paradigms in their API.\n\n### Relation to Job Queues\nJob Queues only let one \"subscriber\" watch for new \"events\" at a time, and keep a queue of unprocessed events.\n- ex. Celery\n\nIt turns out that Postgres generally supersedes job servers as well. You can have your workers \"watch\" the \"new events\" channel and try to claim a job whenever a new one is pushed. As a bonus, Postgres lets other services watch the status of the events with no added complexity.\n\n### Examples\n- Kafka\n- RabbitMQ\n- Redis PUB/SUB","n":0.039}}},{"i":814,"$":{"0":{"v":"Offline First","n":0.707},"1":{"v":"\nTo make an app truly offline first, we primarily need to do two things:\n1. Any code and assets used should be available offline\n2. Any data changes should be made locally first and then synced to the cloud.\n\n### using Apollo and resolver to serve offline-first\n- the local cache can be used to remember what queries resolved to. The resolver helps the cache remember which updates it made. Therefore if the app is offline, the cache will be able to remember updates that were made to the cache, so that up-to-date information can be returned \n![](/assets/images/2021-03-28-19-52-42.png","n":0.103}}},{"i":815,"$":{"0":{"v":"Message Broker","n":0.707},"1":{"v":"\n# Overview\nIn a typical message-queueing implementation, a developer installs and configures message-queueing software (a queue manager or broker), and defines a named message queue. Or they register with a message queuing service.\n- An application then registers a software routine that \"listens\" for messages placed onto the queue.\n- Second and subsequent applications may connect to the queue and transfer a message onto it.\n- The queue-manager software stores the messages until a receiving application connects and then calls the registered software routine. The receiving application then processes the message in an appropriate manner.\n\n## Message Broker\nA message broker is an intermediary computer program module that translates a message from the formal messaging protocol of the sender to the formal messaging protocol of the receiver.\n\n### Examples\n- Kafka\n- RabbitMQ\n- AWS Kinesis\n- Google Cloud Pub/Sub\n- Redis (not a message broker per se, but rather has messaging brokering as one of its capabilities)","n":0.083}}},{"i":816,"$":{"0":{"v":"Edge","n":1},"1":{"v":"\n# Edge Compute\nEdge compute is an architecture where compute and storage resources are decentralized and distributed\n- for example it could mean compute environments are distributed all over the globe, located on every oil rig owned by an oil company or put into every truck in a fleet of food delivery vehicles.\n\nThe goal of edge compute is to minimize the amount of raw, unprocessed data sent to/from applications operating at edge locations (think CCTV cameras, or digital signage) and to reduce the time it takes for instructions to be sent from a central server to devices located at the edge\n- In an edge environment you can have sensors and devices talking directly to applications running on the network, with the application processing information in real time","n":0.089}}},{"i":817,"$":{"0":{"v":"Client Server","n":0.707},"1":{"v":"\nThe frontend aims to be stateful (that is, keep track of the state between requests). If the frontend wasn’t stateful, you would have to log in every time you navigated to a new page.\nThe backend, however, aims to be stateless. This means that the state must be provided on every new invocation. For instance, the API does not keep track of whether you are logged in or not. It determines your authentication state by reading the token in your API request.\n- If you used a Redux store on your Node.js server, the state would be cleared every time the node process stops\n- It becomes even more involved when you consider scaling. If you were to scale your application horizontally by adding more servers, you’d have multiple Node processes running concurrently, and each would have their own version of the state. This means that two identical requests to your backend at the same moment could easily get two different responses.","n":0.079}}},{"i":818,"$":{"0":{"v":"MVC","n":1},"1":{"v":"\n## MVC\nEven with the so-called MVC design pattern, there is some variation between the traditional MVC pattern and its modern interpretation in various programming languages. For example, some MVC–based frameworks will have the view observe the changes in the models, while others will let the controller handle the view update.\n\n### Model\n- Models represent knowledge. A model could be a single object (rather uninteresting), or it could be some structure of objects.\n    - There should be a one-to-one correspondence between the model and its parts on the one hand, and the represented world as perceived by the owner of the model on the other hand.\n- The classes which are used to store and manipulate state, typically in a database of some kind.\n- The model retrieves and populates the data.\n- When a model changes, typically it will notify its observers that a change has occurred.\n\n### View\n- A view is a visual representation of its model. \n    - It highlights certain attributes of the model and suppress others. It is thus acting as a presentation filter.\n\n### Controller\nA controller is the link between a user and the system, or the glue between the view and the model. \n- The controller updates the view when the model changes. It also adds event listeners to the view and updates the model when the user manipulates the view.\n\nIt provides the user with input by arranging for relevant views to present themselves in appropriate places on the screen. It provides means for user output by presenting the user with menus or other means of giving commands and data. The controller receives such user output, translates it into the appropriate messages and pass these messages on to one or more of the views.\n- The brains of the application. The controller decides what the user's input was, how the model needs to change as a result of that input, and which resulting view should be used.\n- job is to provide a bit of orchestration between Models and Views\n","n":0.055}}},{"i":819,"$":{"0":{"v":"Algorithms","n":1},"1":{"v":"\n#### What may cause an algorithm to not be deterministic?\n1. The algorithm uses external state, that is not passed in as an argument to the function\n    - ex. global variable, hardware timer value, random value etc.\n2. Timing is sensitive. If the algorithm is run two times, is there a possibility that different processes of the algorithm finish in different orders (e.g. if there are multiple processes)\n3. hardware error causes its state to change in an unexpected way.\n\nAlthough real programs are rarely purely deterministic, it is easier for humans as well as other programs to reason about programs that are.\n\nmost programming languages and especially functional programming languages make an effort to prevent the above events from happening except under controlled conditions\n\n### Making an algorithm that is open-ended\nConsider a function that counts occurrences in an array\n```js\nconst countVowels = (str) => {\n    const vowels = { a: 0, e: 0, i: 0, o: 0, u: 0 }\n\n    for (let i = 0; i < str.length; i++) {\n        const currentChar = str[i].toLowerCase()\n        if (Object.keys(vowels).includes(currentChar)) {\n            vowels[currentChar]++\n        }\n    }\n    return Object.values(vowels).reduce((acc, sum) => acc + sum)\n}\n```\n\nConceptually, this could be made a bit simpler, because we don't actually care about the breakdown of each vowel. In other words, we don't really care if there are 2 `a`s, or 3 `e`s... All we care about is how many there are in total. Therefore, we could have just iterated on the string and counted the vowels and returned that value. However, there is a benefit to having put the results of the vowel count into an object. It may be a reasonable assumption that the breakdown of values is of value (or future value) to us. Building our algorithm this way makes it a simple adjustment to include the breakdown, and doesn't really add much overhead.","n":0.058}}},{"i":820,"$":{"0":{"v":"Space and Time Complexity","n":0.5},"1":{"v":"\n## Space vs Time\nspace can be reused, while time can't.\n\"Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform.\"\n\n### Space complexity\na function describing the amount of memory that an algorithm takes in terms of input to the algorithm.\n- Space complexity is sometimes ignored because the space used is minimal and/or obvious, but sometimes it becomes as important an issue as time.\n\nex. we might say \"this algorithm takes constant extra space,\" because the amount of extra memory needed doesn't vary with the number of items processed.\n\n### Time complexity\na function describing the amount of time an algorithm takes in terms of the amount of input to the algorithm.\n- \"Time\" can mean...\n\t- the number of memory accesses performed,\n\t- the number of comparisons between integers,\n\t- the number of times some inner loop is executed,\n\t- or some other natural unit related to the amount of real time the algorithm will take.\n- We try to keep this idea of time separate from \"wall clock\" time, since many factors unrelated to the algorithm itself can affect the real time\n\nex. we might say \"this algorithm takes n2 time,\" where n is the number of items in the input.\n\n## Ramen noodles/stu analogy\nanal: consider that if making one cup of ramen noodles takes 3 min, making 5 cups would take 15 min (3x5). Conversely, if I was going to make a beef stu, if one serving takes 50 min, 5 servings will still be roughly 50 min. This is the concept of time. Now consider, that ramen has a 1:1 relationship with space. In other words, if I am filling up a shopping cart, the amount of ramen that takes up space is directly proportional to the number of serving I have to make. 5 servings = 5 cups of ramen noodles. On the other hand, stu for 1 person or 5 will have a less linear relationship. This is the concept of space.\n- We want to build algorithms that are more like the stu.\n![](/assets/images/2021-08-06-21-40-26.png)\n\nAnd in mathematical terms...\n![](/assets/images/2021-08-06-21-44-09.png)\nThe above shows how the left side of the equation is time/space, and the right side is the O Notation\n\nTime and Space Complexity at this level describe the relationship between Inputs and rate of operations/space taken\n\n\n## Big O\nBased on the above, ramen noodles have a time complexity of `O(n)`. In other words, their \"growth\" rates are linear, and each additional operator will add an a period of time that is equal to the average durations of each operator.\n\nCases (listed from best to worst):\n\n### Constant\nIt takes at most the same amount of time or space for one or man\n![](/assets/images/2021-08-06-21-53-20.png)\n\n### Logarithmic\ntime and space complexity increases at first but then it stabilizes and changes less\n![](/assets/images/2021-08-06-21-52-38.png)\n\n### Linear\nhere time and space increase at worst at a fixed/liner rate :\n![](/assets/images/2021-08-06-21-53-54.png)\n\n### Quadratic & Polynomial\nIn general as your inputs increase your, time becomes greater at a certain greater rate which is related to the exponent, ^2 for quadratics, ^K for Polynomials and everything in between:\n![](/assets/images/2021-08-06-21-54-20.png)\n\n### Exponential\nrapidly increases until time and space requirements tend towards the infinite.\n![](/assets/images/2021-08-06-21-55-00.png)\n\n# Resources\n[Find O-notations of common things, like arrays, objects etc](https://www.bigocheatsheet.com/)","n":0.044}}},{"i":821,"$":{"0":{"v":"Practice","n":1},"1":{"v":"\n### Sorted indexes\nGiven an array, sort the elements from the largest to the smallest. Create a result array using the indices of the original array. \n- Example: arr = [4,4,4,10,6,6,5]\n- res = [3,4,5,6,0,1,2]\n\n### Missing results from graphql query\nGiven a graphql-like query string and an object, return an array of paths that exist in the graphql-like string and don't exist in the object.\n- Example\ngiven:\n```\n{\n    age\n    name {\n        first\n        last\n    }\n    parent {\n        name {\n            first\n            last\n        }\n        age\n    }\n    contact {\n        phone\n        email\n    }\n}\n```\n\nand \n```js\n{\n    name: {\n        first: 'joe'\n    },\n    contact: {\n        email: 'joe@joe.joe',\n        phone: 7788713377\n    }\n}\n```\n\nShould return\n```ts\n[\n    'age',\n    'name.last',\n    'parent.name.first',\n    'parent.name.last',\n    'parent.age',\n]\n```","n":0.099}}},{"i":822,"$":{"0":{"v":"Patterns","n":1}}},{"i":823,"$":{"0":{"v":"Two Pointers","n":0.707},"1":{"v":"\nUseful for problems where we deal with sorted arrays (or LinkedLists) and need to find a set of elements that fulfill certain constraints\n- the set of elements could be a pair, a triplet or even a subarray\n\n### Example: find sum of pairs in array\nGiven an array of sorted numbers and a target sum, find a pair in the array whose sum is equal to the given target.\n\nSince the array is sorted, we can just move inwards from the ends of the array, comparing the sums to the `target` until we find the match. We could alternatively brute-force this by using a nested for-loop and iterating over each element twice, but since the array is sorted, we can leverage this fact to be smart about which comparisons we make\n![](/assets/images/2021-10-10-11-45-01.png)\n\n```js\nfunction findSumPairs(arr, target) {\n    let frontIndex = 0\n    let backIndex = arr.length - 1\n\n    let tempSum\n    while (frontIndex !== backIndex) {\n        tempSum = arr[frontIndex] + arr[backIndex]\n        if (tempSum < target) {\n            frontIndex++\n        } else if (tempSum > target) {\n            backIndex--\n        } else {\n            return true\n        }\n    }\n    return false\n}\n```","n":0.075}}},{"i":824,"$":{"0":{"v":"Sliding Window","n":0.707},"1":{"v":"Time Complexity\nThe time complexity of this pattern is `0(N)`\n\nSpace Complexity\nThe algorithm runs in constant space `O(1)`\n\n### Calculate averages\nThis function takes in an array, and sums the first `k` elements, computes its average then sticks it in the array. Then it sums the next `k` elements, computes average, adds to an array, and so on until the end of the array. Consider that we could brute-force this by having each iteration take a sum then average of each `k` sized window. However this has a major inefficiency, which is that the same elements are counted multiple times needlessly.\n\nInstead, our approach here lets us leverage previously summed up numbers by simply removing the start of the window (`frontIndex`) and adding the end of the window (`backIndex`) on each iteration.\n![](/assets/images/2021-10-10-11-11-22.png)\n```js\nconst calculateAverages = (arr, size) => {\n    let backIndex = size - 1\n    let frontIndex = 0\n    const averageResults = []\n    let sum = arr\n        .slice(frontIndex, backIndex + 1)\n        .reduce((acc, val) => acc + val)\n\n    while (backIndex < arr.length) {\n        averageResults.push(sum / size)\n        \n        // remove the front of the window, and add the back of the window\n        sum = sum - arr[frontIndex] + arr[backIndex + 1]\n\n        frontIndex++\n        backIndex++\n    }\n    return averageResults\n}\n\nconst arr = [1, 3, 2, 6, -1, 4, 1, 8, 2]\nconst k = 5\n\nconst averages = calculateAverages(arr, k)\n// [ 2.2, 2.8, 2.4, 3.6, 2.8]\n```\n","n":0.067}}},{"i":825,"$":{"0":{"v":"Approach","n":1},"1":{"v":"\n### Approach for algorithmic problems\nExample: implement a method on a LinkedList class that will delete a node from that list, given a position.\n\n#### Have a clear understanding of what the resulting data will be after the algorithm is run.\nFirst, we have to understand what does it mean to delete an item from a linked list. In technical terms, it means to change the pointer of the previous node to point to the node after the one being deleted. If we had a linked list of 4 elements:\n\nfirst -> second -> third -> fourth\n\nand wanted to remove the node at the second index:\n\nfirst -> second -> fourth\n\nWe would need to modify the second node's `next` value to point to the fourth element. This is effectively deleting a node from a linked list. If this is not clear to us, then it is impossible to write an algorithm that solves the problem.\n\n\"A problem well put is a problem half-solved.\"\n\nA way to fulfill this step might be to satisfy some test cases. For example...\n- If the list does not have a head, then return early, since there are no elements to remove\n- If we want to delete the node at position 0 (the head), then we need to assign a new head. \n    - Therefore, after the removal does `list.head` properly point to the second node instead of the first?\n- If we want to delete any other node, we need to change the `next` value of the previous node so that it refers to the node that was after the deleted node.\n\n### Tips\n- Best starting point might be the end-interface. This answers the question \"how would my code be used by others?\". If you are implementing a Binary Search Tree, then you probably want users to be able to add an element something like this: `tree.insert(32)`. From there, work backwards.\n- Any time you have to map over an array and you have to *remember* something about each iteration, consider using a map/object as a way to remember\n    - ex. The problem \"loop over array, sort it, and return an array of the original indices\" requires us to remember which index the number was at before we sort it. We can solve this by making an object where the key is the original index, and the value is the value at that index. From there, we can sort it and do whatever else we want with it, because we have already \"remembered\" that mapping between the value and its index.","n":0.049}}},{"i":826,"$":{"0":{"v":"Game","n":1}}},{"i":827,"$":{"0":{"v":"Engine","n":1},"1":{"v":"\nthe core part of a game engine is a renderer, a physics system, and other more complicated things like AI.\nGames that run on the same engine will have similar physics and Artificial intelligence, though those values can still be tweaked by developers\n\nif a development team desires to make a game using two different engines, but using the same assets and textures, the game will look exactly the same. The engine's job is to load the images created by the artists.\n- consider that Gimp, windows media player, photoshop all load an image and look exactly the same, even though they're not the same program.\n\n### Making your own?\na game engine is just a bunch of convenience classes and structural decisions for common tasks in games.\nIf you want to do it without an engine, you have to write all those classes and make those decisions yourself.","n":0.084}}},{"i":828,"$":{"0":{"v":"Game Development","n":0.707},"1":{"v":"\n## Production process\n### Pre-production\n- studios produce a design document; a single source of truth for its creative direction (for a film, this would be roughly adjacent to a script or storyboard)\n    - includes information about the story, gameplay, art direction, intended target audience and accessibility.\n- purpose is to iterate quickly while costs to do so are low. This is no different than designing a website mocks/designs/user testing before actually building the website.\n### Production\nRoles involved in production (overseen by the Producer):\n![](/assets/images/2021-08-18-14-27-37.png)\n\n### Post-production\nproject is evaluated, edited, polished, and fixed.\n- includes alpha and beta testing\n\n## Development team\n### Designer\n- conceives of gameplay and the structure/rules of the game\n- often the main visionary of the game\n- One of the roles of a designer is being a writer, often employed part-time to conceive game's narrative, dialogue, commentary, cutscene narrative, journals, video game packaging content, hint system, etc.\n- they are responsible for various parts of the game, such as, game mechanics, user interface, characters, dialogue, graphics, etc\n\n### Artist\n- designs video game art\n- An artists work may be 2d oriented or 3D oriented\n- 2D artists may produce concept art, sprites, textures, environmental backdrops or terrain images, and user interface. 3D artists may produce models or meshes, animation, 3D environment, and cinematics. Artists sometimes occupy both roles.\n\n### Programmer\nThe programmer develops video games or related software (such as game development tools).\n\nIndividual programming disciplines roles include:\n- Physics – the programming of the game engine, including simulating physics, collision, object movement, etc.;\n- AI – producing computer agents using game AI techniques, such as scripting, planning, rule-based decisions, etc.\n- Graphics – the managing of graphical content utilization and memory considerations; the production of graphics engine, integration of models, textures to work along the physics engine.\n- Sound – integration of music, speech, effect sounds into the proper locations and times.\n- Gameplay – implementation of various games rules and features (sometimes called a generalist);\n- Scripting – development and maintenance of high-level command system for various in-game tasks, such as AI, level editor triggers, etc.\n- UI – production of user interface elements, like option menus, HUDs, help and feedback systems, etc.\n- Input processing – processing and compatibility correlation of various input devices, such as keyboard, mouse, gamepad, etc.\n- Network communications – the managing of data inputs and outputs for local and internet gameplay.\n- Game tools – the production of tools to accompany the development of the game, especially for designers and scripters.\n\n### Level designer\n- a person who creates levels, challenges or missions for video games using a specific set of programs. These programs may be commonly available commercial 3D or 2D design programs, or specially designed and tailored level editors made for a specific game (like Wowedit)\n\n## Domain\nAs far as the actual development of the game goes, there are 2 main routes: programming and designing.\n\n### Design\n- 3D artist: create 3D objects and backgrounds; optimize graphics performance\n- 2D artist: create 2D objects and backgrounds; optimize graphics performance; create storyboards and concept images\n- Lighting artist: develop optimized lighting effects; create post-processing effects such as bloom, ambient occlusion, depth of field, and color correction\n- Audio designer: create and implement sound effects, voice overs, and music\n- Character artist: sculpt and model characters and their assets such as hair, accessories, armor, and weapons\n- Technical artist: work with artists and programmers to implement their work within the final product\n- Visual Effects artist: create realistic and stylized visual effects, such as lava, smoke, and fire; implement physical properties such as collisions and falling\n- Animator: create and optimize high-quality animation of all moving characters and objects; ensure that lighting and visual effects operate well in motion\n- Game / Level Designer: construct and implement gameplay; conduct playtesting and implement feedback; validate playability with the testers, level artists, and gameplay programmers\n\n### Programming\n- Quality assurance technician: find and report bugs; write test cases and plans\n- Programmer analyst: write code based on documented design; modify applications to maintain functionality\n- C# developer: develop new features and applications; manage the release of products \n- Unity engineer: develop new features and applications using Unity\n- Research engineer: design, implement, analyze, and optimize state-of-the-art algorithms\n- Software developer: design, develop, test, and release new features and tools\n- VR / AR developer: prototype and build next generation VR/AR products; deploy and maintain VR/AR applications; stay current on the latest trends in VR/AR\n- Mobile developer: develop new mobile features and applications; stay current on mobile technologies \n- UI / interface programmer: code user interfaces to meet the intent of the project's design and flow; integrate UI systems into features\n- Gameplay programmer: analyze and understand the game design documents; develop gameplay systems such player’s action, character’s behavior, game elements, and game progression\n\n* * *\n\nOnly top 20 percentile of games make profit\n\n# Resources\n[Project design document](https://connect-prd-cdn.unity.com/20190524/19ad3c2b-506e-46c9-9700-07180536a9d2_Project_Design_Doc__PDF_.pdf)\n[Unity design document tutorials](https://learn.unity.com/tutorial/lab-1-personal-project-plan?uv=2020.3&courseId=5cf96c41edbc2a2ca6e8810f&projectId=5caccdfbedbc2a3cef0efe63)","n":0.036}}},{"i":829,"$":{"0":{"v":"Fs","n":1}}},{"i":830,"$":{"0":{"v":"Virtual","n":1},"1":{"v":"\n# Virutal FS (VFS)\n- a component of the kernel\n- handles all system calls related to files and file systems\n- serves as an interface between a user and a particular file system\n\t- In other words, it abstracts away the specific filesystem implementation and let's us access it on a command line.\n\t- it accomplishes this by specifying an interface (a contract) between the kernel and the underlying FS\n- we interact with the underlying FS by using the API provided by the VFS. \n- this abstraction allows us to bridge the differences between Windows filesystems, Mac filesystems, and Unix filesystems\n- when an external device attached to the system (such as a USB stick), Unix can run the `mount` command, which will create a new directory on the VFS.\n","n":0.089}}},{"i":831,"$":{"0":{"v":"Network","n":1},"1":{"v":"\n## Network FS\n- a filesystem that is distributed across a network\n\t- distributed means that the FS does not share block level access to the data, instead opting for a network protocol (likely IP)\n- the NFS mounts directly into the file system on mount points, so we can use regular unix commands like `cp`, `ls` etc.\n- ex. a NAS exposes its data via a NFS\n","n":0.125}}},{"i":832,"$":{"0":{"v":"Disk","n":1},"1":{"v":"\n# Disk Filesystem\ndisk file systems are file systems which manage data on permanent storage devices. As magnetic disks are the most common of such devices, most disk file systems are designed to perform well in spite of the seek latencies inherent in such media.","n":0.151}}},{"i":833,"$":{"0":{"v":"Ntfs","n":1},"1":{"v":"\n# NTFS\nContrast with [[fat|fs.fat]]\n- unlike FAT, NTFS supports permissions, a change journal, and other features\n- Mac can only read NTFS, not write. \n","n":0.209}}},{"i":834,"$":{"0":{"v":"Fat","n":1},"1":{"v":"\n## FAT (File Allocation Table)\n- the FAT filesystem uses an index table stored on the device to identify chains of data storage that are associated with a single file. \n- the table is a linked list of entries for each cluster\n\t- a cluster is an atomic unit of space on a hard disk that can be allocated to hold files.\n\t\t- Storing small files on a filesystem with large clusters will therefore waste disk space (slack space)\n\t- being a linked list, each entry contains either the number of the next cluster in the file, or else a marker indicating the end of the file, unused disk space, or special reserved areas of the disk.\n\t\t- also, the root directory of the disk contains the number of the first cluster of each file in that directory\n\t\t- this enables the operating system to traverse the FAT, looking up the cluster number of each successive part of the disk file as a cluster chain until the end of the file is reached\n\t\t- Sub-directories are implemented as special files containing the directory entries of their respective files.\nFAT is a legacy system, and is supported for backward-compatability reasons\n","n":0.072}}},{"i":835,"$":{"0":{"v":"Exfat","n":1},"1":{"v":"\n# exFAT\nlightweight file system like FAT32, but without the extra features and over head of NTFS\n- not as compatible as plain FAT, but much more compatible than NTFS\n- Assuming every device we may want to mount to is compatible with exFAT, this is the filesystem we should probably use.\n","n":0.143}}},{"i":836,"$":{"0":{"v":"Firebase","n":1},"1":{"v":"\n# Google Cloud\n## Terminology\n### Reference \n- a locally existing pointer that points to a location within the cloud where data is stored. The client (ex. mobile app) can interact with the reference in order to interact (CRUD) with the database.\n\n### Task \n- an object that is returned from calling a CRUD operation on a `reference` (ex. `.putFile()`. Holds info similar to `res` object in Express.js\n\n### Cloud functions\n- A service from Google that allows you to run snippets of code within their own infrastructure. If you were to build your own server, you would simply execute these functions in your backend code. Normally they are executed in response to some event. Since Firebase is a Backend as a Service, we don't have this ability. Cloud functions allow us to fulfill this need.\n- The functions can also be fired directly with a simple HTTP request.\n- Normally however, we have an event provider (the origin of the event) such as Firestore that will wait on a certain event to occur \n\t- ex. we can execute functions in response to databse writes, user creation etc.\n- can be used as a webhook - Via a simple HTTP trigger, respond to events originating from 3rd party systems like GitHub, Slack, Stripe, or from anywhere that can send HTTP requests.\n\n#### Features\n- these functions are stateless, meaning that if we were trying to make a counter function that holds the current value, we would get unpredictable results. This is because multiple instances of the same function will be created depending on how many people are using your app and executing that function. Therefore, all storage needs must be delegated to some other Google cloud service (like Firestore) \n\n# Firebase\n- When you create a new Firebase project in the Firebase console, you're actually creating a Google Cloud Platform (GCP) project behind the scenes\n\t- You can think of a GCP project as a virtual container for data, code, configuration, and services. A Firebase project is a GCP project that has additional Firebase-specific configurations and services (put another way, a firebase project is a wrapper around a GCP project). \n\t- therefore, a Firebase project ***is*** a GCP project. Everything that is possible in GCP is also possible in Firebase.","n":0.052}}},{"i":837,"$":{"0":{"v":"Filter","n":1}}},{"i":838,"$":{"0":{"v":"Clause","n":1}}},{"i":839,"$":{"0":{"v":"Filter","n":1},"1":{"v":"\n# FILTER\n- def - adds a filter to an aggregate function\n- ex. if we are making a json array with `json_agg`, we can filter on anything that would appear in that json\n\t- ex. `filter (where \"bucket\".id is not null)`","n":0.16}}},{"i":840,"$":{"0":{"v":"Express","n":1},"1":{"v":"\n- Express is an HTTP server\n- It is built on top of another framework callet Connect\n- express applications are request handler functions that you pass to http or http Server instances\n\n![](/assets/images/2021-03-07-22-10-51.png)\n\n\n# ER\n[Using /services to encapsulate business logic](https://www.coreycleary.me/should-one-express-controller-call-another/)\n- [also](https://www.coreycleary.me/why-should-you-separate-controllers-from-services-in-node-rest-apis/)","n":0.162}}},{"i":841,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\n# UE Resources\nhttps://sammeechward.com/testing-an-express-app-with-supertest-and-jest/\n","n":0.577}}},{"i":842,"$":{"0":{"v":"Router","n":1},"1":{"v":"\n## Router\n- A router is essentially just a container for a set of middleware, grouped by the fact they all have to do with http methods and routes\n- The router is an isolated (meaning it operates independently of other routers) instance of middleware and routes. Therefore it can only perform middleware and routing functions.\n- The router can be thought of as a mini-application\n- A route is a combination of a path and a callback (called the route-handler)\n","n":0.114}}},{"i":843,"$":{"0":{"v":"Objects","n":1}}},{"i":844,"$":{"0":{"v":"App","n":1},"1":{"v":"\nThe app object denotes the Express application.\n\nThe `app` object has methods for:\n- Routing HTTP requests\n    - ex. `app.get()`, `app.param()`\n- Configuring middleware\n    - `app.route()`\n- Rendering HTML views\n    - `app.render()`\n- Registering a template engine\n    - `app.engine()`\n\n### Express variables\n- we can set and get variables that are available throughout express with `app.set` and `app.get`\n- Express has the concept of the \"app settings table\", which is essentially a list of key-value pairs for configging Express that we can manipulate with `app.set`\n\n### Local properties\nThe `app.locals` object has properties that are local variables within the application.\nOnce set, the value of `app.locals` properties persist throughout the life of the application, \n- can be accessed on `req.app.locals`\n\nin contrast with res.locals properties that are valid only for the lifetime of the request.\n","n":0.09}}},{"i":845,"$":{"0":{"v":"Middleware","n":1},"1":{"v":"\n# Middleware\nMiddleware is a stack of functions that gets called in a chain. However, the next function in that chain does not need to get called necessarily. The middleware that is currently handling the request decides if the request will complete right then and there, continue on to the next middleware in the stack with `next()`, or die instantly.\n- MWs are the key to flexibility and modularity in Express\n- A MW is a function that occurs in the lifecycle of a http request before the request hits the server, or before the response gets to the client\n- These functions have access to the request and response objects, and can modify/read however they want\n- When we add middleware to express with `app.use()`, we are appending items to `Server.prototype.stack` in Connect. When the server receives a request, it iterates over the stack, calling the `(req, res, next)` method.\n- Recall that each middleware item will either modify the request object, modify the response object, or call `next()` so the next middleware in the stack is called (?)\n- If a middleware does *not* modify the body or the header of the response, it should call `next()`. If it does, it should not call `next()`\n- If the current mw function does not end the request-response cycle, it must call `next()` so the next mw in the stack can take over.\nTechnically, Express itself is just middleware that occurs before the node.js server\n\n## Types of middleware\n### Application level\nusing `app.use()`, we can bind a piece of application-level MW to the app object\n- `app.use` will be called each time a request is sent to the server\n### Router level\nworks the same as application-level, except the middleware is bound to an instance of `express.Router()` instead of `express()`\n### Error handling\n### Built-in\nex. `express.static`, `express.json`\n","n":0.059}}},{"i":846,"$":{"0":{"v":"Body Parser","n":0.707},"1":{"v":"\n## body-parser\n- when we don't use body-parser, we get the raw request in the request body. In this format, the `body` and `headers` keys are not on the root object of the request parameter (ie. they are nested). This means that we must individually manipulate all the fields","n":0.144}}},{"i":847,"$":{"0":{"v":"Controller","n":1},"1":{"v":"\n## Controllers\n- define the actions to be taken when given a route\n- Performs the database interactions (using Mongoose or an SQL equivalent)\n- Determines the response that the server will give","n":0.183}}},{"i":848,"$":{"0":{"v":"Eslint","n":1}}},{"i":849,"$":{"0":{"v":"Custom","n":1},"1":{"v":"\n### Processors\nAny time we want to make a lint rule for a non-`.js` file, we need to include a processor \n\nA processor is composed of a preprocessor and a postprocessor.\n\n#### Preprocessor\nThe preprocess method takes the file contents and filename as arguments, and returns an array of code blocks to lint\nA code block has two properties text and filename; \n- the text property is the content of the block\n- filename property is the name of the block.\n    - Name of the block can be anything, but should include the file extension, that would tell the linter how to process the current block\nThe linter will consult the `--ext` extensions list of the linting command as executed on the CLI.\n\n#### Postprocessor\n\n* * *\n# E Resources\n[Eslint docs for custom rules](https://eslint.org/docs/developer-guide/working-with-rules)\n[Eslint docs for custom plugins](https://eslint.org/docs/developer-guide/working-with-plugins)\n[Create custom eslint rules](https://www.webiny.com/blog/create-custom-eslint-rules-in-2-minutes-e3d41cb6a9a0)\n[creating an eslint plugin](https://medium.com/@bjrnt/creating-an-eslint-plugin-87f1cb42767f)","n":0.086}}},{"i":850,"$":{"0":{"v":"Enzyme","n":1}}},{"i":851,"$":{"0":{"v":"Terms","n":1}}},{"i":852,"$":{"0":{"v":"Unpack","n":1},"1":{"v":"\n### Unpack\n- render 1+ level deeper (ie. not shallow)\n- `dive()` - shallow render the component child of the wrapper","n":0.229}}},{"i":853,"$":{"0":{"v":"Ember","n":1},"1":{"v":"\n# Overview\n- provides MVC in the client-side\n- opinionated— follows convention over configuration\n- modules import other modules implicitly because they know exactly where to look. Therefore, following the conventional structure is paramount (like Ruby on Rails) \n\t- ex. data that's fetched in the `/routes/index.js` will be available in `/templates/index.hbs` as `@model`\n\n![](/assets/images/2021-03-10-22-39-10.png)\n\n# Decorators\n- they are special functions that make modifications to the following line\n- they can be thought of as wrapper functions in a sense, if they are before a\n  function.\n- ex. this decorator will cause the following getter to cache the result on the\n  first time.\n## Types\n- `@tracked` - in a class component, This annotation on a variable tells Ember to monitor this variable for updates. Whenever this variable's value changes, Ember will automatically re-render any templates that depend on its value.\n- `@action` - in a class component, define a function that is available to the component layout (handlebars html)\n```\n@cache\nget count() {\n  return this._count++;\n}\n```\n- can receive arguments: `@alias('fullName') name`\n\n# Anatomy of Ember app\n![](/assets/images/2021-03-10-22-39-28.png)\n- url determines the current state of the app\n\t- ex. are they looking at a list? a post? are they editing the post?\n- When the url is entered in the address bar, it...\n\t- connects to a route handler which loads a particular model.\n\t- It renders a template, which has access to the model.\n\n## Models\n- Models represent persistent state. \n\t- ex. in an airbnb app, the details of a rental (price, description, photos etc) would be stored in the `rental` model. we'd also have a `user` model to store the state of the currently logged in user\n- models persist information, whether it's to a web server or as local state\n- model layers can be swapped in, so we could use Redux or Apollo\n\n## Templates\n- similar to `ejs`\n- the route handler makes the data of the model available to the template\n\n### application.hbs\n- a wrapper around the whole application. This is where we can specify a footer and header, since they will appear on all pages.\n- use `{{outlet}}` to specify the whole application (think of it as `children`, which contains the rest of the app)\n\n## Components\n- essentially just a template that goes in another template\n- they can take args (just like passing props in React)\n![](/assets/images/2021-03-10-22-39-41.png)\n- You can think of components as Ember's way for letting you create your own HTML tags.\n- use the `{{yield}}` keyword to pass \"the rest of the data\" (similar to `children` in React)\n\n### Namespaced components\n- you can have a component that exists within another component by:\n\ta. creating a folder within `/components` with `ember generate component <parentDir>/<subComponent>`\n- invoked in templates like this `<Parent::Child>`\n- We can pass down HTML attributes just like props in react:\n```\n<div class=\"image\">\n  <img ...attributes>\n</div>\n```\n\n### Class components\n- enable us to add behavior to components (by using states)\n- run `ember generate component-class <nameOfComponent>\n- class components extend `Glimmer` components, giving it functionality similar to classes in React, such as state and lifecycle methods.\n- whenever a component is invoked, an instance of its related class component will be instantiated, allowing us to store state in it and call any relevant lifecycle methods.\n- initial state is stored in the constructor (in Ember, writing out constructor seems to be optional)\n- in the template part of the component (ie. the html) we get access to the instance variables (the component state) defined in the class component\n- Glimmer components have access to `this.args`, which is just like `this.props`\n\t- All arguments that can be accessed from this.args are implicitly marked as @tracked\n\n#### Block parameters\n- a block is any code that is between the opening and closing tags of a component (in react it would be called `children`)\n- What if there is a variable that we want to pass to the block content?\n\t- In this case we can the `as |results|` syntax, which would make `results` available to everything inside the block\n\t- this is similar to when we do a `items.map(item)`, and we have the current iteration available to us as `map`\n```\n  <ul class=\"results\">\n    <Rentals::Filter @rentals={{@rentals}} @query={{this.query}} as |results|>\n      {{#each @rentals as |rental|}}\n\t\t<li>\n\t\t  <Rental @rental={{rental}} />\n\t\t</li>\n      {{/each}}\n    </Rentals::Filter>\n  </ul>\n```\n- this also allows us to pass down the resultings data in the corresponding module that pertains to `rentals/filter.hbs` with `{{yield this.results}}` (see next section)\n\n#### Provider component\n- A pattern we use when we want to set up a piece of state for a component, but don't have any html to render for it. Instead, the html is just passed on down to the next level by using `{{yield this.results}}`\n- The child component then passes data up to it's parent\n\t- look at `rentals.hbs` and `rentals/filter.js`. \n\t\t- `@query={{this.query}} as |results|` passes the `query` variable down to the child, giving it access to it. The child (a class component) uses that variable to make computations, then returns a result, which gets put in the variable `results` (due to the `|results|` line above)\n\n## Routes\n### Model hook\n- The model hook is responsible for fetching and preparing any data that you need for your route. Ember will automatically call this hook when entering a route, so that you can have an opportunity to run your own code to get the data you need. The object returned from this hook is known as the model for the route.\n\t- Usually, this is where we'd fetch data from a server. Since fetching data is usually an asynchronous operation, the model hook is marked as async\n\t\n## Services \n- Services are just singleton objects (ie. they get instantiated only once) to hold long-lived data such as user sessions.\n- serve a similar role to global variables, in that they can be easily accessed by any part of your app\n- For example, we can inject any available service into components, as opposed to having them passed in as an argument. This allows deeply nested components to \"skip through\" the layers and access things that are logically global to the entire app, such as routing, authentication, user sessions, user preferences, etc.\n- A major difference between services and global variables is that services are scoped to your app, instead of all the JavaScript code that is running on the same page. This allows you to have multiple scripts running on the same page without interfering with each other.\n\n### Store service\n- can be injected into a route with `@service store`, making the Ember Data store available to use as `this.store`, and giving us `find` and `findAll` methods.\n```\nimport { inject as service } from '@ember/service';\n\nexport default class IndexRoute extends Route {\n  @service store;\n  async model() {\n    return this.store.findAll('rental');\n  }\n}\n```\n- store service might be compared to Redux in its role to fetch from the database and cache it\n\n## Controller\n- def - an object that receives the return value of the `model()` method (which is found in the associated route).\n- def - an object that receives one property when its associated route is hit: `model` (the return value of the Route's model method)\n- controller is only needed if we want to customize the properties or provide actions to the Route\n\t- in other words, they are an extension of the model loaded from the Route\n- if we don't make a `controller` file, one is generated for us by default (we just don't see it)\n- the controller name must match the route that renders it\n- controller is a singleton (ie. they get instantiated only once) \n\t- this means we shouldn't keep state in the controller \n\t\t- (unless it comes from either the Model or the Query params; since these would persist in between activations such as when a user leaves the Route and then re-enters it)\n- Controllers can also contain actions, Query Parameters, Tracked Properties, and more\n- Basically, use controllers when: \n\t1. we want to pass down actions or variables to the components found in a route. \n\t2. we want to support query params\n\t3. we want to compute a value (that we will ultimately pass down to the route's components) that depends on the model hook \n\t\t- in other words, the controller takes in the result of `model()` as its sole argument. What if we want to pass a variable down to the components that depend on the return value of `model()`?\n\n# Libraries\n## Ember Data\n- a library that helps manage data and application state in Ember applications.\n- built around the idea of organizing your app's data into model objects (in `/models` directory).\n\t- These objects represent units of information that our application presents to the user\n- The model represents the shape of the data\n```\nimport Model, { attr } from '@ember-data/model';\n\nconst COMMUNITY_CATEGORIES = [\n  'Condo',\n  'Townhouse',\n  'Apartment'\n];\n\nexport default class RentalModel extends Model {\n  @attr title;\n  @attr owner;\n  @attr city;\n  @attr location;\n  @attr category;\n  @attr image;\n  @attr bedrooms;\n  @attr description;\n\n  get type() {\n    if (COMMUNITY_CATEGORIES.includes(this.category)) {\n      return 'Community';\n    } else {\n      return 'Standalone';\n    }\n  }\n}\n```\n- Ember Data uses Adapters and Serializers. The idea is that, provided that your backend exposes a consistent protocol and interchange format to access its data, we can write a single adapter-serializer pair to handle all data fetches for the entire application.\n\t- Adapters deal with how and where Ember Data should fetch data from your servers, such as whether to use HTTP, HTTPS, WebSockets or local storage, as well as the URLs, headers and parameters to use for these requests. \n\t- Serializers are in charge of converting the data returned by the server into a format Ember Data can understand.\n\n# Structure\n- the root of the ember application is `templates/application.hbs`\n\n# Tests\n- integration tests - components that exist in isolation. They don't have to interact with the context in which they are placed. They can exist as a unit. Essentualy these are our unit tests.\n- acceptance tests - components that need to interact with other areas of the app (ex. navbar link functionality)\n\n# Mirage\n## Factories\n- allow you to create blueprints for your data. In other words, seed your development database","n":0.025}}},{"i":854,"$":{"0":{"v":"Email","n":1},"1":{"v":"\nNever send to generic emails, like `info@[businessname].com`. This hurts your sender reputation score\n\n# UE Resources\n[Email Sender Reputation](https://www.sparkpost.com/resources/email-explained/email-sender-reputation/)\n","n":0.243}}},{"i":855,"$":{"0":{"v":"Elastic Search","n":0.707},"1":{"v":"\nElasticsearch is an open-source, RESTful, distributed search and analytics engine built on Apache Lucene\n- You can send data in the form of JSON documents to Elasticsearch using the API. Elasticsearch automatically stores the original document and adds a searchable reference to the document in the cluster’s index. You can then search and retrieve the document using the Elasticsearch API\n- ES is NoSQL and is more powerful, flexible, and faster than SQL's LIKE\n\nES is typically used when you have :\n- high data volumes, and are likely to need multiple nodes to process the data\n- unstructured or semi-structured data (log files, text, ...). You ingest the raw data in its original form.\n- the data is never updated. It’s ingested once, queried, and then purged according to some bulk retention policy (e.g. older than 30 days)\n- you need to access aggregate data more than individual records\n\nWhen you're searching for text. ES ranks search results based on how close the phrase or words are. SQL doesn't do this nearly as well.\n- ES starts to shine when you start to do a lot of filtering\n\nElastic search scales horizontally with your requirements.\n\n# Tools\n[Kibana: a data visualization platform for Elasticsearch](https://www.elastic.co/kibana)","n":0.072}}},{"i":856,"$":{"0":{"v":".NET","n":1},"1":{"v":"\n.NET is a framework that contains a large set of programs to be called from the program you are building. These functions/programs can be as simple as *join 2 arrays*, or can be as complex as *translate voice to text*/*recognize red object in an image*.\n\n.NET is made up of an SDK and a runtime environment\n\nthe languages that are used in the framework can all be compiled down to a common language\n- ex. C#, VB, IronPython (a .NET implementation of Python)\n\nThe computers that run a .NET application need to have the corresponding .NET framework installed prior to being able to run it.\n\n### Major components\nCommon Language Runtime (CLR) – This allows the executions of programs written in the .NET framework using C#, VB, Visual C++\n- also used to provide services such as memory management, security, exception handling, loading and executing of the programs.\n\nFramework Class Library (FCL) – This is integrated with the CLR of the .NET framework and allows writing programs using .NET supporting programming languages such as C#, Visual C++, VB, etc.\n\nUser and Program Interfaces – This provides tools to develop desktop and windows applications. Windows forms, web services, Console applications and web forms are some examples of the user and program interfaces.\n\n### ASP.NET vs .NET\n.NET is a (software) development platform that is used to develop, run and execute the applications\n- unified environment is the key value adder of .NET\n\nASP.NET is a web framework that is used to build dynamic web applications\n- ASP.NET is part of the .NET framework\n    - because of this, developers have access to all of the .NET classes and features\n- applications developed by ASP.NET are largely component-based and built on the top of the common language runtime (CLR) and can be written in any of the .NET languages.","n":0.059}}},{"i":857,"$":{"0":{"v":"Docker","n":1},"1":{"v":"\n# Docker\nUsing Docker, we can abstract away the software, OS, and hardware configuration from our application, turning each service into a building block that we can run anywhere\n- when using containers you have to always think of dynamic vs. static parts of your application. You can use your host's file system to store your data and files. A more scalable and efficient way of thinking about this is to store your data to amazon rds and your files to amazon s3. This way you can spin up as many containers of your app/site as you want and have them all point to a single place where they store your dynamic stuff; namely, your data and files.\n\n## .dockerignore\n- Because being lean is a design principle of Docker, it is important to cut out the stuff from the image that is not necessary to running the code.\n\t- This includes git files, travis.yml, .vscode etc. To ensure these files do not become a part of the image, we put these in a .dockerignore file.","n":0.076}}},{"i":858,"$":{"0":{"v":"Tools","n":1},"1":{"v":"\n## Watchtower\n- allows us to update an already running docker container by simply pushing to Docker Hub. Watchtower sees that a new image has been pushed, automatically runs `docker pull`, then starts the container back up again.\n- note: a message from calibre:\n\t- We do not endorse the use of Watchtower as a solution to automated updates of existing Docker containers. In fact we generally discourage automated updates. However, this is a useful tool for one-time manual updates of containers where you have forgotten the original parameters. In the long term, we highly recommend using Docker Compose.\n\n## Portainer\nan open source, platform agnostic tool for managing containerized applications. It works with Kubernetes, Docker, Docker Swarm, Azure ACI in both data centres and at the edge.\n```\ndocker container run -d \\\n  -p 9000:9000 \\\n  -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer\n```\n[Website](https://www.portainer.io/)","n":0.087}}},{"i":859,"$":{"0":{"v":"Registry","n":1},"1":{"v":"\n# Registry\n- ***def*** - a stateless, scalable server-side application that stores and allows us to distribute docker images\n    - In other words, it allows us storage for our images\n- Think of it like github for docker images\n    - So we have commands like `docker pull`, `docker push`x etc.\n- ex. *Docker Hub*\n- Docker registry stores images as 2 parts, with a pointer between them:\n    - image:tag\n    - digest (i.e. the SHA id)\n- we use the registry as a place to store our images \n- Docker Hub is a an available registry that is hosted for us \n","n":0.102}}},{"i":860,"$":{"0":{"v":"Orchestrators","n":1},"1":{"v":"\n# Orchestrators (container schedulers)\n- Tools to manage, scale, and maintain containerized applications\n- ie. Kubernetes, Docker Swarm\n- Orchestration tools are designed to handle Docker containers running stateless applications\n\t- as such, you should not run stateful applications in orchestration tools which are built for stateless apps.\n\t- this makes sense, seeing as containers can be killed and restarted without any issue, multiple can run at once etc.\n\t\t- The same cannot be said for a database, which is why we typically don't dockerize DBs\n\nThe container scheduler is a framework that determines where a service should be deployed and make sure that it maintains the desired run-time specification. The scheduler manages clusters (which are composed of services)\n\nA cluster scheduler has quite a few goals.\n- It makes sure that resources are used efficiently and within constraints.\n- It makes sure that services are (almost) always running.\n- It provides fault tolerance and high availability.\n- It makes sure that the specified number of replicas are deployed.\n- It makes sure that the desired state requirement of a service or a node is (almost) always fulfilled. Instead of using imperative methods to achieve our goals, with schedulers, we can be declarative.\n- We can tell a scheduler what the desired state is, and it will do its best to ensure that our desire is (almost) always fulfilled.\n\t- ex. instead of executing a deployment process five times hoping that we’ll have five replicas of a service, we can tell a scheduler that our desired state is to have the service running with five replicas.\n\nConsider the impact that is had by having declarative methods, as is offered from a container scheduler. With a declarative expression of the desired state, a scheduler can monitor a cluster and perform actions whenever the actual state does not match the desired. Compare that to an execution of a deployment script. Both will deploy a service and produce the same initial result. However, the script will not make sure that the result is maintained over time. If an hour later, one of the replicas fail, our system will be compromised.\n- traditionally, these problems were solved by having alerts, and having DevOps guys manually intervene to get everything back up and running.\n- Think of schedulers as operators who are continually monitoring the system and fixing discrepancies between the desired and the actual state\n\nIn general, the development workflow looks like this:\n1. Create and test individual containers for each component of your application by first creating Docker images.\n2. Assemble your containers and supporting infrastructure into a complete application, expressed either as a Docker stack file or in Kubernetes YAML.\n3. Test, share and deploy your complete containerized application.\n\n* * *\n\nYou should not run stateful applications (like a db) in orchestration tools which are built for stateless apps.\n- Orchestration tools are designed to handle Docker containers running stateless applications. Such applications don’t mind being terminated at any time, any number can run at the same time without communicating with each other and nobody will really notice if a new container will take over on a different machine.","n":0.045}}},{"i":861,"$":{"0":{"v":"Machine","n":1},"1":{"v":"\nDocker engine is a tool that lets us install Docker Engine on virtual hosts and manage those hosts\n\t- Therefore, it is something we install on our own computer.\n\t- with the **driver** concept, we can deploy to 3rd party cloud services, like AWS, DigitalOcean, or even VirtualBox on your local machine\n- *docker-machine* commands let us start, inspect, stop, and restart hosts, as well as configure a docker client to talk to the hosts.\n- it allows us to control the docker engine of a VM created using docker-machine\n- The main reason you would use docker-machine is when you want to create a deployment environment for your application and manage all the micro-services running on it\n- To setup, all we need to do is point our `docker-machine` CLI at our managed host, which enables us to run docker commands directly on that host.\n\n### Connecting to Docker Machine\nThe connection to a docker machine is made available through env variables. By default, they are unset, giving us our default connection\n`docker-machine env -u` will show us how to unset all variables to return to our default connection\nIf we wanted to connect to minikube, we could run `eval $(minikube docker-env)","n":0.072}}},{"i":862,"$":{"0":{"v":"Init","n":1}}},{"i":863,"$":{"0":{"v":"Images","n":1},"1":{"v":"\n# Images\n- an image is an immutable snapshot of a system\n\t- The fact that it is immutable gives it predictability. In other words, it will always work as-is\n- an image has a tree hierarchy. There is a base image (aka Parent Image), which is initiated with the `FROM` command in the Dockerfile. It sets the base for the rest of the images generated in the Dockerfile. \n\t- Therefore, every Dockerfile must have a `FROM` directive\n\n## Layers\n- Docker image is made up of a series of `layers`\n- Each line in the `dockerfile` creates a `layer`\n\t- Therefore, a layer contains only the differences between the preceding layer and the current layer (like consequent git commits).\n- layers are read-only\n- On top of the layers, there is a writable layer (the current one) which is called the container layer\n- layers may be shared between images\n\t- this means if the layer `COPY . /app` is used in multiple places, each iteration of it (from other Dockerfiles) will not contribute to Docker's overall footprint.\n\t- When the Dockerfile is run with `docker build`, each layer gets executed and its result cached\n- The following Dockerfile instructions create a layer and influence the size of the image:\n\t- RUN\n\t- COPY\n\t- ADD\n- The other Dockerfile instructions create intermediate layers, and do not increase the size of the image\n- When we build an image from Dockerfile, we'll notice that it says `removing intermediate container`, rather than what we might expect: `removing intermediate layer`. This is because a build step (a line in the Dockerfile) is executed in an intermediate container, which is no longer needed once the build step is done\n- if we run `docker history <image-id>`, we can recognize the intermediate containers as the ones having 0B size\n\t- There are also a lot of containers labelled `missing`, meaning that those layers are built on a different system and are not available locally.\n\nBelow, The FROM statement starts out by creating a layer from the ubuntu:15.04 image. The COPY command adds some files from your Docker client’s current directory. The RUN command builds your application using the make command. Finally, the last layer specifies what command to run within the container.\n```\nFROM ubuntu:15.04\nCOPY . /app\nRUN make /app\nCMD python /app/app.py\n```\nIn the previous example, we spun up a container whose basis is a Ubuntu server. We can just as easily spin up a continer whose basis is nodejs. \n\n## Dockerfile\n- Think of these Dockerfile commands as a step-by-step recipe on how to build up our image.\n\t- first step to containerizing an application\n- Dockerfiles describe how to assemble a private filesystem for a container, and can also contain some metadata describing how to run a container based on this image\n\t- A Dockerfile specifies the operating system that will underlie the container, along with the languages, environment variables, file locations, network ports, and other components it needs— and, of course, what the container will actually be doing once we run it.\n\nExample Dockerfile\n```\nFROM node:6.11.5 \nWORKDIR /usr/src/app \nCOPY package.json . \nRUN npm install \nCOPY . . \nCMD [ \"npm\", \"start\" ]\n```\n\nBuilding up our images takes the following steps:\n\n1. Start FROM the pre-existing node:6.11.5 image. This is an official image, built by the node.js vendors and validated by Docker to be a high-quality image containing the node 6.11.5 interpreter and basic dependencies.\n\n2. Use WORKDIR to specify that all subsequent actions should be taken from the directory /usr/src/app in your image filesystem (in other words, not the FS on your machine).\n\n3. COPY the file package.json from your host to the working directory in your image (so in this case, to /usr/src/app/package.json)\n\n4. RUN the command npm install inside your image filesystem (which will read package.json to determine your app’s node dependencies, and install them)\n\n5. COPY in the rest of your app’s source code from your host to your image filesystem.\n\nThese above commands effectively set up the filesystem of our image\n\nCMD specifies how to run a container based off *this* particular image\n- In this case, it’s saying that the containerized process that this image is meant to support is npm start.\n- i.e. it is a metadata specification\n- there can only be one `CMD` instruction per Dockerfile\n\nENTRYPOINT allows us to configure the container to run as an executable \n- the commands specified in ENTRYPOINT will always be run.\n- we also have CMD, whose commands will only run if we are spinning up a container and not explicitly setting any CLI arguments \n\t- if we specify arguments when spinning up a container, CMD is ignored, but ENTRYPOINT commands will still be executed \n\t- ex. `docker run -it <image> <arg1>`\n\t- CMD therefore are default arguments\n\n## Tags\n- an alias to the ID of an image. ie. they are just a a way to refer to a specific image\n- anal: git tags can be used to refer to a specific commit (ex. map tag SHA 3fhd883nnf9 to v1.4)","n":0.035}}},{"i":864,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n### `docker images`\n- show all images\n\n### `docker build`\n- purpose is to build an image from a Dockerfile. The command will find the `Dockerfile` and will build an image based on it.\n\n## Build image\n- from your application folder:\n`docker image build -t <APP NAME> .`\n  - this causes each line of Dockerfile to be executed, building up the image as it goes\n- Images are static once they are created\n","n":0.122}}},{"i":865,"$":{"0":{"v":"Engine","n":1},"1":{"v":"\n# Docker Engine\n- Docker engine is a client-server application made up of the Docker daemon, a REST API allowing us to interact with the daemon, and a CLI for talking to the daemon. \n- Docker Engine accepts docker commands from the CLI (`docker run` etc)\n- When people say “Docker” they typically mean Docker Engine\n","n":0.136}}},{"i":866,"$":{"0":{"v":"Debug","n":1},"1":{"v":"\n- `docker logs <container name>` - see what happened during container initialization\n- `docker inspect <container-name>` - get low-level info about a container, such as its IP, port mappings,  \n- remove all containers based on one image (ex. monica) - `docker ps -a | awk '{ print $1,$2 }' | grep monica | awk '{print $1 }' | xargs -I {} docker rm {}`","n":0.126}}},{"i":867,"$":{"0":{"v":"Containers","n":1},"1":{"v":"\n# Containers\n- Container - the instance of an *Image*\n- Each container had its own filesystem (provided by a Docker image)\n- Image includes everything needed to run an application\n    - inc. the application code, runtimes, dependencies etc\n- Containers are designed to be transient and temporary, but they can be stopped and restarted, which launches the container into the same state as when it was stopped.\n- Containers can communicate with each other by attaching (`docker attach`). They do this by attaching stdin, stdout and stderr steams to one another so that one container's output can be piped into another container as their input \n\t- When we run `docker attach` with a specified container, we are piping our stdin/stdout/stderr to the container, so we are effectively able to write commands in the container's terminal\n\n## From the container's perspective \nThe container by default has no access to the outside world. When the docker host specifies the `-p` option when spinning up a new container, the container opens up the specified ports and maps them to the docker host's port.\n- For all the container knows, it is a regular computer. It has a network interface, including an IP address, a gateway, a routing table etc. \n\n![9ede10f99d18b464b0087150a5679308.png](:/26150f69cbf24c1583bf667d69c6ac9b)\n\n- Containers guarantee our applications will run the same anywhere, whether it's our own machine, a data centre, or anything else\n\n## Run container\n`docker run --name *<NAME OF CONTAINER>* -d -v /tmp/mongodb:/data/db -p 27017:27017 *<NAME OF IMAGE:IMAGE TAG>*`\n- `--name`: Name of the container.\n- `-v`: Attach the /tmp/mongodb volume of the host system to /data/db volume of the container.\n- `-p`: Map the host port to the container port.\nLast argument is the name/id of the image.\n\n## Running commands in the container\n- we can use `docker exec` to run commands.\n- by default, the command is executed in the WORKDIR variable that is declared in the Dockerfile\n- often, we see `docker exec -it <containerID> /bin/bash`\n\t- this executes the command `/bin/bash` within the specified container, opening a new bash session for us.\n\t- `-t` tells docker to open a terminal session\n\t- `-i` for interactive, ensures that our stdin input stream remains open \n\n## Actions\n- get IP of container - `docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <my-container-name>`\n","n":0.053}}},{"i":868,"$":{"0":{"v":"Volumes","n":1},"1":{"v":"\n## Volumes\n- The primary way to persist data that exists in a Container\n- in a node app, we specify the following `volume` field attributes\n```\nvolumes:\n      - '.:/app'\n      - '/app/node_modules'\n```\n- This first line is saying \"map the host's PWD (.) to the `/app` directory in the container\". The next line says \"persist the node-modules directory in the container so that when we mount the host directory (the first line) at runtime, the node-modules won't get overwritten\"\n\t- In other words, this would happen:\n\t\t- Build - The node_modules directory is created in the image.\n\t\t- Run - The current directory is mounted into the container, overwriting the node_modules that were installed during the build.\n\n## Bind Mount\n- Instead of Docker creating a directory on the host that a container can have full access to (volume), we can make use of bind mounts. A bind mount lets us create something similar to a symbolic linked directory, whereby a directory or file on the host machine gets mounted into the container.\n\t- This means that we can make changes to the directory/file on the host machine, and those changes will reflected in the docker container's version of the directory/file. \n- bind mounts are created in `docker-compose.yml` with the `volumes` directive\n\n# UE Resources\n[Primer](https://docs.docker.com/storage/volumes/)","n":0.07}}},{"i":869,"$":{"0":{"v":"Networks","n":1},"1":{"v":"\n# Networks\n- Networks enable containers to be able to communicate with each other and with non-Docker processes (such as a host) \n- Networks are natural ways to isolate containers from other containers or other networks. As such, they provide complete isolation for containers\n- Docker’s networking subsystem is pluggable, made possible by having drivers\n\t- depending on which driver you are using, you will have different core networking functionality \n- `docker network ls` - show all networks\n- all containers within a network can communicate with each other. This can be shown by the fact that you can ping the IP address of one container from another (within the same network) \n\t- you can also simply `ping <container-name>`\n- Docker networking allows you to attach a container to as many networks as you like.\n- to see if 2 containers are properly on the same network, try pinging one container's IP from another.\n\nThere are 2 main types of network: bridge and overlay\n\n## Bridge\n- this is the default\n\t- Docker creates a bridge named `docker0`, and both the docker host and the docker containers have an IP address on that bridge.\n- Bridge networks are usually used when your applications run in standalone containers that need to communicate over the same docker host (ex. a pod?)\n- Limited to a single host running Docker Engine.\n- default type\n- if our `docker-compose.yml` does not explicitly specify a network to use, a special network called *bridge* will be the network that our containers are launched in \n\t- visible with `docker network ls`\n\n## Overlay\n- Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other.\n- can include multiple hosts and is a more advanced configuration\n\n## Host network\n- if we have a standalone container, network isolation between the container and the Docker host will be removed, and the container will use the host's network directly\n- host is only available for swarm services\n","n":0.057}}},{"i":870,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### Remove all containers built from Image 7401323eb08e\n```\ndocker ps -a | awk '{ print $1,$2 }' | grep 7401323eb08e | awk '{print $1 }' | xargs -I {} docker rm {}\n```\n","n":0.18}}},{"i":871,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n## Commands\n### run a command in a new container\n`docker run`\n- spec: Each time we execute `docker run`, the Dockerfile is read. This is why calling `docker run` on an existing container wouldn't make any sense.\n\n### show all running containers in docker engine\n`docker ps`\n- pass `-a` to see all containers\n- `docker-compose ps` will list containers in the docker engine that are related to the images specified in `docker-compose.yml` (therefore `dc ps` is a subset of `docker ps`)\n\n## Start a container\n`docker container run --publish 8000:8080 --detach --name bb <APP NAME>`\n- `--publish` asks Docker to forward traffic incoming on the host’s port 8000, to the container’s port 8080\n  - containers have their own set of ports\n- notice, we didn’t specify what process we wanted our container to run. We didn’t have to, since we used the CMD directive when building our Dockerfile; thanks to this, Docker knows to automatically run the process npm start inside our container when it starts up\n- visit application at localhost:8000\n\n### Show all containers\n`docker container ls`\n\n#### Filter containers by those created by *mongo* image\n`docker container ls -f ancestor=mongo`","n":0.075}}},{"i":872,"$":{"0":{"v":"Compose","n":1},"1":{"v":"\n# Docker Compose\ncompose is a tool for defining and running multi-container applications\n- use `docker-compose.yml` to configure the app's services, and start them with a single command\n\t- this file will be parsed everytime you run `docker-compose up`. Therefore, if you make changes to `docker-compose.yml`, all you need to do is `docker-compose down`, then `dc up` again. This is unlike the Dockerfile, which will only be run once when you are building an image\n\nusing compose is a 3 step process:\n1. Define your app’s environment with a Dockerfile so it can be reproduced anywhere.\n2. Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.\n3. Run docker-compose up and Compose starts and runs your entire app.\n\nCompose has commands for managing the whole lifecycle of your application:\n- Start, stop, and rebuild services\n- View the status of running services\n- Stream the log output of running services\n- Run a one-off command on a service\n\n## Anatomy of docker-compose.yml\nThis file will be run with `docker-compose up`, building up the services (and with it, the containers) of the whole application\n- if the image has already been built, then by default the build command will be ignored\n\nThe docker-compose.yml file is organized by service. The `build` field designates the location of the service's dockerfile (this location is called the \"build context\") so that it can be run and the image can be built.\n\nthe docker-compose.yml file can specify environment variables to be executed during process of building the images with the `args` field under `build`\n- these variables are accessed in the dockerfiles by specifying `arg`:\n\ndockerfile:\n```\narg buildno\narg gitcommithash\n\nrun echo \"build number: $buildno\"\nrun echo \"based on commit: $gitcommithash\"\n```\n\ndocker-compose.yml\n```\nbuild:\n  context: .\n  args:\n    buildno: 1\n    gitcommithash: cdc3b19\n```\n\n### Publish vs. EXPOSE\n- these commands make the services inside the containers available to outside the containers\n- expose makes the processes in the container accessible to other containers within Docker (but not from *outside* Docker)\n\t- note: this is not entirely correct, but useful for understanding. [see](https://stackoverflow.com/questions/22111060/what-is-the-difference-between-expose-and-publish-in-docker/47594352#47594352)\n- publish (`-p`) makes the services available to outside Docker\n\t- Therefore, publish implicitly EXPOSES\n\n## Service\n- there is one image per service, but there can be multiple containers for a service. In other words, a docker \"service\" is one or more containers from one image\n- In our development set up, we are likely to have a single container per image. When we start to scale and need for additional containers per service arises, we can use `docker service` to create multiple containers from the same image\n\t- consider how this relates to the `docker run` command, which starts up 1+ containers from an equal amount of image (ie. one container per image)\n\t- what arises from running this command is Docker's swarm mode\n- Swarm mode is a container orchestration tool, giving the user the ability to manage multiple containers deployed across many different host machines\n\t- ex. imagine there are 5 containers deplayed across 5 different machines across USA. Swarm mode allows you to interact with all 5 containers at once, instead of having to manage one container at a time\n- This above distinction between a container and a service is precicely what differentiates `docker exec` and `docker-compose run`\n\t- naturally, `docker-compose` runs `docker` commands under the hood.\n\t- `docker exec` exectutes a command within a docker container\n\t- `docker-compose run` executes a command on a service (with the help of a swarm manager(?)\n\n# UE Resources\n[Definitive Guide to Docker Compose](https://gabrieltanner.org/blog/docker-compose)","n":0.042}}},{"i":873,"$":{"0":{"v":"dns","n":1},"1":{"v":"\nDNS can be thought of as a directory service for the internet.\n- Consider that a real-life directory service works like this: You have the name of someone, and you consult an agent to get \"directions\" (or a phone number) on how to reach them. A DNS is a similar entity which when given a domain name, can give us the address that it represents.\n\nDNS illustrates how a piece of core network functionality (network-name to network-address translation) can be implemented at the application layer of the internet.\n\n## History\nDNS is based on a distributed database that takes time to update globally. In the early internet days, the database was small and could be updated by hand. As the internet grew, this became unfeasible, so a new management structure was introduced: the concept of *domain name registrars*. The idea was that the updates to the database would be handled by the registrars\n- nowadays, when you make updates to domain name management settings, the registrar will push out the updated information to other DNS servers. \n\nThe DNS can be thought of the administrative assistant of the internet. It carries all the administrative responsibility of making things work, albeit behind the scenes. \nThe DNS is made up of Internet nameservers and a communication protocol\n\n## Components of DNS\n- [[DNS Cache|dns.cache]]\n- [[DNS Lookup|dns.lookup]]\n- [[DNS Resolver|dns.resolver]]\n\n## Config\n`/etc/resolv.conf`\n- we can see what our system's DNS info is with this file\n\t- run `scutil --dns` on Mac to see DNS configuration\n\n## Names\n### Apex domain\nThe apex domain is the root domain, without any `www` subdomain.\n\n### Fully Qualified Domain Name (FQDN)\n- The full domain name, rather than its parts separated. The FQDM is notable because it is fully unambiguous. It therefore points to a very specific place, and cannot be interpreted in any other way.\n- It always ends in the TLD (therefore, paths don't count)\n- FQDN in DNS records should always be appended with a `.`\n\t- This is to differentiate them from paths.\n\n* * *\nThere is no way to specify port numbers in DNS. If you are running a website, your server must respond to HTTP requests on port 80\n\n# Resources\nhttps://shapeshed.com/unix-traceroute/\nhttps://activedirectorypro.com/dns-best-practices/#dns-aging-scavenging\n\n# UE Resources\n- [DNS sinkhole](https://en.wikipedia.org/wiki/DNS_sinkhole)","n":0.053}}},{"i":874,"$":{"0":{"v":"Zone","n":1},"1":{"v":"\nDomains can be very large, so they are further organized into smaller books, called, “zones.”\n- The DNS is broken up into different zones. A DNS zone is a portion of the DNS namespace that is managed by a specific organization or administrator\n- A DNS zone is a subset, often a single domain, of the hierarchical domain name structure of the DNS\n\t- if DNS was a filesystem, a DNS zone would be each directory.\n\t- ex. if we are talking TLD DNS zone, then examples are `.com`, `.net`. If we are talking Domain-level DNS, then examples are `facebook.com`, `google.com`\n- A portion of the domain name space where administrative responsibility has been delegated to a single manager\n- The DNS Zone is described by the Zone File (aka. DNS record), which serves as the database for each nameserver.\n\t- The zone file contains mappings between domain names and IP addresses, along with other resources. This file is organized around resource records (A, CNAME etc.). In other words, resource records form the basis of the database. \n\t- We recognize the zone file when we go to a domain registrar, click onto one of our domains, and see all of the RRs that we have made.\n- a 'zone' is an area of control over namespace. A zone can include a single domain name, one domain and many subdomains, or many domain names. \n\t- In some cases, 'zone' is essentially equivalent with 'domain,' but this is not always true.\n\t- each zone has a *zone serial number*, which is a unique identifier\n\t\t- A DNS server can quickly look up a zone's records in its database via the serial number, which will bring up the SOA record.\n- A common mistake is to associate a DNS zone with a domain name or a single DNS server\n\t- In fact, a DNS zone can contain multiple subdomains and multiple zones can exist on the same server\n- We can decide which URLs should be their own zone, and which should be combined into a single zone.\n\t- ex. Below, as far as Cloudflare subdomains go, we have `blog`, `support`, and `community`. Support and community are small, so we put them in the same zone as the main `cloudflare.com`. However, the `blog` subdomain is a robust independent site that needs separate administration, so we give it its own zone.\n\nEach below is an example of a zone:\n![](/assets/images/2021-03-07-15-16-30.png)\n![](/assets/images/2021-03-07-15-16-44.png)\n\n### Zone Apex\n- Where `SOA` and `NS` records live. They are records whose names are the same as the zone itself.","n":0.049}}},{"i":875,"$":{"0":{"v":"Subdomain","n":1},"1":{"v":"\nThe most common subdomain is `www`\n- The www subdomain is so widely used that most domain registrars include it with domain name purchases.\n\nThe concept of subdomain can be achieved in 2 different ways:\n1. The DNS zone file pertaining to the parent domain can be edited\n    - in other words, subdomains are contained within parent domains (this is not the case for the second)\n2. a record can be made to map a name to the A record.\n    - network operations teams consider the second to not really be a subdomain, and instead restrict the definition to only including those subdomains which are provided by the zone NS records, and any server-destination other than that.","n":0.094}}},{"i":876,"$":{"0":{"v":"Servers","n":1},"1":{"v":"\n# DNS Request\nA DNS request is executed by the browser on a device. The first thing the OS checks is the hosts file. If the hosts file has an entry for the DNS, then this entry is always used, regardless of what comes next.\n- If the hosts file turns up no result, then the network card settings will be queried. This can come from one of 2 places:\n\t1. IP addresses of DNS servers configured in router\n\t2. IP addresses of DNS servers configured on device itself (in which the DNS settings on router would be set to *manual*)\nWhen the DNS resolves, the browser is enabled to connect to a web server or a CDN edge server \n\nDNS queries and responses are sent in plaintext (via UDP), which means they can be read by networks, ISPs, or anybody able to monitor transmissions (even when using HTTPS)\n\n# Nameservers\n- A DNS nameserver is a server that stores the DNS records for a domain\n\t- a DNS nameserver responds with answers to queries against its database.\n- A nameserver is a computer designated to translate domain names into IP addresses\n- Nameservers can be “authoritative”, meaning that they give answers to queries about domains under their control. Otherwise, they may point to other servers, or serve cached copies of other name servers’ data.\n\n# DNS Server\nThere are only 4 types of DNS Server:\n1. DNS Recursor\n\t- the server that responds to a DNS query and asks another DNS server for the address, or already has the IP address for the site saved.\n2. Root nameserver\n\t- A root name server is the name server for the root zone. It responds to direct requests and can return a list of authoritative name servers for the corresponding top-level domain.\n3. TLD nameserver\n\t- The top-level domain server (TLD) is one of the high-level DNS servers on the internet. When you search for www.varonis.com, a TLD server for the ‘.com’ will respond first, then DNS will search for ‘varonis.’\n4. Authoritative nameserver\n\t- The authoritative name server is the final stop for a DNS query. The authoritative name server has the DNS record for the request.\n\n- In instances where the query is for a subdomain such as `foo.example.com`, an additional nameserver will be added to the sequence after the authoritative nameserver, which is responsible for storing the subdomain’s `CNAME` record.\n- the ISP typically supplies the nameserver, but you can use public servers, like those offered by Google (which have IP `8.8.8.8` and `8.8.4.4`) or Cloudflare (`1.1.1.1` and `1.0.0.1`)\n\t- You could consider each IP address here to be a resolver.\n- The DNS server has expanded its role beyond only resolving domain names, and has other anciliary functionality\n\t- ex. a real-time blackhole list for spam\n- The DNS database is traditionally stored in a structured text file (the *Zone File*)\n\t- The Zone File describes a DNS Zone, and contains all RR for every domain in the zone.\n\nYour home network typically relies on a DNS Server supplied by your ISP\n- therefore your ISP's DNS servers see every domain you request.\n- some ISPs have found a way to monetize their DNS service. When you hit an erroneous domain, one that has no actual IP address, they divert your browser to a search and advertising page preloaded with a search phrase derived from the domain name\n\n### Library Analogy\n- *Resolver* - a librarian who, given a title, is asked to go fetch a book\n- *Root NS* - the blocks of bookshelves in the library\n- *TLD NS* - the specific rack within the bookshelf block\n- *Authoritative NS* - the specific book you asked for.\n\n## Misc\nthe same domain name may have multiple IP addresses associated with it.\n\nAnything that can be done with a DNS address can also be done with an IP address, since all a DNS does is translate from hostname (www.____.com) to IP.\n","n":0.04}}},{"i":877,"$":{"0":{"v":"Tld","n":1},"1":{"v":"\n### TLD nameserver\n- Holds information about all domains sharing the common TLD\n\t- ex. a `.com` TLD nameserver contains information for all the `.com` sites.\n- broken into 2 groups:\n\t1. Generic TLD, like .com, .org, .net, .edu, and .gov.\n\t2. Country code TLD, like .ca, .uk\n- overseen by IANA (a branch of ICANN)","n":0.141}}},{"i":878,"$":{"0":{"v":"Self Hosted","n":0.707},"1":{"v":"\nA Synology NAS can be configured to run a DNS server. Even once the server is set up, of course by default nothing will happen, since no one will be configured to ask the NAS for DNS records\n- There are 2 ways we can achieve this: by host, or by the router\n    - by host means we configure it on every single client on our network\n    - \n\n### Host-based method\nIn Mac Network Preferences you can find a place to add DNS server IP addresses. You might have your ISP's DNS server IP, you may have Cloudflare's (`1.1.1.1` and `1.0.0.1`), you may have Google's (`8.8.8.8` and `8.4.4.8`)\n\nYou can replace one of these IP addresses with the IP of your NAS. It's important to retain one of the Cloudflare/Google IP addresses, so that if the NAS is down, the whole network isn't down.\n- note: we want to configure our NAS DNS server so that it forwards requests on to the Cloudflare/Google DNS servers if the NAS doesn't have the IP address in its cache (rule: enable forwarders)\n\n* * *\n\n## Router DNS \nWhen our Mac has its DNS server configured as the router IP address, your router is running a caching DNS server, and setting itself as the DNS server via DHCP.\n- Your router is acting as a DNS forwarder, you ask your router and your router asks a DNS server for you (the DNS server it forwards it to on your behalf can be configured in the Router settings)","n":0.064}}},{"i":879,"$":{"0":{"v":"Root","n":1},"1":{"v":"\n### Root nameserver\n- Accepts the domain name from the DNS resolver, and returns the appropriate TLD address (`.com`, etc.). The resolver then uses this to find the TLD nameserver.\n- The \"root\" here is a reference to the `.` in `.com`. If you think of the `.` as a `/` instead, it becomes clear why it's called root (it is a directory)\n- There are 13 types of root nameservers (therefore 13 different addresses)\n- ex. **F-Root Server**\n- overseen by ICANN","n":0.113}}},{"i":880,"$":{"0":{"v":"Authoritative Nameserver","n":0.707},"1":{"v":"\nWhen a DNS server queries other DNS servers, it’s making an “upstream” query. Queries for a domain can go “upstream” until they lead back to domain’s authority, or “authoritative name server.”\n- An authoritative name server is where administrators manage server names and IP addresses for their domains. Whenever a DNS administrator wants to add, change or delete a server name or an IP address, they make a change on their authoritative DNS server (sometimes called a “master DNS server”).\n\nTherefore, the authoritative nameserver is the server that actually holds, and is responsible for, DNS resource records\n\t- This could be considered the \"domain's nameserver\"\n- Holds information specific to the actual domain name it serves (eg. google.com), and will send the IP address back to the recursive resolver.\n\t- The IP address it sends is found in the DNS A record.\n- Each domain has its own authoritative NS\n- spec: this nameserver holds the records that we see on our domain registrar dashboard for our domain. ","n":0.079}}},{"i":881,"$":{"0":{"v":"Resolver","n":1},"1":{"v":"\n### DNS Resolver (aka. Recursive Resolver)\nthe recursive resolver is at the beginning of the DNS query (with the authoritative nameserver at the end).\n- the resolver receives requests from web browsers, and in turn makes additional requests to the rest of the servers in a final effort to resolve the query. \n- a resolver will tell us what the IP is, given a domain name. By default, if a resolver cannot tell us, then that is it. However, the fact that it is recursive means that not only can we query a resolver, but a resolver can make queries itself. Put another way, if it doesn't have the answer we are looking for, it has the capacity to make queries itself. \n- After receiving a request for a webpage, the resolver will either respond with cached data (given to it by the nameservers), or will send a request to the *root nameserver*, followed by a request to a *TLD nameserver*, and then one last request to an *authoritative nameserver* (This is called [[DNS Lookup|dns.lookup]])\n\t- At each step, there is a possibility that a cached version of the website will be found, and it can immediately be returned to the client, rather than continuing to recurse down the DNS tree.\n- After receiving the requested IP address (from the authoritative nameserver), the recursive resolver sends a response to the client\n\nDNS Client (aka *Resolver*) is a client machine configured to send name resolution queries to a DNS server\n- in other words, our computer is a DNS client when it sends requests to port 53 of a DNS server, with the intention of resolving an IP address  ","n":0.061}}},{"i":882,"$":{"0":{"v":"Registry","n":1},"1":{"v":"\n### Domain name registry\n- a large database of all domain names and the associated registrant information in the TLDs of the DNS\n\t- With a database, third party entities (companies) can request administrative control of a domain name.\n- Most registries operate on the top-level and second-level of the DNS.\n- while registries manage domain names, they delegate the *reservation* of domain names to registrars","n":0.127}}},{"i":883,"$":{"0":{"v":"records","n":1},"1":{"v":"\n# Resource Records (RR)\nDNS records (aka zone files) are instructions that live in authoritative DNS servers and provide information about a domain\n- ex. You can think of a set of DNS records like a business listing on Yelp. That listing will give you a bunch of useful information about a business such as their location, hours, services offered, etc\n- a DNS record is a mapping between domain name and IP address. \n\nwhen we enter `facebook.com` in an address bar, we request an A record from the DNS. When we send an email, we request an MX record from the DNS. \n- A record is therefore the thing that we (the client) query for when engaging with the DNS\n\n## RR Field Values\nValues\n- `@` - indicates that this is a record for the root domain\n- `*` - indicates a wildcard, which will match all subdomains.\n\n### TTL (Time to Live)\n- How long the website will live in the caching nameserver before a new one is requested. If the TTL is 1 hour, then the server will only request a new version of the site every hour. All requests made to that URL will be made to the cached version, until the hour is up, and a new version is used.\n- As we know, there are many layers to DNS. It could be the case that we are able to resolve the domain name in the ISP nameserver, but we also may need to recurve further. The TTL will give us the max amount of time that particular resolution can exist on the ISP nameserver\n\t- In other words, how long can a given mapping of domain to IP address live for in the cache, before we need to make a more recursive query? \n\n### Record Data\n- Think of it like environment variables. We are passing data to the record. Naturally, whatever data we pass will vary in result depending on what type of record we have designated. \n\n### Record Class\n- this is the namespace of the record information.\n\t- The most commonly used namespace is that of the Internet (`IN`)\n\n# UE Resources\n- [DNS records - an introduction](https://www.linode.com/docs/networking/dns/dns-records-an-introduction/)","n":0.053}}},{"i":884,"$":{"0":{"v":"Txt","n":1},"1":{"v":"\n### TXT\n- originally meant to hold human-readable notes, nowadays they are able to be used to pass in machine-readable code.\n- two of the most important uses for TXT records are email spam prevention and domain ownership verification","n":0.164}}},{"i":885,"$":{"0":{"v":"Srv","n":1},"1":{"v":"\n### SRV\n- SRV records are how a port can be specified within the DNS\n- must point to an A record.","n":0.224}}},{"i":886,"$":{"0":{"v":"soa","n":1},"1":{"v":"\n### SOA (Start of Authority)\n- This is the most important address record, and must be specified.\n- Contains the authoritative master nameserver for the zone or domain, as well as an admin email that we specify.\n- Also contains administrative information about the zone\n- *MNAME* - the primary nameserver for the zone","n":0.141}}},{"i":887,"$":{"0":{"v":"Ns","n":1},"1":{"v":"\n### NS \n- nameserver record - indicate which authoritative nameserver contains the actual DNS records\n\t- Basically, NS records tell the Internet where to go to find out a domain's IP address\n- A domain often has multiple NS records which can indicate primary and backup nameservers for that domain. \n\t- if one nameserver goes down or is unavailable, DNS queries can go to another one\n\t- Typically there is one primary nameserver and several secondary nameservers, which store exact copies of the DNS records in the primary server\n\t\t- Updating the primary nameserver will trigger an update of the secondary nameservers as well.\n- Without properly configured NS records, users will be unable to load a website or application.\n- When multiple nameservers are used (as in most cases), NS records should list more than one server\n- NS records must point to an A record\n- ex. the resolver may have the NS records, but no A record, and it will still be able to query those nameservers directly, rather than having to go through the TLD server\n\nUpdating NS records\n- Domain administrators should update their NS records when they need to change their domain's nameservers\n- update NS records if you want a subdomain to use different nameservers than the domain (ex. example.com and blog.example.com have 2 different nameservers)","n":0.069}}},{"i":888,"$":{"0":{"v":"Mx","n":1},"1":{"v":"\n### MX \nMail exchange (SMTP)\n- MX Records point to the incoming smtp server for a domain\n- the record indicates how email messages should be routed (in accordance with SMTP)\n\t- ex. When a user sends an email to john.smith@gmail.com, the *Message Transfer Agent* (MTA) sends a DNS query to identify the mail servers for that email address. The MTA establishes an SMTP connection with those mail servers, starting with the prioritized domains\n- must point to an A record\n- `priority` - lower number indicates preference. In the result of a send failure, the next priority domain will be attempted \n\t- `mailhost1.tycholiz.com` might have `priority` of 10, and `mailhost2.tycholiz.com` might have `priority` of 20, which would mean `mailhost2` only gets used when the first message fails to send. \n\t- if we use the same priority, then both servers will receive equal amount of mail (effectively a load balancer) ","n":0.083}}},{"i":889,"$":{"0":{"v":"Cname","n":1},"1":{"v":"\n### CNAME \nCanonical name record\n- points to a domain, not an IP address\n- CNAME records allow a machine to be known by more than one hostname\n- map an alias name to a true (canonical) domain name\n- typically used to map a subdomain, like `www.stuff.com` to `stuff.com`, or `stuff.com` to `blog.stuff.com`)\n\t- This is good, because if the host IP address changes, then we only need to change the A record. The CNAME record depends on the domain, not the IP address it's associated with.\n- anal. Imagine a scavenger hunt where each clue points to another clue, and the final clue points to the treasure. A domain with a CNAME record is like a clue that can point you to another clue (another domain with a CNAME record) or to the treasure (a domain with an A record).\n- ex. imagine we give `blog.tycholiz.com` a CNAME with value `tycholiz.com`. This means that any time a DNS server hits the DNS records for `blog.tycholiz.com`, it actually triggers another DNS lookup to `tycholiz.com`, since we specified that as the CNAME\n\t- in this example, the canonical name (true name) is `tycholiz.com`\n\t- from this example, you can see how CNAMEs are kind of like relays, since they don't map to an IP at all, but point to a domain name, which maps to an IP. In other words, CNAME records cause A records to resolve domain names. \n- The CNAME record only points the client to the same IP address as the root domain\n\t- Therefore, the CNAME record does not have to resolve to the same website as the domain it points to.\n\t- ex. in the case where we hit `blog.example.com`, the DNS will return us the same IP as if we hit `example.com`.  \n\t\t- when the client actually connects to that IP address, the web server will look at the URL, see that it is blog.example.com, and deliver the blog page rather than the home page.\n- Pointing a CNAME to another CNAME is possible, but there is no point\n\n TEST","n":0.055}}},{"i":890,"$":{"0":{"v":"Aname","n":1},"1":{"v":"\n### ANAME \n- Like a *CNAME record*, but at the root of the domain\n- Allows us to point the \"naked\" version of domain (eg. example.com) to another host\n- Common use case is CDN","n":0.174}}},{"i":891,"$":{"0":{"v":"A (Address) Record","n":0.577},"1":{"v":"\nThe `A` record is the most fundamental record\nmaps the IP address to a given domain\n- Therefore, the `A record` serves as a lookup for the IP address of a given domain\n- it is normal to have just one A record. \n- `AAAA` for IPv6\n\na value of 14400 for TTL means that if an A record gets updated, it takes 240 minutes (14400 seconds) to take effect.\n\nMost websites have a single A record, but with multiple A records, you can implement round robin load balancing","n":0.109}}},{"i":892,"$":{"0":{"v":"DNS Lookup","n":0.707},"1":{"v":"\n## Process\n1. When `www.facebook.com` is searched, the browser DNS cache is checked to see if the domain and IP address key-value pair is stored, after that, the OS's cache, then the DNS server configured on the system (which might be the DNS server in the home router, the ISP (Internet Service Provider), or the public DNS server.)\n2. after the browser cache is checked, the resolver intercepts the request and checks its own cache. \n3. If it doesn't have anything, it will make a request to the root server to see if it has it cached. \n\t- the part of the URL corresponding to root server is the final `.` of www.facebook.com.\n4. If it does not, then it proceeds down the chain until the authoritative nameserver. \n\t- the next parts of the chain are `com`, then `facebook`, then `www`. each of these is essentially a zone\n\nAt any point, if the nameserver does indeed have the website cached, then it will return it to the resolver, who will proceed to return it to the client in domain form. \n\n\n\n<!-- During a new DNS lookup, the lookup passes through the resolver, root server, and TLD server. -->\n\n\n\nAt each step of the DNS lookup, information is gathered and cached for later use\n- Therefore, in a DNS lookup, the resolution process runs until either it reaches the DNS server (?) and gets the IP, or one of the stages returns a cached version of the website. \n\nDNS Lookup:\n![DNS lookup](/assets/images/2021-08-01-21-31-10.png)\n\n*Below may be roughly the same info as above, but it's a good resource. combine the 2 snippets when we have more time*:\n1. A DNS request starts when you try to access a computer on the internet. For example, you type www.varonis.com in your browser address bar.\n2. The first stop for the DNS request is the local DNS cache. As you access different computers, those IP addresses get stored in a local repository.  If you visited www.varonis.com before, you have the IP address in your cache.\n3. If you don’t have the IP address in your local DNS cache, DNS will check with a recursive DNS server. Your IT team or Internet Service Provider (ISP) usually provides a recursive DNS server for this purpose.\n4. The recursive DNS server has its own cache, and if it has the IP address, it will return it to you. If not, it will go ask another DNS server.\n5. The next stop is the TLD name servers, in this case, the TLD name server for the .com addresses. These servers don’t have the IP address we need, but it can send the DNS request in the right direction.\n6. What the TLD name servers do have is the location of the authoritative name server for the requested site. The authoritative name server responds with the IP address for www.varonis.com and the recursive DNS server stores it in the local DNS cache and returns the address to your computer.\n7. Your local DNS service  gets the IP address and connects to www.varonis.com to download all the glorious content. DNS then records the IP address in local cache with a time-to-live (TTL) value. The TTL is the amount of time the local DNS record is valid, and after that time, DNS will go through the process again when you request Varonis.com the next time.\n\n## Reverse DNS Lookup\n- Forward DNS lookup is using an Internet domain name to find an IP address. Reverse DNS lookup is using an Internet IP address to find a domain name\n\t- when you put a URL in the address bar, the address is transmitted to a nearby router which does a forward DNS lookup in a routing table to locate the IP address\n\t- `PTR Record` is an RR for enabling reverse DNS lookups, which is the exact opposite of an A record.\n- Reverse lookups are commonly used by email servers, who check and see if an email message came from a valid server before bringing it onto their network.\n\t- Many email servers will reject messages from any server that does not support reverse lookups (the absense of a `PTR record` means reverse lookups aren't supported)\n","n":0.038}}},{"i":893,"$":{"0":{"v":"Dynamic DNS","n":0.707},"1":{"v":"\nDDNS is a method of automatically updating a nameserver in the Domain Name System with the active DDNS configuration of its configured hostnames, addresses or other information.\n\t- each time your IP address changes, a program redirects the new address to the domain name and makes it permanently available on the internet.\nDDNS solves the problem of having your public IP address change. Since getting a static public IP address from an ISP can be expensive, this is another solution to that problem.\n\nThe term is used to describe two different concepts\n1. **dynamic DNS updating** - systems that are used to update traditional DNS records without manual editing\n2. ?\n\nDNS is only suitable for devices that don't change their IP often.\n- DDNS is a system that addresses the problem of rapid updates.","n":0.088}}},{"i":894,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Check which DNS server is configured for the device\nMac\n`scutil --dns | grep 'nameserver\\[[0-9]*\\]'`\n\nLinux\n`cat /etc/resolv.conf`\n","n":0.258}}},{"i":895,"$":{"0":{"v":"DNS Cache","n":0.707},"1":{"v":"\nThere are multiple caches involved in the entire DNS. They are checked in sequential order. \n1. Browser\n2. OS - the process that handles this query is called a **stub resolver**, or **DNS Client** \n\t- the stub resolver will first check to see if it has the cached data, and if not, will call the DNS query that gets handled by the resolver (which is hosted by the ISP).\n\nA residential router internally runs a DNS cache and DNS proxy server\n- it also advertises itself as the DNS server in all [[DHCP|network.lan#Dynamic-Host-Configuration-Protocol-(DHCP)]] responses.\n\nThere are actually DNS caches at every hierarchy of the lookup process\n- The computer reaches your router, which contacts your ISP, which might hit another ISP before ending up at what's called the \"root DNS servers.\" Each of those points in the process has a DNS cache for the same reason, which is to speed up the name resolution process.\n- spec:the DNS cache caches records.","n":0.08}}},{"i":896,"$":{"0":{"v":"Deploy","n":1},"1":{"v":"\n### Blue-green deployments\nA blue-green deployment is one without any downtime. In contrast to rolling updates, a blue-green deployment works by starting a cluster of replicas running the new version while all the old replicas are still serving all the live requests. Only when the new set of replicas is completely up and running is the load-balancer configuration changed to switch the load to the new version. A benefit of this approach is that there’s always only one version of the application running, reducing the complexity of handling multiple concurrent versions.\n\n# Trends\n[GitOps](https://www.atlassian.com/git/tutorials/gitops)\n\n# UE Resources\n[Kent C. Dodds Deployment pipeline breakdown](https://kentcdodds.com/blog/how-i-built-a-modern-website-in-2021)","n":0.101}}},{"i":897,"$":{"0":{"v":"Serverless","n":1}}},{"i":898,"$":{"0":{"v":"Functions","n":1},"1":{"v":"\nA cold start for serverless functions is directly related to the size of the function itself. The reason why cold starts exist is because the function is stored as a zip file, and it needs to be unzipped before it is mounted. Therefore, the solution to having shorter cold starts is to have smaller functions\n\nEach function should have its own package.json\n\n# Tools\nbegin.com","n":0.127}}},{"i":899,"$":{"0":{"v":"Scaling","n":1},"1":{"v":"\n### Scaling Types\nVertical Scaling \n- getting bigger machines to handle the increasing workload.\n    - ie. make each node more powerful (adding CPUs etc)\nHorizontal Scaling \n- getting many more smaller machines to handle the increase.\n    - ie. add more nodes\n- ex. Kubernetes does this by spinning up new containers\n- ex. Azure functions does this by spinning up new function handlers","n":0.13}}},{"i":900,"$":{"0":{"v":"Pricing","n":1},"1":{"v":"\n### per Second\nThis is normally for something like Serverless Functions, where we pay for the time that the server is executing our functions. \n- ex. if we are charged $0.01 per second and the function takes 333ms to run, then the function can be executed 3 times for $0.01\n\n### per Uptime\nThis is likely for container-based deployments (ex. Heroku), where we can pay per hour of uptime. This usually comes with the idea of \"cold starts\", where if no one is using our website, then the server will go offline (and therefore we won't get charged)","n":0.103}}},{"i":901,"$":{"0":{"v":"Jamstack","n":1},"1":{"v":"\n# Jamstack\nJAM: JavaScript, APIs, and markup\n- JavaScript: any dynamic programming during the request/response cycle is handled by JS, running entirely on the client. This could be any frontend framework or library, or even vanilla JavaScript.\n- APIs: all server-side processes or database actions are abstracted into reusable APIs, accessed over HTTPS with JavaScript. These can be custom-built or leverage third-party services.\n- Markup: templated markup should be prebuilt at deploy time, usually using a site generator for content sites, or a build tool for web apps.\n\n![](/assets/images/2021-03-20-18-45-27.png)\n\n## Examples\n### Site Generators\nNext.js, Gatsby, Jekyll\n\n### Headless CMS\nNetflify\n\n## Architecture\nPurpose is to make the web faster, more secure, and easier to scale\n- core principles are pre-rendering, and decoupling\n    - pre-render - to generate the markup which represents a view (front-end) in advance of when it is required. This happens during a build rather than on-demand so that web servers do not need to perform this activity for each request recieved.\n- entire front end is prebuilt into highly optimized static pages and assets during a build process. This process of pre-rendering results in sites which can be served directly from a CDN. Since the CDN only has to serve already-rendered markup, it can be delivered very quickly and securely\n    - On this foundation, Jamstack sites can use JavaScript and APIs to talk to backend services, allowing experiences to be enhanced and personalized.\n- The Jamstack philosophy is to be modular and have a strong capacity to be able to hook into various 3rd party services/APIs\n\n## Benefits\n### Security\n- Jamstack naturally results in less moving parts, meaning there are naturally less surfaces to have to protect against.\n- Since CDNs only serve pre-generated files, meaning we can use read-only hosting, further reducing the damage that a third party can do.\n\n### Scaling\nsince everything is pre-generated, the CDN can cache the whole site.\n\n### Performance\nJamstack sites remove the need to generate page views on a server at request time by instead generating pages ahead of time during a build.\n\n### Maintenance\nJamstack apps are easier to maintain, as they only need to be served directly from a simple host (or CDN)\n- The work was done during the build, so now the generated site is stable and can be hosted without servers which might require patching, updating and maintain.\n\n## Approaches\n#### Thinking from CDN perspective\nBecause Jamstack projects don’t rely on server-side code, they can be distributed instead of living on a single server. Serving directly from a CDN unlocks speeds and performance that can’t be beat. The more of your app you can push to the edge, the better the user experience.\n\n#### Automating builds\nBecause Jamstack markup is prebuilt, content changes won’t go live until you run another build. Automating this process will save you lots of frustration. You can do this yourself with webhooks, or use a publishing platform that includes the service automatically.\n\n#### Atomic Deploys\nAs Jamstack projects grow really large, new changes might require re-deploying hundreds of files. Uploading these one at a time can cause inconsistent state while the process completes. You can avoid this with a system that lets you do “atomic deploys,” where no changes go live until all changed files have been uploaded.\n\n#### Instant Cache Invalidation\nWhen the build-to-deploy cycle becomes a regular occurrence, you need to know that when a deploy goes live, it really goes live. Eliminate any doubt by making sure your CDN can handle instant cache purges.\n\n#### Everything Lives in Git\nWith a Jamstack project, anyone should be able to do a git clone, install any needed dependencies with a standard procedure (like npm install), and be ready to run the full project locally. No databases to clone, no complex installs. This reduces contributor friction, and also simplifies staging and testing workflows.\n\n","n":0.041}}},{"i":902,"$":{"0":{"v":"Hosting","n":1}}},{"i":903,"$":{"0":{"v":"Vps","n":1},"1":{"v":"\n## VPS Hosting vs Shared Hosting\n### Shared Hosting\n- share space on a server along with other websites.\n- best utilized for static sites.\n\n### VPS Hosting\n- have superuser access to system\n- for practical purposes, they are functionally equivalent to a dedicated physical server.\n- underlying hardware is shared with other VPSes, which may result in lower performance \n- the virtualization aspect provides a high level of security.\n- Because each virtual server is isolated from the others, it can run its own OS\n- the physical server runs a hypervisor, which creates, manages, and allocates resources to the guest OSes (VMs)","n":0.102}}},{"i":904,"$":{"0":{"v":"Gitops","n":1},"1":{"v":"\nGitOps is implemented by using the Git distributed version control system (DVCS) as a single source of truth for declarative infrastructure and applications. Every developer within a team can issue pull requests against a Git repository, and when merged, a \"diff and sync\" tool detects a difference between the intended and actual state of the system. Tooling can then be triggered to update and synchronise the infrastructure to the intended state.\n\n# UE Resources\nhttps://www.weave.works/blog/gitops-operations-by-pull-request","n":0.117}}},{"i":905,"$":{"0":{"v":"Distributed Computing","n":0.707},"1":{"v":"\nA distributed system is a system whose components are spread across many machines on a single network. Therefore, when we think of a distributed system we think of it as a network— rather than a single computer.\n\nData partition and replication strategies lie at the core of any distributed system.\n\nDistributed systems typically have the following characteristics:\n- concurrency of components\n- lack of a global clock\n- failure of components\n\nKey characteristics of a distributed system include Scalability, Reliability, Availability, Efficiency, and Manageability. \n\nMessages may be passed between nodes of a distributed system by using technologies like HTTP, [[RPCs|deploy.distributed.RPC]], or message queues\n\nHistorically, *distributed* referred to the fact that there were multiple computers all working in unison to solve some problem. Nowadays (as of 2021), the term is much broader, and can be used to refer to a system of multiple autonomous processes existing on a single machine. In this case, a node of the \"network\" would be a single process, rather than an entire computer.\n- In the case of distributed computing, a node is simply something that passes messages and has its own local memory.\n\nThere is no clear distinction between distributed systems and parallel systems, and there is a lot of overlap, thought one distinction is that the nodes of a parallel system all share the same memory, while nodes of a distributed system manage their own:\n![Distributed system vs Parallel system](/assets/images/2021-07-16-13-10-56.png)\nExamples of distributed systems: MMOs\n\n## Considerations for distributed systems\nAccording to [[CAP theorem|deploy.distributed.CAP-theorem]], out of consistency, availability and partition tolerance, you can only really achieve 2/3.\n\n\n* * *\n\n### Distributed Databases\nImagine having billions of users, along with trillions of other pieces of data associated with these users. Of course, a single machine is not enough to handl\n\n# UE Resources\n[Good Speaker (Kafka Guy)](https://www.youtube.com/watch?v=Y6Ev8GIlbxc)\n[Considerations for distributed systems](https://www.aosabook.org/en/distsys.html)\n[Distributed Transactions Without Atomic Clocks](https://vimeo.com/545130381)\n","n":0.059}}},{"i":906,"$":{"0":{"v":"Strategies","n":1},"1":{"v":"\n### Approaches to common problems arising because of a distributed system\n#### Key Generation System\nImagine we were making a URL shortener like TinyURL. As a database solution, we opt for a NoSQL approach key-value store, where each key is the shortform URL, and the value is the longform URL.\n\n##### Naive approach\nEach time a write request is made (ie. user creates a new shortform URL), the application server generates a new random 6 character string, and attempts to insert it into the DB. If that key exists already, then it tries again, until there is no failure. This is naive because it involves a lot of back and forth between application server and database, and results in an unpredictable time complexity.\n\n##### Smarter approach\nHave a standalone Key Generation Service (KGS), which generates the random 6 character strings in advance and stores them in a separate database (the Key-DB). Whenever a user wants to generate a new TinyURL, we take a key from the Key-DB and use it. This takes out the worry of collision and duplications of keys.\n\nThe purpose of the 2 tables is to handle read concurrency issues. The problem we are trying to solve is to not give the same key to two servers (as we don’t want two URLs to have the same short key)\n\nIn our Key-DB, we can have 2 tables to store the keys, for:\n1. `unused_keys`\n2. `used_keys`\n\nThe reason we have 2 tables is to solve read concurrency issues. We are trying to avoid a situation where we give the same key to two application servers. Since there is only 1 KGS in the system at a time, there are inherently no write concurrency issues.\n\nThe KGS can always keep some unused keys in memory, so that whenever a server needs them, it can provide them quickly. As soon as the KGS loads some keys in memory, it moves them from `unused_keys` to `used_keys`, so that we can ensure each server gets unique keys. It's true that if the KGS goes offline, all those keys will be lost (and will still exist in the `used_keys` table). This is ok, since there are such a large amount of keys anyway.\n\nSince the KGS would be a single point of failure, we can have a standby replica, and whenever the primary server dies it can take over to generate and provide keys.\n\n![](/assets/images/2021-10-13-11-24-19.png)\n","n":0.051}}},{"i":907,"$":{"0":{"v":"Locks","n":1},"1":{"v":"\nA *distributed lock manager* (DLM) runs in every machine in a cluster\n\n## Purpose\n[source](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)\nThe purpose of a lock is to ensure that 2+ nodes in a distributed system are not doing the same work. That work might be to write some data to a shared storage system, to perform some computation, to call some external API etc.\n\nAt a high level, there are 2 reasons you might want a lock: efficiency, or ensure correctness.\n- the efficiency problem is less worrysome, and usually results in nothing more than a nuisance. For instance, a user might get 2 password reset emails, or we might make 2 database reads for the same data.\n- the correctness problem is the one to be concerned about. If 2 nodes concurrently work on the same piece of data, we may very well result in a corrupted file, data loss, or permanent inconsistency.\n","n":0.084}}},{"i":908,"$":{"0":{"v":"Load Balancer","n":0.707},"1":{"v":"\nA Load Balancer helps to spread the traffic across a cluster of servers\n- Purpose is to improve responsiveness and availability of applications, websites or databases.\n\nLoad balancing helps you scale horizontally across an ever-increasing number of servers\n\nThe LB sits between a client and server\n- We can add LBs at three places:\n    1. Between the user and the web server\n    2. Between web servers and an internal platform layer, like application servers or cache servers\n    3. Between internal platform layer (e.g. application server) and database/cache.\n\n![](/assets/images/2021-10-12-09-10-37.png)\n\nLB also keeps track of the status of all the resources while distributing requests\n- If a server is not available to take new requests or is not responding or has elevated error rate, LB will stop sending traffic to such a server.\n- Load balancers should only forward traffic to “healthy” backend servers. To monitor the health of a backend server, “health checks” regularly attempt to connect to backend servers to ensure that servers are listening. If a server fails a health check, it is automatically removed from the pool, and traffic will not be forwarded to it until it responds to the health checks again.\n\nSince the LB can itself be a single point of failure, it's considered a good practice to have a second redundant LB to form a cluster.\n- Each LB monitors the health of the other and, since both of them are equally capable of serving traffic and failure detection, in the event the main load balancer fails, the second load balancer takes over.\n\n### Purpose of LB\n- Users experience faster, uninterrupted service.\n    - Users won’t have to wait for a single struggling server to finish its previous tasks. Instead, their requests are immediately passed on to a more readily available resource.\n- Service providers experience less downtime and higher throughput. \n    - Even a full server failure won’t affect the end user experience as the load balancer will simply route around it to a healthy server.\n- Smart load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen.\n\n### LB Algorithms\n- *Least Connection Method* — This method directs traffic to the server with the fewest active connections. This approach is quite useful when there are a large number of persistent client connections which are unevenly distributed between the servers.\n- *Least Response Time Method* — This algorithm directs traffic to the server with the fewest active connections and the lowest average response time.\n- *Least Bandwidth Method* - This method selects the server that is currently serving the least amount of traffic measured in megabits per second (Mbps).\n- *Round Robin Method* — This method cycles through a list of servers and sends each new request to the next server. When it reaches the end of the list, it starts over at the beginning. It is most useful when the servers are of equal specification and there are not many persistent connections.\n    - A problem with Round Robin LB is that we do not consider the server load. As a result, if a server is overloaded or slow, the LB will not stop sending new requests to that server.\n- *Weighted Round Robin Method* — The weighted round-robin scheduling is designed to better handle servers with different processing capacities. Each server is assigned a weight (an integer value that indicates the processing capacity). Servers with higher weights receive new connections before those with less weights and servers with higher weights get more connections than those with less weights.\n- *IP Hash* — Under this method, a hash of the IP address of the client is calculated to redirect the request to a server.\n","n":0.041}}},{"i":909,"$":{"0":{"v":"Cache","n":1},"1":{"v":"\nCaches take advantage of the locality of reference principle: recently requested data is likely to be requested again.\n\nCaches are used in almost every computing layer: hardware, operating systems, web browsers, web applications, and more. \n- while they can exist at all levels in architecture, they are often found at the level nearest to the front end, where they are implemented to return data quickly without taxing downstream levels.\n\n### Cache invalidation\nCache Invalidation is the idea that if the data is modified in the database, it should be invalidated in the cache; if not, this can cause inconsistent application behavior.\n\nTo solve this, there are three main schemes that are used:\n1. *Write-through cache*: Under this scheme, data is written into the cache and the corresponding database simultaneously. The cached data allows for fast retrieval and, since the same data gets written in the permanent storage, we will have complete data consistency between the cache and the storage. Also, this scheme ensures that nothing will get lost in case of a crash, power failure, or other system disruptions.\n    - Although write-through minimizes the risk of data loss, since every write operation must be done twice before returning success to the client, this scheme has the disadvantage of higher latency for write operations.\n\n2. *Write-around cache*: This technique is similar to write-through cache, but data is written directly to permanent storage, bypassing the cache. This can reduce the cache being flooded with write operations that will not subsequently be re-read, but has the disadvantage that a read request for recently written data will create a “cache miss” and must be read from slower back-end storage and experience higher latency.\n\n3. *Write-back cache*: Under this scheme, data is written to cache alone, and completion is immediately confirmed to the client. The write to the permanent storage is done after specified intervals or under certain conditions. This results in low-latency and high-throughput for write-intensive applications; however, this speed comes with the risk of data loss in case of a crash or other adverse event because the only copy of the written data is in the cache.\n\n#### Cache eviction policies\nCache eviction means to free the memory of the old, unused data in the cache.\n\nThere are many cache eviction strategies, and what makes it difficult is that there is no one-size-fits-all approach. Agreeing to use one and not another can be a difficult endeavor.\n\nFollowing are some of the most common cache eviction policies:\n- Least Recently Used (LRU): Discards the least recently used items first (most commonly used).\n- Most Recently Used (MRU): Discards, in contrast to LRU, the most recently used items first.\n- Least Frequently Used (LFU): Counts how often an item is needed. Those that are used least often are discarded first.\n- First In First Out (FIFO): The cache evicts the first block accessed first without any regard to how often or how many times it was accessed before.\n- Last In First Out (LIFO): The cache evicts the block accessed most recently first without any regard to how often or how many times it was accessed before.\n- Random Replacement (RR): Randomly selects a candidate item and discards it to make space when necessary.\n\n#### Why is cache invalidation considered difficult?\nSuccess in a caching implementation involves seeing into the future, or multiple possible futures, which is hard in any non-deterministic situation.\n- Sources of non-determinism include both your own workload as well as other workloads that may affect the cost of things you depend on (cache fill / eviction actions, for example).\n\nThe two main reasons why cache invalidation is difficult:\n1. Space complexity issues\n    - You have to guess when the data is not likely to be needed in memory; It can't be too short that the cache is useless, and too long that you'll get a memory leak.\n    - Suppose we had infinite memory, then cache all the data; but we don't so we have to decide what to cache that is meaningful to have the cache implemented (is a ??K cache size enough for your use case? Should you add more?) - It's the balance with the resources available.\n\n2. Time complexity issues\n    - The underlying data might get changed by another process and then your process that uses the cache will be working with incorrect data\n    - Suppose all your data was immutable, then cache all the data indefinitely. But this isn't always to case so you have to figure out what works for the given scenario (A person's mailing address doesn't change often, but their GPS position does).\n\n\n#### Shopping cart example\nLet's say User1 adds 5 different things in their shopping cart. The inventory count of those items are cached in Redis or Memcache.\n\nWhile the user is checking out, a Manual Inventory has been completed and gets loaded into the system. A manual inventory is the result of someone walking through a warehouse and counting items by hand. They discover that there are actually zero of a certain item that happens to be in User1's digital cart. The inventory person obviously doesn't know that.\n\nUser1 pays for the items that the system says they still have inventory for. Ideally, an error occurs as the purchasing system attempts to move a number of items from \"in stock\" to \"sold\", as there are actually none \"in stock\". This happens a lot when one item is next to another item and they have very similar packaging. Warehouse employees will accidentally ship the wrong item. If the cart checkout code was less defensive, it may just set a negative \"in stock\" value and the company would have a more serious problem of selling something they didn't own.\n\nHopefully, the person who wrote the Manual Inventory Entry code knows about the caching that the shopping cart does and sends them some sort of message that \"inventory has been updated. Please invalidate these cache entries.\" But that requires that they know exactly what is being cached and how it's being cached; otherwise they may not give the right information to ensure that data is being invalidated. ie- \"Please invalidate the following SKU's - \" vs \"Please invalidate all SKU's associated with Vendor XYZ\".\n\n* * *\n\n## Parameter cache requirements\n### Cache memory\nHow much cache memory should we have? We can start with 20% of daily traffic and, based on clients’ usage patterns, we can adjust how many cache servers we need.\n- Alternatively, we can use a couple of smaller servers to store all the cached data.\n- Since a modern-day server can have 256GB of memory, we can easily fit all the cache into one machine.\n\n## When to cache\n- We don't want to cache many keys that change continuously.\n- We don't want to cache many keys that are requested very rarely.\n- We want to cache keys that are requested often and change at a reasonable rate. For an example of key not changing at a reasonable rate, think at a global counter that is continuously INCRemented.\n","n":0.03}}},{"i":910,"$":{"0":{"v":"RPC","n":1},"1":{"v":"\nA Remote Procedure Call (RPC) is when we call a function that lives on a different machine, without the programmer explicitly coding the details for the remote interaction\n- That is, the programmer writes essentially the same code whether the subroutine is local to the executing program, or remote\n- caller is client, executor is server\n\nThe RPC model implies a level of location transparency, namely that calling procedures are largely the same whether they are local or remote, but usually they are not identical, so local calls can be distinguished from remote calls.\n\nRemote calls are usually orders of magnitude slower and less reliable than local calls, so distinguishing them is important.\n\nRPCs are a form of inter-process communication\n- ie. different processes have different address spaces\n\n# UE Resources\n[node and Go tutorial](https://blog.logrocket.com/introduction-to-rpc-using-go-and-node/)","n":0.089}}},{"i":911,"$":{"0":{"v":"CDN","n":1},"1":{"v":"\nCDNs are a kind of cache that comes into play for sites serving large amounts of static media. In a typical CDN setup, a request will first ask the CDN for a piece of static media; the CDN will serve that content if it has it locally available. If it isn’t available, the CDN will query the back-end servers for the file, cache it locally, and serve it to the requesting user.\n- If the system you are building is not large enough to have its own CDN, you can ease a future transition by serving the static media off a separate subdomain (e.g., static.yourservice.com) using a lightweight HTTP server like Nginx, and cut-over the DNS from your servers to a CDN later.\n\n- A website server normally has a caching server along with it, which will cache the webpage. All requests to the URL will retrieve the content from the cache, until the expiry time is hit. At that point, the user will make a fetch, which will see that the cached content is expired, and will then hit the server, which will retrieve the content, pass it to the caching server, which passes it to the user. All subsequent calls (from other users) will be intercepted by the caching server until it is expired (warm caching)\n![](/assets/images/2021-03-11-15-50-18.png)\n- A CDN provider will place servers in many locations, but some of the most important are the connection points at the edge between different networks\n- Without a CDN, transit may take a slower and/or more convoluted route between source and destination\n- Originally, a CDN was like a cache for static assets that don't change, like images, logos, stylesheets. Nowadays, that benefit only make up ~10% of everything a CDN can do. \n- while CDNs perform caching, not everything that performs caching is a CDN\n- CDNs reduce the importance of where your web server is located, since this would theoretically only impact the timecost of the first fetch of data of each expiry cycle.\n\t- In practice, this is only true for data that doesn't change often. If we are talking about a Facebook news feed that updates frequently, then you are still going to be interfacing a lot with the server, making the location of your server ultimately still important. \n- The DNS plays a central role in the CDN \n- a CDN is known as a distributed internet service\n\n![](/assets/images/2021-03-11-15-50-32.png)\n\nIf any of the following is true for your website, you should definitely opt for a CDN:\n- Large amounts of traffic\n- A scattered, international user-base\n- Expected growth\n- Lots of media content, such as images and videos\n\nIf the answer to all 4 is no, you won’t notice a difference in speed, but you will still get the benefit of DDoS protection.\n","n":0.047}}},{"i":912,"$":{"0":{"v":"CAP-theorem","n":1},"1":{"v":"\nCAP theorem states that it is impossible for a distributed system to simultaneously provide all three of: consistency, availability, and partition tolerance. Only 2 can be achieved.\n- We cannot build a general data store that is continually available, sequentially consistent, and tolerant to any partition failures.\n- Therefore, the theorem can really be stated as: \"In the presence of a network partition, a distributed system must choose either Consistency or Availability.\"\n    - note: this seems to be at least somewhat of a controversial tone, given that SQL databases occupy CA. This view above would state that CA systems are incoherent, though this opinion might not appear to be reputable, given the prevalence of relational databases.\n    - regarding CA systems then, what CAP theorem would argue is that these systems have an inherent weakness, which is that in the case of a network partition, they will be forced to give up either consistency or availability.\n\n### Consistency (C) \nAll nodes see the same data at the same time. This means users can read or write from/to any node in the system and will receive the same data. It is equivalent to having a single up-to-date copy of the data.\n\nDepending on the business needs of the application, we may wish to make the trade-off of sacrificing consistency for availability. Say we are implementing a facebook newsfeed. It is acceptable for the application to miss some data points here and there. For instance, if a friend of yours uploads a new post, it's not of critical importance that you get that data right away. That is, if one client is getting its data from a data store that is not up-to-date, then it's not the end of the world (assuming everything can be made up-to-date in a timely manner).\n\n### Availability (A) \nAvailability means every request received by a non-failing node in the system must result in a response. Even when severe network failures occur, every request must terminate. In simple terms, availability refers to a system’s ability to remain accessible even if one or more nodes in the system go down.\n\nIn an AP (Availability/Partition-tolerant) system, the system is essentially saying “I will get you to a node, but I do not know how good the data you find there will be”; or “I can be available and the data I show will be good, but not complete.”\n\n### Partition tolerance (P) \na.k.a robustness\n\nPartition tolerance is the ability of a data processing system to continue processing data even if a network partition causes communication errors between subsystems\n- A single node failure should not cause the entire system to collapse.\n- A partition is a communication break (or a network failure) between any two nodes in the system, i.e., both nodes are up but cannot communicate with each other. \n\nA partition-tolerant system continues to operate even if there are partitions (ie. communication breakdowns) in the system. Such a system can sustain any network failure that does not result in the failure of the entire network. Data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages.\n\n![](/assets/images/2021-10-12-10-38-14.png)\n\n## PACELC Theorem\nOne place where the CAP theorem is silent is what happens when there is no network partition? What choices does a distributed system have when there is no partition?\n\nThe PACELC theorem states that in a system that replicates data:\n- if there is a partition (‘P’), a distributed system can tradeoff between availability and consistency (i.e., ‘A’ and ‘C’);\n- else (‘E’), when the system is running normally in the absence of partitions, the system can tradeoff between latency (‘L’) and consistency (‘C’).\n![](/assets/images/2021-10-12-10-43-34.png)\n\nThe first part of the theorem (PAC) is the same as the CAP theorem, and the ELC is the extension. The whole thesis is assuming we maintain high availability by replication. So, when there is a failure, CAP theorem prevails. But if not, we still have to consider the tradeoff between consistency and latency of a replicated system.\n\n### Examples\n- Dynamo and Cassandra are PA/EL systems: They choose availability over consistency when a partition occurs; otherwise, they choose lower latency.\n- BigTable and HBase are PC/EC systems: They will always choose consistency, giving up availability and lower latency.\n- MongoDB can be considered PA/EC (default configuration): MongoDB works in a primary/secondaries configuration. In the default configuration, all writes and reads are performed on the primary. As all replication is done asynchronously (from primary to secondaries), when there is a network partition in which primary is lost or becomes isolated on the minority side, there is a chance of losing data that is unreplicated to secondaries, hence there is a loss of consistency during partitions. Therefore it can be concluded that in the case of a network partition, MongoDB chooses availability, but otherwise guarantees consistency. Alternately, when MongoDB is configured to write on majority replicas and read from the primary, it could be categorized as PC/EC.\n","n":0.035}}},{"i":913,"$":{"0":{"v":"Db","n":1},"1":{"v":"\nReplicated tables are typically static data that does not change very often. Replicating them allows for read scalability.","n":0.236}}},{"i":914,"$":{"0":{"v":"Continuous Integration","n":0.707},"1":{"v":"\n\"continuous\" means that it happens with each \"checkin\" of the code (updating code on the host\n\na CI system like Travis CI will automatically build code and run tests, and provide immediate feedback on the result of those tests and build \n- a CI can also automate other parts of your development process by managing deployments and notifications.\n\n# Travis CI\n- when you instruct a build to be made, Travis clones the github repo into a brand-new virtual env, and runs the tests and creates the build.\n\t- if any of those tasks fail, the build is considered broken\n\t- if all tasks succeed, the build passes and Travis CI can deploy the code\n\nGithub Actions is github's CI system\n\nbuilding and testing happens in CI time\n\n## UE Resource\n[Good primer](https://hackernoon.com/continuous-integration-circleci-vs-travis-ci-vs-jenkins-41a1c2bd95f5)\n","n":0.09}}},{"i":915,"$":{"0":{"v":"CD","n":1},"1":{"v":"\n## Continuous Deployment\nThis process is about taking all of the commits we make during the development process, and putting them out into production. As part of this process, we subject our code to a multitude of tests (unit, e2e etc.)\n\n## Continuous Delivery\nslightly broader, in the sense that not all changes that go through the pipeline need to necessarily make it into production.\ncontinuous delivery is about making sure that every change a developer makes can make it through the pipeline that includes subjecting our code to a series of tests (1. does it compile? 2. does it pass unit tests? 3. does it pass E2E? etc)\n- This shows how putting code into production is then a business-decision, not a technical one. The executives of a company may decide \"we only want to deploy new releases every 2 weeks, so that the amount of change that user's experience isn't so jarring\". The point is that from a technical standpoint, we can do this— we just decide not to for business reasons.\n\nThis above explanation shows that in order to do Continuous Delivery, we have to already be doing Continuous Deployment\n\n### Why?\n- reduced deployment risk\n    - with smaller deployments, it's easier to spot when/where something goes wrong\n- user feedback\n    - with incremental and automated change, it's easier to get more reliable feedback from users, rather than having to depend on what they tell you they want.","n":0.066}}},{"i":916,"$":{"0":{"v":"Dendron","n":1}}},{"i":917,"$":{"0":{"v":"Vault","n":1},"1":{"v":"\n### Local Vault\nA local vault is a folder in your file system.\n\n### Remote Vault\nA remote vault is a git repository. If you choose a remote vault, you can choose from a vault from the registry or enter a custom git url.\n- Note that when you add a remote vault, the url can also point to a remote workspace. In that case, dendron will inspect the dendron.yml to get a list of all vaults within the workspace and add all vaults from inside the workspace.\n","n":0.109}}},{"i":918,"$":{"0":{"v":"Searching","n":1},"1":{"v":"\n#### Scoping search to a sub-tree\n1. Open vscode advanced search with `<Cmd+shift+f>`\n2. Input search term\n2. In \"files to include\", include as many hierarchies (separated by `.`), followed by `**` at the end\n    - ex. Search within `postgres` domain - `postgres**`\n    - ex. `graphql.operators**`\n","n":0.152}}},{"i":919,"$":{"0":{"v":"Dendron Commands","n":0.707},"1":{"v":"\nGo up one level\n`cmd+shift+up`\n\nOpen Lookup with current path pre-populated\n`cmd+shift+down`\n\nCycle between note siblings\n`cmd+shift+[`/`]`\n\nCopy to clipboard a reference link of the highlighted header\n`cmd+shift+r`\n\n### Backlinks\n\nLink to a note\n- `[[code.git.cli]]` will link to a note of the same hierarchy\n- we can generate a new note by entering a new hierarchy within the braces and hitting `F12`\n- we can alias a backlink like this `[[git cli|code.git.cli]]`\n- all backlinks of current note can be seen on sidebar\n\nEmbed a note/section of note in another note\n- You can create a note reference by using CMD+SHIFT+R while inside a note, then paste that reference into another note. This creates an embedded note in the current note.\n\t- Instead of embedding a whole note, we can embed just what's within a header, by highlighting the head before using CMD+SHIFT+R\n\t\t- If we want to retain our original header, remove the `,1` in the reference\n\nLink to particular section of a note\n- You can use `CMD+SHIFT+C`\n\t- can do it for the whole note, or just a highlighted section\n\nExtracting out text to put into a lower-level note\n1. select text to extract\n2. cmd+shift+s\n3. turn \"scratch note\" off, and \"selection extract\" on\n4. rename file\n5. cmd+shift+r from the extracted file\n6. paste into the original file to create the ref to the lower-level file\n\n\n## Navigating\ngo up one level\n- cmd+shift+up","n":0.069}}},{"i":920,"$":{"0":{"v":"Debugging","n":1},"1":{"v":"\nThere are 3 categories of bug origin in our code (LSD)\n1. Logic - Are the steps you used to carry out the operation correct?\n2. Syntax - Is what you are trying to portay in the code the right way to write it given the programming language being used?\n3. Data - Is the data passing through in a way that you'd expected? Does the programmatic state of the program align with what you'd expect from your code?\n","n":0.115}}},{"i":921,"$":{"0":{"v":"Stack Track","n":0.707},"1":{"v":"\nA stack trace is a \"treasure map\" of what happened up until things went wrong","n":0.258}}},{"i":922,"$":{"0":{"v":"Dbeaver","n":1}}},{"i":923,"$":{"0":{"v":"Cmds","n":1}}},{"i":924,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Change Keyboard Shortcuts\n`Preferences` -> `User interface` -> `Keys`\n","n":0.333}}},{"i":925,"$":{"0":{"v":"Db","n":1},"1":{"v":"\nWhen you are designing a database with relations, you have to consider that the database is a living object. At any moment in time, you have definitions and purposes of each specific model. However, as the needs of the database change and grow with your business, you need to be flexible to change the definition of models that no longer makes sense, given that business logic\n- For instance, we previously had the concept of a payment profile in the database, which was basically a credit card. Therefore, you might find a payment profile as \"a way to pay\". However, within corporations have coupons into the payment system, it became apparent that a coupon was really not to do similar from a credit card in the way that it would function. Therefore, it made sense to Change the way that we thought of a payment profile. Incorporating coupons into this model met reconsidering the name. Instead of payment profile, they would be called payment methods. Now the definition became something more like \"some type of exchange good that you can pay with to receive a good in return\". By allowing this definition to change, we were able to leverage existing functionality to incorporate new and unexpected designs into the database model.\n","n":0.069}}},{"i":926,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\n# UE Resources\nhttps://sammeechward.com/mocking-a-database-with-jest-in-javascript/\n","n":0.577}}},{"i":927,"$":{"0":{"v":"Strategies","n":1}}},{"i":928,"$":{"0":{"v":"Sharding","n":1},"1":{"v":"\nSharding is a method of splitting and storing a single logical dataset in multiple databases (running on different servers). By distributing the data among multiple machines, a cluster of database systems can store larger dataset and handle additional requests. Sharding is necessary if a dataset is too large to be stored in a single database\n- Sharding is also referred to as horizontal partitioning\n\nWith sharding, you would know in which shard the data lies based on the ID.\n- illustration: imagine if there were 10 different shards, and the ID of the person was prefixed with a number from 0-9, signifying which shard the data could be found in. This is obviously not how it works, but a database object would have metadata about which shard it could be found in.\n\nEvery sharded cluster needs to have some logic in how it will place each piece of data. For instance, you might implement a simple round-robin using modulus:\n`photoId % 10`\nThis will spread the photos evenly around all 10 of the databases, so that where to retrieve them is predictable.\n\nAn obvious use-case for sharding is how to store locale information in MMORPGs. Consider that the quest text for a quest in French does not have any benefit to being stored together with the English version of it. This is the nature of your sharding strategy: let's increase lookup speed by removing the amount of rows in our database table. We can create a different table, and call it a shard of the first table. As long as the requester of the data knows which shard the data can be found in, then the result can be substantially faster queries.\n\nAnother possibility is that you have a large user table which is accessed heavily. You could create 12 shards for the user data, and give each database server the responsibility over one of the twelve months.","n":0.057}}},{"i":929,"$":{"0":{"v":"Pools","n":1},"1":{"v":"\n# Connection Pool\n- a cache of database connections maintained so that the connections can be reused when future requests to the database are required\n\t- after a connection is created, it is placed in the pool and it is used again so that a new connection does not have to be established\n- The client pool allows you to have a reusable pool of clients you can check out, use, and return\n\t- used to enhance the performance of executing commands on a database\n- If you're working on a web application or other software which makes frequent queries you'll want to use a connection pool.\n- Connecting a new client to the PostgreSQL server requires a handshake which can take 20-30 milliseconds. During this time passwords are negotiated, SSL may be established, and configuration information is shared with the client & server. Incurring this cost every time we want to execute a query would substantially slow down our application.\n- Typically, each application server instance would have its own pool\n\t- ex. If there are 4 Express servers running, then there will be 4 pools\n\t- between those 4 servers and the users sending requests to them sits a *load balancer*\n- The pool sits between the application server(s) and the database:\n```\nCLIENTs → load balancer → Express → pool → database\n```\n\nWith Postgraphile\n- The rootPgPool is not normally what is used with PostGraphile because its privileges are too elevatedt. typically you use rootPgPool for authentication tasks like Passport.js.\n","n":0.065}}},{"i":930,"$":{"0":{"v":"Partitioning","n":1},"1":{"v":"\nThe justification for partitioning a database is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers.\n\nThere are two challenges when we try to distribute data:\n1. How do we know on which node a particular piece of data will be stored?\n2. When we add or remove nodes, how do we know what data will be moved from existing nodes to the new nodes? Additionally, how can we minimize data movement when nodes join or leave?\n\n## Partitioning Methods\nThere are many different schemes one could use to decide how to break up an application database into multiple smaller DBs. Three of the most popular schemes used by various large-scale applications are:\n\n### Horizontal Partitioning (aka Range-based partitioning; Data Sharding)\nSee [[sharding|db.strategies.sharding]]\nIn this scheme, we put different rows into different tables. \n- ex. if we store different places in a table, we can decide that locations with ZIP codes less than 10000 are stored in one table and places with ZIP codes greater than 10000 are stored in a separate table.\n    - The key problem with this approach is that if the value whose range is used for Partitioning isn’t chosen carefully, then the partitioning scheme will lead to unbalanced servers. In the previous example, splitting locations based on their zip codes assume that places will be evenly distributed across the different zip codes. This assumption is not valid as there will be a lot of places in a thickly populated area like Manhattan as compared to its suburb cities.\n\nConsider the flexibility afforded when you take a horizontal partitioning strategy. We can have a large number of logical partitions to accommodate future data growth, such that in the beginning, multiple logical partitions reside on a single physical database server. Since each database server can have multiple database instances running on it, we can have separate databases for each logical partition on any server. So whenever we feel that a particular database server has a lot of data, we can migrate some logical partitions from it to another server. We can maintain a config file (or a separate database) that can map our logical partitions to database servers; this will enable us to move partitions around easily. Whenever we want to move a partition, we only have to update the config file to announce the change.\n\n### Vertical Partitioning\nIn this scheme, we divide our data to store tables related to a specific feature in their own server. For example, if we are building an Instagram-like application - where we need to store data related to users, photos they upload, and people they follow - we can decide to place user profile information on one DB server, friend lists on another, and photos on a third server.\n\nVertical Partitioning is straightforward to implement and has a low impact on the application. The main problem with this approach is that if our application experiences additional growth, then it may be necessary to further partition a feature specific DB across various servers (e.g. it would not be possible for a single server to handle all the metadata queries for 10 billion photos by 140 million users).\n\n### Directory-Based Partitioning\nA loosely coupled approach to work around issues mentioned in the above schemes is to create a lookup service that knows your current partitioning scheme and abstracts it away from the DB access code. So, to find out where a particular data entity resides, we query the directory server that holds the mapping between each tuple key to its DB server. This loosely coupled approach means we can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application.\n\n## Problems that arise from partitioning a database\nMost of these constraints are due to the fact that operations across multiple tables or multiple rows in the same table will no longer run on the same server.\n\n1. *Joins and Denormalization*: Performing joins on a database that is running on one server is straightforward, but once a database is partitioned and spread across multiple machines it is often not feasible to perform joins that span database partitions. Such joins will not be performance efficient since data has to be compiled from multiple servers. \n- A common workaround for this problem is to denormalize the database so that queries that previously required joins can be performed from a single table. Of course, the service now has to deal with denormalization’s perils, such as data inconsistency.\n\n2. *Referential integrity*: just like how performing a cross-partition query on a partitioned database is not feasible, trying to enforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult.\n- Most RDBMS do not support foreign keys constraints across databases on different database servers. This means, applications that require referential integrity on partitioned databases often have to enforce it in application code. Often in such cases, applications have to run regular SQL jobs to clean up dangling references.\n\n3. *Rebalancing*: There could be many reasons we have to change our partitioning scheme:\n    - The data distribution is not uniform, e.g., there are a lot of places for a particular ZIP code that cannot fit into one database partition.\n    - There is a lot of load on a partition, e.g., there are too many requests being handled by the DB partition dedicated to user photos.\n\nIn such cases, either we have to create more DB partitions or have to rebalance existing partitions, which means the partitioning scheme changed and all existing data moved to new locations. Doing this without incurring downtime is extremely difficult. Using a scheme like directory-based Partitioning does make rebalancing a more palatable experience at the cost of increasing the complexity of the system and creating a new single point of failure (i.e. the lookup service/database).\n\n","n":0.032}}},{"i":931,"$":{"0":{"v":"Pagination","n":1},"1":{"v":"\n# Pagination\n## Limit/Offset Pagination\nA naive approach to pagination is to think like this: \"We want page 3, with a page size of 10, so we should load 10 items, starting after item 20\". This might look like this:\n```\nSELECT * FROM posts ORDER BY created_at LIMIT 10 OFFSET 20;\n```\nThis approach is naive because it has some major downfalls\n- If an item was added to the list while the user is switching pages, we will inevitably be skipping over items (ex. product site, chat app)\n- If an item was added at the top of the list while switching pages, then we might see the same item twice.\nThese previous examples show that the mental model of pages in a book is a poor analogy for pagination, since the data set is not static. This goes to show that for certain apps, the very concepts of page1 and page2 don't really make any sense, because the set of data and the boundaries between loaded sections is constantly changing.\n\n## Cursor-Based Pagination\nWhat if we could just specify the place in the list we want to begin, and then specify how many items we want to fetch? Then it doesn’t matter how many items were added to the top of the list in the meanwhile, since we have a constant pointer (cursor) to the specific spot where we left off.\n\nA cursor-based paginator needs 2 things to fetch more data: the current cursor position, and the number of items to fetch.\n\n*\"Cursor-based pagination is the most efficient method of paging and should always be used where possible.\"*\n- besides cursor-based pagination, the most\n","n":0.062}}},{"i":932,"$":{"0":{"v":"Migrations","n":1},"1":{"v":"\na migration is just a script that performs a set of synchronous database interactions\npreservation of data during migrations in general is not guaranteed because schema changes such as the deletion of a database column can destroy data\n- Therefore, you should never `drop table` in a migration if there is still data in that table.\n\nmigrations should always be backwards compatible. This gives us the freedom to rollback the migration, and have a database that is still valid.\n- The proper approach to migrations is to expand the schema to work with the old version as well as the new version, then contract it to work only with the new version\n- Imagine 2 different users have different versions of an application. Upon signup, the older version asks for first and last name, while the newer asks for middle name as well.\n\n# Migrations without downtime\nhttps://thorben-janssen.com/update-database-schema-without-downtime/","n":0.084}}},{"i":933,"$":{"0":{"v":"Indexing","n":1},"1":{"v":"\nIndexing makes querying by a column faster, at the slight expense of create/update speed and database size.\n- For example, you will often want to query all comments belonging to a post (that is, query comments by its `post_id` column), and so you should mark the `post_id` column as indexed.\n- However, if you rarely query all comments by its author, indexing `author_id` is probably not worth it.\n- In general, most `_id` fields are indexed. Sometimes, boolean fields are worth indexing if you often use it for queries. However, you should almost never index date (`_at`) columns or string columns.\n\nIn the case of data sets that are many terabytes in size, but have very small payloads (e.g., 1 KB), indexes are a necessity for optimizing data access. Finding a small payload in such a large dataset can be a real challenge, since we can’t possibly iterate over that much data in any reasonable time.\n- Furthermore, it is very likely that such a large data set is spread over several physical devices—this means we need some way to find the correct physical location of the desired data. Indexes are the best way to do this.\n\nIndexes are hidden objects that can exist against a table. You can create an index against a set of columns and the SQL engine can jump to the specific record based on the index.\n- One way of optimizing queries is to create an index on a column or columns you might commonly filter/join against.\n\n- spec: so we should index the things that we are likely to query on. If we frequently want to retrieve books from the db filtered by a certain year, we'd want to add an index onto the \"year\" column, making that data easier to retrieve.\n\t- spec: otherwise we'd have to essentially search through every single book to see if it satisfies the filter requirements for the year we are seeking.\n\nAn index allows the system to have new options to find the data that your queries need\n- In the absence of an index, the only option available to your database is a sequential scan of your tables. The index access methods are meant to be faster than a sequential scan, by fetching the data directly where it is.\n\n### Umbrella Analogy\nThe metal tips on the edge of an umbrella represent the items that are available for retrieval in a database. If you were not using an index, you'd have to look at each tip to see if it's the item you want or not. With an index, you look from the top of the umbrella, and each arm leading to the tip would light up, indicating which items should be retrieved.\n- therefore, indexes basically tell us how to get a certain piece of data.\n\n### Textbook Analogy\nImagine studying for a test and you stumbled upon a topic you don't understand or forgot. You might consider looking at the INDEX at the back of the textbook for the topic. The index will tell you which page to go find the information. You can then jump straight to the correct page. Now imagine not having an index at the back of the textbook. You'd have to skim through the ENTIRE book until you found what you were looking for.","n":0.043}}},{"i":934,"$":{"0":{"v":"Etl","n":1},"1":{"v":"\n# ETL Pipeline\nStands for **Extract**, **Transform**, **Load**\n\nThe ETL Pipeline can be thought as a series of processes whose goal is to take data from some external source, transform it to fit our needs, then loading that transformed data into our own database. \n- With this capability, we are able to enhance reporting, analysis and data synchronization.\n\n## Extract\n- data might be extracted from business systems, APIs, data from physical sensors, marketing tools, transaction databases (eg. Stripe)\n\n## Transform\ndata is temporarily stored in at least one set of staging tables as part of the ETL process\n\n## Load\nthe load phase doesn't have to be the end of the pipeline. Once the data has been successfully inserted into our database, it can trigger webhooks in other systems to perform more actions.\n","n":0.089}}},{"i":935,"$":{"0":{"v":"Cursors","n":1},"1":{"v":"\n# Cursor\n- a cursor is a piece of data (likely just an ID) that represents a location in a paginated list.\n- the cursor is the thing that allows us to traverse over records in a database\n- A cursor can be viewed as a pointer to one row within a set of rows\n- cursors also facilitate retrieval of the records, as well as creating and removing records\n- a cursor is conceptually and behaviourally very similar to an iterator\n- Cursors are used to process individual rows returned by database system queries\n- a cursor makes it possible to define a result set and perform complex logic on it, on a row by row basis","n":0.095}}},{"i":936,"$":{"0":{"v":"Concurrency","n":1},"1":{"v":"\nConcurrency control is about the database ensuring that correct results for concurrent operations are generated, while getting those results as quickly as possible","n":0.209}}},{"i":937,"$":{"0":{"v":"Cleaning","n":1},"1":{"v":"\n### TinyURL example\nImagine a database where the main data entry is a shortened URL. In this structure, the shortform of the URL is matched with the longform. At some point, it will make sense to delete entries. For instance, the user may choose to have the URL only last for 24 hours, after which point it will no longer be a valid URL. In this scenario, all we need to do is introduce an `expired` property/column to the object/table, and check it any time a user tries to access the shortform URL. Now this leads to the principle issue of cleaning: when do we go about deleting items from the database?\n- If we chose to continuously search for expired links to remove them, it would put a lot of pressure on our database. Instead, we can slowly remove expired links and do a lazy cleanup.\n- additionally, \n    - Whenever a user tries to access an expired link, we can delete the link and return an error to the user.\n    - A separate Cleanup service can run periodically to remove expired links from our storage and cache. This service should be very lightweight and scheduled to run only when the user traffic is expected to be low.\n    - We can have a default expiration time for each link (e.g., two years).\n    - After removing an expired link, we can put the key back in the key-DB to be reused.\n    - Should we remove links that haven’t been visited in some length of time, say six months? This could be tricky. Since storage is getting cheap, we can decide to keep links forever.\n","n":0.061}}},{"i":938,"$":{"0":{"v":"Backward Compatability","n":0.707},"1":{"v":"\n# UE Resources\n[Recommendation from Juan](https://spring.io/blog/2016/05/31/zero-downtime-deployment-with-a-database)\n[Recommendation from Juan](https://thorben-janssen.com/update-database-schema-without-downtime)\n","n":0.378}}},{"i":939,"$":{"0":{"v":"Local","n":1},"1":{"v":"\n# Local storage\n- anything inside local storage is accessible by any script inside your page. This is why storing JWTs in local storage is a bad idea, but storing state about the app would be fine.\n\n# Resources\n### real-time data\n[RethinkDB](https://github.com/rethinkdb/rethinkdb)\n[realtime](https://github.com/supabase/realtime)","n":0.16}}},{"i":940,"$":{"0":{"v":"Cluster","n":1},"1":{"v":"\n# Cluster\n- a collection of databases servers (ie. nodes, instances) that are connected to a single database file. It is made up of one master node, and 1+ slave nodes.\n- The benefits of clustering are: data redundancy, load balancing, high availability, and monitoring and automation.\n- a collection of databases managed by a single PostgreSQL server instance constitutes a database cluster.\n- a cluster is created with the `initdb` command\n- in file system terms, a database cluser is a single directory under which all data is stored.\n\t- normally `/usr/local/pgsql/data` or `/var/lib/pgsql/data` (determined by the package)\n- the unix user postgres should own this directory\n\n## Node\n- **node** - an instance of a database server\n- the master node is typically the only one that gets written to. \n\t- The master can operate without slaves, and if there is an emergency, the slave node can be promoted to master, since it has the most up to date data.\n- the slave node typically exists as a backup, or as a read replica\n\t- a read replica means that the read and write traffic can be split between the two","n":0.074}}},{"i":941,"$":{"0":{"v":"Acid","n":1},"1":{"v":"\n# ACID\n\"As a developer, we should think of a database as something that is ACID-compliant.\" —Dmitri Fontaine\n\n* * *\n\nFrom the Postgres-XL docs, here is an implication of it being fully ACID:\n*When you start a transaction or query in Postgres-XL, you’ll see a consistent version of your data across the entire cluster. While you are reading your data on one connection, you could be updating the same table or even row in another connection without any locking. Both connections are working with their own versions of the rows, thanks to global transaction identifiers and snapshots.  Readers do not block writers and writers do not block readers.*","n":0.098}}},{"i":942,"$":{"0":{"v":"Isolated","n":1},"1":{"v":"\n# Isolated\n- the opposite side of atomicity.\n- while we are doing our queries, are we allowed to see what is happening concurrently in the rest of the system?\n\t- ex. what if we want to make a backup with `pg_dump` that needs to run for several hours? That backup needs to be a consistent snapshot of the production database. If during the backup someone is doing inserts, we don't want these to be in the backup, since we want a snapshot that doesn't move.\t\t- to do this, postgres uses an isolation mode that prevents this from happening.\t\n","n":0.102}}},{"i":943,"$":{"0":{"v":"Durable","n":1},"1":{"v":"\n# Durable\n- the idea that anything that has been known to have been committed by the client should still be there if we were to theoretically remove the power from our machine partway through the operation. \n- In Postgres, Durability is provided through write-ahead logs\n","n":0.149}}},{"i":944,"$":{"0":{"v":"Consistent","n":1},"1":{"v":"\n- The idea that we have business-logic that we can share with Postgres, and have it implement those guarantees for us. \n\t- ex. each employee must have a non-negative integer for their salary\n- Includes things like what data types we use, the shape of our schema, what constraints we have (not null, fkey etc.), relations...\n\t- ex. The schema and the data types. When we define these structures in our database, all data that is entered must comply with the rules set out. For instance, if we have an id column of type int, then the data entering *must* be an int.\n\t- ex. the fact that MongoDB is schemaless means that we do not have Consistency. Nor does it have transactions (the A and I)\n","n":0.09}}},{"i":945,"$":{"0":{"v":"Atomic","n":1},"1":{"v":"\n# Atomic\n- The idea that if we have multiple things to do in a single transaction, we can roll it back as a chunk\n\t- Imagine a transaction consists of 2 inserts and 1 update. The fact that we have atomicity means that we can rollback that whole transaction. This distinction is even more important when you consider multi-table transactions, which MongoDB does not offer.\n\t\t- ex. In old versions of MongoDB if you needed to remove an item from inventory and add it to someone's order at the same time, you could not.\n\t- Imagine a situation we want to update the schema of our database (eg introduce a new table or modify an existing one). If our system crashed in the middle of the update and our database didn't have Atomicity, then the result would be a mix between v1 and v2.\n- In Postgres, Atomicity is provided through write-ahead logs. Shadow Paging is another technique to provide atomicity.\n\n### Transaction\n- logically clumping together multiple database interactions (CRUD) as if they were one action\n\t- ex. in a balance sheet, we want to treat the asset change that corresponds to the liability+shareholer's equity change as atomic. Without transactions, if we were to make one change and for some reason the second change fell over, we would have an imbalanced balance sheet. At least with Transactions, we can be guaranteed that this would never occur\n- When an execution of transaction is interrupted, the transaction is not executed at all.\n\n### Write-Ahead Logging (WAL)\n- Before the results of a transaction are written to the database, the changes are first recorded in a log. This log is written to stable storage to ensure that the whole complete file is saved. Only once this log is known to be securely stored is the change made to the database. \n\t- ex. imagine the machine hosting the database lost power midway through performing some operation. When the machine boots back up, we can use WAL to determine what atomic changes were *supposed* to have been made, and compare that to what actually changed. With this information, we can determine whether or not we should rollback. This guarantees both atomicity and durability.\n","n":0.053}}},{"i":946,"$":{"0":{"v":"Database Management System","n":0.577},"1":{"v":"\n# DBMS\n*\"SQL tells the database what information you want, and the DBMS (Postgres, MySQL) determines the best way to provide it.\"*\n- its role in your software architecture is to handle concurrent access to live data that is manipulated by several applications, or several parts of an application.\n\nWhen a select query is made, The database translates the query into a \"query plan\" which may vary between executions, database versions and database software. This functionality is called the \"query optimizer\" as it is responsible for finding the best possible execution plan for the query, within applicable constraints.\n\nGoal of a DBMS is to handle concurrent access to live data that is manipulated by several applications, or several parts of an application\n- at the core of this is the concept of a transaction (RDBMS)\n\n### Object–relational Impedance Mismatch\nThe object–relational impedance mismatch is a set of conceptual and technical difficulties that are often encountered when a relational database management system (RDBMS) is being served by an application program (or multiple application programs) written in an object-oriented programming language or style, particularly because objects or class definitions must be mapped to database tables defined by a relational schema.\n\n\n# UE Resources\n- [Evolutionary database design](https://martinfowler.com/articles/evodb.html)","n":0.071}}},{"i":947,"$":{"0":{"v":"CSS","n":1},"1":{"v":"\n## Mobile-first\n`@media` queries should favor `min-width`\n- Max-width queries come into play when you want styles to be constrained below a certain viewport size. A combination of both min-width and max-width media queries will help to constrain the styles between two different viewport sizes.\n\n`min-width: 800px` => \"these styles will apply once you hit 800px\"\n`max-width: 1200px` => \"these styles no longer apply once you hit 1200px\"\n\n# E Resources\n[Writing mobile-first CSS: good examples](https://zellwk.com/blog/how-to-write-mobile-first-css/)\n","n":0.12}}},{"i":948,"$":{"0":{"v":"Transform","n":1},"1":{"v":"\n3dtransform() will use GPU, transform(x) will use the CPU\n- 3dtransform will be a smoother experience, and is therefore the best practice","n":0.218}}},{"i":949,"$":{"0":{"v":"Rendering","n":1},"1":{"v":"\n## Block Formatting Context (BFC)\nA BFC is a part of a visual CSS rendering of a web page. It's the region in which the layout of block boxes occurs\n\nA BFC is basically a mini-layout within the main layout.\n- This means, for instance, that floated elements will be contained within (achieved with `overflow: auto` on container).\n\nBFCs prevent margin collapsing\n\nCreated by (non-exhaustive):\n- `<html>` tag\n- elements with `position` equal to `absolute`/`fixed`\n- elements with `display` equal to `inline-block`\n- Block elements where `overflow` has a value other than `visible` and `clip`.\n- Flex items, provided they are not flex/grid containers themselves\n- Grid items, provided they are not flex/grid containers themselves","n":0.098}}},{"i":950,"$":{"0":{"v":"Prop","n":1}}},{"i":951,"$":{"0":{"v":"Position","n":1},"1":{"v":"\n### Static\nThe element goes with the normal document flow. `z-index` has no meaning here, and neither do `top`/`left`/`bottom`/`right`\nThis is default\n\n### Relative \nThe element goes with the normal document flow, and then is offset relative to itself (using `top`/`left`/`bottom`/`right`)\nThe element does not impact any other elements in the document\nThe space given to the component in the page layout is the same as if position were `static`\n\n### Absolute\nThe element is removed from the normal document flow, and no space is created for the element in the page layout.\nInstead, the element is positioned relative to it's closest positioned ancestor (ie. an ancestor with a position value that is not `static`)\n- If there are no positioned ancestors, then the element is positioned relative to the initial containing block (ie. the parent of the `<html>` tag)\n\nCreates a new stacking context as long as `z-index` is not set to `auto`\n\n### Fixed\nThe element is removed from the normal document flow, and no space is created for the element in the page layout.\nThe element is positioned relative to the initial containing block (ie. the parent of the `<html>` tag)\nIf one of the element's ancestors has any of: `transform`, `perspective`, or `filter`, then that ancestor is used as the containing block.\nCreates a new stacking context\n\n### Sticky\nThe element goes with the normal document flow, and then is offset relative to 2 containing blocks: the nearest scrolling ancestor, and the nearest block-level ancestor.","n":0.066}}},{"i":952,"$":{"0":{"v":"Layout","n":1},"1":{"v":"\n## Layout-isolated component\n- def - A component that is unaffected by the parent it is placed within, and does not itself affect the size and position of its siblings.\n    - this only applies to the root element of a reusable component\n![](/assets/images/2021-03-28-19-49-24.png)\n- avoid any properties on the root element of a component that affect, or are affected by elements outside of the bounds of that component.\n    - ex. margin, because it acts on elements outside of the component's scope\n    - ex. align-self, as it will stretch the width or height of the component depending on the flex-direction of its parent\n    - by contrast, padding is fine, as it is confined to the scope of the component\n    - Basically, if a property depends on, or impacts other components outside of its scope, discourage its use.\n```js\n// Does NOT conform to layout isolation principals\nfunction MyComponent() {\n  return (\n    <div style={{alignSelf: 'center'}}>\n      <div />\n    </div>\n  \n}\n\n// This component is layout isolated, since the potentially dangerous property is not on the root element\nfunction MyComponent() {\n  return (\n    <div>\n      <div style={{alignSelf: 'center'}}/>\n    </div>\n  )\n}\n```\n\n# E Resources\nhttps://visly.app/blogposts/layout-isolated-components","n":0.075}}},{"i":953,"$":{"0":{"v":"Flexbox","n":1},"1":{"v":"\n- using explicit margins breaks flexbox's ability to position things nicely, since it doesn't play nicely with the math that it's relying on\n- when doing flexbox row don't use margin left/right, and when doing column, don't use margin top bottom, because it interferes with the math that flex is doing\n    - try and use justify-content: space-between instead, and set flex: 0 49% for example, which will almost serve as our margin\n \n note: all examples below assume `fd: row`. If `column`, then substitute width for height.\n## Flex-Grow\n- when there is available space after all boxes have taken up their room, `flex-grow` will allow the item to grow proportionally to the other item's `flex-grow` property.\n    - therefore, all items having flex-grow: 1 is the same as all items having flex-grow: 100. Think of it as a proportional growth rate. \n    - an item with flex-grow: 3 will grow 3 times faster than an item with flex-grow: 1 \n- a value of `0` means the item won't be resized during the size calculation to\n    accommodate the flex container's full main axis size\n- Imagine we use different flex-basis for each flex item. Even if we have `flex-grow: 1` on each item, they will still grow at a different rate, because of the basis at which they started.\n\n### UE Resources\n[flex grow article](https://css-tricks.com/flex-grow-is-weird/)\n\n## Flex-Shrink\n- The rate at which a flex item will shrink when the page is resized to a size smaller than the flex-containers most \"comfortable\" size.\n\n## Flex-Basis\n- initial main size of a flex item before any available space is distributed\n    among the flex items (which is determined according to flex-shrink and\n    flex-grow). \n- It will override any width property, unless left as `auto`\n    - when set to `auto`, it checks for a width property (if fd: row). If none found, it uses the content inside the flex-item to determine its width.\n    - `flex-basis` still obeys `min/max-width` settings\n- In a sense, it is similar to `min-width`\n    - (although `flex-shrink` determines how the flex item will behave when the\n    size of the whole flexbox shrinks below initial size)\n    - If we were to use `min-width` and shrink the browser size, the browser would make us scroll horizontally to see all of the content.\n\n## Flex Shorthand\n- by default, it is `flex: 0 1 auto` (?)\n- `flex: 1 1 0` - ==flex-grow== ==flex-shrink== ==flex-basis==\n    - also can be written `flex: `\n","n":0.05}}},{"i":954,"$":{"0":{"v":"Elements","n":1},"1":{"v":"\n### The problem with building isolated components using CSS+HTML\nA fundamental problem of building isolated components with HTML+CSS is that the CSS `display` property sets both an element's inner layout (how it lays out its children) and its outer layout (how it is laid inside it parent).\n- ex. you have to make an element `display: inline-flex` to make it an inline flexbox. You can't just make it `flex` and have the parent decide whether it is inline, a block or whatever.\n\nThe consequence of this is that a parent component cannot properly layout a child component without knowing its internal layout structure. And if it doesn't don't know the child's layout (if it's dynamically supplied for example) then it can't safely do anything with it. So long as this limitation exists, there will never be true layout encapsulation on the web platform.\n\n* * *\n\n## Element Types: the `display` property\n### Block-level elements\nOccupies the entire horizontal space of its parent element, and vertical space equal to the height of its contents, thereby creating a \"block\".\nBrowsers display the block-level element with a newline both before and after the element. \n- You can visualize them as a stack of boxes.\n\nThe block-level element\n\n#### Examples\n`<div>`, `<p>`, `<h1>`, `<ul>`\n\n### Inline elements\nOnly occupy the space bounded by the tags defining the element.\nDoes not start on a new line, and only takes up the width it needs.\nDoes not break the flow of the content.\nYou can't put block elements inside inline elements.\nThe block-direction margin is ignored entirely\nThe padding on inline elements doesn’t affect the height of the line of text\n- ex. `padding-top` on an `<a>` tag won't cause the height of the line of text to change\n\nAll inline elements (not just text) can be laid out by a parent with the `text-align` property\n- also with `vertical-align`, which would be the inverse property, manipulating upon the vertical axis.\n\n#### Inline-block elements\nAble to have width/height values set.\nWhile `transform` property can't be used on inline elements, it can be used on inline-block.\n\nBe careful with inline-block elements. The components you build with them may look fine, but the quirks may cause them not to place nicely with their parents:\n![](/assets/images/2021-11-02-10-53-16.png) \n\n#### Inline-flex\nVirtually the same as `display: flex`, except the flex container itself is inline, instead of block.","n":0.052}}},{"i":955,"$":{"0":{"v":"Best Practice","n":0.707},"1":{"v":"\n## General Readability Principles\n### Line-height\npick a minimum line-height: 1.4 for body copy. Headings should be tighter.\n\n### Line-width\nKeep it to 60-80 characters wide\n\n## Size\n- use `rem` for `font-size`\n- use `px` for `border-width`\n- use `em` for everything else\n\nDifferent font sizes on a webpage should be related by a common ratio ([source](https://www.type-scale.com))\n- ex. if we choose that ratio to be 1.25, then our ems will be: 0.64, 0.8, 1, 1.25, 1.563, etc.\n\nWe can set the base-size font on a container using `rem`, and then we set size of the children content using `em`. This allows the children using `em` to leverage the `font-size` set on the parent (the `rem` value). Now, when we adjust the `rem` value, all the `em` values depending on it will adjust as well, while keeping their proportions.\n![](/assets/images/2021-11-02-16-11-32.png)\n- This method works well for images as well\n\nThis method can be synergized with:\n- `@media` queries\n- `vw` unit with `font-size`. This will cause the font-size to grow/shrink smoothly with no abrupt breakpoints. We need to do some math to define min size (0.5em):\n    - `font-size: calc(0.5em + 1vw)`\n\n* * *\n\n### Magic numbers\nWhen using magic numbers, add a comment to explain how it was derived:\n```css\n.gallery__item {\n    @media (min-width: 800px) and (max-width: 1200px) {\n        width: 21.25%; /* (100% - 15%) / 4 */\n        &:nth-child (4n) {\n            margin-right: 0;\n        }\n    }\n}\n```","n":0.068}}},{"i":956,"$":{"0":{"v":"Crypt","n":1}}},{"i":957,"$":{"0":{"v":"Public Key","n":0.707},"1":{"v":"\n## Public Key Cryptography\n- In a simple cryptographic system, I have some message that I can encrypt using a key. I can then give you that key along with the information, and you can decrypt it and read the message.\n\t- This is known as Symmetric Encryption\n\t- The problem with this, is that I need to send you the key. Since we have not yet established a secure connection, we cannot be sure that no one is listening in.\n- in Public Key Cryptography, we generate a key pair: a private key and public key.\n\t- This is known as Asymmetric Encryption\n\t- It is Asymmetric because key A is used to encrypt the message, while key B is used to decrypt it.\n\t\t- Anything we encrypt with key A can only be decrypted with key B, and anything encrypted with key B can only be decrypted with key A\n- Now we can have a situation where both you and I have our own key-pairs, with each person's public key being widely distributable. \n\t- Now if I want to send something to you, I just need your public key. I can encrypt the message like this, and you are the only person that can decrypt it, since you have the private key. \n- This system also allows the receiver of a message know exactly who sent it. Consider that if I use my private key to encrypt a message and send it out, anyone with my public key can decrypt it. This has an important implication, which is that if you were able to successfully decrypt my message, then you know for a fact that I was the one who encrypted it. \n- In the public key system, the public key is for encrypting, while the private key is for decrypting.\n- Diffie-Helman\n\n### Mail Poker Example\n- *note:* is this an actual analogy of public key, or something else?\n- [source](https://www.youtube.com/watch?v=mthPiiCS24A&list=PLt5AfwLFPxWLXe-ZqZyu0kSsaWd4FjXbj)\n- Imagine we wanted to play a game over poker over the mail. Of course, each player needs to be assured that the same rules apply as if the game was in person (neither side knows the other's cards, each side only takes 5 cards, etc). \n- Player 1 puts each card in an envelope, and attaches a lock. This lock is only openable by player 1, but only one key is needed to open all the locks.\n- Player 1 mails all 52 cards to Player 2. Player 2 then puts his own lock on all envelopes. Again, his key will open all envelopes. Then Player 2 will send the double-locked envelopes back to Player 1. (the randomized nature of the mail process acts as a secure and trusted method of shuffling) \n\t- note: we have created a situation where neither player can reveal any card on their own. \n- Player 1 now has 52 double-locked envelopes, so now must choose 5 cards for Player 2. Once they are chosen, Player 1 takes his lock off those cards and mails just those 5 cards. Once Player 2 receives those cards, he can unlock them and now knows his hand. \n- Player 1 now has 47 double-locked envelopes, so now he must choose 5 for himself. Once he chooses, he sends them to Player 2. Player 2 removes his lock from those 5 cards, and sends them back to Player 1. Player 1 removes his lock, and now knows his own hand.\n\n- This system of double-locking creates what is called *commutativity*. A commutative crypographic algorithm is one that is order-independent (ie. it doesn't matter which order the locks are taken off)  \n\n### Bridging the Analogy Gap\n- If this game of poker were to take place over the internet, each of the 52 cards would have a numeric value (1-52). We need a way to \"lock\" each card. Player 1 creates his key (`k`) by choosing a random number. He then takes that number and raises each card to that value.\n\t- ex. if `k = 3`, then each card value is raised to the 3rd power. This is equivalent of locking the envelopes, as long as Player 2 doesn't know what `k` is.\n- Player 2 then can create his own key (`j`), and raise each (already encrypted) card value to that value. Now we have double-locked cards.\n\t- It's important to note that `(2^5)^3` is mathematiacally equivalent to `(2^3)^5`. In other words, the locks can be applied or removed in any order. \n- Now, to remove the locks, each player simply needs to multiple the exponent by the inverse of their key (if Player 1 wants to remove the locks of 5 cards, then he just needs to multiply the exponent by `-3`)\n","n":0.036}}},{"i":958,"$":{"0":{"v":"Hashing","n":1},"1":{"v":"\n# Hashing \nlet's say we have a function that takes a number from zero through nine, adds three and, if the result is greater than ten, subtracts ten. So f(2) = 5, f(8) = 1, etc. Now, we can make another function, call it f', that goes backwards, by adding seven instead of three. f'(5) = 2, f'(1) = 8, etc.\n- Theoretically, any mathematical functions that maps one thing to another can be reversed. In practice, though, you can make a function that scrambles its input so well that it's incredibly difficult to reverse.\n- taking an input and applying a one-way function is called \"hashing\" the input\n- SHA1 is an example of this kind of \"one-way\" function","n":0.092}}},{"i":959,"$":{"0":{"v":"K Anon","n":0.707},"1":{"v":"\n# K Anonymity\n- When entering a password, the client hashes it into a 130 bit hash, then sends only the first several characters to the server. The server checks its database of passwords hashes, and returns all of the passwords that begin in the same way. Once the client receives this, it selects the hash that matches with its password\n- The benefit of this is that a password hash is never actually sent from the client to the server. MITM attacks are less threatening. The server also doesn't gain any valuable information, since it won't know if any of the hashes it is sending actually matches the one owned by the client.\n- ex. run `curl https://api.pwnedpasswords.com/range/f42b7e\n`. We will get in return a list of hashes for passwords that have been cracked (ie. leaked in plain white text)\n","n":0.085}}},{"i":960,"$":{"0":{"v":"Hashing Algorithm","n":0.707},"1":{"v":"\n# Hashing Algorithms\n## RSA\nRSA security is based on 2 matching keys. There is a public key for each user, and everybody can (should) know it. There is also a private key that only the user should know. A message encrypted by the public key can only be decrypted by the private key, and visa versa.\n- Thus, if I want to send you a message that only you can read, I get (from the network) your public key, encrypt the message with that key and you are the only person who can decrypt it.\n- Or, if I want to prove to you that I sent a message, I can encrypt the message with my private key, tell you (in open text or in another message) how it was encrypted. Then you could decrypt the message with my public key, and if it becomes readable, you know it came from me.\n- RSA-129 is a publicly available 129 digit number that is derived from multiplying 2 very large prime numbers (those numbers are unknowable)\n","n":0.076}}},{"i":961,"$":{"0":{"v":"Cracking","n":1},"1":{"v":"\n# Password Cracking\n- with `hashcat`, dedicated servers for cracking passwords can hash a word and check for the presence of the hash in a big list of hashes 40 billion times per second (if the hashes were hased with md5 algo). If a more secure algo is used like SHA512 or Bcrypt, it would be in the thousands instead. \n- In a brute force attack, a 7 letter word all lower-case has 26^7 possible combinations, which is just over 8 billion combinations. This makes an md5 stored 7 letter word breakable in less than a second.\n","n":0.102}}},{"i":962,"$":{"0":{"v":"Colors","n":1},"1":{"v":"\nUse warmer colors for foreground elements, cooler colors for background elements.\n\nUse saturated colors (pure hues) when attracting attention is the priority, and desaturated colors when performance and efficiency are the priority.\n\nGenerally, desaturated, bright colors are perceived as friendly and professional; desaturated, dark colors are perceived as serious and professional; and saturated colors are perceived as more exciting and dynamic.\n\n### High-dynamic-range (HDR)\nOutside this range, no features are visible because in the brighter areas everything appears pure white, and pure black in the darker areas. The ratio between the maximum and the minimum of the tonal value in an image is known as the dynamic range\n\n# Resources\n[UI palettes]( https://visme.co/blog/website-color-schemes/ )\n","n":0.096}}},{"i":963,"$":{"0":{"v":"Cloudflare","n":1}}},{"i":964,"$":{"0":{"v":"Workers","n":1},"1":{"v":"\nCloudflare workers are serverless functions that allow us to intercept HTTP requests (run code in between request and response\n\nThere are notable differences with serverless functions:\n- Workers run on the edge. This means that they are available on a number of Cloudflare servers around the world so your user will get the lowest possible latency. This is not how cloud functions usually work. For example in AWS you have Lambda and Lambda at Edge.\n- Since workers are running in Cloudflare's infrastructure they have access to some unique API to interact with the CDN and caching which is the main Cloudflare product. So you can receive a request to your worker and then decide \"hey, put this image in the cache for next time\".\n- Workers have access to a Key Value (KV) store which is storing data on the edge.\n\nWorkers runs code on Cloudflare's edge. And each Worker is assigned to at least one HTTP endpoint. So these are scripts that live ~10ms away from most users and can be updated globally easily.\n\nex. Workers can be used to apply http headers if you use a serverless setup for hosting.\nex. You can host every static website / react SPA on cloudfare workers\nex. You can use workers to inject headers into responses without having to change Nginx configs","n":0.068}}},{"i":965,"$":{"0":{"v":"Kv","n":1},"1":{"v":"\nCloudflare Workers KV provides access to a secure low latency key-value store at all of the data centers in Cloudflare's global network\n- usage ex. save the cart and checkout in Workers KV and when our webhook is notified by Stripe of a successful payment_intent, we create the order in our backend.\n\t- ex. this decouples your server having to be up and running from being able to process orders","n":0.121}}},{"i":966,"$":{"0":{"v":"Warp","n":1},"1":{"v":"\nSimilar to a VPN, in that it will encrypt your traffic, but it doesn't fake our IP like a VPN does.\n\nSince Workers are distributed across 190+ datacenters, you can get the website loading in milliseconds all around the world.","n":0.16}}},{"i":967,"$":{"0":{"v":"Chrome","n":1}}},{"i":968,"$":{"0":{"v":"Vimium","n":1},"1":{"v":"\n## Vimium\n- `<c-u>` move tab to new window (unpin)\n- `<c-n>` - pin the tab back to the main chrome window\n- `^` - go to most previous tab\n- `yt` - duplicate current tab\n- `gE` - edit the current url and open in new tab\n- `gi` - focus first (or nth) input box on page\n- `yy` - copy url that you are currently on\n- `gu` - go up a level in the url hierarchy","n":0.118}}},{"i":969,"$":{"0":{"v":"Devtools","n":1}}},{"i":970,"$":{"0":{"v":"Tabs","n":1}}},{"i":971,"$":{"0":{"v":"Performance","n":1},"1":{"v":"\nPerformance tab allows us to see how the Javascript actually performs on our site.\n\n## Usage\n1. Hit record (`Cmd + E`)\n2. Do the interaction on the webpage that you want to measure performance for.","n":0.174}}},{"i":972,"$":{"0":{"v":"Network Tab","n":0.707},"1":{"v":"\nWe should probably have the \"disable cache\" checkbox enabled. This will give us a more realistic view of the actual user experience.\n\nWhen looking at the  load amounts, there is two numbers. The first number is how much data is in the browser, and the second is how much data was actually loaded over the wire. The second number is more significant in terms of debugging performance\n\n## Unofficial APIs\nOften companies don't offer open APIs. We can \"open it up\" to ourselves by mimicking the request as if it's coming from us interacting with the webpage.\n1. In the website's UI, do the interaction that will trigger a network request (eg. clicking a button)\n2. In the network tab, find the request and gather the API endpoint that was hit.\n3. Right-click and *copy as* fetch/cURL. We now have a fully formatted request that has all of the headers needed to make a successful request.\n\n## Image caching status\nWe can check to see if image caching is working or not (eg. if we are using Cloudflare image caching worker)\n1. In Network tab, filter for images, and find the image in question\n2. Check response headers for an indicator (in this example, `cf-cache-status=hit`) that the image came from a cached location.\n\n* * *\n\nWhen right-clicking a request, we get the option to `Copy As...`. If we `copy as Node.js fetch`, we also get anything that came along with the request (inc. all of the cookies, session Ids, headers etc).\n- a normal fetch request doesn't include these, because it assumes we are firing off the request from the browser. However, if we are making a fetch request from a server, then we would manually need to send those cookies/headers etc along.","n":0.06}}},{"i":973,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n# Google Chrome\n## Devtools\n- swap between chrome tabs - `cmd+[`/`cmd+]`\n- move dock position - `cmd+shift+d`","n":0.258}}},{"i":974,"$":{"0":{"v":"Debugger","n":1},"1":{"v":"\n## Debugging outbound network issues\n1. Sources tab, hit `Pause on exceptions` (stop sign with pause icon)\n2. Reload the page, and code will pause on the first exception that happens\n3. Hover over the failing line, and investigate\n\t- are args to the function call what you'd expect them to be? Does anything stand out? Why is an error being thrown?","n":0.131}}},{"i":975,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\nOpen Clear browsing data modal - `cmd+shift+delete`\n","n":0.378}}},{"i":976,"$":{"0":{"v":"Caddy","n":1},"1":{"v":"\nif Caddy is running at port `:2019`, then the config file is at `:2019/config/`\n\nmost(?) caddy commands are just running GET/POST requests. For instance, `caddy stop` sends a POST to `http://localhost:2019/stop`\n\n#### Workflow\nTo administer Caddy, we can do so in 2 ways:\n1. using the API\n    - ex. POST to `https://caddyserver.mywebsite.com:443/load`\n2. using the CLI\n    - ex. run `caddy load`\n\nCaddyfile+CLI combo is the more easy-going route, but is more difficult to scale. Otherwise, if looking for more control, users typically go for the JSON+API combo.","n":0.111}}},{"i":977,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n#### Start Caddy in background\n`caddy start`\n\n#### Start Caddy in foreground\n`caddy run`\n\n#### Give Caddy a new config without stopping server\n`caddy reload`\nnote: this works with `caddy start`; not `run`","n":0.192}}},{"i":978,"$":{"0":{"v":"Caddyfile","n":1},"1":{"v":"\nCaddy config natively uses a JSON structure. However, because writing JSON by hand can be error-prone, we have config adapters to help us.\n- The standard config adapter coming with Caddy converts a `Caddyfile` into JSON.\n- At the end of the day, JSON is more flexible, but not quite as simple. If we stay on the well-beaten path, then we probably can just get by with a Caddyfile.\n\nWe can use a `caddy.json` config file like this:\n```json\n{\n\t\"apps\": {\n\t\t\"http\": {\n\t\t\t\"servers\": {\n\t\t\t\t\"example\": {\n\t\t\t\t\t\"listen\": [\":2015\"],\n\t\t\t\t\t\"routes\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"handle\": [{\n\t\t\t\t\t\t\t\t\"handler\": \"static_response\",\n\t\t\t\t\t\t\t\t\"body\": \"Hello, world!\"\n\t\t\t\t\t\t\t}]\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\n}\n```\n\nOr, the equivalent can be expressed in a `Caddyfile`\n```Caddyfile\n:2015\n\nrespond \"Hello, world!\"\n```\n\nIf we run `caddy adapt` in a directory where there is a `Caddyfile`, then we can see the json that the Caddyfile contents are being converted into. This is the *config adaptor* at work.\n\nIf we are serving multiple apps from a single Caddy server, then we need to use curly braces (`{}`) in the Caddyfile to wrap the details of a single app. Otherwise, curly braces aren't needed","n":0.079}}},{"i":979,"$":{"0":{"v":"Directives","n":1},"1":{"v":"\n## Matchers\nA request matcher is used to filter requests.\n\nSyntax-wise, matchers immediately follow directives\n\n## Addresses\nThe address part may be any of these forms:\n- `host`\n- `host:port`\n- `:port`\n- `/path/to/unix/socket`\n    - ex. `unix//path/to/socket`\n\nsome config fields may allow us to specify a port range, like `:8080-8085`\n\n## Directives\n\n### root\nsets the root path of the site\n- the root path (ie. root directory) holds all the files related to serving an application, whether they are private or public, (with respect to being exposed to the internet).\n\nspecifying multiple `root`s in the same block is legal. \n- Each `root` directive is mutually exclusive with others in the same block.\n    - multiple `root`s will not cascade and overwrite each other.\n\n`root` is usually used together with `file_server`, since it does not enable serving static files on its own.\n```\nroot * /var/www\n```\n\n### file_server\n`file_server` works by taking the request's URI path and appending it onto the root path\n\nex. if a user visits `https://example.com/jokes`\nWith the Caddyfile:\n```Caddyfile\nroot * /var/www\n```\nCaddy will serve content from `/var/www/jokes`","n":0.08}}},{"i":980,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Serving static files\nie. Caddy performing the duties of a static file server\n```conf\nexample.com {\n    # if `root` omitted, pwd (of Caddyfile) is assumed\n    # `*` is a wildcard matcher, so it matches all requests.\n    root * /var/www\n    file_server\n}\n```\n\n#### Reverse proxy load balance\nThis will round-robin load balance requests to 3 different hosts on the same local network\n```conf\nreverse_proxy /api/* 192.168.0.30:80 192.168.0.31:80 192.168.0.32:80 {\n\tlb_policy round_robin\n}\n```","n":0.127}}},{"i":981,"$":{"0":{"v":"C","n":1},"1":{"v":"\n- C is designed to interface with the output of the computer hardware. Therefore, it is a natural language to use when building operating systems. When you write code in C, you can better intuit what the resulting assembly language will look like. C effectively enables you to write code from a hardware-first perspective.\n\nIn C, if a function signature doesn’t specify any argument, it means that the function can be called with any number of parameters or without any parameters.\n\n- `main` is a function just like any other, and thus returns a value to the environment that executed the c program to begin with.\n\n# Header Files\n- purpose is to share functions and macros across source files.\n\n* * *\n\n## Aspects\n### Const\n- a *constant expression* involves only constants, and is therefore evaluated during compilation \n- an `emun` is a type of constant. By default, the first element has value 0, the second has value 1, and so on.\n\n## Operators\n### Token-pasting operator (##)\n- if 1+1=2, then 1##1=11\n\n## Concepts\n\n* * *\n\n### Symbolic Constants (`#define`)\n- `#define` creates a macro\n- Any constant defined in this manner will find all occurrences and replace it with the corresponding value *before* compilation \n\t- This contrasts with variables, in that data is actually stored inside of them (while macros are more like aliases) \n- Symbolic Constants are valauble for defining magic numbers\n\n* * *\n\n## Debugging\nErrors involving “token” almost always mean that you either missed a semicolon or your {}s, ()s, or []s aren’t matched.\n\t\t\n* * *\n\n### Indirection\n- The act of referencing something using a name, rather than using the value itself.\n- A common form of indirection is when we manipulate a value via its memory address\n\t- ex. accessing a variable by using a pointer.\n- aka Dereferencing\n\n### Handle vs Pointer vs Reference\n- While a pointer contains the address of the item it refers to, a handle is an opaque reference to an object. \n- The type of the handle is unrelated to the element referenced\n- using handles adds an extra layer of indirection, meaning that we can change the details at this level without breaking the program. The same couldn't be said for a pointer. \n- A pointer is the combination of an address in memory and the type of the object that resides in that memory location","n":0.052}}},{"i":982,"$":{"0":{"v":"Preprocessors","n":1},"1":{"v":"\n### Preprocessors (header of the file)\n- contains C function declarations and macro definitions to be shared between several source files.\n\t- In essence, they are like importing modules.\n- ex. `stdio.h` – Defines core input and output functions\n- In C, all lines that start with # are processed by preprocessor which is a program invoked by the compiler\n\t- a preprocessor takes a C program and produces another C program without any `#`\n- When we use `include` directive,  the contents of included header file (after preprocessing) are copied to the current file.\n\t- `<` and `>` instruct the preprocessor to look in the standard folder\n\t- `“` and `“` instruct the preprocessor to look into the current folder\n- When we use `define` for a constant, the preprocessor produces a C program where the defined constant is searched and matching tokens are replaced with the given expression\n\t- ex. `#define max 100` — `max` is searched for in the program, and is replaced with `100`\n","n":0.08}}},{"i":983,"$":{"0":{"v":"Makefile","n":1},"1":{"v":"\n## Purpose\n- to help decide which parts of a program need to be recompiled.\n- Makefiles allow us to give a series of instructions to run depending on what files have changed.\n- similar to the sript section of the `package.json`\n\n## Structure\n- a Makefile consists of a set of rules, which take the form:\n```\ntargets: prerequisites\n   command\n   command\n   command\n```\n- `targets` are filenames.\n- `commands` are a series of steps, normally used to make the target(s)\n- `prerequisites` are dependencies that are needed before the commands can be run.\n\n## Running \"make\" command\n- when we run `make` without arguments, the first target that doesn't begin with `.` is processed. This is known as the *default goal*.\n\t- To do this, it may have to process other targets, specifically ones that the first target depends on.\n- Often the *default goal* is called `all`, though this is just a convention.\n\n### Building from source with `make`\n1. run `make`\n2. run `sudo make install`\n\n## Syntax\n- `PG_CONFIG ?= pg_config` - set PG_CONFIG variable only if it isn't already set\n\n### UE Resources\n- [Makefile Tutorial](https://makefiletutorial.com/)\n","n":0.077}}},{"i":984,"$":{"0":{"v":"Lang","n":1}}},{"i":985,"$":{"0":{"v":"Variables","n":1},"1":{"v":"\n### Variables\n- `external` and `static` variables are guaranteed to be initialized to zero if not explicitly declared.\n\t- The value of these variables is determined conceptually before the program begins execution \n- automatic and register variables have undefined (i.e., garbage) initial values, if not explicitly declared.\n\t- The value of these variables is determined when the function or block is entered.\n\nStatic\n- Decalaring a variable or fn *static* does 2 things:\n1. it becomes scoped to wherever it was defined (scoped to source file, fn etc)\n2. When static is declared inside a function, then that variable will live on between calls of the function, giving us permanent stoage. \n- Static variables are allocated memory in data segment, not stack segment\n- Unless initialized with a value, static variables default at ‘0’\n\nRegister\n- Declaring a variable with `register` advises the compiler that it will be heavily used, placing it in a machine register for quicker access\n- Only a few variables in each function may be kept in registers, and only certain types are allowed.\n\t- Excess register declarations are harmless, however, since the word register is ignored for excess or disallowed declarations\n\t\t- The actual limit varies from machine to machine.\n\t- It is impossible to get the memory address of a register variable. ","n":0.07}}},{"i":986,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n## Types\nIn the early days of C, pretty much everything was either an int or pointer (which is why int is the default type)\n\n### Numbers\nPrefixes\n- A leading `0` on an integer constant means octal\n- a leading `0x` means hexadecimal\n\nSuffixes\n- spec:an int is by default short, and has no suffix:\n`int num = 1234;`\n- but to make a long constant (double by default), we need to append the number with `L`:\n`long num = 123456789L`\n\t- note: even if `long` is not specified, the compiler will know it is long and declare it as such \n- to make an unsigned const, append `U`\n- to make a float const, append `F`","n":0.097}}},{"i":987,"$":{"0":{"v":"String","n":1},"1":{"v":"\n# String\n- The following are equivalent:\n```\nchar pattern[] = \"ould\";\n// and\nchar pattern[] = { ′o′, ′u′, ′l′, ′d′, ′\\0′ };\n```\n- C does not have a native string type. Instead, the language uses arrays of char terminated with a null char (`'\\0'`)\n\t- Therefore, the physical storage required is one more than the number of characters\n\t-  Therefore, the physical storage required is one more than the number of characters\n\t- The `\\0` gets automatically appended for us when array is initialized. Therefore if we access the last element, we get back `\\0`\n- there is a standard header `<string.h>` that gives us some string utilities\n- In C, arrays are of fixed length at the time they are declared. For this reason, it may make sense to declare an array with a length that is far more than you need, just so you know it can handle everything that gets thrown at it. \n\nSingle vs Double Quotes\n- In C, `'x'` is an integer representing the ASCII value, while `\"x\"` is a character array with 2 elements: `x` and `\\0` \n\n## Initialization\n- Character arrays can be initialized in different ways:\n\t- `char name[13] = \"StudyTonight\";`\n\t- `char name[] = \"hello\";` (spec:wide char array)\n\t- `char name[10] = {'L','e','s','s','o','n','s','\\0'};`\n- These character arrays can either be read-only or manipulatable\n\t- read-only: `char * str = \"Hello\";`\n\t- manipulatable: `char str[] = \"Hello\";`\n\n## Format specifiers \n- `%s` - Take the next argument and print it as a **string**\n- `%d` - Take the next argument and print it as an **int**\n\n## printf (print formatted)\n- The compliment to `printf` is `scanf`, in the sense that print is stdout and scanf is stdin. \n\t- `scanf` terminates its input on the first white space it encounters. Tip: use \"edit set conversion code `%[..]` \" to get around this.\n\t\t- alternatively, use `gets()`, which will terminate once hitting `\\n`\n\n","n":0.058}}},{"i":988,"$":{"0":{"v":"Pointer","n":1},"1":{"v":"\n# Pointer\n- `&` returns the memory address of the object\n\t- called \"address of\" operator\n\t\t- Therefore, this gets applied to the value to get the memory address\n\t- ie. `p = &c;`\n- `*` either: \n\t1. declares a pointer variable, or\n\t\t- ex. `int *p;`\n\t2. dereferences an existing pointer (indirection through a pointer)\n\t- called \"indirection\" or \"dereferencing\" operator\n\t\t- Therefore, this gets applied to the pointer to get the value in storage\n\t- `int a = 5` then `int *ptr = &a;`\n\t\t- this means \"locate\" the address where `a` is stored, and assign its value to `ptr`\n\t\t\t- In other words, the value of `ptr` will now be the address where `a` is stored. \n\t- `*ptr = 8`\n\t\t- this means take the address of `ptr`, \"locate\" that address in memory, and set its value to 8.\n\t- `int *ptr`\n\t\t- \"`ptr` is a pointer that points to an object of type `int`\", or simply: \"`ptr` is a pointer to `int`.\"\n- If p points to the integer x, then *p can occur in any context where x could, so:\n```\n*p = *p + 10;\n// is the same as\nx = x + 10;\n```\n\n![996035c6b28b5cf88b9156c920cc0d58.png](:/454c71a68a8845c99466685e0c038c4d)\n- This picture shows how a pointer references a storage location, which holds a variable `c`. When we use the `&` operator, we are talking about the place where `c` is stored. When we use the `*` operator, we are talking about the variable `c` itself. \n\n- each pointer points to a specific data type, which is why we declare a pointer variable `int *p;`\n\t- The exception is \"pointer to void\", which can hold any kind of pointer, but cannot be dereferenced itself.\n- C does not implicitly initialize storage duration of memory locations. Therefore, we should be careful that the address that the pointer points to is valid.\n\t- For this reason, some suggest initializing pointers to `NULL` (*null pointer*/*null reference*)\n\t- null pointer shown as `0x00000000`\n- Like other languages, manipulating a function argument will have no effect on the original variable that was passed to the function. This is because when we pass an argument, a copy is made and we are merely mutating the copy. \n\t- However, we are also able to call a function, passing in the variable's *address* as the argument `passByAddr(&x)`.\nfrom within the function, if we change the value like so `*m = 14`. this changes the value at the address, so outside the function we'll notice that the value changed\n","n":0.05}}},{"i":989,"$":{"0":{"v":"Opaque","n":1},"1":{"v":"\nAn opaque type is a type that \"wraps\" lower-level types, and is often used when either the underlying implementation is complex, or the user simply does not need to know about the inner workings\n- ex. there is a type in Swift called `CFString`. It is an opaque type that provides a series of methods that allow for string manipulation and string conversion. For example, we have a `.length` and `.indexOf` methods. The implementation details of these methods is unimportant to the user of this type, so they have been hidden. \n\t- This is the essence of an opaque type, in that a native type has had some extra functionality added to it by being \"wrapped\".\n\nAn \"opaque type\" is a type where you don't have a full definition for the struct (or class in the case of C++)\n- In C, you can tell the compiler that a type will be defined later by using a forward declaration:\n```c\n// forward declaration of struct in C and C++ \nstruct Foo;\n```\nhere, the compiler only has enough info to be able to declare pointers to `Foo`, which is sometimes all we need to do\n- Allows library and framework creators to hide implementation details, allowing the users of that library to call helper functions to create, change or destroy instances of a forward declared `struct` (also `class` in C++) ","n":0.067}}},{"i":990,"$":{"0":{"v":"Int","n":1},"1":{"v":"\ntypically, a `short int` is 16 bits (2 bytes), and `long int` 32 bits (4 bytes)\n- therefore, an int is a 32-bit data type\n\t- this depends on the natural size of integers on the host machine\n- in reality, 31 bits are available for the number, since 1 has to be reserved for the sign (+/-)\n- Whenever a number is being assigned to an ‘int’ type variable, it is first converted to its binary representation then it is kept in memory at specific location.\n","n":0.11}}},{"i":991,"$":{"0":{"v":"Float","n":1},"1":{"v":"\n### float/double\n- floating-point types.\n\t- float - single-precision floating point\n\t- double - double-precision floating point\n","n":0.267}}},{"i":992,"$":{"0":{"v":"Char","n":1},"1":{"v":"\n### Char\n- A char holds the ASCII value, rather than the character itself.\n\t- Therefore, by definition a `char` is just a small integer\n\t- We can leverage the fact that a char will evaluate to its ASCII value, by performing math on them\n\t\t- ex. we can test if something is a digit like with `if (val >= '0' && val < '9')`. Here, the `'0'` and `'9'` get converted to their ASCII values as the comparison is made. As it so happens, any ASCII value between 48 and 57 is a digit.\n- a char is a single byte\n","n":0.102}}},{"i":993,"$":{"0":{"v":"Array","n":1},"1":{"v":"\n# Array\n- In C, arrays can be thought of as pointers to consecutive areas of memory\n\t- elements in an array are stored in physically adjacent blocks of memory \n\t\t- spec:This is why a pointer pointing to an array only points to the first element— because we can essentially keep checking \"is the next memory block part of that array?\"\n- the name of an array is a synonym for the memory location of the first element of that array. \n\t- therefore, the assignment `p = &x[0]` is identical to `p = x`\n\t- also, what follows from this is that a reference to `x[i]` can also be written as `*(x+i)`\n\t\t- We can understand this as \"`x[5]` is the address of the 5th element beyond `x`\".\n- When an array name is passed to a function, what is passed is the location of the initial element. Within the called function, the argument is a local variable, and so an array name parameter is a pointer (a variable containing an address)\n- the syntax for accessing arrays is identical for that which can be used to dereference pointers\n- `array[i]` is equivalent to `*(array + i)`. \n\t- this fact shows that arrays are pointers to consecutive blocks of memory\n- An array can be initialized with values like so: `int days[] = { 1, 2, 3 }` \n\t- If the size of the array is omitted in initialization, the compiler will count the number of elements \n","n":0.065}}},{"i":994,"$":{"0":{"v":"Structs","n":1},"1":{"v":"\n### Structs\n- A struct is a composite type \n- Stands for “user defined data structure”\nIn C++, it is similar to a class (though by default the access level is public)\n- A struct is similar to an interface, in that we define a shape, which effectively becomes our new composite type. We can then initialize new variables that take this shape\n- The primary use of struct is for the construction of complex datatypes\n- Consider that a composite type such as this is analogous to a record in a database table. In a table, we implicitly create a type that has the same shape of the table  (in this sense the composite type is called a record)\n- All variables contained in a struct are physically located together. The consequence of this is that we can access the struct with a single pointer\n","n":0.085}}},{"i":995,"$":{"0":{"v":"Statements","n":1},"1":{"v":"\n### Statements\n- all statements in C end in semicolon `;`\n- braces `{`/`}` can be used to group declarations and statements together into a *compound statement* (or block)\n\t- this makes them syntactically equivalent to a single statement\n- the fact that `;` denotes the end of a statement rings true for *for loops* too. The first and third are assignments (or fn calls), while the second is a relational expression.\n\t- All are optional, so `for (;;)` is an infinite loop","n":0.113}}},{"i":996,"$":{"0":{"v":"Functions","n":1},"1":{"v":"\n### Functions\n- In C, we must declare a function prototype before calling the function so that the signature is known to the compiler \n\t- this is no different from declaring an int with `int e`\n\t- The prototype looks identical to the function, expect the body is replaced by a semi-colon:\n\n```c\nchar *do_something(char *dest, const char *src);\n```\t\n\nThis is not strictly necessary, but unless we declare it, the compiler is left to guess the signature based on how the function is called, and it is often wrong.\n- if we are passing an argument that should remain constant within the function body, we can prepend `const` to the parameter: `int strlen(const char[]);`\n","n":0.096}}},{"i":997,"$":{"0":{"v":"Compiler","n":1},"1":{"v":"\n### Compiler\n- unlike other languages, the C compiler does not look ahead to hoist definitions. It doesn't look backward or forward, nor does it scan the file multiple times to understand relationships. The compiler only scans forward in the file exactly once. Connecting function calls to function declarations is part of the linker's job, and is only done after the file is compiled down to raw assembly instructions.\n\t- This means that as the compiler scans forward through the file, the very first time the compiler encounters the name of a function, one of two things have to be the case: It either is seeing the function declaration itself, in which case the compiler knows exactly what the function is and what types it takes as arguments and what types it returns — or it's a call to the function, and the compiler has to guess how the function will eventually be declared.\n\n#### Compilation process\nwe can get all intermediate files with `clang -Wall -save-temps filename.c -o filename`\n\n1. Pre-processing\n\t- takes the source file and handles...\n\t\t- Removal of Comments\n\t\t- Expansion of Macros\n\t\t- Expansion of the included files\n\t\t- Conditional compilation\n\t- creates `filename.i`\n\t- At the end of the file, our source code is preserved\n2. Compilation\n\t- takes the `filename.i` file and compiles it, creating assembly level instructions that assembler can understand\n\t- creates `filename.s`\n\t- compilation phase is useful because it provides a common output language for different compilers for different high-level languages\n\t\t- ex. compilers of C and Fortran produce the same assembly language.\n3. Assembly\n\t- converts assembly language to machine level instructions\n\t- creates `filename.o`\n\t\t- produces a *relocatable object file*\n4. Linking\n\t- all the linking of function calls with their definitions are done\n\t- at this phase, we take code that has already been compiled (like `printf`, which comes from the `printf.o` file), and merge it with our own `.o` file.\n\t\t- We can see the effect of this phase by running `size filename.o` and comparing it to `size filename`. We see that `filename` is much larger\n\t\t- produces a *executable object file*, which can be loaded into memory and executed by the system.\n\n#### Different compilers\nIn C, it's possible that one compiler fails while another succeeds.\n- In the gcc compiler, `main()` cannot return `void`, but Turbo C compiler allows this.\n- To ensure we are writing proper C code, we have to look at the C Standard.\n- Therefore, just because C code compiles doesn't mean it is up to C standard.  \n","n":0.05}}},{"i":998,"$":{"0":{"v":"C#","n":1},"1":{"v":"\n# Resources\n[Float notation in C#](https://answers.unity.com/questions/282128/what-does-0f-and-5f-mean.html)\n","n":0.447}}},{"i":999,"$":{"0":{"v":"Lang","n":1},"1":{"v":"\n### Enumerables vs Enumerators\n#### Enumerable\n\"Enumerable\" defines an object that is meant to be iterated over, passing over each element once in order.\n- arrays and objects are examples of *enumerables*\n- an Enumerable is an object like an array, list, or any other sort of collection that implements the `IEnumerable` interface\n- Enumerables standardize looping over collections, and enables the use of useful extension methods like `List.Where()` or `List.Select()`.\n\n`IEnumerable` is an interface that defines one method `GetEnumerator` which returns an `IEnumerator` interface, this in turn allows readonly access to a collection. A collection that implements `IEnumerable` can be used with a foreach statement.\n\n#### Enumerator\n\"Enumerator\" is an object that can return each item in a collection. Therefore, the enumerator knows the order of items and keeps track of where it is in sequence. It then returns the current item when it is requested.\n\n`IEnumerator` provides two methods, `MoveNext()` and `Reset()`. It also has a property `Current`. \n- `MoveNext` is the method used to step over each item, applying any kind of custom iterator logic in the process\n- `Current` is a method used to get the current item after `MoveNext` is done. You end up with an interface that defines objects that can be enumerated, and how to handle that enumeration.\n\n![](/assets/images/2021-08-24-12-52-32.png)\nin the above example:\n\n- It gets the object’s enumerator by calling its GetEnumerator method.\n- It requests each item from the enumerator and makes it available to your code as the iteration variable, which your code can read\n\nevery time you write a foreach loop, you’re using enumerators. You actually can’t use foreach on a collection that doesn’t implement `IEnumerable`. When it gets compiled, a foreach loop like this:\n\n```cs\nforeach (Int element in list)\n{\n    // your code\n}\n```\n\nGets turned into a while loop, that processes until the Enumerable is out of items, calling MoveNext each time and setting the iterator variable to .Current.\n\n```cs\nIEnumerator enumerator= list.GetEnumerator();\nwhile (list.MoveNext())\n{\n    element = (Int)enumerator.Current\n    // your code\n}\n```\n\n[resource](https://www.csharpstar.com/difference-between-ienumerator-and-ienumerable-interface-csharp/)\n\n### Coroutine\nCoroutines are special functions that can pause execution and return back to the main thread.\n- Coroutines return an enumerator\n    - therefore, Coroutines can use functions that have a return type of `IEnumerator`\n- They’re commonly used for executing long actions that can take some time to finish, without causing the application to hang while waiting on the routine. \n    - For example, in games where the framerate of the application matters a lot, large hitches even on the order of a few milliseconds would hurt the user experience.\n\nto break up execution over different frames, you’ll just need to `yield return null` whenever you’d like to pause execution and run more on the main thread.\n- to use this Enumerator, you’ll need to call the function and assign it to a variable, which you can call `MoveNext()` on at regular intervals.\n\n#### in Unity\nthe Coroutine controller in Unity handles the processing of coroutines for us. All it's really doing is just calling `MoveNext()` once per frame, checking if it can process more, and handling the return value if it’s not null.","n":0.045}}},{"i":1000,"$":{"0":{"v":"Keywords","n":1},"1":{"v":"\n### Access modifiers\nin C# you have:\n- public\n- protected - only inherited classes can access this method\n- private\n- internal\n\n* * * \n\n#### override\nwhen we specify `override`, we override virtual method which was implemented scope above\n","n":0.171}}},{"i":1001,"$":{"0":{"v":"Browser","n":1},"1":{"v":"\nEvents are the native way to deal with user input in browser based web applications. Events are not part of the language of JavaScript itself, but they are part of the browser environment that JavaScript runs in\n\nBrowsers are single threaded and this single thread (The UI thread) is shared between the rendering engine and the js engine.\n- Therefore, if the thing you want to do takes a lot of time, it could halt the rendering (flow and paint).\n- In browsers there also exists \"The bucket\" where all events are first put in wait for the UI thread to be done with whatever it´s doing. As soon as the thread is done it looks in the bucket and picks the task first in line.\n    - Using `setTimeout` you create a new task in the bucket after the delay and let the thread deal with it as soon as it´s available for more work.\n\n","n":0.081}}},{"i":1002,"$":{"0":{"v":"Worker","n":1},"1":{"v":"\n### Workers in the browser\n#### Web workers\nA web worker is a JavaScript file that runs independently of the website off of the main thread of the app\n\n#### Service workers\nA service worker is a type of web worker which acts as a proxy between the browser and the server.  It also acts as a proxy between the browser and the cache.\n- Put another way, service workers act as a caching agent, and can store content for offline use.\n- They also give you more control over network requests and allow you to handle push-messaging, too. Since service workers are web workers, they run independently of your app, meaning that they can run even when the app is not open. Progressive web apps use service workers, and can thus work offline or on very slow networks.\n\nService workers are based on JavaScript promises They also use JavaScript’s fetch and cache APIs.\n\n\n","n":0.082}}},{"i":1003,"$":{"0":{"v":"DOM","n":1},"1":{"v":"\nThe DOM is just a big nested object that can be represented as a tree:\n![](2021-11-06-09-53-30.png)\n\nAnother way to view the tree is like this, which is a more accurate representation of what it actually looks like:\n![](2021-11-06-09-54-44.png)\n\nHTML becomes the DOM after the browser parses it.\n\nDOM is an API for HTML or XML documents and it creates a logical structure which can be accessed and manipulated.\n\nThe DOM was created so we could have a standard API that could be used with any programming language.\n\nThis object would include everything that's inside the scope of your browser, including `window`, `navigator`, `document`, global variables\n\nThe inspector tab looks like HTML, but in fact it is just an HTML-like representation of the DOM. The reason it looks like HTML is because someone thought it would be a good idea to show the DOM as HTML tags instead of objects. This makes sense, since the developer writes HTML, which is parsed into the DOM, and it looks similar to how the HTML was authored.","n":0.078}}},{"i":1004,"$":{"0":{"v":"shadow","n":1},"1":{"v":"\nShadow DOM is a tool used to build component-based apps and websites.\n- Shadow DOM comes in small pieces, and it doesn’t represent the whole DOM\n- Therefore, it can be seen as a subtree or as a separate DOM for an element.\n\nThe shadow DOM is mostly used when building without a front-end framework.\n\n### Shadow DOM vs Regular DOM\nThe DOM and Shadow DOM differ in how they are created and how they behave.\n- DOM nodes are placed within other elements.\n- Shadow DOM node (a scoped tree) is connected to the element but separated from the children elements\n    - the element it's attached to is called a Shadow Host.\n\nEverything that is added to the Shadow DOM is local, including styles.\n\nIf you have an element in the shadow dom and run `document.querySelector` to find it, it will not be returned.\n\nIf we are using [Web Components](https://developer.mozilla.org/en-US/docs/Web/Web_Components), then we are using the shadow DOM.\n\n* * *\n\nThe shadow DOM API is part of the browser specification, supported by most browser versions post 2018.\n\nShadow DOM should not be confused with the virtual DOM, which is an implementation used in frameworks like React and Vue.\n\nShadow DOM allows hidden DOM trees to be attached to elements in the regular DOM tree — this shadow DOM tree starts with a shadow root, underneath which can be attached to any elements you want, in the same way as the normal DOM.","n":0.066}}},{"i":1005,"$":{"0":{"v":"Bios","n":1},"1":{"v":"\n- Bios/UEFI serves as an interface between a system's OS and the software","n":0.277}}},{"i":1006,"$":{"0":{"v":"Binary","n":1},"1":{"v":"\n# Bits and Bytes\n- Bit - a zero or 1 \n\t- portmanteau of \"binary digit\"\n- Byte - an atomic unit of digital information. From a memory standpoint, it is the smallest unit that can be stored and read.\n- Though 8 bits make up a byte, there is no reason for that number aside from the benefits gained by raising that number to the power of 2.\n\t- There have been many other instances of varying-bit bytes, such as 10-bit bytes, 4-bit bytes etc.\n- 1 octet equals 1 byte (as long as  the bytes exist in an 8-bit system, which is almost always)\n- by definition, binary numbers ending in 0 are even, and those ending in 1 are odd.\n\n## Bit\nthe number 255 represents 8 bits. \n- Therefore, it is the largest binary number that can be represented by 8 individual bits. \n\t- in binary, it is *11111111*\n\n### Conversion\n- the image below shows how the placement of the bit determines its numerical value on the whole. \n\t- just like in base 10, each placement to the left multiplies the number by 10, in base 2, each placement to the left multiplies the number by 2 \n\t- if we add up the value of each numerical position (128, 64...), we will get 255 \n![](/assets/images/2021-03-09-21-26-04.png)\n- This chart above can be used to convert to/from binary format\n\t- ex. if we want to represent 135 in binary, we just need to add each slot, with the biggest fitting number first. In this case, the 8th slot, the 3rd slot, the 2nd slot and the 1st slot (or, 10000111) \n\n### How bits are read\n- a bit can be stored by any device that can exist in one of 2 possible states\n\t- ex. lightswitch, presence/absense of a hole in a punchcard, thickness of barcode line, presence of a microscopic pit on a CD ROM\n\t\t- therefore, any of these can be represented by 0's and 1's\n\t- in the case of modern computers, that bit is stored by way of: electrical pulse, or no electrical pulse\n\n### Bit field\n- a type of data structure that directly stores bits\n- the bit field is made up of adjacently-located memory locations.\n- think of a bit field as if it were an array of bits","n":0.052}}},{"i":1007,"$":{"0":{"v":"Bitwise","n":1},"1":{"v":"\n\n\n*Bitwise operation* - an operation done on a series of bits. Therefore, it is supported directly by the CPU and thus more efficient\n- the nature of bitwise is to treat a value as a series of bits, rather than as a numerical quantity\n- used to manipulate values for comparisons and calculations\n\t- the values must be an array of chars (string), array of ints, or a binary numeral\n- ex. NOT, AND, OR, XOR\n- ex. Say we perform the bitwise operation `4&1`. What happens is that a check is performed on each bit:\n```\n  00000011\n& 00000001\n__________\n  00000001\n```\n\nIt is *not* their main use to substitute arithmetic operations. Cryptography, computer graphics, hash functions, compression algorithms, and network protocols are just some examples where bitwise operations are extremely useful.\n- It is true that in most cases when you multiply an integer by a constant that happens to be a power of two, the compiler optimises it to use the bit-shift.\n\n*Bitmask* (or simply mask) - in the example above (with `&`) we utilized the bitwise operator to make a mask. Basically, what comes after the `&` is the mask, and it specifies which bits from the leftside will \"shine through\".\n- the mask is determined by the bitwise operator (i.e. AND, OR XOR) and the value following it.\n\t- `&` (AND) will extract a subset of bits from the value\n\t- `|` (OR) will set a subset of the bits in the value\n\t```\n\t  00110110\n\t| 11010011\n\t__________\n\t  11110111\t\n\t```\n\t- `^` (XOR) will toggle a subset of the bits in the value\n\t```\n\t  00110110\n\t^ 11010011\n\t__________\n\t  1110001\t\n\t```\n- anal. imagine a mask in Inkscape who's job is to mask anything that isn't skin. The resulting mask would be black wherever there is not skin in the photo. When we apply this mask to the photo, the resulting image would be just skin. \n\n*Shifting* (`<<`/`>>`) - the action of shifting all bits to the left or right. Shifting to the left effectively doubles the value. Conversely, shifting to the right divides the value by 2.\n- when shifting, if there is no bit to take a place, it defaults to 0\n- Declaring the first argument to be unsigned ensures that when it is right-shifted, vacated bits will be filled with zeros, not sign bits\n","n":0.052}}},{"i":1008,"$":{"0":{"v":"Bash","n":1},"1":{"v":"\n# High-level\n- A shell program is typically an executable binary that takes commands that you type and (once you hit return), translates those commands into (ultimately) system calls to the Operating System API.\n\t- a shell language is a language designed to ‘glue’ together other programs.\n- a shell is simply a macro processor that executes commands.\n\t- The term macro processor means functionality where text and symbols are expanded to create larger expressions.\n\nSince other shells are also programs, they can be run from within one another\n\n*\"Although most users think of the shell as an interactive command interpreter, it is really a programming language in which each statement runs a command. Because it must satisfy both the interactive and programming aspects of command execution, it is a strange language, shaped as much by history as by design.\"*\n- GNU specifies the shell as both a programming language AND a command interpreter. The command interpreter part provides the user interface to the GNU utilities, while the programming language part allows these utilities to be combined.\n\n- environment variables exist in the same environment as the code that is executed\n\n### Precedence of Synonyms\n1. Alias\n2. Keyword (ie. syntax of the shell)\n3. Function\n4. Builtin\n\t- commands that are built into the shell. These are executed directly, as opposed to the shell having to load and execute the executable\n\t\t- ex. pwd, cd\n\n* * *\n\ncheck to see if alias exists in shell - `which g`\n\n*interactive* means that input is accepted from the command line, while *non-interactive* means that input is accepted from a file.\n\nif the first word of a given command does not correspond to a built-in shell command, then the shell assumed that it is the name of an executable file.\n\nwhen we run `bash -c`, a script, or other executable in the shell, it becomes a child of the current environment.\n- ex. `bash -c 'echo \"child [$var]\";` will only have access to `$var` if it was exported beforehand from the shell that the command is run in.\n\nIn bash utilities, the `--` signifies the end of command options, after which only positional parameters are accepted.\n- often, `--` can signify the separation from the command options, and the following Regex\n\n### Status code 127\nThink of it as \"Error: the program you tried to use was not found\"\n- it's returned by your shell `/bin/bash` when any given command within your bash script or on bash command line is not found in any of the paths defined by PATH system environment variable.\n- to fix, make sure the command we are using is discoverable through `$PATH`\n\n### $()\nwhen we execute commands like this, they are executed in a subshell\n\n## UE Resources\n[GNU High-quality documentation](https://www.gnu.org/software/bash/manual/bash.html)","n":0.048}}},{"i":1009,"$":{"0":{"v":"Variables","n":1},"1":{"v":"\nvariables should be **snake_case**\n- kebab-case does not work","n":0.354}}},{"i":1010,"$":{"0":{"v":"Internal","n":1},"1":{"v":"\n`$?`\n- Exit status of a command, function, or the script itself\n\n`$$`\n- Process ID (PID) of the script itself\n    - The `$$` variable often finds use in scripts to construct \"unique\" temp file names\n\n`$_`\n- Special variable set to final argument of previous command executed.\n\n`$!`\n- PID (process ID) of last job run in background","n":0.139}}},{"i":1011,"$":{"0":{"v":"Test","n":1},"1":{"v":"\n[ -e FILE ] - True if FILE exists\n[ -d FILE ] - True if FILE exists and is a directory.\n[ -f FILE ] - True if FILE exists and is a regular file.\n[ -z STRING ] - True if the length if \"STRING\" is zero.\n[ -n STRING ] or [ STRING ] - True if the length of \"STRING\" is non-zero. \n\n```sh\nhttp_status=$(curl ___)\nif [ $http_status != \"200\" ]\n    then\n        echo \"command failed\"\nfi\n```\n\n### Test based on exit status of previously executed command\n- The `?` variable holds the exit status of the previously executed command (the most recently completed foreground process).\n```sh\nif [ $? -eq 0 ]\nthen echo 'That was a good job!'\nfi\n```","n":0.095}}},{"i":1012,"$":{"0":{"v":"Strings","n":1},"1":{"v":"\n- Single quotes `''` - preserves the literal value of each character within the quotes (don't allow anything to be interpolated inside). Take it exactly as it appears.\n\t- ex. if `$MYVARIABLE` is within the `''`, the string `$MYVARIABLE` will be interpreted.\n- Double quotes `\"\"` - evalutate the functions within, interpolating into the string\n\t- ex. if `$MYVARIABLE` is within the `\"\"`, the value that `$MYVARIABLE` stands for will be interpreted.\n\nnm: double quotes are bigger, so they have more capabilities (the ability to interpret and expand variables)\n\n```sh\n# NO STRING INTERPOLATION\n$ echo '$(echo \"upg\")'\n# $(echo \"upg\")\n\n# js equivalent:\n# console.log(\"$(echo \"upg\")\")\n\n# STRING INTERPOLATION\n$ echo \"$(echo \"upg\")\"\n# upg\n\n# js equivalent\n# `${}`\n```\n- The implication of this is that when we use double quotes, since the inerpolated functions are called when defined, the variable that gets set to the string will only be executed once.\n- ex:\n```sh\nPROMPT=\"$(git_status) $\"\n# defining this variable will cause git_status to run, interpolating the return\n# value of that function. The only way to update this variable, is by sourcing .zshrc\n\nPROMPT='$(git_status) $'\n# here, we literally pass the string '$(git status)' to the shell (?),\n# and let it interpolate (?) on its own, each time it is displayed in the terminal\n```\n\n## Globbing\n- quotes used in a globbing context do not behave as described above. Globs are not expanded when in either single or double quotes","n":0.068}}},{"i":1013,"$":{"0":{"v":"Script","n":1},"1":{"v":"\n[Using colors effectively in bash scripts](https://github.com/mikebranski/the-art-of-postgresql-docker/blob/master/fetch-f1db-data.sh)","n":0.408}}},{"i":1014,"$":{"0":{"v":"Redirection","n":1},"1":{"v":"\n# Redirection\n- the redirect operator `<`/`>` takes the stdout from the preceding command and redirects it to a file\n- implicity, `>` === `1>`\n","n":0.209}}},{"i":1015,"$":{"0":{"v":"Pipeline","n":1},"1":{"v":"\n# Pipeline\n- a pipe takes stdout from one process and passes it as stdin to another\n\t- spec: if the first program fails, then a `0` will be passed as stdout. (maybe not true)\n- Each command in a pipeline is executed in its own subshell, which is a separate process\n","n":0.143}}},{"i":1016,"$":{"0":{"v":"Parameters","n":1},"1":{"v":"\n# Special Parameters\n- `$1, $2, $3` are the positional parameters.\n- `\"$@\"` is an array-like construct of all positional parameters\n\t- ex. `{$1, $2, $3 ...}`\n- `\"$*\"` is the IFS expansion of all positional parameters\n\t- ex. `$1 $2 $3`\n- `$#` is the number of positional parameters.\n- `$-` current options set for the shell.\n- `$$` pid of the current shell (not subshell).\n- `$_` most recent parameter (or the abs path of the command to start the current shell immediately after startup).\n- `$IFS` is the (input) field separator.\n- `$?` is the most recent foreground pipeline exit status.\n- `$!` is the PID of the most recent background command.\n- `$0` is the name of the shell or shell script.\n","n":0.094}}},{"i":1017,"$":{"0":{"v":"Lang","n":1}}},{"i":1018,"$":{"0":{"v":"Loops","n":1},"1":{"v":"\n#### Loop over regular files in directory\n```sh\nfor f in *; do\n  echo \"File -> $f\"\ndone\n```\n\n#### Loop over all files in directory (inc. dotfiles)\n```sh\nfor f in $(ls -a ~/Documents); do\n    echo \"Processing $f file..\"\ndone\n```","n":0.174}}},{"i":1019,"$":{"0":{"v":"Glob","n":1},"1":{"v":"\n# Glob Patterns\n`*` is not the only globbing primitive. Other globbing primitives are:\n- `?` - matches any single character\n- `[abd]` - matches any character from a, b or d\n- `[a-d]` - matches any character from a, b, c or d\n\n- `.` has no meaning as a glob.\n\n## Globbing vs Regex\n- spec: Globbing is interpreted by the shell, while regex is interpreted by a program (like `rename`)\n- commands surrounded by single-quotes are not interpreted by the shell.","n":0.115}}},{"i":1020,"$":{"0":{"v":"Expansion","n":1},"1":{"v":"\n# History Expansion\n[source](https://www.thegeekstuff.com/2011/08/bash-history-expansion/)\n- History expansion is performed immediately after a complete line is read, before the shell breaks it into words, and is performed on each line individually\n\nDesignators:\n- Event Designators - refer to a particular command in history\n\t- start with `!`\n- Word Designators - refer to a particular word of a history entry\n\t- often gets combined with Event Designators, which are demarcated by `:`\n\t- ex. `!cp:^` finds the most recent `cp` command in history and grabs the 1st argument\n- Modifiers - modify result of the Event/Word Designator\n\nCommands:\n- `!ls` - execute most recent `ls` command\n\t- ex. if I ran `git log`, then `!git`, bash will say \"oh ok, you want the last executed bash command\", and will run `git log`\n- `!?apache` - execute most recent command that contains the keyword `apache`.\n\t- `!%` will refer to the word that was matched by the previous `!?<pattern>` search\n- `!-3` - execute 3rd most recent command\n\t- `!!` === `!-1` (most recent command)\n- `!$` - reuse last part of the most recent command\n\t- ex. `less ~/myfile` then `vim !$` will run `vim ~/myfile`\n- `!*` - reuse all parts of most recent command\n- `^ls^cat^` - modify the pattern `ls` with `cat` in the previous command\n- `!!:s/ls -l/cat/` - replace `ls -l` of previous command with `cat`\n\t- `!!:gs/...` for global substitution\n- `!cp:^` - get 1st arg of the most recent `cp` command\n- `!cp:$` - get last arg of the most recent `cp` command\n- `!cp:2` - get 2nd arg of the most recent `cp` command\n- `!cp:*` - get all args of the most recent `cp` command\n\n* * *\n\n# Tilde Expansion\n### Tilde prefix\nThe tilde prefix includes all of the characters before the first slash `/`\n\n`~`\n`$HOME`\n- in reality, this is shorthand for `~kyletycholiz` (the current user)\n\n`~+`\n`$PWD`\n\n`~-`\n`$OLDPWD`\n- or, the directory we were in previous to our PWD","n":0.058}}},{"i":1021,"$":{"0":{"v":"Variable (aka Parameter) Expansion","n":0.5},"1":{"v":"\nThe `$` character introduces parameter expansion, command substitution, or arithmetic expansion.\n- The parameter name or symbol to be expanded can be enclosed in braces, which are optional but serve to protect the variable to be expanded from characters immediately following it which could be interpreted as part of the name.\n\n### Substitution\nwhen we write `${parameter}`, the value of `parameter` is substituted.\n\n# UE Resources\n- [Overview](https://wiki.bash-hackers.org/syntax/pe)\n- [Explanation](https://ss64.com/bash/syntax-expand.html#parameter)","n":0.125}}},{"i":1022,"$":{"0":{"v":"Brace Expansion","n":0.707},"1":{"v":"\nWith brace expansion, we can generate arbitrary strings\n- similar to filename expansion (ex. `*.env`), except the generated strings actually need to exist.\n\nTo be a proper brace expansion, there must be opening and closing braces, and at least one `,`\n```sh\n$ echo a{d,c,b}e\n# ade ace abe\n```\n\nBrace expansion is performed before any other expansions, and any characters special to other expansions are preserved in the result\n- Bash does not apply any syntactic interpretation to the context of the expansion or the text between the braces.\n\n### Alpha example\n```sh\n$ ls\nalpha.py alphabeta.py\n\n### Brace expansion\n$ mv alpha{,beta}.py ../\n```","n":0.105}}},{"i":1023,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### See the command attached to a bash alias\nrun `type <command>`\n\n#### Run multiple commands in sequence\n```sh\n$ command1; command2\n```\n\nThis is distinct from `&&`, since commands will be run regardless of exit status\n\n#### Run previous command with `sudo` in front\n```sh\n╰─➤ $ rm -rf /var/www\nrm: /var/www: Permission denied\n╰─➤ $ sudo !!\n╰─➤ $ sudo rm -rf /var/www\n```\n","n":0.137}}},{"i":1024,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n### Line Navigation\n- go to start of line - `<C-a>`\n- go to end of line - `<C-e>`\n\n`<C-R>` to search through history. We can start to type a command, then by hitting this shortcut, the history will be searched for that string.\n- `d` - show previous 10 directories visited\n    - press the number beside it to go to that directory\n","n":0.13}}},{"i":1025,"$":{"0":{"v":"CLI","n":1}}},{"i":1026,"$":{"0":{"v":"Eval","n":1},"1":{"v":"\n`eval` will run the arguments as a command in the current shell\n- variables will be expanded before executing, so we can execute commands saved in shell variables\n\nSometimes we have bash commands that output shell commands (ex. ssh-eval)\nWe could run these outputted shell commands in 2 steps:\n1. run the command\n2. take the output of that command, and run it in the shell\n\nAlternatively, we can just wrap the command in `eval`\n- ex. `eval $(ssh-agent)`\n\n### Say Hello example\nsay-hello.sh\n```sh\necho \"echo hello\"\n```\n\n```sh\n$ ./say-hello.sh\n> echo hello\n\n$ eval $(./say-hello.sh)\n> hello\n```\n","n":0.11}}},{"i":1027,"$":{"0":{"v":"Azure","n":1},"1":{"v":"\nAzure gives you many different ways to interact with its resources, such as Azure Portal, Azure SDKs, az (command-line tool) etc.\n\n## Deployment\n- To deploy Functions to the cloud, we need to create 3 different resources: Resource group, storage account, and function app\n\n### Storage Account\n- maintains state and other information about your projects\n- created with command `az storage account create`\n\n### Function app\n- provides the environment for executing your function code\n- A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources\n- created with command `az functionapp create`\n\t- When running this command, we also determine the `<APP_NAME>`. This will serve as the default DNS domain for the function app (ie. we invoke the function with an HTTP request to `<APP_NAME>.azurewebsites.net`)\n\n* * *\n\n## Artifacts\n- Artifacts allows us to create/share/manage dependencies (ex. npm packages) in our project (client-read, client-publisher)\n\t- usually we explicitly define each feed that we pull our package from. Artifacts allows us to use a single point of entry for multiple existing feeds.\n- Github Package Manager is analogous to Artifacts\n\nArtifact Feed\n- \"feed\" is an Azure-specific term\n- a feed is a construct that allows us to store, manage and group packages (like npm), and control who we share it with.\n- a feed is like an npm registry, similar to an endpoint that specifies where these packages can be found.\n- a feed gives us access to a collection of packages.\n- a feed is made up of artifacts\n\nBuild Artifacts\n- build artifact is the output of running the azure-pipeline.yml CI file.\n- Build Artifacts are different than Artifacts\n- ex. build artifact is the result of taking some input data, processing it in some way, then stamping it with a commit SHA as well as a build #, letting us track it.\n\nAzure has 2 types of pipelines: build pipeline and release pipeline.\n- the build pipeline is CI (build, test and create artifacts), while release pipeline is CD\n* * *\n\n### Resource Manager\nWhen a user sends a request from any of the Azure tools, APIs, or SDKs, Resource Manager receives the request. Then it:\n1. authenticates and authorizes the request\n2. sends the request to the Azure service\n- since it is a central API that all requests pass through, we can see logs and everything related to these services and the requests they receive.\n- the Resource Manager allows us to declaratively manage resources (as opposed to having to use scripts)\n\n#### Scope\nAzure provides four levels of scope: management groups, subscriptions, resource groups, and resources.\n![](/assets/images/2021-03-08-21-28-23.png)\n- management settings can be applied to any of these levels of scope, and the level we choose will determine how widely the setting is applied (lower levels inherit from higher).\n\n## Terminology\n**Resource**\n- Any manageable item that is available through Azure.\n- ex. VMs, storage accounts, web apps, databases, blob storage, virtual networks, resource groups, management groups etc.\n\n## Questions\n- What would the connection string be a connection to?\n\t- would it be the host of the function?","n":0.045}}},{"i":1028,"$":{"0":{"v":"Resource Group","n":0.707},"1":{"v":"\nA Resource group is a logical container for related resources. We would group together resources that we want to manage together as a group.\n- all resources in a RG should be a part of the same lifecycle, meaning we deploy, update, and delete them together.\n- resource groups can be stored in a different location than the resource that is comprises. This is possible because the location of the RG is merely the location of the RGs metadata about its resources.\n- created with command `az group create`\n- ex. `AzureFunctionsQuickstart-rg`\n","n":0.107}}},{"i":1029,"$":{"0":{"v":"Db","n":1}}},{"i":1030,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Allow IP address to database\nFrom within resource panel (ie. database panel) of the resource group, select `connection security` and add your IP address","n":0.204}}},{"i":1031,"$":{"0":{"v":"Aws","n":1},"1":{"v":"\n# Lambda\nlike an anonymous function (or a callback) that runs code in response to events.\n- Think of them like event handlers, but for web services, not components within a webpage\n\nEffectively, everything is abstracted away aside from a function interface.\n\nIf EC2 is a complete computer in the cloud, Lambda is a code runner in the cloud.\n- With EC2 you get an OS, a file system, access to the server’s hardware, etc. \n- With Lambda, you just upload some code and Amazon runs it for you.\n\n### When to use\nLambda is most suitable for small snippets of code that rarely change.\n- think of Lambda functions as part of the infrastructure rather than part of the application\n\nLambda can be used as a plugin system for other AWS services, for example:\n- S3 doesn’t come with an API to resize an image after uploading it to a bucket, but with Lambda, you can add that capability to S3.\n\n### Limitations\n- Your functions will suffer a cold start when a function is invoked after a period of inactivity\n- limit of 250 MB for your code bundle, including all your dependencies\n\n# EC2 (Elastic Compute Cloud)\nAn instance of EC2 is a VM on an AWS server\nThe nice thing about EC2 is that the computer you get will be very similar to the computer you use to develop your software. If you can run your software on your computer, you can almost certainly run it on EC2 without any changes.\n- This is one of EC2’s main advantages compared to other types of compute platforms (such as Lambda): you don’t have to adapt your application to your host.\n\nElastic refers to the fact that it can scale up/down as needed automatically\n\nEC2 has dozens of options you will probably never need, and thus it comes with sensible defaults.\n-  This is the result of the highly varied workloads and use cases serviced by EC2\n\nSelecting an instance type will be the most consequential decision when provisioning an EC2 instance.\n- There are 256+, and can be narrowed to a few categories, defined by what they're optimized for:\n\t- CPU\n\t- Memory\n\t- Network\n\t- Storage\n\nIf you were building your own server, there would be an infinite number of ways to configure it, but with EC2 you get to pick an instance type from its catalog.\n- This is the tradeoff you make as opposed to getting you build your own server, but most likely it shouldn't even be a concern.\n\n### Security\nthe defaults are sensible, but we may have to modify:\n- The security group\n\t- You can think of security groups as individual firewalls for your instances. With security groups, you can control what goes in and out of your instances.\n- The VPC ACL.\n\t- You can think of VPC ACL as a network firewall. With the VPC ACL, you can control what goes in and out of your network.\n\n### Scaling\nThe auto part of Auto Scaling allows us to automatically add/remove EC2 instances (therefore horizontal scaling). In theory it works, but is often impractical.\nThe main premise of Auto Scaling is that once you decide how much headroom you want, you’ll be able to make that headroom a constant size\n\nUnless the following 2 are true, then it's probably not worth messing around with auto-scaling:\n- Are your EC2 costs high enough that any reduction in usage will be materially significant?\n- If your EC2 bill were to go down by 30%—would that be a big deal for your business?\n\n#### How many EC2 instances to run?\nCapacity Headroom - we need to have a buffer between the expected peak demand and the maximum capacity that our system can handle.\n\nPricing\n- only pay for the number of seconds your instance is running.\n\n# ELB (Elastic Load Balancing)\nComes in 3 variants:\n- Classic \n\t- Legacy software.\n- Application (ALB)\n\t- a proper reverse proxy that sits between the internet and your application\n\t- Every request to your application gets handled by the load balancer first. The load balancer then makes another request to your application and finally forwards the response from your application to the caller.\n- Network (NLB)\n\t- behave like load balancers, but they work by routing network packets rather than by proxying HTTP requests\n\n# S3 (Simple Storage Service)\nFundamentally, you can think of S3 as a highly-durable hash table in the cloud. \n- The key can be any string, and \n- the value any blob/object of data up to 5 TB.\n\nObject storage is used for purposes such as storing photos on Facebook, songs on Spotify, or files in online collaboration services, such as Dropbox.\n\nData gets streamed at a rate of around 90 MB/s.\n\nYou can have as many parallel uploads and downloads as you want, thus, the infinite bandwidth.\n\nS3 offers built-in redundancy.\n\nAt first you can start with the default storage class and ignore all the other classes. Don't bother with the implications of the different storage classes until you really need to start saving money from S3 storage costs.\n\n### Limitations\n- cannot append to objects.\n- It doesn’t support HTTPS when used as a static website host, so you cannot host static websites\n\n- Alternatives: Azure Blob\n\nPricing\n$23.55/TB/month\n$5/million uploads and $0.40/million downloads\n\n# Fargate\n- serverless compute engine for containers\n- adds about 20% in price, but removes a lot of the admin overhead\n- removes the need to provision and manage servers\n\t- don't have to worry about scaling, patching, securing, and managing servers\n- Fargate automates how much computing power you need and will scale up/down automatically\n- with Fargate, you only interact with your containers\n![](/assets/images/2021-03-08-21-25-27.png)\n\n# RDS (Relational Database Service)\n- databases shouldn't be run in a container\n- by default will spin up a single instance in a single availability zone\n\t- if we want more redundancy, we can add an active backup instance\n- doesn't support downloading postgres extensions\n \n# Amplify\n- toolbox for front-end/mobile development, with some overlap with Firebase\n\t- ex. tools for creating onboarding flow, tools for implementing AI/ML features, tools for auth \n- also includes tools to help implement cloud-based features in the app\n- includes tools to make real-time apps (ex. news feed, chat)\n\n# Elastic Beanstalk\nPurpose is to deploy + scale web apps/services built with common web languages (Javascript, PHP etc) onto webservers like Nginx/Apache\n- Elastic beanstalk hosts all the services needed for deployment/scaling\n\nHandles things like:\n- Provisioning services\n- loadbalancing\n- scaling infra\n- updating platform with latest patches\n- app health monitoring\n\nAlso allows us to reimplement portions of the code as containerized services, allowing us to achieve a more [[distributed|deploy.distributed]] architectural design\n\nAllows to retain control over the resources powering your application, so you can decide on how much you want to manage.\n\nAlternatives: Azure App Service, Google Cloud App Engine\n\n### Process\n1. upload source code bundle to Elastic Beanstalk console\n2. Elastic Beanstalk return a new URL for the webapp\n\n# DynamoDB\nData is stored as a partitioned B-Tree.\nUnlike Redis, in that it is immediately consistent and highly-durable, centered around that single data structure.\n- If you put something into DynamoDB, you’ll be able to read it back immediately and, for all practical purposes, you can assume that what you have put will never get lost.\n\nDynamoDB data is schemaless. The DB engine can manage structured or semi-structured data, including JSON documents.\nFully managed.\nsupports key–value and document data structures\nThe general rule of thumb is to choose Dynamo for low throughput apps as writes are expensive and consistent reads are twice the cost of eventually consistent reads\nHigh vendor lock-in.\nprovides seamless integration with services such as Redshift (large scale data analysis), Cognito (identity pools), Elastic Map Reduce (EMR), Data Pipeline, Kinesis, and S3. Also, has tight integration with AWS lambda via Streams and aligns with the server-less philosophy; automatic scaling according to your application load, pay-per-what-you-use pricing, easy to get started with, and no servers to manage. \n\nWhen to use DynamoDB?\n- In case you are looking for a database that can handle simple key-value queries but those queries are very large in number\n- In case you are working with OLTP workload like online ticket booking or banking where the data needs to be highly consistent\n\nWhen not to use DynamoDB?\n- In cases where you have to do computations on the data. \n\t- Relational databases run their queries close to the data, so if you’re trying to calculate the sum total value of orders per customer, then that rollup gets done while reading the data, and only the final summary (one row per customer) gets sent over the network. However, if you were to do this with DynamoDB, you’d have to get all the customer orders (one row per order), which involves a lot more data over the network, and then you have to do the rollup in your application, which is far away from the data.\n\nPricing\n$256/TB/month\n\nBy default, you should start with DynamoDB’s on-demand pricing and only consider the provisioned capacity as cost optimization. On-demand costs $1.25 per million writes, and $0.25 per million reads.\n- Then, if your usage grows significantly, you will almost always want to consider moving to provisioned capacity (significant cost savings).\n- if you believe that on-demand pricing is too expensive, then DynamoDB will very likely be too expensive, even with provisioned capacity. In that case, you might want to consider a relational database.\n\n# CloudFormation\nWhen working in AWS, you almost always want to use some CloudFormation (or a similar tool).\nlets you create and update the things you have in AWS without having to click around on the console or write fragile scripts.\n- for instance, gives us the ability to tear down everything cleanly and recreate your AWS set up in one click\n\nrule of thumb is to let CloudFormation deal with all the AWS things that are either static or change very rarely, like load balancers, deployment pipelines, VPC configs, security groups\n\n### Overview\n- define your AWS resources as a YAML script\n- point CloudFormation to your AWS account, and it creates all the resources you defined idempotently.\n- Updates can be rolled back.\n\n# SQS (Simple Queue Service)\nhighly-durable queue in the cloud\n- put messages on one end, and a consumer takes them out from the other side.\nMessages are consumed *almost* in FIFO, but there is no strictness to adhere to this.\n- Strictness *can* be guaranteed, but there is a performance cost.\n\nSQS requires zero capacity management.\n- no limit on the rate of messages enqueued or consumed\n- don’t have to worry about any throttling limits.\n- number of messages stored in SQS (the backlog size) is also unlimited.\n\ngreat default choice for dispatching asynchronous work.\n\n# Kinesis\nA highly-durable linked list in the cloud\n\nUse-cases are similar to SQS—you would typically use either Kinesis or SQS when you want to enqueue records for asynchronous processing.\n\nDifference with SQS:\n- SQS can only have one consumer, while Kinesis can have many.\n- Once an SQS message gets consumed, it gets deleted from the queue. But Kinesis records get added to a list in a stable order, and any number of consumers can read a copy of the stream by keeping a cursor over this never-ending list.\n- Multiple consumers don’t affect each other, and if one falls behind, it doesn’t slow down the other consumers.\n- Whenever consumers read data out of Kinesis, they will always get their records in the same order.\n- Often cheaper than SQS\n- Kinesis can carry a significant operational burden with the need to provision capacity (shards).\n\n# CloudFront\nCloudFront is a CDN in AWS which operates at Edge Locations around the world closer to the users. It can cache the static assets and deliver them to the end-users quite fast.\n\nLambda Edge functions run here","n":0.023}}},{"i":1032,"$":{"0":{"v":"Ecs","n":1},"1":{"v":"\n# ECS (Elastic Container Service)\n- running containers via AWS proprietary orchestrator\n- There are 2 main models for running containers: ec2 and fargate\n- ECS clusters can be run a few different ways:\n\t- run on vms\n\t- managed docker container runtime\n\t- EC2\n\t- Fargate\n- **ECS Control Plane** - The tools to manage ECS\n\t- handles autohealing\n- by default, containers behave like other Linux processes with respect to access to resources like CPU and memory. this means they get access to all of the host's CPU and memory capacity\n  - ecs provides mechanisms through which we can limit this (in the task)\n\n### Task\n**Task definition** - To prepare your application to run on Amazon ECS, you create a task definition, which is a JSON file that describes between 1 and 10 containers\n- Think of it as a blueprint for your application\n- Task definition parameters might be:\n\t- Which containers should be used?\n\t- Which ports should be opened?\n\t- Which data volumes should be used within the containers?\n\t- How are container linked together\n\n- a `task` is an instantiation of a `task definition`\n\t- the basic unit of deployment is a `task`\n- a `task` is a logical construct that models 1+ containers\n\t- therefore, the ECS API operates on tasks rather than individual containers\n- in ECS you run a task, which in turn runs a container\n- each fargate task has its own isolation boundary and doesn't share kernel, cpu resources, memory resources etc\n\n- **ECS Cluster** - a regional grouping of container instances, upon which we can run task requests\n\t- When tasks are run on Fargate, cluster resources are managed for us\n\t- ECS cluster is a regional grouping of one or more container instances on which you can run task requests\n\nECS objects and how they relate\n![](/assets/images/2021-03-08-21-29-20.png)","n":0.06}}},{"i":1033,"$":{"0":{"v":"Auth","n":1},"1":{"v":"\nBecause of SSL, sending plaintext passwords from the client to the server is perfectly fine.\n","n":0.258}}},{"i":1034,"$":{"0":{"v":"Tokens","n":1}}},{"i":1035,"$":{"0":{"v":"Refresh Token","n":0.707},"1":{"v":"\nThe Refresh Token contains basically only the session ID which can be used to look up in the database to see if it’s an active session or not. If it is, then it can generate a new [[access token|auth.jwt.access-token]] and that can be sent to the user\n- A refresh token is a special token that is used to generate additional jwt tokens. This allows you to have short-lived access tokens (JWTs) without having to collect credentials every time one expires. The server sends the client this token alongside the access (JWT) and/or ID tokens as part of a user's initial authentication flow\n\t- The refresh token should be saved in the database in the relevant row of the user's table.\n\t\t- Therefore, we can handle the renewing login with Postgres.\n\t- the refresh token can be sent from the server to the client as an `HTTPOnly` cookie\n- A refresh token is not capable of authenticating a user on its own— its only use is to look up active sessions. If it finds one, then the refresh token can be used to generate a new access token.\n- Every time a new access token is generated, a refresh token should be generated as well\n- a Refresh token is what allows us to login to a website, close the browser, and still be logged in upon reopening it.\n\t- When a new session starts (reopening the browser tab), the app is able to see that there is no JWT in memory, so it triggers a *silent refresh*\n- Imagine we set a jwt to have a lifetime of 15 minutes. Without the refresh token, this means that the server would send back an http 401: unauthorized every 15 minutes (probably at which point your app will log the user out and display the login screen)\n- A refresh token has 2 properties:\n\t1. It can be used to make an API call (say, `/refresh_token`) to fetch a new JWT token before the previous JWT expires.\n\t2. It can be safely persisted across sessions on the client!\n\n### Why use refresh tokens?\nUsing refresh tokens in conjunction with access tokens helps us alleviate the problem of stale tokens. e","n":0.053}}},{"i":1036,"$":{"0":{"v":"Access Token","n":0.707},"1":{"v":"\nAn access token is used to inform an API that the bearer of the token has been authorized to access the API\n\nThe access token doesn't have any concept of sessions. All it cares about is user information. If we are leaving out refresh tokens, then we would only care if the user has an access token or not. If they do, then we trust that they are who they say they are, and we allow access.\n\nThe access token contains only the session ID and the user ID, and it expires when the user closes that browser window.","n":0.102}}},{"i":1037,"$":{"0":{"v":"Session","n":1},"1":{"v":"\n# Session\n- When user logs in, the server creates a session for the user. The sessionId is stored on a cookie in the user's browser. That cookie gets sent with each request to the server, where it gets compared against what is in the db.\n- sessions are not an option for mobile apps.\n\n- sessions are stateful (unlike JWTs, which are stateless)\n\t- This means that one of the communicating parties must keep track of (store) information about the current state, and save information about session history.\n- Sessions piggyback off of stateless HTTP requests to make a stateful connection.\n\t- anal: the way TCP on top of IP\n- When there is an active session, the server responds to the request with an ETag (entity tag), which is an id corresponding to a particular \"version\" of data. If the data arrives and it is identical to the ETag, then we know it is the same as the cached version.\n- In a session-based workflow, a session is active until the user hits the logout button\n\n## Persistent Session Store\na place to store all session-related data (but persistent so it doesn't get erased when the user closes the tab or app)\n- session store is like the next evolution after cookies. Cookies were being used to remember user's preferences and some searching data about them. When those client-side cookies reached their capacity, session storage starting taking over for that sort of data storage about a user.\n\n# Resources\n[A new session should be created during user authentication](https://rules.sonarsource.com/typescript/type/Vulnerability/RSPEC-5876)","n":0.064}}},{"i":1038,"$":{"0":{"v":"Passport","n":1},"1":{"v":"\n## Serializing/Deserializing\n```\npassport.serializeUser(function(user, done) {\n    done(null, user.id); \n   // where is this user.id going? Are we supposed to access this anywhere?\n});\n\n// used to deserialize the user\npassport.deserializeUser(function(id, done) {\n    User.findById(id, function(err, user) {\n        done(err, user);\n    });\n});\n```\n\n- the `user.id` we pass as an arg to `done` (inside `serializeUser`) is saved in the session. We can later use it to retrieve the whole user object (`deseralizeUser`)\n- `serializeUser` determined which data in the `User` object shoould be saved in the session \n\t- the result of `serializeUser` is attached to the session as `req.session.passport.user`\n- the `id` that is passed to `deseralizeUser` is used to find the session object that has already been stored. The key it looks for is the same key provided to the `done` funtion in `serializeUser`\n\t- the fetched object is attached to the request object as `req.user`","n":0.086}}},{"i":1039,"$":{"0":{"v":"Jwt","n":1},"1":{"v":"\nA JWT is a security token format that is used for exchanging authentication and authorization data between parties\nThe most common implementations of JWT that we will see are as [[access tokens|auth.jwt.access-token]] and [[refresh tokens|auth.jwt.refresh-token]]\n\n### Token (ex. JWT)\n- When user logs in, the server creates a JWT with the secret, and sends it to the client. The client stores the JWT in local storage, and includes that JWT in every request.\n- The biggest difference here is that the user’s state is not stored on the server, as the state is stored inside the token on the client side instead\n\n- the whole point of JWTs is to not require centralised coordination\n\t- this is why there is no `/logout` endpoint to hit. \n- A JWT is just a regular javascript object that is stringified, hashed, and cryptographically signed.\n- Your token is signed with the secret, known only by the server. If someone changes the token on client side, it would fail validation and the server side framework would reject it. Therefore you can trust your token. Of course, the jwtSecret should be a secret only known by your authentication server and resource server.\n- JWTs are agnostic to what form of authentication you are using (ex. email, OAuth etc). Regardless, the response will contain the JWT.\n- You generate the token only if you trust the user who requested it.\n- You trust the token as long as it has not expired and can be verified with the secret.\n- The information in a JWT can be read by anyone, so do not put private information in a JWT. What makes JWTs secure is that unless they were signed by our secret, we can not accept the information inside the JWT as truth.\n- JWTs guarantee that the bearer of the token also owns the data that he is requesting.\n\t- However JWTs don't guarantee encryption, which is why HTTPS is required. Otherwise, a man in the middle could take that server response (with the jwt) and use it to authenticate itself on your behalf, gaining access to all data.\n- JWTs come with a death sentence— that is, by nature they have an expiry date.\n\t- this value is stored in the `iat` property. This can be thought of as the `date_of_death` property.\n\t- The server determines the lifespan of the JWT, since it controls the expiry date. Therefore, JWTs give a uniform lifespan to all JWTs, but like God, it has control over ending your life prematurely in order to deny access.\n- JWTs are stateless (while sessions are stateful). This fact enables them to be verified on the server, without having to make a database call. This effectively makes them faster than using sessions (since sessions need to be stored)\n\t- In reality, it's likely that the times we need to authenticate ourseleves with the JWT are also times that we need to interact with the database. This makes \"saving trips to the database\" more of a pipe-dream than a reality. At the end of the day, if we need to make a database interaction *and* authenticate ourselves, we are quicker just using a session rather than authenticating with a JWT (due to their size)\n- JWTs are huge. Storing a userid in a cookie is 6 bytes, while storing in the JWT (along with headers+secret) makes it 304 bytes.\n\n![](/assets/images/2021-03-08-16-41-50.png)\n\n- unlike a cookie, a JWT can contain an unlimited amount of data\n\n### Logout\nThere is no `/logout` endpoint to hit, as all we need to do is delete the JWT kept on the client.\n- This means the token is still valid even after you logout. This is why keeping a short expiry date is important\n\n### Analogy\n*\"Pretend I’m blind and hard of hearing. Let’s also pretend that last week you bought me lunch, and now I need your bank account number to pay you back. If I ask you for your bank account number in person, and someone else shouts their bank account number, I might accidentally send them the money I owe you.\nThat’s because I heard someone shout a bank account number, and I trusted that it was you, even though in this case, it wasn’t.\nJWTs were designed to prevent this sort of thing from happening. JWTs give people an easy way to pass data between each other, while at the same time verifying who created the data in the first place.\nIf I received 1,000,000 different JWTs that contained a bank account number, I’d easily be able to tell which one actually came from you.*\"\n\n# E Resources\n[Good high level overview. Includes links to other good content](https://blog.logrocket.com/jwt-authentication-best-practices/#:~:text=A%20JWT%20needs%20to%20be,storage%20(or%20session%20storage).)","n":0.036}}},{"i":1040,"$":{"0":{"v":"Signature","n":1},"1":{"v":"\n### Signature\n- created using the header, the claims AND the secret. Therefore, this unique combination creates a hash, and if something in the claim were to change, then the signature would be different and would no longer match up. \n\t- What this means is that if the 1st and 2nd set of the JWT don't change, than neither will the 3rd (of course assuming the secret remains unchanging)\n\t\t- in practice jwt will always be different between sign ins of the same user, since the `exp` variable will always be different\n![8ded9f2b6239d7464fa7e85badce77c6.png](:/fa5c3e7e2d95477e85f51bed49c6b4d9)","n":0.105}}},{"i":1041,"$":{"0":{"v":"Jwk","n":1},"1":{"v":"\n### JWK (JSON Web Key)\n- The JWK is a JSON object that contains a well-known public key which can be be used to validate the signature of a signed JWT\n- If the issuer of your JWT used an asymmetric key to sign the JWT, it will likely host a file called a JSON Web Key Set (JWKS). The JWKS is a JSON object that contains the property keys, which in turn holds an array of JWK objects.\n- The service may only use one JWK for validating web tokens, however the JWKS may contain multiple keys if the service rotates signing certificates.\n\t- we can use a service like Auth0 or Okta to store our JWKs, which will be located at an endpoint hosted at their domain. \n\t- Any time your application validates a JWT, it will attempt to retrieve the JWK(S) from the issuer in order to ensure the JWT signature matches the content.\n\nLogin\n1. user enters email/pw combination\n2. email/pw sent to server. The server takes in that password, appends the salt to it, then hashes it and compares it to the hashed pw stored in the database.\n3. if the passwords match, then the server will send back a signed JWT \n\t- the jwt is composed of 3 parts:\n\t\t- header - usually contains type of token and signing algo used:\n\t\t\t```\n\t\t\t{\n\t\t\t  \"alg\": \"HS256\",\n\t\t\t  \"typ\": \"JWT\"\n\t\t\t}\n\t\t\t```\n\t\t- claims (aka payload)\n\t\t\t- this is the data that we can encode into the token\n\t\t- signature\n\nHTTP requests\n1. server receives the request, along with the JWT.\n2. JWT header and claims are combined with the secret (stored on the server) to generate a \"test signature\", to compare against the original signature (also received in the JWT) ","n":0.06}}},{"i":1042,"$":{"0":{"v":"Claim","n":1},"1":{"v":"\n### Claim\n- the core of a JWT, since they are the data contained in the JWT\n- claims are pieces of information that are \"claimed\" about a subject (most often a user). In other words, they are just properties of an object.\n\t- ex. name, sub, admin\n- the claim refers to the key, not the value\n- usually not encrypted, meaning if we don't use https, this information is potentially compromised.\n- anyone will be able to decode them and to read them, we cannot store any sensitive data in here\n\t- not an issues because of the secret\n- ex:\n```\n{\n\t\"sub\": \"1234567890\",\n\t\"name\": \"John Doe\",\n\t\"admin\": true\n}\n```\n- when the server receives a JWT from an HTTP request's authorization header, like so:\n```\nAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhIjoxLCJiIjoyLCJjIjozfQ.hxhGCCCmGV9nT1slief1WgEsOsfdnlVizNrODxfh1M8\n```\nit will verify the token using the secret, and then will serialize the claims in that token to the database. Therefore, we will be able to access the data in `current_settings` (if using Postgres)\n\n- claims can be used as a means to differentiate users. Imagine we are building a postgres database and decide that all signed-in users will have the group role `user_login`. We can use the claims in the jwt to distinguish users\n- in postgres, a claim can be retrieved with `current_setting('request.jwt.claim.email', true)`\n\n*Registered claim* - recommended pre-defined claims\n\t- ex. **iss** (issuer), **exp** (expiration time), **sub** (subject), **aud** (audience)\n\t\n","n":0.069}}},{"i":1043,"$":{"0":{"v":"Cookies","n":1},"1":{"v":"\nWhen a browser sends a request to a server, all of the cookies are automatically sent along with that request.\n\nOne of the benefits of using cookies versus local storage is that the server receives all of these cookies on initial request. If we are storing a JWT in local storage, then the user will be shown the non-authenticated version of the page initially. The reason is that local storage uses JavaScript, which only gets loaded after the HTML and CSS have been already received. This is a perceivable delay, over just storing the JWT in an http-only cookie.\n\nIf you use an HTTP only cookie for authentication, the user's data will be available on the server for the initial render","n":0.092}}},{"i":1044,"$":{"0":{"v":"Cookies","n":1},"1":{"v":"\n### httpOnly Cookie\na cookie that is only sent in the http requests to the server.\n- Therefore, never accessible to the client-side javascript (making it immune to XSS attacks.)\n\nCookies and local storage can be accessed by anything client-side. This includes things like extensions. This makes storing sensitive information in a cookie a security risk. However, an HTTP-only cookie is different, in that its contents can only be set and read server side. This is why it is OK to store JWTs in an HTTP-only cookie.\nThe cookie is set via the response payload from the server\n\nThe clients only real job with an HTTP-only cookie is to store the cookie itself","n":0.096}}},{"i":1045,"$":{"0":{"v":"Ascii","n":1}}},{"i":1046,"$":{"0":{"v":"Emojis","n":1},"1":{"v":"\nಠ_ಠ)\n¯\\_(ツ)_/¯\n\n( •_•)\n( •_•)>⌐■-■\n(⌐■_■)\n\n(╯°□°)╯︵ ┻━┻\n┬─┬ ノ( ゜-゜ノ\n\nლ(ಠ益ಠლ)\n\n(∩ ` -´)⊃━━☆ﾟ.*･｡ﾟ\n\n(－‸ლ)","n":0.354}}},{"i":1047,"$":{"0":{"v":"Arduino","n":1},"1":{"v":"\nArduino abstracts the circuit board part of electronics away from us, by giving us a pre-made integrated circuit\n- Arduino allows us to perform logical computations\n    - ex. When I press *this*, *that* happens\n\n### Parts of Arduino\nHardware\n1. a Printed Circuit Board (PCB), with contains the Integrated Circuit\n2. Pin headers, so we can easily connect to other devices\n3. USB port to connect to a computer\n\nSoftware\n1. IDE","n":0.125}}},{"i":1048,"$":{"0":{"v":"Apollo","n":1},"1":{"v":"\ntesttesttest\n# Overview\nIdea is that all the lower-level networking tasks (HTTP requests etc) as well as storing the data (caching) should be abstracted away and the declaration of data dependencies should be the dominant part.\n- This is precisely what GraphQL client libraries like Relay or Apollo will enable you to do. They provide the abstraction that you need to be able to focus on the important parts of your application rather than having to deal with the repetitive implementation of infrastructure.\n\nApollo is completely separate from the view-layer, making it framework agnostic.\n\nApollo allows us to send queries from the view-layer\n\n### As state management\n[high-level overview](https://www.apollographql.com/blog/dispatch-this-using-apollo-client-3-as-a-state-management-solution/)\n\n### vs Relay\nRelay is more opinionated, Apollo is unopinionated\n\n### vs URQL\n[Open source alternative to Apollo](https://formidable.com/open-source/urql/)","n":0.093}}},{"i":1049,"$":{"0":{"v":"Links","n":1},"1":{"v":"\n# Apollo Link\n- a system of modular components for GraphQL networking\n\t- Each link represents a subset of functionality that can be composed with other links to create complex control flows of data.\n\t- In a few words, Apollo Links are chainable \"units\" that you can snap together to define how each GraphQL request is handled by your GraphQL client.\n- When you fire a GraphQL request, each Link's functionality is applied one after another, allowing you to control the request lifecycle in a way that makes sense for your application\n\t- ex. Links can provide retrying, polling, and batching\n- At a basic level, a link is a function that takes an operation and returns an observable\n\t- an operation is an object with information like: query, variables, context\n- links are required when creating an Apollo client instance.\n- the core of a link is the `request` method, which accepts the `operation` (ex. query, mutation) as an argument\n\t- this method is called every time execute is run on that link chain (typically each time an operation is passed through the link chain)\n\n## HTTP Link\n- most common Apollo Link\n- The http link is a terminating link that fetches GraphQL results from a GraphQL endpoint using http\n\t- This can be used for authentication, persisted queries, dynamic uris, and other granular updates.\n\n## State link\n- allows us to use graphql query operations on client state. \n\n## Auth link\n- allows us to add an authorization header to each request","n":0.065}}},{"i":1050,"$":{"0":{"v":"Hooks","n":1},"1":{"v":"\n# Query & Mutation\n- `useQuery` will execute on component mount, giving us back the loading, error and data state. Therefore, it is declarative \n- `useLazyQuery` will execute the query on command. This is perfect to use in events other than component mount, like on button click \n\t- To be clear, you could put this in `useEffect` and it would operate similarly to `useQuery`, since `useEffect` gets run on component mount\n- `useMutation` will return a function that we can execute to perform the mutation \n\n### Errors\nwe need to stringify errors:\n`console.log('error', JSON.stringify(err, null, 2))`\nspec: alternatively, we can use `apollo-link-error`\n\n","n":0.102}}},{"i":1051,"$":{"0":{"v":"Client","n":1}}},{"i":1052,"$":{"0":{"v":"Core","n":1},"1":{"v":"\nthe core part of apollo that is framework-agnostic. In other owrds it doesn't care about react and has no concept of hooks or anytihng like that","n":0.196}}},{"i":1053,"$":{"0":{"v":"Cache","n":1},"1":{"v":"\n# Cache\n- Apollo caches the result of every graphql query in a normalized cache\n- when a single resource is updated with a mutation, Apollo handles caching for us (as long as we return the `id` and fields that were updated). When we create or delete, or if we update many at once, we will have to manually take care of caching.\n\t- `useMutation` has an `update` function, whose purpose is to modify your cached data to match the modifications that a mutation makes to your back-end data\n- we can interact directly with the cache with `readQuery, writeQuery, readFragment, writeFragment`, accessible from `ApolloClient` class\n- we can get the cached version of an entity with `cache.identify`\n\t- an input `{ id: 1, title: '', mediaItems: [{...}] }` gives us `Nugget:1`\n\nreadQuery\n- like a regular graphql query, only it is performed on the cache, rather than the GraphQL API\n\nwriteQuery\n- we can use this function to update the apollo cache \n\nWe can search only data that is in the cache (ie no API call made) by using the `@client` decorator\n\n## Normalization\n- The `InMemoryCache` normalizes query response objects before it saves them to its internal data store.\n- normalization happens in the following steps:\n\t1. The cache generates a unique ID for every identifiable object included in the response. \n\t\t- ID will be in format: `<__typename>:<id>` (ex. `Bucket:232`)\n\t2. All the objects are stored by that generated ID in a flat lookup table.\n\t3. Whenever an incoming object is stored with the same ID as an existing object, the fields of those objects are merged.\n\t\t- this means that the only time anything is overwritten is when the field names are the same. If the incoming object has different fieldnames than the existing one, they will be preserved \n- The apollo cache is an object where each key is the ID (`typename:id`), and the value is an object of the corresponding record from the database:\n```\ndata: {\n\t'Bucket:1': {\n\t\tid: 1,\n\t\ttitle: 'Psychology',\n\t\t__typename: 'Bucket'\n\t}\n}\n```\n\n### Updating the cache\n- we can pass an `update` method to `useMutation`\n\t- this method allows us to interact with the cache as if we were interacting with a graphql API. \n\t\t- ex. we can make general queries, as well as use fragments to help\n- `cache.modify` is a method we can execute on our cache that lets us modify individual fields directly \n\t- differs from `writeQuery` and `writeFragment` in that it will circumvent any `merge` function we have defined, meaning that fields are always overwritten with exactly the values you specify.\n\n### Type Policies\n- By defining type policies, we can determine how the cache interacts with specific types in the schema\n\t- done by mapping a `__typename` to the whole `TypePolicy` object.\n\t- in other words, the `typePolicies` object has `key`-`values` of `__typename`-`TypePolicy Object`\n- each field in a `typePolicies` object is a type's `__typename`\n- we can customize how a particular field within our Apollo cache is written to and read. For this, we have 2 methods: `merge` and `read`.\n\t- with `read`, the cache calls that function whenever your client queries for the field. In the query response, the field is populated with the read function’s return value, instead of the field’s cached value.\n\t\t- Read is useful for manipulating values when they’re read from the cache, for example, things like formatting strings, dates, etc.\n\t- with `merge`, the cache calls that function whenever the field is about to be written with an incoming value. When the write occurs, the field’s new value is set to the merge function’s return value, instead of the original incoming value.\n\t\t- Merge can take incoming data and manipulate it before merging it with existing data. Suppose you want to merge arrays or non-normalized objects.\n\t- to define the policy for a single field, we need to first know which TypePolicy object the field corresponds to.\n- `FieldPolicy` lets us customize how individual fields in the Apollo Client cache are read and written.\n","n":0.04}}},{"i":1054,"$":{"0":{"v":"API","n":1},"1":{"v":"\nAn api is really an interesting thing, because it allows you to do the work once, and have its effect (result) be replicated ad infinitum. Consider the way the cursor works in WoW. The cursor points over something, and somehow it knows what it is hovering over. Well, For a mouse to able to locate something in that sense, it has to have the coordinates of the item (ex. a chest). If the coordinates that you are hovering over match the coordinates of the chest, then you know you are hovering over the chest. The function to determine this is complex, but it is at the end of the day reducible to a function. We can still walk away with an atomic piece that can be transported anywhere. So imagine we take the functionality of determining where the coorinate of the cursor is and wrap it up into a function. Now all we need to do is run that function every time the mouse moves. We have just created an API that easily replicable across the entire application. \n\n# UE Resources\n[difference between API and SDK](https://nordicapis.com/what-is-the-difference-between-an-api-and-an-sdk/#:~:text=By%20definition%2C%20an%20SDK%20is,to%20allow%20communication%20between%20applications.)\n","n":0.074}}},{"i":1055,"$":{"0":{"v":"Apache","n":1},"1":{"v":"\nApache is an HTTP server (just like NGINX)\n\n# How it works\n- Apache listens to the IP addresses identified in its config file (HTTPd.conf). Whenever it receives a request, it analyzes the headers, and takes action; considering the rules specified for it in the Config file,\n\n## Four main directories on apache server\n- `htdocs` contains the files to be served to the client upon receiving HTTP requests\n\t- files and sub-directories under htdocs are available to the public \n- `conf` contains all server configuration files \n\t- similar to Dockerfile, it is a manifesto of instructions for the apache server\n- `logs` contains server logs\n- `cgi-bin` contains CGI scripts\n\n# E Resources\n[primer on apache](https://code.tutsplus.com/tutorials/an-introduction-to-apache--net-25786)","n":0.096}}},{"i":1056,"$":{"0":{"v":"Reverse Proxy","n":0.707},"1":{"v":"\nWe need to load the proxy and proxy_http modules in Apache, and the most basic configuration would look like this (for one domain):\n\n```\n<VirtualHost *:80>\n    ServerName foo.example.com\n    ProxyPass / http://192.168.2.x\n</VirtualHost>\n```","n":0.186}}},{"i":1057,"$":{"0":{"v":"Android","n":1},"1":{"v":"\n### Hermes\n- a engine for optimizing RN apps on Android","n":0.316}}},{"i":1058,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n# Android\n- open developer menu - `cmd+m`\n","n":0.378}}},{"i":1059,"$":{"0":{"v":"Adb","n":1},"1":{"v":"\n# Adb reverse and port forwarding\n`adb -s 192.168.76.101:5555 reverse tcp:8081 tcp:8081`\nIn either case, it's basically just port forwarding. It's probably called \"reverse\" because it's actually setting up a \"reverse proxy\" in which a http server running on your phone accepts connections on a port and wires them to your computer... Or vice versa. The port is specified twice because you have the ability to control the listening port on both sides of the proxy.\n\nWhen the RN packager is running, there is an active web server accessible in your browser at 127.0.0.1:8081. It's from this server that the JS bundle for your application is served and refreshed as you make changes. Without the reverse proxy, your phone wouldn't be able to connect to that address.\n\nNow when your phone tries to access http://localhost:3000/ your request will be routed to localhost:3000 of your laptop. Just recompile your app to use localhost:3000 as the API endpoint.\n\nif we had written `adb reverse tcp:80 tcp:3000`, it would redirect your phone’s port 80 to your computer’s port 3000","n":0.076}}},{"i":1060,"$":{"0":{"v":"Raspberry Pi","n":0.707},"1":{"v":"\n[Setup headless RPi (inc. WiFi)](https://brandonb.ca/raspberry-pi-zero-w-headless-setup-on-macos)\n[Build your own NAS w/ RPi](https://pimylifeup.com/raspberry-pi-nas/)","n":0.316}}},{"i":1061,"$":{"0":{"v":"OS","n":1}}},{"i":1062,"$":{"0":{"v":"scheduling","n":1},"1":{"v":"\nscheduling is the action of assigning resources to perform tasks\n- The resources may be processors, network links or expansion cards. \n- The tasks may be threads, processes or data flows.\n\n### Cooperative multitasking (non-preemptive multitasking)\n- In this scheduling paradigm, the OS has it's role as a scheduler reduced. Instead of the scheduler determining which process' turn it is to get run, the processes themselves voluntarily perform this duty. That is, each process knows when its turn is over, and cedes processing power to the next process in line.\n- In this scheme, the process scheduler of an operating system is known as a cooperative scheduler, having its role reduced down to starting the processes and letting them return control back to it voluntarily\n\nThis paradigm is widely used in memory-constrained embedded systems","n":0.088}}},{"i":1063,"$":{"0":{"v":"MVC","n":1}}},{"i":1064,"$":{"0":{"v":"CMS","n":1},"1":{"v":"\nTraditionally, CMSs managed all content on a website, including \n- content, \n- images, \n- HTML, \n- CSS\n\nThis made it impossible to reuse the content because it was commingled with code.\n","n":0.183}}},{"i":1065,"$":{"0":{"v":"Headless CMS","n":0.707},"1":{"v":"\n- a Headless CMS can be thought of an an API-first CMs\n\nany type of back-end content management system where the content repository “body” is separated or decoupled from the presentation layer “head\". \n- if the presentation layer of a website is the “head” of a CMS, then cutting off that presentation layer creates a headless CMS.\n\nThe idea is *not* that we have no head, but that we get to *choose* our head. In traditional CMSs, there is an integrated head, meaning the presentation layer is coupled to the data-handling layer. Suppose we already have a React app set up, so the visual layer is already taken care of. Now, all we need is the editing interface to alter our content, and the database to store it. A Headless CMS provides these for us.\n\nAll Headless CMSs pretty much boil down to some database backend with a web-based user interface, and content made accessible through an API.\n","n":0.08}}},{"i":1066,"$":{"0":{"v":"CGI","n":1},"1":{"v":"\n# Common Gateway Interface (CGI)\n- a specification for web servers (like Nginx, Apache) to execute command line programs that run on a server and generate web pages dynamically.\n\t- Such programs are known as CGI scripts or simply as CGIs\n\t- Normally, a CGI script executes at the time a request is made and generates HTML.\n- ex. an HTTP GET or POST request from the client may send HTML form data to the CGI program (via STDIN). Simultaneously, the CGI program will receive other data like URL path and HTTP headers as a list of environment variables. \n\nPurpose\n- The HTTP server (ex. Express) will have a directory that is designated as a document collection (ie. each file in the directory is a document).\n\t- These files can be sent to users accessing the website. \n- ex. if our website was www.example.com and we had a document collection stored at `/usr/local/apache/htdocs` of the local filesystem, the Web server will respond to a request for `http://example.com/index.html` by sending to the browser the (pre-written) file `/usr/local/apache/htdocs/index.html`\n\n## FastCGI\n- this is an alternative approach and variation of CGI.\n- it is a protocol for interfacing interactive programs with a web server\n- purpose is to reduce overhead by allowing a single, long-running process to handle more than one user request\n\t- result is that the server can handle more web page requests per unit of time.\n- FastCGI applications remain independent of the web server.\n- FastCGI can be implemented in any language that supports network sockets\n\t- ex. Node, PHP, Python\n- both Nginx and Apache implement FastCGI","n":0.063}}}]}
