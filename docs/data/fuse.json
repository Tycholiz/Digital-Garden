{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Digital Garden","n":0.707},"1":{"v":"This Dendron vault of tech knowledge is organized according to domains and their sub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git would include things like `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner`, and so on.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the \"aha\" moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nAs a result, I have allowed myself to use potentially arcane concepts to explain other ones. For example, in my note on [[unit testing|testing.method.unit]], I have made reference to the [[microservices|general.arch.microservice]] note. If these notes were made with the public in mind, this would be a very bad strategy, given that you'd have to understand microservices to be able to draw that same parallel that I've already drawn. Since these notes are written for myself, I have been fine with taking these liberties.\n\nWhat I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course this is all\nvague sounding, and really depends on the domain along with the ends I am\ntrying to reach.\n\nTo achieve this, the system should be steadfast to:\n- be able to put information in relatively easily, without too much thought\n\trequired to its location. While location is important, Dendron makes it easy\n\tto relocate notes, if it becomes apparent that a different place makes more\n\tsense.\n- be able to extract the information that is needed, meaning there is a\n\thigh-degree in confidence in the location of the information. The idea is\n\tthat information loses a large amount of its value when it is unfindable.\n\tTherefore, a relatively strict ideology should be used when determining\n\twhere a piece of information belongs.\n\t- Some concepts might realistically belong to multiple domains. For instance, the concept of *access modifiers* can be found in both `C#` and `Typescript`. Therefore, this note should be abstracted to a common place, such as [[OOP|paradigm.oop]].\n\nThis Dendron vault is the sister component to the [General Second Brain](https://tech.kyletycholiz.com).\n\n### Tags\nThroughout the garden, I have made use of tags, which give semantic meaning to the pieces of information.\n\n- `ex.` - Denotes an *example* of the preceding piece of information\n- `spec:` - Specifies that the preceding information has some degree of *speculation* to it, and may not be 100% factual. Ideally this gets clarified over time as my understanding develops.\n- `anal:` - Denotes an *analogy* of the preceding information. Often I will attempt to link concepts to others that I have previously learned.\n- `mn:` - Denotes a *mnemonic*\n- `expl:` - Denotes an *explanation*\n\n## Resources\n### UE (Unexamined) Resources\nOften, I come across sources of information that I believe to be high-quality. They may be recommendations or found in some other way. No matter their origin, I may be in a position where I don't have the time to fully examine them (and properly extract notes), or I may not require the information at that moment in time. In cases like these, I will add reference to a section of the note called **UE Resources**. The idea is that in the future when I am ready to examine them, I have a list of resources that I can start with. This is an alternative strategy to compiling browser bookmarks, which I've found can quickly become untenable.\n\n### E (Examined) Resources\nOnce a resource has been thoroughly examined and has been mined for notes, it will be moved from *UE Resources* to *E Resources*. This is to indicate that (in my own estimation), there is nothing more to be gained from the resource that is not already in the note.\n\n### Resources\nThis heading is for inexhaustible resources. \n- A prime example would be a quality website that continually posts articles.  - Another example would be a tool, such as software that measures frequencies in a room to help acoustically treat it.\n","n":0.037}}},{"i":2,"$":{"0":{"v":"Patterns","n":1}}},{"i":3,"$":{"0":{"v":"Structural","n":1}}},{"i":4,"$":{"0":{"v":"Facade","n":1},"1":{"v":"\nA facade is a wrapper that provides a simpler interface to a more complex one.\n- it is about encapsulating a complex subsystem within a single interface object. \n- an added benefit is it also promotes decoupling the subsystem from its potentially many clients\n- ex. AWS offers [[lib packages|aws.SDK#lib-packages,1:#*]] alongside their lower level packages, as a way to simplify the commands and make it easier to use.\n- using a facade notably does not prevent users from bypassing the facade and engaging directly with the more complex layers, but gives them the option to interact with a simplified interface.\n\nA facade differs from an [[adapter|general.patterns.structural.adapter]], since a facade creates a new interface, while an adapter re-uses the old interface.\n\n![](/assets/images/2022-04-11-12-33-11.png)","n":0.093}}},{"i":5,"$":{"0":{"v":"Music","n":1}}},{"i":6,"$":{"0":{"v":"Instr","n":1}}},{"i":7,"$":{"0":{"v":"Elec Guitar","n":0.707},"1":{"v":"\n## Knobs\nIf playing lots of harmonics, you'll want a lot of treble in the tone.\n\nIf playing fast note runs with a relatively clean tone, too much treble exposes the weakness in playing, and rolling it off a bit can help make the performance sound better.\n### Overdrive\nThere are 2 distinct types of overdrive: \n- the kind generated by the pre-amp (where gain and tone controls live), and is characterized by a fuzzy sound quality. \n- the kind generated by overdriving the power amp (where the master volume control lives). This type of overdrive sounds more musical, with the guitar still feeling responsive with all the sustain you need, but with a smooth timbre and none of the buzzy abrasiveness of pre-amp distortion.\n  - valve amps are prized for their power amp distortion, which are responsible for most of the classic rock tones\n\nTo make solos sound a little more clearly, boost the mids.\n\n“Volume knob” is a bit of a misnomer\n- If your amp is on a clean setting then the volume knob does pretty much what it says. However, in a more overdriven context, volume also increases the fatness of what’s being played.\n- however it does also apply in a clean context. Imagine we’re trying to get a nice clean tone, so we turn off the overdrive and use the neck pickup, but now the guitar sounds overly boomy. If you back off the volume a bit you’ll notice that it thins out the tone. ","n":0.064}}},{"i":8,"$":{"0":{"v":"React","n":1}}},{"i":9,"$":{"0":{"v":"React Typescript","n":0.707},"1":{"v":"\n# Resources\n[React+Typescript Cheatsheet](https://github.com/typescript-cheatsheets/react)\n","n":0.577}}},{"i":10,"$":{"0":{"v":"Stat","n":1},"1":{"v":"\nSetting state only changes it for the *next* [[render|react.architecture#rendering]]\n- ex. if we have a button that sets the state multiple times, using the existing state as a dependency, we would get unexpected behaviour, and the `number` state would only increment once per click. If the initial state of `number` is `0`, then that is the value that is used each time `setNumber` is called:\n```jsx\n<button onClick={() => {\n    setNumber(number + 1);\n    setNumber(number + 1);\n    setNumber(number + 1);\n}}>\n    +3\n</button>\n```\n\nPhrased another way, on any given render, the state (and props) are *always* the same.\n\nIf you would like to update the same state variable multiple times before the next render, instead of passing the next state value like `setNumber(number + 1)`, you can pass a function that calculates the next state based on the previous one in the queue, like `setNumber(n => n + 1)`\n- this is a way to tell React to “do something with the state value” instead of just replacing it.\n- this is called an **updater function**, and when you pass it to a state setter:\n    1. React queues this function to be processed after all the other code in the event handler has run.\n    2. During the next render, React goes through the queue and gives you the final updated state.\n- technically, `setNumber(6)` is identical to `setNumber(x => 6)`\n\nWhen something can be calculated from the existing props or state, don’t put it in state. Instead, calculate it during rendering.\n\n### Avoiding State Paradoxes\nex. `isTyping` and `isSubmitting` can’t both be true, so instead of having these as distinct states, replace with with a `status` state that can be `typing`, `submitting`, or `success`.\n\n### Batching\nAs of React 18, React will wait until all code in the event handlers have run before processing your state updates.\n- therefore, re-rendering doesn't occur until the event handler has finished execution\n- anal: a waiter doesn't return to the kitchen with an order after each guest gives their order— Instead, they let you finish your order, let you make changes to it, and even take orders from other people at the table, before notifying the kitchen.\n\nThe problem is that when we update multiple pieces of state in sequence, React will re-render the component. We need a way to tell React \"hey, don't bother re-rendering until I've completed my whole sequence of state updates.\"\n\nWhen we implement a click handler, we are implementing the same benefits that batching gives us. Only a single re-render would be triggered from the following click handler:\n```js\nfunction handleClick() {\n  setIsFetching(false);\n  setError(null);\n  setFormStatus('success')\n}\n```\n\nHowever, what if we wanted to change the 3 pieces of state *not* in response to an event handler? Imagine we want the state to be updated when a fetch resolves?\n\n### Forcing a Re-render\nImagine we had a chat application with 2 components: `<ContactList />` and `<Chat />`. The `<Chat />` component takes `contact` as a prop, which is an object with the shape: `{ name: 'Taylor', email: 'taylor@mail.com' }`.\n- by default, the `<Chat />` component won't re-render, even if the object changes. We can however, force the component to re-render by passing it the `key` prop like this: `<Chat key={to.email} contact={to} />`. That way, when the `to.email` value changes, the component will be re-rendered.\n- [original example](https://react.dev/learn/managing-state#preserving-and-resetting-state)","n":0.043}}},{"i":11,"$":{"0":{"v":"Patterns","n":1}}},{"i":12,"$":{"0":{"v":"Composition","n":1},"1":{"v":"\nInstead of this:\n```js\nfunction App() {\n  const [someState, setSomeState] = React.useState('some state')\n  return (\n    <>\n      <Header someState={someState} onStateChange={setSomeState} />\n      <LeftNav someState={someState} onStateChange={setSomeState} />\n      <MainContent someState={someState} onStateChange={setSomeState} />\n    </>\n  )\n}\n```\n\nWe can do this:\n```js\nfunction App() {\n  const [someState, setSomeState] = React.useState('some state')\n  return (\n    <>\n      <Header\n        logo={<Logo someState={someState} />}\n        settings={<Settings onStateChange={setSomeState} />}\n      />\n      <LeftNav>\n        <SomeLink someState={someState} />\n        <SomeOtherLink someState={someState} />\n        <Etc someState={someState} />\n      </LeftNav>\n      <MainContent>\n        <SomeSensibleComponent someState={someState} />\n        <AndSoOn someState={someState} />\n      </MainContent>\n    </>\n  )\n}\n```\n","n":0.119}}},{"i":13,"$":{"0":{"v":"Libraries","n":1},"1":{"v":"\n## State management\n- React Query (server state)\n- Zustand (UI/FE state)\n- Tanstack\n\n## Storage\n- MMKV (persistence)","n":0.267}}},{"i":14,"$":{"0":{"v":"Lang","n":1}}},{"i":15,"$":{"0":{"v":"Suspense","n":1},"1":{"v":"\nSuspense refers to React’s ability to “suspend” rendering while components are waiting for something, and display a loading indicator\n\nImagine a component that needs to do some asynchronous task before it can render, e.g. fetching some data.\nBefore Suspense, such a component would keep an isLoading state and return some kind fallback (an empty state, skeleton, spinner, ...) based on it.\nWith Suspense, a component can now, during rendering, shout \"HALT! I'm not ready yet. Don't continue rendering me, please - here's a pager, I'll ping you when I'm ready!1\"\nReact will walk up the tree to find the nearest Suspense boundary, which is set by You and contains the fallback that'll now be rendered until the pager is ringing.\n\nIn the traditional way of handling data fetching, the paradigm is: \"wait for Promise to resolve and run some user code\". With suspense, it's \"React will retry rendering when it resolves\". \n- In some ways it's simpler but it takes some \"unlearning\" because the user loses some control they might be used to from traditional Promise-base programming.\n\nsolves the problem of having some of the data, but not being able to render until all data is in. With suspense, we can load some of the UI without all of the data having to be available.\n- Suspense lets you pause rendering a component while it’s loading async data\n\nReact Suspense will specify a fallback component in case the main component suspends. \n\n<!-- TODO: broken image -->\n<!-- ![28a01e0b811e84e2c3c20a4faca82641.png](:/3b101f50bf264119a5a3d57e387812f4) -->\n\n## Process for data fetching and rendering in browser\n1. Browser downloads the code\n2. React begins to render the `<Home />` component\n3. React will attempt to use the data needed for the component, then will realize that the data doesn't exist yet. React will then look up the component tree, find its nearest Suspense boundary. Then it will then render the fallback component, and fetch the data simultaneously.\n4. Once data has been fetched (signified by a function like `useQuery` having been completed), React will render `<Home />` (which contains an `<img />` tag)\n5. The browser sees the `<img />` tag and makes a fetch to get that image.\n* * *\n- If we have multiple components within a `<Suspense />`, then we have to wait\n    for all components inside to be ready before anything renders. Conversely if\n    each component is nested within its own `<Suspense />`, then each can render\n    when it is done\n```jsx\n<Suspense fallback={<DotLoader />}>\n    <NewsFeed />\n</Suspense>\n```\n\n- If we want to control the order in which something will render, we can use a\n    `<SuspenseList>` component. This component takes a prop `revealOrder`.\n    - if we set `revealOrder=\"forwards\"`, then we ensure that the first\n        component will always render first. \n    - We might want to do this if component1 and component2 are stacked on top\n        of each other, and component1 has dynamic dimensions (content determines\n        the height). If component2 were to load first, then once component1\n        loads, it would push everything down.\n        - ex. Facebook's `<Composer>` and `<NewsFeed>`\n- \"In the long term, we intend Suspense to become the primary way to read asynchronous data from components — no matter where that data is coming from.\"\n    - from React team\n  \n","n":0.044}}},{"i":16,"$":{"0":{"v":"Render Prop","n":0.707},"1":{"v":"\nA render prop is a prop of the signature: `(data) => JSX` (ie. a function that returns JSX)\n\nRender props allow us to communicate with any rendered component without having to couple implementation details.\n- we do this by passing data to the render prop as arguments\n\nWith render props, we enable the child component to communicate with its parent before actually rendering\n\nA component with a render prop takes a function that returns a React element and calls it instead of implementing its own render logic.\n\n```tsx\n<DataProvider render={data => (\n  <h1>Hello {data.target}</h1>\n)}/>\n```\n\n## Enhancement\nThe traditional way of enhancing components is to have a component with the enhancement logic, and have it wrap another component, thereby giving that child component access to the enhancements. Render props flips this model on its head, and instead we have the component that needs access to the enhancements render the component providing those enhancements:\n\nFor example, a common implementation is to provide a component with the ability to be dismissed. A naive implementation would be to have a `Dismiss` component which adds some dismissal logic, then renders the children it was passed:\n\n```tsx\nconst Dismiss = ({ children }) => {\n  const dismiss = () => {\n    // ...code to implement dismissal animations etc\n  }\n\n  return (\n    <div>\n      {children}\n      <button onClick={dismiss}>dismiss</button>\n    </div>\n  )\n}\n\nconst Notification = () => {\n  return (\n    <Dismiss>\n      { /* ...code */ }\n```\n\nThe problem with the above implementation is that:\n- the `dismiss` function can only be called from the `Dismiss` components\n- we can't pass data to `children`\n\n```tsx\nconst Dismiss = (props) => {\n  const dismiss = () => {\n    // dismissal animations etc.\n  }\n\n  return props.render(dismiss)\n}\n\nconst Notification = () => {\n  return (\n    <Dismiss render={\n      dismiss => <Content dismiss={dismiss} />\n    } />\n  )\n}\n```\n\nWith this render prop implementation, we are no longer hard-coding the rendered JSX in the `Dismiss` component. Instead, now it is accepting the JSX to be rendered via props, and can render by simply calling `props.render()`.\n\nThis implementation gives us flexibility to pass data to the render prop function, meaning `Dismiss` can communicate with the component being rendered, and vice versa.\n\n### Implementing Higher-Order Components\nrender props also allow us to implement HOCs with minimal effort.\n\n```jsx\nconst withDismiss = (Component) =>\n  (props) => (\n    <Dismiss>\n      {(dismiss) => (\n        <Component dismiss={dismiss} {...props} />\n      )}\n    </Dismiss>\n  )\n```\n\n## E Resources\n- https://blog.logrocket.com/react-reference-guide-render-props/","n":0.052}}},{"i":17,"$":{"0":{"v":"Ref","n":1},"1":{"v":"\n## What is it?\nA ref is a mutable value that is shared by all re-renders of a single component.\n- Think of a ref as a piece of state that doesn't abide by the same re-rendering rules that apply to normal state.\n    - normally this state controls imperative DOM elements, such as if an `<input>` is focused or not, or the current width of a resizable window.\n- a ref can be a function or an object\n\nRefs are for when you want a component to \"remember\" some information, but you don’t want that information to trigger new renders.\n- The `.current` value is like a secret pocket of your component that React doesn’t track. Therefore, it is an \"escape hatch\" from React's one-way data flow.\n\nThe `useRef()` hook creates a mutable variable (ie. `ref.current`) inside a functional component that won’t update on every render.\n\nWhen we attach a ref to a `<div>` element, we are creating a reference to that particular `<div>` element.\n- under the hood, React automatically assigns the `current` property of the ref to the corresponding DOM node when the component renders.\n- once we have that reference to the JSX element, we can access properties and methods of the DOM node through the ref.\n    - ex. if we attach a ref to a `<div>`, we can access the `innerText` property\n\nWith refs, we can also create a reference to another React component. To do this, the component to be referenced needs to use `forwardRef`, which allows us to forward the ref prop from a child component to a DOM element or a class or functional component.\n\n## Why use it?\nA ref should be used when:\n1. the \"React-way\" of interacting with children (ie. Passing props) doesn't cut it \n2. we want to manually access DOM nodes or React elements created in the render method\n\nTraditionally, refs were meant to let you directly access an html element.\n- ex. focus an input field\n- use refs To manage focus, text selection, or media playback\n\nWhen a piece of information is only needed by event handlers and changing it doesn’t require a re-render, using a ref may be more efficient.\n\n### Use-cases\n- Focus Management: Setting focus on an input field or a button programmatically, for example, in response to a specific user action.\n    - ex. imagine we have a Parent component which renders 2 `<input>`. We want to enact the business logic that *upon filling out the first `<input>` and hitting enter, the second `<input>` should automatically be focused*.\n- Integration with Non-React Libraries: When you need to incorporate third-party libraries that require direct DOM access or manipulation, refs can be useful.\n- Measurements: If you need to measure elements, like their width, height, or position, refs can be necessary.\n- Animations: While there are React libraries for animations, sometimes you might need to use refs for more granular control.\n\n### When not to use Refs\n- Overusing for State Management: component state should be managed within the confines of React's prop and state system.\n    - ex. using a ref to capture and validate an input value instead of utilizing the component's state and controlled components might be seen as not idiomatic.\n- Bypassing React's Reconciliation: If refs are used to make direct DOM manipulations, like changing a node's text or toggling a class, instead of relying on React's re-render mechanism, it's probably a misuse. React works best when it's allowed to handle the DOM based on its state.\n- Mixing Logic: Mixing up business logic with direct DOM operations through refs can make the component harder to reason about, test, and maintain.\n- Overhead: Using too many refs can make a component overly complex, harder to read, and harder to maintain. If every other element has a ref attached, it's a red flag that the code might not be optimally structured.\n- Not Utilizing Available Abstractions: Sometimes developers use refs because they aren't aware of the higher-level abstractions available. \n    - ex. they might use a ref to set focus on an input field, unaware that some UI libraries provide components with built-in focus management.\n\n## How does it work?\nImagine we had 2 counters: one made with state, and the other with refs. As expected, when we increment the state count, the component understands that it needs to re-render, so it does so and reads in the new value of count. When we increment the ref count however, the component understands that it should not update, so we end up still seeing the old value, even though underneath the hood, the count has still risen. Since nothing has caused the component to re-render, this value will remain static. Now imagine that we increment the state count. React will recognize that an updated state means that it needs to re-render the component. Upon doing so, the current value of the ref will be read.\n\n- The variables created with `useRef()` persist in between component renders (like state).\n- The nature of refs is that we can change the value of the ref, and it will not cause the component to re-render.\n- Refs have a value called `.current()`, which gives us the value of the ref in the current rendering of the component.\n    - this value is intentionally mutable\n- Most elements inside of your document have a ref attribute, which facilitates the use of useRef to reference elements inside of your HTML.\n- refs created using createRef are not persisted between re-renders. A new ref is always created whenever the component is re-rendered.\n- Updating a ref value is considered a side effect\n\nThis is the reason why you want to update your ref value in event handlers and effects and not during rendering\n\n- ex. Imagine we have a list of users. When we click one, it logs out the username after 1 second. Now imagine that we click on a user, then click on another before the `console.log` occurs.\nIn this scenario, class components would log out user2, while functional components would log out user1.\n- This shows that functional components attach the call made to the user selected at the time. If we want to make a functional component that behaves like a class, we can use a ref, which is a value that is shared between all renders (in this sense it's sort of like an instance variable) <-- (in retrospect I think this should read \"static variable\", since it exists across all renders, similar to how static variables exist across all instances of that class; however this information came from React docs)\n\n### Forward Ref\n- What if we want to take a **ref** that is currently existing in the component and pass it down the tree?\n\t- ie. from `<Parent />`, we can reference a component rendered by its `<Child />`\n\t- ex. we have `<App />` and `<Form />`. We have a `ref` defined in `App` and want to attach it to an input box inside `Form`. We wrap `Form` with `forwardRef`, enabling the component to receive the `ref` from `App`. in `Form`, we pass the ref to the input. Now, we can control that component from `App`.\n- the component that has `forwardRef` in it is the component that is receiving the ref (ie. the child)\n- `forwardRef()` will create a component, accepting 2 arguments: `props` and `ref` (a regular component just accepts props)\n\n#### Example\nImagine we have a CheckoutPage component which renders a PaymentForm. The CheckoutPage handles all logic related to network calls in the app, and PaymentForm handles all form logic, including managing its own state. This is a reasonable separation of concerns. However, what happens if we want to access a piece of state in the parent from the child? Imagine the network call our parent made needed to get the address from the form. The \"React Way\" of accomplishing this is to bring the form state up one level to live in the parent component. However, this breaks our separation of concerns pattern. We can use an escape hatch forward ref in this circumstance.\n\nIllustration:\n```js\nconst Parent = () => {\n    const myRef = useRef();\n    return <Child ref={myRef} />;\n}\n\nconst Child = React.forwardRef((props, ref) => {\n    const [myState, setMyState] = useState('This is my state!');\n    useImperativeHandle(ref, () => ({getMyState: () => {return myState}}), [myState]);\n})\n```\nThen you should be able to get myState in the Parent component by calling: `myRef.current.getMyState()`\n\n#### Combined ref\n- for low-level UI development, it's common to want to use both a local ref and an external one (`forwardRef`).\n\n\n### `useImperativeHandle`\nThis hooks lets us customize the ref (exposed via `forwardRef`) that is exposed to the parent who passed it.\n- in other words, when a parent component renders a child component that accepts a ref, the child component can then choose to customize the values/methods that should be exposed to the parent.\n\n#### Use case #1: limiting methods exposed to parent\n- ex. imagine we had a `<Parent />` and a `<Child />`. The `<Child />` renders an `<input />`. From the `<Parent />`, we define a ref and pass it to the `<Child />`, because we want to imperatively control some of the `<input />` methods from the `<Parent />`. However, we don't need access to all the methods, so instead of attaching that forwarded ref directly to the `<input />`, we use it to create an imperative handle (`useImperativeHandle`). Then, we create another ref (`inputRef`) in the `<Child />` which gets attached to the `<input />`, and we use that `inputRef` in the `useImperativeHandle`. The result is that two methods (`focus` and `scrollIntoView`) are exposed to the parent, instead of the whole reference to the `<input />`.\n```jsx\nconst MyInput = forwardRef(function MyInput(props, ref) {\n  const inputRef = useRef(null);\n\n  useImperativeHandle(ref, () => {\n    return {\n      focus() {\n        inputRef.current.focus();\n      },\n      scrollIntoView() {\n        inputRef.current.scrollIntoView();\n      },\n    };\n  }, []);\n\n  return <input {...props} ref={inputRef} />;\n});\n```\n\n#### Use case #2: Combining the methods of multiple refs into one function call\n- ex. Imagine we had a blog with a button which when clicked, would scroll the page down to the bottom of the comments list then highlight the `<input />`. From our top-level component, we want to be able to just call `postRef.current.scrollAndFocusAddComment()`. From the child component, what we would do is define the `useImperativeHandle` which exposes that method, and define 2 more refs that get passed down to its children:\n```jsx\nconst Post = forwardRef((props, ref) => {\n  const commentsRef = useRef(null);\n  const addCommentRef = useRef(null);\n\n  useImperativeHandle(ref, () => {\n    return {\n      scrollAndFocusAddComment() {\n        commentsRef.current.scrollToBottom();\n        addCommentRef.current.focus();\n      }\n    };\n  }, []);\n\n   return (\n    <>\n      <CommentList ref={commentsRef} />\n      <AddComment ref={addCommentRef} />\n    </>\n  );\n```\n\n\n### UER\n[Dan Abramov on refs](https://overreacted.io/making-setinterval-declarative-with-react-hooks/)\n","n":0.024}}},{"i":18,"$":{"0":{"v":"Memo","n":1},"1":{"v":"\na HOC that memoizes the rendered output of a component\n- `Memo` accepts a second arg that lets us determine whether or not the component should re-render (defaults to `true`, meaning it won't re-render)\n- When deciding to update DOM, React first renders your component, then compares the result with the previous render result. If the render results are different, React updates the DOM.\n\t- To be clear, React will always render, but won't always update DOM\n- When we wrap our component with `Memo`, the first render will be memoized. Every subsequent render will involve the usual check of oldProps/oldState vs nextProps/nextState, but instead of rendering the component *then* comparing the output to the DOM, it will just take the memoized output. The logic is: since props or state didn't change, we shouldn't need to re-render it (which may well be the case for some components; in which circumstance we should certainly wrap the component with `Memo`\n- Components using hooks can be freely wrapped in React.memo() to achieve memoization\n- use mostly when component trees get too wide or too deep\n\n## When to use\n1. the component is purely functional (if class, use PureComponent and modify SCU), and given same props, will always return the same component\n2. the component renders often\n\t- often what causes this is a Parent with state/props that change often, and a Child who would receive the same props for a longer period of time. In this case, memoize the Child\n\t\t- ex. we have a `<MoviePage />` component and inside it a `<MovieInfo title={props.title} />`. MoviePage holds the state to determine how many thumbs up a movie has, and it pings the server every second. Therefore, when the number changes, the component re-renders, and thereby forces MovieInfo to render. The issue here is that `props.title` is not going to change each time, so React is needlessly rendering the component each time. \n3. the component often receives the same props in between renders\n4. the component contains a decent amount of components that are subject to conditional rendering (and depend on a prop)\n","n":0.054}}},{"i":19,"$":{"0":{"v":"JSX","n":1},"1":{"v":"\nattributes written in JSX become keys of JavaScript objects\n\nWhy do multiple JSX tags need to be wrapped? \n- Because JSX is transformed into plain JavaScript objects. You can’t return two objects from a function without wrapping them into an array. This explains why you also can’t return two JSX tags without wrapping them into another tag or a Fragment.","n":0.13}}},{"i":20,"$":{"0":{"v":"Hooks","n":1},"1":{"v":"\nThe value of using Hooks is being able to decouple React-y logic things, like state management and side effects, from the actual component that it is actually existing in\n\nHooks should be [[idempotent|general.terms.idempotent]].\n\nHooks are functions, but they aren't pure functions.\n- this is why testing hooks is a little more involved— we can't just call the function and assert its output.\n\nEach incovation of a hook is indepdendent. Therefore, if the hook manages state (e.g. via a `useState` or `useContext` hook), that state will be independent on each invocation of the hook.\n- Therefore, if we have a hook `useStagingArea` that manages state with `useState` and use it in 2 different components, the state of those two hooks will be independent\n  - if we wish to share the state, then we must use something like a [[context|react.lang.context]].\n\n## Testing\nIf we can, it's generally not a bad idea to test hooks by simply testing the component that they are used in.\n- naturally, if the component tests pass, then the hook must be working properly.\n\nAlternatively, we can use a setup helper that renders a dummy component (since a rule of hooks is that they must be called from within a component):\n```js\nfunction setup(...args) {\n  const returnVal = {}\n  function TestComponent() {\n    Object.assign(returnVal, useUndo(...args))\n    return null\n  }\n  render(<TestComponent />)\n  return returnVal\n}\n\ntest('should ...', () => {\n  const undoData = setup('one')\n\n  // assert initial state\n  expect(undoData.loading).toBe(false)\n\n  // add second value (set is a function returned from the hook)\n  act(() => {\n    undoData.set('two')\n  })\n\n  // assert next state\n  expect(undoData.loading).toBe(true)\n```\n\nEven more alternatively, we can use `@testing-library/react-hooks`, which gives us:\n- similar benefit to the `setup` function above\n- Utility to \"rerender\" the component that's rendering the hook (to test effect dependency changes for example)\n- Utility to \"unmount\" the component that's rendering the hook (to test effect cleanup functions for example)\n- Several async utilities to wait an unspecified amount of time (to test async logic)\n\n## Resources\n- [Rehooks: premade react hooks](https://github.com/rehooks)","n":0.056}}},{"i":21,"$":{"0":{"v":"useReducer","n":1},"1":{"v":"\nPrefer `useReducer` over `useState` when either:\n1. The state is more complex than a scalar or simple object.\n2. the next state depends on the previous state\n3. you are storing an array in state and the user can edit each item in the array.\n4. you have components with many state updates spread across many event handlers\n  - this gets simplified because we can consolidate all the state update logic outside your component in a single function, called \"reducer\". The event handlers become concise because they only specify the user \"actions\"\n\n`useReducer` also lets you optimize performance for components that trigger deep updates because you can pass dispatch down instead of callbacks.\n\n### Throw and error on `default`\nThere is little point in the default to a switch statement to simply return `state`:\n```js\ndefault:\n  return state;\n```\n\nThis is like allowing a silent error to pass through, as actions should be explicit. We want to be notified of any non-existent actions and typos immediately, so we throw an error.\n\n### Example: useUndo\n```js\nimport * as React from 'react'\n\nconst UNDO = 'UNDO'\nconst REDO = 'REDO'\nconst SET = 'SET'\nconst RESET = 'RESET'\n\nfunction undoReducer(state, action) {\n  const {past, present, future} = state\n  const {type, newPresent} = action\n\n  switch (action.type) {\n    case UNDO: {\n      if (past.length === 0) return state\n\n      const previous = past[past.length - 1]\n      const newPast = past.slice(0, past.length - 1)\n\n      return {\n        past: newPast,\n        present: previous,\n        future: [present, ...future],\n      }\n    }\n\n    case REDO: {\n      if (future.length === 0) return state\n\n      const next = future[0]\n      const newFuture = future.slice(1)\n\n      return {\n        past: [...past, present],\n        present: next,\n        future: newFuture,\n      }\n    }\n\n    case SET: {\n      if (newPresent === present) return state\n\n      return {\n        past: [...past, present],\n        present: newPresent,\n        future: [],\n      }\n    }\n\n    case RESET: {\n      return {\n        past: [],\n        present: newPresent,\n        future: [],\n      }\n    }\n    default: {\n      throw new Error(`Unhandled action type: ${type}`)\n    }\n  }\n}\n\nfunction useUndo(initialPresent) {\n  const [state, dispatch] = React.useReducer(undoReducer, {\n    past: [],\n    present: initialPresent,\n    future: [],\n  })\n\n  const canUndo = state.past.length !== 0\n  const canRedo = state.future.length !== 0\n  const undo = React.useCallback(() => dispatch({type: UNDO}), [])\n  const redo = React.useCallback(() => dispatch({type: REDO}), [])\n  const set = React.useCallback(\n    newPresent => dispatch({type: SET, newPresent}),\n    [],\n  )\n  const reset = React.useCallback(\n    newPresent => dispatch({type: RESET, newPresent}),\n    [],\n  )\n\n  return {...state, set, reset, undo, redo, canUndo, canRedo}\n}\n\nexport default useUndo\n```","n":0.052}}},{"i":22,"$":{"0":{"v":"useMemo","n":1},"1":{"v":"\n`useMemo` allows you to memoize (cache) a value between renders of a component.\n- By default, defining a variable inside a component means that it will be redefined and recalculated every time the component renders. `useMemo` allows us to say \"ok, I know that only one of the props of this component really matters to the calculation of this variable, so only recalculate it when that one changes. All other prop changes shouldn't cause it to re-render. Therefore, in the event that none of the dependencies change, just return me the cached value.\n- the cache is local to the component position within the tree\n- this idea of \"remembering\" the value is in a way conceptually similar to a ref, in that the value will exist between renders. The difference is that while refs exist between *all* renders of a component, a variable defined by `useMemo` will exist only between the renders where none of the dependencies change.\n\nNaturally, we should only use `useMemo` with pure functions. That makes sense, since impure functions by definition have dependencies that are different each time.\n\nReact will only cache one version of a memoized value. \n- ex. imagine we had a variable defined with `useMemo` that is dependent on `value`. The first render, `value` is `kyle`. Then `value` changes to `kyle t`, so the component re-renders and the variable is calculated again. Finally, `value` changes back to `kyle`, so the variable is recalculated again. Even though the value is the same as the first render, it won't be retrieved from the cache.\n\n`const MyComponent = memo(() => null)` is an anonymous component and will appear as unnamed in React Devtools.\n\n### Relation to `useCallback`\nLike [[useCallback|react.lang.hooks.useCallback]], the purpose of `useMemo` is to prevent unnecessary variable re-calculations and make your code more efficient.\n\nIt is similar to `useCallback`, but instead of returning a function, it calls the function and gives us the return value of the function\n- In other words, `useMemo` calls the passed function only when necessary and it returns a cached value on all the other renders.\n- `useMemo` \"caches\" the value, while `useCallback` \"caches\" the function itself.\n\n### Checking how expensive a calculation actually is\nIf the overall logged time adds up to a significant amount (say, 1ms or more), it might make sense to memoize that calculation. \n- Keep in mind that your machine is probably faster than your users’ so it’s a good idea to test the performance with an artificial slowdown. \n    - ex. Chrome offers a CPU Throttling option for this.\n\n```js\nconsole.time('filter array');\nconst visibleTodos = useMemo(() => {\n  return getFilteredTodos(todos, filter); // Skipped if todos and filter haven't changed\n}, [todos, filter]);\nconsole.timeEnd('filter array');\n```","n":0.048}}},{"i":23,"$":{"0":{"v":"useEffect","n":1},"1":{"v":"\n## Overview\n`useEffect` lets you specify side effects that should be triggered by the rendering of a component, rather than by a particular user-triggered event (like a button click). \n- ex. setting up a connection to a remote server happens once the component renders— not when a user performs some action.\n\nYou can think of `useEffect` as \"attaching\" a piece of behavior to the render output. \n\nThe purpose of `useEffect` is to allow us use an escape hatch from React and synchronize with some external system like a non-React widget, network, or the browser DOM. If there is no external system involved, then we shouldn't need a `useEffect`\n\n### Relationship to closures\nWhen you define a function inside the `useEffect` hook, that function forms a [[closure|js.lang.closures]] over the variables in the component's scope at the time the effect was created.\n- ex. In the following example, the `handleClick` function forms a closure over the `count` variable. This means that when `handleClick` is invoked, it will capture the value of `count` from the outer scope where the effect was created.\n```js\nfunction MyComponent() {\n  const [count, setCount] = useState(0);\n\n  useEffect(() => {\n    const handleClick = () => {\n      setCount(count + 1); // This will capture the count value from the outer scope\n    };\n\n    document.addEventListener('click', handleClick);\n\n    return () => {\n      document.removeEventListener('click', handleClick);\n    };\n  }, [count]);\n\n  // Rest of the component...\n}\n```\n\nEffects from each render are isolated from each other. This is due to the closure.\n\n### When the effect gets run\n`useEffect` runs after the JSX has been reconciled and the DOM has been updated.\n- contrast this will all other non-JSX code in a component body which gets run *while* the component is rendering.\n    - For this reason, we must pay especially close attention to component body code that would attempt to modify any DOM elements that are generated by the component's JSX.\n    - This is the value of `useEffect`. It allows us to wait until a component has finished rendering before we attempt to manipulate any DOM elements\n    - ex. if our `App` component had a child `VideoPlayer` component, we would not be able to attach a [[ref|react.lang.ref]] to the `<video>` tag and expect to be able to just call `ref.current.play()` on it. We would have to wrap that code in a `useEffect`, which would essentially be saying \"don't try to access this ref until the component has finished rendering all its JSX (see example [here](https://react.dev/learn/synchronizing-with-effects#step-2-specify-the-effect-dependencies))\n\n\nKeep in mind that `useEffect` is typically used to \"step out\" of your React code and synchronize with some external system \n- ex. \"external system\" might mean browser APIs, third-party widgets, network\n\nWe should also add a cleanup function (ie. the `return` code) if needed. Some Effects need to specify how to stop, undo, or clean up whatever they were doing. \n- ex. \"connect\" needs \"disconnect\", \"subscribe\" needs \"unsubscribe\", and \"fetch\" needs either \"cancel” or \"ignore\".\n\nReact always cleans up the previous render’s Effect before the next render’s Effect.\n- that is, if we are in a state where `useEffect` needs to run again (e.g. a value in the dependency changed), the cleanup will always get run before the next `useEffect` gets called.\n\nIn development mode, React remounts the component once to verify that you’ve implemented cleanup well.\n- that is, in a `useEffect`, the main body will get run once, then the cleanup gets run, then the main body will get run again.\n\n#### Dependency array\nBy default, `useEffect` will run after every render. However, most of the time, it should only re-run when needed rather than after every render. This can be controlled with the *dependency array*.\n- ex. a fade-in animation should only trigger when a component appears. Connecting and disconnecting to a chat room should only happen when the component appears and disappears, or when the chat room changes.\n\n*The question is not \"when does this effect run\", the question is \"with which state does this effect synchronize with:\"*\n- `useEffect(fn)` - all state\n- `useEffect(fn, [])` - no state\n- `useEffect(fn, [these, states])`\n\nIf one of the variables in the dependency array changes, `useEffect` runs again. If the array is empty the hook doesn't run when updating the component at all, because it doesn't have to watch any variables.\n\nYou only need to include a variable in the dependency array if that variable changes due to a re-render.\n- ex. if we define a variable that is defined outside the component, then it wouldn't change between renders. For this reason, it doesn't have to be a part of the dependency array.\n- Props, state, and other values declared inside the component are *reactive* because they’re calculated during rendering and participate in the React data flow. Therefore, they must be included in the dependency array.\n  - by extension, any variable that is declared that uses any reactive variable is also reactive\n- `ref.current` is mutable and since changing it doesn’t trigger a re-render, it’s not a reactive value\n\nIf you have no dependency array and are getting infinite loops, see if there are functions being defined each time in the component. It might be a simple fix to memoize them with `useCallback`\n- simply adding an empty dependency array is a bandaid solution and is not really addressing the root of the problem.\n\nThe dependency array is used to specify which values from the component's scope should be captured by the closure. \n- If a variable used in the `useEffect` is not listed in the dependency array and changes between renders, the effect will still capture the original value.\n    - spec: verify this\n\n## Shortcoming of useEffect\nCompared to class component lifecycle methods, the shortcoming of `useEffect` is that while we can set new state, we are unable to access current state (because of stale closure)\n\n```js\nuseEffect(() => {\n    const intervalId = setInterval(() => {\n        setCount(count + 1)\n    }, 1000)\n    return () => clearInterval(intervalId)\n}, [])\n```\nIn this example, `count` is always pointing to the previous reference.\n- we can work around this shortcoming with [[refs|react.lang.ref]]. Essentially, because refs exist between renders, that value is never lost between mounts. We simply take the value from the ref, and update the state with that value:\n\n```js\nuseEffect(() => {\n    const intervalId = setInterval(() => {\n        countRef.current = countRef.current + 1\n        setCount(countRef.current)\n    }, 1000)\n    return () => clearInterval(intervalId)\n}, [])\n```\n\n[[this|js.lang.feat.this]] is mutable state, and the problem with mutable state is that it is always up to date. The good thing about hooks in react is that there is no `this` to retrieve values from. State always stays the same within a given render of a component.\n\n### `useEffect` that depends on a function\nif your effect depends on a function, storing that function in a ref is a useful pattern. Like this:\n\n```js\nconst funcRef = useRef(func)\n\nuseEffect(() => {\n    funcRef.current = func\n})\n\nuseEffect(() => {\n    // do some stuff and then call\n    funcRef.current()\n}, [/* ... */])\n```\n\n* * *\n\n### Using multiple `useEffect`s\nEach Effect should represent a separate synchronization process, and we should resist adding unrelated logic to our Effect only because this logic needs to run at the same time as an Effect we already wrote.\n- ex. say you want to send an analytics event when the user visits the room. You already have an Effect that depends on roomId, so you might feel tempted to add the analytics call there:\n\n```jsx\nfunction ChatRoom({ roomId }) {\n  useEffect(() => {\n    logVisit(roomId);\n    const connection = createConnection(serverUrl, roomId);\n    connection.connect();\n    return () => {\n      connection.disconnect();\n    };\n  }, [roomId]);\n  // ...\n}\n```\nBut imagine you later add another dependency to this Effect that needs to re-establish the connection. If this Effect re-synchronizes, it will also call logVisit(roomId) for the same room, which you did not intend. Logging the visit is a separate process from connecting. Write them as two separate Effects:\n\n```jsx\nfunction ChatRoom({ roomId }) {\n  useEffect(() => {\n    logVisit(roomId);\n  }, [roomId]);\n\n  useEffect(() => {\n    const connection = createConnection(serverUrl, roomId);\n    // ...\n  }, [roomId]);\n  // ...\n}\n```\n\nIn the above example, deleting one Effect wouldn’t break the other Effect’s logic. This is a good indication that they synchronize different things, and so it made sense to split them up. On the other hand, if you split up a cohesive piece of logic into separate Effects, the code may look “cleaner” but will be more difficult to maintain. This is why you should think whether the processes are same or separate, not whether the code looks cleaner.\n\n## UE Resources\n- https://react.dev/learn/you-might-not-need-an-effect\n- https://react.dev/learn/escape-hatches","n":0.027}}},{"i":24,"$":{"0":{"v":"useCallback","n":1},"1":{"v":"\n`useCallback` returns a memoized callback.\n- it accepts a function, and returns the same instance of the function being passed instead of creating a new one when a component re-renders, which is the default behavior.\n\n`useCallback` allows you to cache an instance of a function between renders.\n- by default, all variables (including functions) defined in a component body will be regenerated on each rendering of the component. `useCallback` allows us to say \"I know that this function body will not change unless *this* prop (or state) changes. Therefore, I want to opt-out of the automatic regeneration of the function *unless* that prop changes.\"\n\nThe purpose of `useCallback` is to prevent unnecessarily re-defining functions between component renders, making our code more efficient.\n- [[useMemo|react.lang.hooks.useMemo]] \"caches\" the value, while `useCallback` \"caches\" the function itself.\n- In the days of class components, we had a `render()` function. Anything placed inside would get re-calculated on every render, but we also had the option of defining functions as class methods (by putting functions outside of `render()`). These methods would not get recalculated on each render, and would lead to performance gains. Since functional components naturally call everything (since `render()` method is implicit), they inherently have a performance issue. `useCallback` and `useMemo` exist to alleviate this problem.\n\nYou should consider using useCallback and/or useMemo hooks on the following situations:\n1. Processing large amounts of data\n2. Working with interactive graphs and charts\n3. Implementing animations\n4. Incorporating component lazy loading (`useMemo` specifically)\n\n\n```js\nconst additionResult = useCallback(add(firstVal, secondVal), [firstVal, secondVal])\n```\n- In this example, the `additionResult` function only gets re-defined if either `firstVal` or `secondVal` have changed between those 2 renders.\n\n### Example\nImagine we had a function that we passed down to a child component.\n```js\nconst getItems = () => {\n\treturn [number, number + 1, number + 2]\n}\n```\n\nThis can get expensive. Instead we can memoize the callback:\n```js\nconst getItems = useCallback(() => {\n\treturn [number, number + 1, number + 2]\n}, [number])\n```\n\nNow, the `getItems` function:\n```js\n() => {\n\treturn [number, number + 1, number + 2]\n}\n```\n\nOnly gets redefined when the `number` prop changes.\n\n## Additional reading\n- [when to use](https://kentcdodds.com/blog/usememo-and-usecallback)\n","n":0.055}}},{"i":25,"$":{"0":{"v":"HoC (Higher-Order Component)","n":0.577},"1":{"v":"\n### Downsides of HoCs\n- If we use HoCs to enact business logic, then we need to change the structure of our component tree so that the HoC wraps the sub components that use the business logic\n  - this is a case where [[hooks|react.lang.hooks]] are more appropriate, as we don't have to alter our component structure.","n":0.135}}},{"i":26,"$":{"0":{"v":"Controlled Components","n":0.707},"1":{"v":"\n***[Controlled Component](https://dev.to/stanleyjovel/simplify-controlled-components-with-react-hooks-23nn)*** - components within a form whose state is controlled with `setState`, rather than the built-in functionality provided by HTML forms\n- ie. an HTML `input` that receives its `value` property from a prop in the component\n```js\nconst ControlledInput = ({ value, eventHandler }) => (\n\t<input value={value} onChange={eventHandler} />\n)\n```\n- It means that when the user types the letter “J” on the input, what is visible is not the same “J”, it may be an identical “J” that comes from the state, or whatever the event handler has put in there.\n- Controlled components are the highly recommended approach\n","n":0.102}}},{"i":27,"$":{"0":{"v":"Context","n":1},"1":{"v":"\nA Context allows us to manage the global state of a sub-tree of components\n- ex. perhaps we have an ApplicationForm page with many fields, and we want to store that state in a form.\n\nIf your goal with Context is only to avoid having to pass props down many levels, then consider using [[inversion of control|general.patterns.IoC]] instead (bake those props into the component at the top level, then just pass the enhanced component itself down as a prop)\n\n## Components\nThe React Context API consists of:\n- The context\n- Provider\n- Consumer\n\n### The Context\nThis is the state. This is the part that becomes available to consumers under the provider\n\n```ts\n// the type in the generic is the shape of the context state.\nconst StagingNuggetContext = createContext<\n  undefined,\n)\n```\n\n### Provider\nThe Provider is a component that wraps a subtree, allowing components lying underneath to consume the context state.\n- it does this by taking a `value` prop which is the state (ie. the context) that will be made available to the consumers\n```tsx\n<StagingNuggetContext.Provider value={state}>\n  {children}\n</StagingNuggetContext.Provider>\n```\n\nThe state we pass to `value` is our underlying state-management mechanism.\n- ex. we could use a simple `useState` hook for our context state-management, or we could get a little more complex and use `useReducer`\n```tsx\nexport function StagingNuggetProvider({ children }: StagingNuggetProviderProps) {\n  const [state, dispatch] = useReducer(reducer, initialState)\n  const value = { state, dispatch }\n  return (\n    <StagingNuggetContext.Provider value={value}>\n      {children}\n    </StagingNuggetContext.Provider>\n  )\n}\n```\n\n### Consumer\nA consumer is a component that subscribes to state from the context.\n- A context may have many consumers\n\nAll consumers that are descendants of a Provider will re-render whenever the Provider’s value prop changes\n\n#### Hook (optional)\nIn the past, we would have made a `<MyContext.Consumer>` and wrapped any components that needed to access the context. In modern React, we can just use a hook to consume it:\n\n```ts\nexport const useStagingNugget = () => {\n  const context = useContext(StagingNuggetContext)\n\n  if (context === undefined) {\n    throw new Error('useStagingNugget must be used within a StagingNuggetProvider')\n  }\n  return context\n}\n```\n\n* * *\n\n### How re-renders are handled\nThe updates to context values doesn't trigger re-render for all the children of the provider, rather only components that are rendered from within the Consumer\n- [source](https://stackoverflow.com/questions/50817672/does-new-react-context-api-trigger-re-renders)\n\nContext renders all consumers throughout the system on every change.\n- In a larger scale app everything, the entire component tree, will render on every keystroke when you update an input field for instance\n\n### Should you use context?\nIf you need to make a couple of values that don't update often available to other components then context is what you want.\n\nIf you have non-trivial global state that updates frequently, requires complex updates and is used in lots of places then you should use a state management library (like [[Redux|redux]] Zustand, or Jotai).","n":0.048}}},{"i":28,"$":{"0":{"v":"Components","n":1},"1":{"v":"\nPurity and immutability between components is critical. Mutability within a component is fine, as long as that mutation doesn't affect state/props.\n- ex. pushing onto an array within a component is fine.\n\nWhen React is interpreting JSX, it will treat custom components (`<Form>`) as recursive functions, and regular elements (`<form>`) as the finality of the chain\n- ex. We say \"React, render a `<Sidebar>` for me\". React responds \"ok what's in a `<Sidebar>`?\", and it will go on like this until React gets to the html elements and it has therefore built up the entire component tree.\n\nA component that receives another component as props will always re-render, since [[JSX|react.lang.jsx]] syntax always produces a new immutable React component\n\n## Re-rendering\nReact components will re-render for various reasons:\n- Props values changed: When a component's parent passes new props to it, the component will re-render. React will compare the new props with the previous props to determine if the component needs to update.\n- State changed: When the `setState` function is called in a component, it triggers a re-render. \n- If a parent component re-renders, all of its child components will also re-render. \n\n### Higher-order components\nHigher-Order Components are generic functions, as they only care about the argument being a component. The type of the component’s properties is not important.\n","n":0.069}}},{"i":29,"$":{"0":{"v":"Form","n":1},"1":{"v":"\n### Controlled Form\n```ts\nconst OuterForm = () => {\n  const [inputValue, setInputValue] = useState<string>(\"\");\n  const handleChange = (event: React.FormEvent<HTMLInputElement>) => {\n    const { value } = event.currentTarget;\n    setInputValue(value)\n  };\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    // submit logic\n  }\n  return (\n    <form action=\"\" method=\"get\" onSubmit={handleSubmit}>\n      <div>\n        <label htmlFor=\"name\">Enter your name: </label>\n        <input\n          type=\"text\"\n          name=\"name\"\n          id=\"name\"\n          placeholder=\"Enter your name\"\n          value={inputValue}\n          onChange={handleChange}\n          required\n        />\n      </div>\n      <div>\n        <input type=\"submit\" value=\"Submit!\" />\n      </div>\n    </form>\n  )\n}\n```","n":0.118}}},{"i":30,"$":{"0":{"v":"Architecture","n":1},"1":{"v":"\n## Rendering\nTo \"render a component\" is to run the function that defines the component.\n- The JSX you return from that function is like a snapshot of the UI in time. Its props, event handlers, and local variables were all calculated using its state at the time of the render.\n- therefore, since state persists between renders, state actually lives in React itself— not within the function (ie. the component).\n    - `useState` is still called on each render. This helps to illustrate the fact that `useState` is not creating the state, but is instead accessing it. When you call `useState`, React gives you a snapshot of the state for that render.\n\nWhen React re-renders a component:\n- React defines variables and calls your function again (unless cached with [[react.lang.hooks.useMemo]] or [[react.lang.hooks.useCallback]], respectively)\n- Your function returns a new JSX snapshot.\n- React then updates the screen to match the snapshot your function returned.\n\nA client-side rendered React app starts out as a simple html file with virtually nothing but `<script>` tags in its `<body>`. When the browser downloads the base HTML, it runs the scripts (ie. the bundle/chunks), and builds up the rest of the HTML content (ie. the `<body>`, and everything between).\n- Once the browser downloads and parses those scripts, React will build up a picture of what the page should look like, and inject a bunch of DOM nodes to make it so.\n- therefore, a browser must be Javascript-enabled for client-side rendering to work. If we are using [[general.patterns.SSR]], like [[nextjs]] does, we can load the pages (but not interact with them) without being Javascript-enabled.\n\n### Virtual DOM\n- note: the virtual DOM in React in distinct from the [[browser.DOM.shadow]], which is a browser technology for scoping variables and CSS.\n\nThe virtual DOM is an object that keeps an ideal version of the UI in memory. It is then synced with the real [[browser.DOM]] by a library, such as ReactDOM.\n- this process of synchronization is called *reconciliation* (the reconciliation engine of React 16+ is called *Fiber*)\n- this idea of a virtual DOM is what makes React declarative. We no longer have to imperatively manipulate an attribute of a DOM node, or handle events. Instead we use a virtual DOM to update values that represent these actual DOM nodes, and when the process of reconciliation happens (ie. on re-renders), the DOM is updated for us.\n\nVirtual DOM is more of a pattern, than a specific technology.\n- usually, Virtual DOM refers to React Elements, since they are the objects representing the user interface.\n    - recall: a React Element is simply an object that describes a UI element. An HTML node can be generated from a React Element.\n- internally, React also uses objects called *fibers* that are used to hold additional information about the component tree.\n\nWhen we talk about \"re-rendering\" in react, we are talking about\n\nReact splits all work into the \"render phase\" and the \"commit phase\".\n- Render phase is when React calls your components and performs reconciliation.\n- Commit phase is when React touches the host tree. It is always synchronous.\n\n* * *\n\nIn React, anything other than updating the page is considered a side-effect. If you’re not using React to update state or render HTML, that’s a side effect. It’s any non-React thing.\n\nImages that you are going to import inside of components should be in your `src/` directory, not `public/`\n\n# UE Resources\n- [Implementing Pub/Sub in React](https://www.pluralsight.com/guides/how-to-communicate-between-independent-components-in-reactjs)\n    - [[pubsub Dendron node|general.patterns.messaging.pubsub]]\n- [React Fiber (reconciliation engine) Architecture](https://github.com/acdlite/react-fiber-architecture)\n","n":0.042}}},{"i":31,"$":{"0":{"v":"React Animation","n":0.707},"1":{"v":"\n- [Framer motion](https://www.framer.com/motion/) \n  - for less control but more out of the box\n- [React Three Fiber](https://docs.pmnd.rs/react-three-fiber/getting-started/introduction) \n  - for more control but less out of the box\n","n":0.189}}},{"i":32,"$":{"0":{"v":"Pulumi","n":1}}},{"i":33,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Show stacks\n`pulumi stack ls`\n\n#### Switch a stack (preproduction, staging, production)\n`pulumi stack select <stackname>`\n\n#### Show the details of a stack\n`pulumi stack output -j --show-secrets | less`\n","n":0.196}}},{"i":34,"$":{"0":{"v":"Ascii","n":1}}},{"i":35,"$":{"0":{"v":"Emojis","n":1},"1":{"v":"\n- ಠ_ಠ)\n- ¯\\_(ツ)_/¯\n\n- ( •_•)\n- ( •_•)>⌐■-■\n- (⌐■_■)\n\n- (╯°□°)╯︵ ┻━┻\n- ┬─┬ ノ( ゜-゜ノ\n\n- ლ(ಠ益ಠლ)\n\n- (∩ ` -´)⊃━━☆ﾟ.*･｡ﾟ\n\n- (－‸ლ)\n","n":0.236}}},{"i":36,"$":{"0":{"v":"Zookeeper","n":1},"1":{"v":"\nZookeeper is a fault-tolerant distributed coordination service\n\nZookeeper and Kafka work in conjunction to form a Kafka Cluster.\n\nNote: Zookeeper is set to be deprecated and replaced by [Kafka Raft](https://www.openlogic.com/blog/kafka-raft-mode)\n\n## Why use it?\nIn general, ZooKeeper provides an in-sync view of the Kafka cluster.  \n\nZookeeper solves the same problem as [[k8s.node.master.components.etcd]]: distributed system coordination and metadata storage\n\n### Metadata management\nZooKeeper stores metadata about the Kafka cluster, including information about brokers, topics, partitions, and consumer group coordination. It maintains a hierarchical namespace in a distributed and replicated manner, allowing Kafka to store and retrieve metadata efficiently.\n\n### Leader election\nZooKeeper helps with leader election of brokers in Kafka. Each partition in Kafka is handled by a leader broker, and ZooKeeper is responsible for selecting and maintaining the leader for each partition. In case a leader fails, ZooKeeper facilitates the process of electing a new leader for the partition.\n\n### Cluster Membership\nZooKeeper keeps track of the live brokers in the Kafka cluster. It allows brokers to register themselves and provides a mechanism for detecting when brokers join or leave the cluster. This information is crucial for maintaining the overall health and availability of the Kafka cluster.\n\n### Topic Configuration and Partition Assignment\nZooKeeper helps with topic configuration and partition assignment. It stores the topic configurations, such as the number of partitions and replication factor, and ensures that the partitions are evenly distributed across the brokers in the cluster.\n\n### Consumer Group Coordination\nZooKeeper is used to manage consumer group coordination in Kafka. It keeps track of the consumer groups, their assigned partitions, and the offsets consumed by each consumer group. This enables Kafka to ensure that each consumer group is processing messages consistently and allows for fault tolerance and load balancing.\n\n### Watchers and Notifications\nZooKeeper provides a mechanism called watchers, which allows clients to receive notifications when certain events occur in the ZooKeeper cluster. Kafka leverages these watchers to be notified of changes to the cluster, such as broker failures or topic configuration updates.\n\n## How does it work?\n### Znode\n- short for \"Zookeeper Node\"\n\nThe znode is the fundamental data entity that represents a point in the ZooKeeper's distributed hierarchical namespace tree.\n- Znodes can have data associated with them\n- It can be created, modified, deleted, and queried by clients interacting with the ZooKeeper service.\n\nZnodes form the foundation for implementing distributed locking, leader election, configuration management, and other distributed coordination patterns.\n\nIt is similar to a file or directory in a traditional file system, but it is stored in memory and allows for coordination and synchronization among distributed systems.\n- similar to the path of a file or directory, each znode has a unique path within the namespace\n\n* * *\n\n## Other\nBesides Kafka, Zookeeper is also utilized by services such as [[apache.hadoop]], HBase, SOLR, Spark, and NiFi among others.","n":0.047}}},{"i":37,"$":{"0":{"v":"Yarn","n":1},"1":{"v":"\n### yarn.lock\n- The purpose of a lock file is to lock down the versions of the dependencies specified in a package.json file\n\t- This means that in a `yarn.lock` file, there is an identifier (ie. exact version specified) for every dependency and sub-dependency that is used for a project\n\t\t- sub-dependencies are the dependencies of a dependency\n- The equivalent of `yarn.lock` for npm is `package-lock.json`. If using both npm and yarn, we need both of them, and they need to remain in sync (use yarn's import directive to accomplish this)\n- if we didn't have a `yarn.lock`, then if a co-worker cloned our repo and ran `yarn install`, they may get different versions of a dependency, since `package.json` can specify version ranges. \n\t- Instead, since `yarn.lock` is checked into version control, when the co-worker clones the repo and runs `yarn install`, `yarn.lock` will be checked and the version specified will be installed.\n- critical to have if working on a team or if working alone with a CI server.\n- `yarn.lock` gets updated any time a dependency is added, removed or modified\n\t- If we want to ensure `yarn.lock` is not updated, use `--frozen-lockfile`\n\t\t- The difference between `--frozen-lockfile` and `--pure-lockfile` is that the former will fail if an update is needed \n- In a perfect world, yarn.lock is unnecessary, because the point of semver is that unless the major version changes, the upgraded package will still work. In other words, if the version in package.json is listed as ^16.0.1, then `yarn install` is free to go to the latest minor version, which doesn't matter since semver defines that as fully backwards compatible.\n\t- however, in the real world not everyone follows semver best practices, and sometimes it is just mistakes which ruin backward compatibility \n\n#### Upgrading packages\n- if we have a dependency version in `package.json` specified at `^3.9.1`, this means that any version between 3.9.1 and 4.0.0 will be acceptable. Of course, since we have a lockfile, upgrades will not automatically happen.\n- `yarn upgrade` allows us to upgrade all dependencies in `package.json`. If we use the `^` specifier, then the latest version within the range will be added. This will be reflected in `yarn.lock`\n- we can ignore the version range by passing the `--latest` flag.\n\t- This modifies both `yarn.lock` and `package.json`\n- We can see all packages that can be upgraded with `yarn upgrade-interactive --latest`\n\n### Link\n- `yarn link` allows us to create symlinks to local projects, from within the project (with package.json) we are currently in.\n- ex. if we have a `rn-client` project and a `components` project, and we want to use `components` within `rn-client`, we can do the following:\n\t1. go to `components` project and run `yarn link`\n\t2. go to `rn-client` project and run `yarn link components` (name field of package.json)\n\t\t- this creates a symlink at `rn-client/node_modules/components`\n\t4. from `rn-client` project, `import components from 'components'` \n- It is meant for development-only purposes\n- spec: think of `yarn link` as exporting the package, and `yarn link <package>` as importing it.\n","n":0.045}}},{"i":38,"$":{"0":{"v":"Yarn Workspaces","n":0.707},"1":{"v":"\nWorkspaces allow us to install dependencies from multiple package.json files at once.\n- Yarn can also create symlinks between workspaces that depend on each other, ensuring directories are correct and consistent\n- When there are repeated dependencies in the `node_modules` of sub-modules, yarn workspaces will pull up (hoist) the common dependencies to live in the root-level `node_modules`.\n\t- This is why we don't need to run `lerna bootstrap --hoist` when using yarn + lerna — yarn already hoists.\n- if package1 depends on package2, then a symlink to package2 will also be created in the root `node_modules`. This allows package2 to `require` in the package, and take advantage of node's recursive resolver to find the package.\n- workspaces uses a single yarn.lock file at the root.\n- any module (either your own code or the code of a dependency) that contains native code (ie. Swift, Java etc) must not be hoisted.\n\n## How to implement\n1. Add a `workspaces` key to the root `package.json`:\n- `@app` holds all the modules\n```json\n\"workspaces\": {\n\t\"packages\": [\n\t\t\"@app/*\"\n\t]\n}\n```\n2. Change the name field of each module's `package.json` to follow a pattern so that they can be referenced easily\n3. Add module script shortcuts to root `package.json`, using the module names as changed in the previous step:\n```json\n\"scripts\": {\n\t\"db\": \"yarn workspace @tycholiz/db\",\n}\n```\n4. Add `private: true` property to root `package.json`.\n5. Add script in root `package.json` that allows us to execute multiple commands at once (note: consider using `concurrently` package to run servers in parallel)\n```json\n\"scripts\": {\n    \"dev\": \"concurrently --kill-others-on-fail \\\"yarn sanity dev\\\"  \\\"yarn web dev\\\"\",\n}\n```\n","n":0.064}}},{"i":39,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n### List all recognized workspaces\n`yarn workspaces info`\n\n### Run the `start` command in each workspace\n`yarn workspaces run start`\n","n":0.243}}},{"i":40,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### package.json script to ensure using yarn\n```json\n{\n    \"scripts\": {\n        \"preinstall\": \"node -e \\\"if(process.env.npm_execpath.indexOf('yarn') === -1) throw new Error('You must use Yarn to install, not NPM')\\\"\",\n    }\n}\n```\n\n### Install package with an alias\n```sh\nyarn add pouchdb-md5@npm:react-native-pouchdb-md5 react-native-quick-md5\n```\n\nwill result in package.json dependencies property:\n```json\n\"pouchdb-md5\": \"npm:react-native-pouchdb-md5\",\n```","n":0.158}}},{"i":41,"$":{"0":{"v":"Xcode","n":1},"1":{"v":"\n### Project\nAn project is a repository for all the files, resources, and information required to build one or more software products. \n\n### Target\nA target specifies a product to build and contains the instructions for building the product from a set of files in a project or workspace.\n\nyou ship targets. Your end products are targets.\n\nMany targets can belong to a single projet\n\n### Scheme\nAn Xcode scheme defines a collection of targets to build, a configuration to use when building, and a collection of tests to execute. \n\n","n":0.108}}},{"i":42,"$":{"0":{"v":"Webserver","n":1},"1":{"v":"\nWeb servers like Apache, Nginx and Caddy can perform various enhancements to our requests\n- Reverse proxy with caching\n- IPv6\n- Load balancing\n- FastCGI support with caching\n- WebSockets\n- Handling of static files, index files, and auto-indexing\n- TLS/SSL with SNI\n\nWeb servers are built to handle a lot of incoming requests simultaneously. Nginx and Apache each take a different approach to how they take a request from the incoming request queue and hand it off to a worker process.\n","n":0.115}}},{"i":43,"$":{"0":{"v":"Proxy Server","n":0.707},"1":{"v":"\nA proxy server is a [[proxy|general.patterns.structural.proxy]] that acts as a gateway from one network to another \n- ex. the Cloudflare proxy server gives the local network access to the network that holds that cached information.\n- a proxy server acts on behalf of the client(s), while a reverse proxy server acts on behalf of the server(s)\n\n## Caching\nAnother advantage of a proxy server is that its cache can serve a lot of requests. If multiple clients access a particular resource, the proxy server can cache it and serve it to all the clients without going to the remote server.\n\nWhen you think about it, HTTP requests have the same benefits that pure functions do: for a given request, there is a given response. This means that if we remember all of the details of the request, we may be able to store that corresponding response, and serve that data to all subsequent identical requests. \n- The proxy server figures out whether or not it should cache data based on the headings that are found in the request. \n- a cache is used by a proxy server. Think of it as storage that the proxy server returns to the client making the request. \n\nWhen we send a request to a server for a particular resource, the proxy server intercepts the message (establishing a TCP connection between it and the browser) and checks if it has a copy of the resource cached. If it does, then it returns it. If it doesn't then it opens a new TCP connection between it and the origin server, gets the resource, then returns it to the client. The client is none the wiser that any of this has happened.\n\nWeb caches are not computation heavy, and therefore can be low-cost machines.\n","n":0.059}}},{"i":44,"$":{"0":{"v":"Reverse Proxy","n":0.707},"1":{"v":"\nReverse proxies are servers that sit between the request-response process that retrieve resources from a server. These resources are then returned to the client, appearing as if they originated from the proxy server itself.\n- ex. This might include services such as caching, security (inc. SSL), load balancing, and content optimization\n\nA reverse proxy acts on behalf of a server, intercepting client requests and forwarding them to the appropriate backend servers.\n\nThe backends that are receiving the proxied requests are called *upstreams*\n- ie. these would be your application servers/database etc.\n\n- Proxying is typically for load balancing, to seamlessly show content from different websites, or to pass requests for processing to application servers over protocols other than HTTP.\n- It's called a reverse proxy because the client makes a request, and before that request can hit the server, it's intercepted by a server (the proxy server), which gives the request what it wants, and sends (reverses) it back to the client.\n- A common scenario is that we run our REST API behind a reverse proxy. Among other reasons, we might want to do this so our API server is on a different network/IP than our front-end application. Therefore, we can secure this network and only allow traffic from the reverse proxy server.\n- A reverse proxy like one from Nginx can be used to implement SSL\n\t- [guide](https://nginx.org/en/docs/http/configuring_https_servers.html)\n\nExample:\n- Imagine we had content stored in 3 different places. We could use a reverse proxy with rewrite rules to determine where each request would go depending on the actual endpoint of where the request was sent.\n- in this case, each request is forwarded on to a different origin\n![](/assets/images/2021-12-07-12-13-42.png)\n\n### Cook\n#### Proxy requests to different local backends (ie. upstreams)\nImagine we want to have a RPi that exposes 3 websites to the internet, without having to specify port at the end of the URL. As it is, we can't expose them all on port 80/443. To get around this, we can configure a webserver like Apache/Nginx/Caddy as a reverse proxy to forward the requests to each specific application hosted on the RPi. With a reverse proxy configured, the webserver is able to know which requests go to which server based on the domain (or even the URL path). We can set this up by exposing the webserver to the internet via ports 80 & 443, and when the webserver receives requests, it will proxy them on to each individual app.\n\n![](/assets/images/2024-02-28-18-29-37.png)\n\n# E Resources\n- [AWS Reverse Proxy Architecture](https://aws.amazon.com/blogs/architecture/serving-content-using-fully-managed-reverse-proxy-architecture/)\n","n":0.05}}},{"i":45,"$":{"0":{"v":"Forward Proxy","n":0.707},"1":{"v":"\nAlso known simply as \"proxies\"\n- From the server's perspective, requests came from the proxy server (in this case, the proxy server is just passing along the request made by the client)\n- It's called a forward proxy because when a request is detected, the proxy server intercepts it, and forwards it on to the next destination\n\nA forward proxy is typically used to retrieve resources on behalf of clients from other servers on the internet\n- Clients send their requests to the forward proxy, which then forwards those requests to the target servers. \n\nThe forward proxy is usually used to provide:\n- Anonymity: The client's identity is hidden from the target server.\n- Caching: The forward proxy can cache frequently requested resources, improving performance and reducing bandwidth usage.\n- Content Filtering: The forward proxy can filter and block certain types of content based on configured rules.\n- Access Control: The forward proxy can enforce access policies, allowing or denying access to certain websites or resources.\n\n## Web Proxy Server\n- It's common to run web apps behind a proxy such as NGINX\n\t- Nginx listens on port 80, then forwards traffic to the app on another port, like 4000\n- An early limiting factor of Nginx that will be encountered will be related to the number of open sockets that the Nginx server can handle. The result is delays on the client side. \n\t- the next limiting factor would be the number of ports available (there are only 64k). These 64,000 ports can handle 500 requests/second \n","n":0.064}}},{"i":46,"$":{"0":{"v":"Webpack","n":1},"1":{"v":"\nNode.js is built with a modular-first mentality. However, the web has been slow to adopt this philosophy of having clearly defined and swappable modules. Webpack aims to solve this by allowing us to implement modules in our code\n- unlike Node, webpack can also treat css/sass files, and image URLs as modules.\n\nWebpack is eager, in that javascript modules are figured out ahead of time, and are delivered in one big file. This negates the need to make network requests to get more of the javascript. The consequence of this fact is:\n- we cannot import dynamically in a data-driven way (since the importing will already have been done by the time the data-driven code is run.)\n- we cannot import in a function, since a function occurs after webpack does its job.\n- Module objects are frozen. There is no way to hack a new feature into a module object, polyfill style.\n- import errors are detected at compile-time\n- There is no hook allowing a module to run some code before its dependencies load. This means that modules have no control over how their dependencies are loaded.\n\n# Concepts\n- each webpack config has an entrypoint from which the dependency graph starts to be made. Unless specified, this defaults to `./src/index.js`. This lets Webpack know what to launch when the final bundle is loaded by the browser.\n- each webpack config also has an output, which specifies where the bundle will be emitted. Unless specified, this defaults to `./dist/main.js` for the main file, and the rest of the files also end up in `dist/`\n\n## Loaders\n- in Webpack 4 and below, webpack only understands js and json files. For this reason, we use loaders to allow different files to form a part of the dependency graph.\n- ex. `{ test: /\\.txt$/, use: 'raw-loader' }`\n\t- basically, loaders say \"hey Webpack, when you come across `.txt` files whenever you are trying to import, use the `raw-loader` to transform it before adding it to the bundle.\n- loaders are declared in `module.rules`\n\n## Types of Modules\n- There are more, but the 2 main types of modules we are concerned with are ECMAScript Modules, and Asset Modules\n\n### Asset Modules\n- In webpack 5, Asset Modules allow us to use asset files (ie. fonts, icons), without configuring additional loaders.\n\t- before Webpack 5, we had to use `raw-loader`, `url-loader`, `file-loader` etc., since out of the box, those versions of webpack only understood js and json files.\n\n### Module Resolution\nA resolver is a library which helps in locating a module by its absolute path\n- Effectively, webpack can be in charge of locating the files, and creating an alias for that path, so that that alias can be used throughout the codebase, giving us the benefit of only having to change that path once, in the event that it needs to change.\n\n* * *\n### Resolve\n#### resolve.modules\n- in webpack.config, we can tell webpack where it should go to look for certain modules.\n\t- ex. `node_modules` will be a default location, but we can specify paths that have higher priority so they are checked first before `node_modules`\n- if relative paths are used, then webpack will scan for that file recursively, similar to how node resolves `node_modules`\n\n#### resolve.alias\n- allows us to import more easily in our project.\n- we basically tell webpack, \"hey, any time you see me `import _ from 'ui-components`, just look for that module at whatever path I specify.\"\n\nwebpack continuously builds our app as we go.\n\n* * *\n\n### Contexts (require.context)\n- allows us to create our own context by passing a pattern that will match a filename.\n- One of the main functions of webpack's compiler is to recursively parse all modules to form a dependency graph.\n- A context is simply a base directory that resolves paths to modules (ie. it is a directory and its dependencies)\n\t- ex. in a React app, we can choose to declare the `components/` directory a context. This will allow us to search for files within that directory \"tree\"\n- Realistically, the root directory (current working directory) is a context for webpack to actually resolve the path to `index.js`\n- The intention is to tell webpack at compile time to transform that expression into a dynamic list of all the possible matching module requests that it can resolve, in turn adding them as build dependencies and allowing you to require them at runtime.\n- In short, you would use require.context in the exact same situation when in Node.js at runtime you would use globs to dynamically build a list of module paths to require. The return value is a callable object that behaves like require, whose keys contain the necessary module request data that can be passed to it as an argument to require the module.\n\n# Alternatives\n- Snowpack: very fast\n- Rollup\n- Parcel: simple config that works out of the box\n\t- uses swc.\n- Vite: Built by Evan You (of Vue.js). Out of the box with Sveltekit\n\t- uses ESBuild\n\n# UE Resources\n[Chunking in Webpack](https://medium.com/hackernoon/the-100-correct-way-to-split-your-chunks-with-webpack-f8a9df5b7758)\n\n","n":0.035}}},{"i":47,"$":{"0":{"v":"Module Federation","n":0.707},"1":{"v":"\nModule Federation allows multiple webpack builds to work together.\n- From the runtime environment's perspective, modules from multiple different builds will behave like a huge connected module graph.\n- From the developer's perspective, we will be able to import modules from specific remote builds, and use them with minimal restriction.\n\n### Motivation\nImagine we want to implement a micro frontend [[general.arch.microservice.micro-frontend]], which would have these qualities:\n- It should take multiple separate builds to form a single application.\n- These separate builds should not have dependencies between each other, so they can be developed and deployed individually\n\nThere is a distinction between local and remote modules\n- Local modules are normal modules which are part of the current build.\n- Remote modules are modules that are not part of the current build and loaded from a so-called container at the runtime.\n\t- Loading remote modules is considered async (due to the network call involved). When using a remote module these asynchronous operations will be placed in the next chunk of loading operations that is between the remote module and the entrypoint. Chunk loading is necessary.\n\nA container is created through a container entry, which exposes asynchronous access to the specific modules. The exposed access is separated into two steps:\n1. loading the module (asynchronous)\n\t- done during the chunk loading\n2. evaluating the module (synchronous).\n\t- done during the module evaluation interleaved with other local and remote modules\n\t\t- This way, evaluation order is unaffected by converting a module from local to remote or the other way around.\n\n### High-level Overview\nEach build acts as a container and also consumes other builds as containers. This way each build is able to access any other exposed module by loading it from its container.\n\n### Goals of the system\n- It should be possible to expose and use any module type that webpack supports.\n- Chunk loading should load everything needed in parallel (web: single round-trip to server).\n- Control from consumer to container\n\t- Overriding modules is a one-directional operation.\n\t- Sibling containers cannot override each other's modules.\n- Concept should be environment-independent.\n\t- Usable in web, Node.js, etc.\n- Relative and absolute request in shared:\n\t- Will always be provided, even if not used.\n\t- Will resolve relative to `config.context`.\n\t- Does not use a `requiredVersion` by default.\n- Module requests in shared:\n\t- Are only provided when they are used.\n\t- Will match all used equal module requests in your build.\n\t- Will provide all matching modules.\n\t- Will extract `requiredVersion` from package.json at this position in the graph.\n\t- Could provide and consume multiple different versions when you have nested node_modules.\n- Module requests with trailing `/` in shared will match all module requests with this prefix.\n\n### Use cases\n- separate builds per page\n\t- Each page of a Single Page Application is exposed from container build in a separate build. This way each page can be separately deployed\n- Components library as container\n\t- Many applications share a common components library which could be built as a container with each component exposed. Each application consumes components from the components library container.\n\t- Changes to the components library can be separately deployed without the need to re-deploy all applications.\n\n# E Resources\n[Webpack docs](https://webpack.js.org/concepts/module-federation/)\n","n":0.045}}},{"i":48,"$":{"0":{"v":"Web","n":1},"1":{"v":"\n## Website vs Webapp\n- Web sites are documents; they are content‐centric. Sites are geared towards content consumption.\n- webapps are tools; they are behaviour‐centric. Apps are geared towards content creation and manipulation.\n  - in some sense, state management is what distinguishes a webapp.\n\nIt can be argued that there is a false dichotomy between a webapp and website, and that it is really more of a continuum\n- a site, built out of static documents, connected via links, is on the left end and a pure behaviour driven, contentless application like an online photo editor is on the right.\n\nIf you would position your project on the left side of this spectrum, an integration on webserver level is a good fit. With this model a server collects and concatenates HTML strings from all components that make up the page requested by the user. Updates are done by reloading the page from the server or replacing parts of it via ajax.\n\nWhen your user interface has to provide instant feedback, even on unreliable connections, a pure server rendered site is not sufficient anymore. To implement techniques like Optimistic UI or Skeleton Screens you need to be able to also update your UI on the device itself.\n\nOf course, some are firmly in one camp or the other:\n- ex.  a simple online brochure for a university programme that contains nothing but text, images, and hyperlinks is clearly a document. It is clearly a site. An HTML‐based Photoshop clone, on the other hand, is clearly an app — it has no inherent content of its own. Instead, it is used to create and manipulate content.\n","n":0.062}}},{"i":49,"$":{"0":{"v":"API","n":1}}},{"i":50,"$":{"0":{"v":"WebSockets","n":1},"1":{"v":"\n### Limitations of Websockets\n- web components such as firewalls and load balancers are built and maintained around HTTP\n\n* * *\n\n### Websockets vs Server Sent Events (SSE)\nWebsockets and SSE are both capable of pushing data to browsers, however they are not competing technologies. \n- Websockets connections can both send data to the browser and receive data from the browser. \n    - A good example of an application that could use websockets is a chat application.\n- SSE connections can only push data to the browser. Online stock quotes, or Twitter's updating timeline or feed are good examples of an application that could benefit from SSE.\n\nIn practice since everything that can be done with SSE can also be done with Websockets, Websockets is getting a lot more attention and love, and many more browsers support Websockets than SSE. However, it can be overkill for some types of application, and the backend could be easier to implement with a protocol such as SSE.\n\n#### Caveats\n- SSE suffers from a limitation to the maximum number of open connections, which can be specially painful when opening various tabs as the limit is per browser and set to a very low number (6).\n- Only WS can transmit both binary data and UTF-8, SSE is limited to UTF-8.","n":0.069}}},{"i":51,"$":{"0":{"v":"Streams","n":1},"1":{"v":"\nLarge swathes of the web platform are built on streaming data\n- that is, data that is created, processed, and consumed in an incremental fashion, without ever reading all of it into memory.\n- the web has the Streams Standard which provides a common set of APIs for creating and interfacing with such streaming data, embodied in *readable streams*, *writable streams*, and *transform streams* (which consist of both a readable stream and a writable stream).\n  - these APIs are designed to efficiently map to low-level I/O primitives\n  - they allow us to efficiently compose streams together into *pipe chains*\n\nStreams are primarily used by piping them to each other.\n\n## Source\n### Underlying Source\nThe underlying source is a lower-level I/O source. Most readable streams will wrap the underlying source.\n\nThere are two types of underlying source: *push sources* and *pull sources*. \n\n#### Push Source\nPush sources push data at you, whether or not you are listening for it. \n- They may also provide a mechanism for pausing and resuming the flow of data. \n- ex. a [[TCP|protocol.TCP]] socket, where data is constantly being pushed from the [[os]] level, at a rate that can be controlled by changing the TCP window size.\n\n#### Pull Source\nPull sources require you to request data from them. \n- The data may be available synchronously \n  - e.g. if it is held by the operating system’s in-memory buffers, or asynchronously, \n  - e.g. if it has to be read from disk. \n- ex. a file handle is a pull source, where you seek to specific locations and read specific amounts.\n\n### Readable Streams\nA readable stream represents a source of data, from which you can read (ie. data comes out of a `ReadableStream` instance)\n\nA readable stream can be piped directly to a writable stream, using its `pipeTo()` method, or it can be piped through one or more transform streams first, using its `pipeThrough()` method.\n\nReadable streams are designed to wrap both *push sources* and *pull sources* behind a single, unified interface.\n\n* * *\n\n## Sink\n### Underlying Sink\nAnalogously to readable streams, most writable streams wrap a lower-level I/O sink, called the underlying sink. \n- Writable streams work to abstract away some of the complexity of the underlying sink, by queuing subsequent writes and only delivering them to the underlying sink one by one.\n\nCode that writes into a writable stream using its public interface is known as a producer.\n\n### Writable Stream\nA writable stream represents a destination for data, into which you can write (ie. data goes into a `WritableStream` instance).\n\n* * *\n\n### Chunk\nA chunk is a single piece of data that is written to or read from a stream.\n\nIt can be of any type; streams can even contain chunks of different types.\n\nA chunk will often not be the most atomic unit of data for a given stream\n- ex. a byte stream might contain chunks consisting of 16 KiB `Uint8Arrays`, rather than single bytes.\n\n### Pipe chain\nOnce a pipe chain is constructed, it will propogate signals giving information as to how fast chunks should flow through it. If any step in the chain cannot yet process more chunks, it will propogate a signal backwards through the pipe chain until it eventually reaches the original source, which is told to stop producing chunks so fast. This process is called *backpressure*.","n":0.043}}},{"i":52,"$":{"0":{"v":"Intersection Observer","n":0.707},"1":{"v":"\nAsynchronously observe changes in the intersection of a target element with an ancestor element (often, this ancestor element is the viewport itself)\n- ex. we can detect when a certain element has entered the viewport\n\n### Why use this?\n- Lazy-loading of images or other content as a page is scrolled.\n- Implementing \"infinite scrolling\" websites, where more and more content is loaded and rendered as you scroll, so that the user doesn't have to flip through pages.\n- Reporting of visibility of advertisements in order to calculate ad revenues.\n- Deciding whether or not to perform tasks or animation processes based on whether or not the user will see the result.\n- Tracking user engagement as they scroll through a page.\n\n### Example\nImagine you have a web page with a long list of items, and you want to know when an item becomes visible in the user's viewport as they scroll down the page. In this case:\n\nBy using the Intersection Observer API, you can set up observers on each item element to detect when it becomes visible within (ie. intersects) the viewport. When an item enters or leaves the viewport, you can asynchronously trigger actions, such as lazy loading images or animating the item's appearance.","n":0.071}}},{"i":53,"$":{"0":{"v":"Fetch","n":1},"1":{"v":"\nThe input to the `fetch` function is a request, which consists of...\n- a [[method|protocol.http.methods]]\n- a [[URL|protocol.http.URL]]\n- a list of headers\n- a body\n- ...and many [others](https://fetch.spec.whatwg.org/#requests), which are mostly implicitly set.\n\nThe response of the `fetch` function is a response, which consists of...\n- a list of headers\n- a body\n- status\n- ...and many [others](https://fetch.spec.whatwg.org/#responses), which are mostly implicitly set.\n\nA response evolves over time. That is, not all its fields are available straight away.\n\n### Request/Response Body\nA body consists of:\n- A [[stream|web.api.streams]] (a `ReadableStream` object).\n- A source (null, a byte sequence, a `Blob` object, or a `FormData` object), initially null.\n- A length (null or an integer), initially null.\n\n### Credentials\nFor cross-origin requests, `credentials` allows us (as the client) to specify whether or not credentials should be sent along for the ride in HTTP requests.\n- Therefore this gets set on the client.\n\nCredentials allow the server to maintain state about a particular user across multiple requests. \n- ex. it's how Twitter shows you your feed, it's how your bank shows you your accounts.\n\nCredentials are [[cookies|browser.cookies]], authorization headers, or TLS client certificates (not to be confused with server certificates). \n- Basically, like the email/password credentials we are most familiar with, credentials verify identity and are a way to establish trust.\n\nBoth the client and the server must indicate that they’re opting into including credentials.","n":0.068}}},{"i":54,"$":{"0":{"v":"Analytics","n":1}}},{"i":55,"$":{"0":{"v":"Pixel","n":1},"1":{"v":"\nA tracking pixel is also known as a *web beacon*\n\nIt is a lightweight piece of HTML that is included in a third-party website in order to grant us more visibility into how users interact with the site.\n\n## How does it work?\nThe pixel itself is a 1 x 1 (ie. 1-pixel) image.\n- the fact that the pixel is an `<img>` is vital. When a webpage loads and an `<img>` tag is encountered, it will send a GET request to the host company's web server (known as the *beacon web server*) to get the image. In that request is included varying identifying information about the requester, allowing the host to keep track of them.\n\nThe identifying information provided by the user's computer typically includes:\n- IP address\n- the time the request was made\n- the type of web browser or email reader that made the request\n- the existence of cookies previously sent by the host server\n\nThe host server can store all of this information, and associate it with a session identifier or tracking token that uniquely marks the interaction.\n\n- Instead of using a Javascript-based API call, the information is sent in the parameters of the image GET request and in the HTTP headers themselves.\n    - as a result, we can include the pixel in any component that supports HTML.\n```html\n<img src=\"https://myCompany.com/trackingpixel?userid=aws_user&thirdpartyname=example.hightrafficwebsite.com”>\n```\n\n### Components\n- A beacon web server to receive tracking information (or a [[lambda|aws.svc.lambda]])\n- A streaming engine to ingest incoming information (e.g. Aws Kinesis Data Firehose)\n- A storage layer to store information retrieved from the pixel (e.g. [[S3|aws.svc.S3]])\n\n## E Resources\n- [Building a serverless tracking pixel in AWS](https://aws.amazon.com/blogs/big-data/building-a-serverless-tracking-pixel-solution-in-aws/)\n\n{ state: State; dispatch: Dispatch }","n":0.061}}},{"i":56,"$":{"0":{"v":"WASM","n":1},"1":{"v":"\nWASM is the answer to the question \"how do we run languages other than Javascript in the browser?\"\n- any language can run as a WASM language\n\nConceptually, what is happening is that there is a container within the browser that includes whatever runtime and language environment that we need, and it interfaces with a minimal amount of javascript that is running natively in the browser.\n\nWASM becomes useful when there is an intense amount of computation happening in the browser\n- ex. Figma\n","n":0.112}}},{"i":57,"$":{"0":{"v":"WatermelonDB","n":1},"1":{"v":"\n## Philosophy\ndeclaratively define the connection between the component, and the data you want it to display. When the data changes, every component that is connected will automatically update.\nwatermelon is fast in part because it uses a declarative API. The declarative API means that all of the expensive computation is being done natively (Java or Swift). Since Javascript is quite slow compared to these 2 languages, this allows our computations to be done more efficiently.\n\n## Tables\n#### Columns\nColumns have one of three types: string, number, or boolean\nFields of those types will default to '', 0, or false respectively\n\nTo allow fields to be null, mark the column as isOptional: true\n\n#### withObservables\n- This is the principal way that we connect WatermelonDB to our component\n- let's us enhance a component by turning a non-reactive component to become reactive, meaning that UI will update in accordance with localdb changes\n- we make our component reactive by feeding it an observable for the data we want to display\n\n```js\nwithObservables(['post'], ({ post }) => ({\n  post: post.observe(), // inject enhanced props into the component\n  author: post.author.observeCount()\n}))\n```\n- above:\n\t- `({ post })` are the input props for the component\n\t- The first argument: `['post']` is a list of props that trigger observation restart. So if a different post is passed, that new post will be observed\n\t- Rule of thumb: If you want to use a prop in the second arg function, pass its name in the first arg array\n\t- This is also the place that we should make relations\n\t\t- the relation is enabled by the `@children` decorator on the parent model\n\n## Actions\nMutation (Create, Update) queries can be made from anywhere in the app, but the preferred way is to execute them through actions\n- An action is a function that can modify the database\n\n## Migrations\nEach migration must migrate to a version one above the previous migration\n- of course, each migration simply builds on the previous ones, meaning that when we want to make changes, we need to add the new changes as an item in the `migrations` array (to the front) and mark it with the next integer `toVersion`.\n\nSteps to making schema changes:\n1. make the change in migrations.js\n2. wait for the error in the simulator:\n\t- `Migrations can't be newer than schema. Schema is version 1 and migrations cover range from 1 to 2`\n3. if the error is there, make the change in schema.js, updating the `schemaVersion` to the latest migration\n\n#### Testing migrations work properly\n1. *Migrations test*: Install the previous version of your app, then update to the version you're about to ship, and make sure it still works\n2. *Fresh schema install test*: Remove the app, and then install the new version of the app, and make sure it works\n\n## Q Module\n- This module provides us to make SQL-like clauses to help construct our query\n\t- This is where we can use WHERE, JOIN (on), AND, OR, LIKE etc.\n\t\t- note: remember to escape `Q.like`\n- JOINs are done through `Q.on`\n\n## Observable\n- `.observe()` will return an observable\n- we can hook up observables to components\n- Because WatermelonDB is fully observable, we can create a @lazy function that will observe a database value and give us updated results in real-time (ie. without having to query the database)\n\t- ex. imagine we have a blog site, and blog posts can have a \"popular\" banner if they have at least 10 comments. We can make a function on the model layer that will observe the number of comments and will reactively give us the correct flag for the boolean:\n\t```js\n\tclass Post extends Model {\n\t\t@lazy isPopular = this.comments.observeCount().pipe(\n\t\t\tmap$(comments => comments > 10),\n\t\t    distinctUntilChanged()\n\t\t)\n\t}\n\t```\n\tand then directly connect it to the component:\n\t```js\n\tconst enhance = withObservables(['post'], ({ post }) => ({\n\t\tisPopular: post.isPopular,\n\t}))\n\t```\n\t- since this is reactive, a rise above/fall below the 10 comment threshold will cause the component to re-render.\n\t- Dissecting:\n\t\t- `this.comments.observeCount()` - take the Observable number of comments\n\t\t- `map$(comments => comments > 10)` - transform this into an Observable of boolean (popular or not)\n\t\t- `distinctUntilChanged()` - this is so that if the comment count changes, but the popularity doesn't (it's still below/above 10), components won't be unnecessarily re-rendered\n\t\t- `@lazy` - also for performance (we only define this Observable once, so we can re-use it for free)\n\n## Sync\nAny backend will work, as long as it complies with the following spec:\n![6d0a0837a90af34681ce452b015d4b19.png](:/860953c1a363424aac94bdff9d490b90)\n- `changes` is an object with a field for each model (table) that has changes. For each model, there are 3 fields: `created`, `updated`, `deleted`.\n\t- When the `changes` object is received from a Pull Request, it is the selection of changes that were made on the server since our last sync, that we need to now update locally.\n\t- When the `changes` object is sent with a Push Request, it is the selection of changes that we've made locally, that have not yet been sent to the remote database.\n\n#### Pulling\nWhen Watermelon runs `synchronize()`, `pullChanges` will get run, which will pass along with it information about the last time a pull was made (`lastPulledAt`). `pullChanges` will call an endpoint to the backend, passing along that `lastPulledAt` timestamp, and the server will confer with the backend DB, and send back all of the changes made since the last pull, along with the current timestamp. When the mobile app receives the response, it will then proceed to apply those changes to the local db.\n\n#### Pushing\nWe send to the server a `change` object, containing everything that needs to be updated remotely, as well as a timestamp of the last time a pull was made (`lastPulledAt`). When the server receives the request, it will use `lastPulledAt` to check conflicts with the remote db. If there is no conflict, the server will then update the db with the provided changes.\n\n#### Sync limitations\nThere are currently limitations of Sync, as outlined in this blog: [How to Build WatermelonDB Sync Backend in Elixir | Fahri NH](:/f615cdc32a5c4be4a768caee30774aa9)\n\n#### How does it know when to re-render?\nfor individual records, just listen to changes, and if the record changes, re-render\n\nfor queries, like \"tasks where a=b\", listen to the collection of tasks, and when a record in that collection changes, check if the record matches the query. If it does: if record was on the rendered list, and was deleted — remove from rendered list. if it wasn't on the rendered list, and now matches — add to rendered list.\n\nfor multi-table queries like \"tasks that belong to projects where a =b\", listen to all relevant collections, and if there's a change in any of them, re-query the database. There's ways to make it more efficient, but need to measure if the perf benefit is worth it\n\n## Solutions to:\n- [prop drilling](https://nozbe.github.io/WatermelonDB/Components.html#database-provider)\n\n## UE Resources\n[Logrocket Tutorial](https://blog.logrocket.com/offline-app-react-native-watermelondb/)\n[how sync works](https://fahri.id/posts/how-to-build-watermelondb-sync-backend-in-elixir/)\n[conf](https://www.youtube.com/watch?v=uFvHURTRLxQ)\n[Pull/Push changes synchronization controller API](https://github.com/FLVieira/sync-api/blob/master/src/app/controllers/SynchronizationController.js)\n[#2 Pull/Push changes synchronization controller API](https://github.com/rodrigosuman/rn-offline-first-app/tree/main/backend/src/services/sync)\n","n":0.03}}},{"i":58,"$":{"0":{"v":"Vscode","n":1}}},{"i":59,"$":{"0":{"v":"Workspace","n":1},"1":{"v":"\nA collection of one or more folders that are opened in a VS Code window (instance)\n- Think of a workspace as the root of a project that has extra VS Code knowledge and capabilities.\n\nThe concept of a workspace enables VS Code to:\n- Configure settings that only apply to a specific folder or folders but not others.\n- Persist task and debugger launch configurations that are only valid in the context of that workspace.\n- Store and restore UI state associated with that workspace (for example, the files that are opened).\n- Selectively enable or disable extensions only for that workspace.\n\n## Multi-root workspaces\nallow us to pick and include other folder trees to exist within the current workspace, allowing us the benefits that having all files under a workspace gives us.\n\n# UI\n### Workbench\n\"Workbench\" refers to the overall Visual Studio Code UI that encompasses the following UI components:\n- Title Bar\n- Activity Bar\n- Side Bar\n- Panel\n- Editor Group\n- Status Bar\n","n":0.081}}},{"i":60,"$":{"0":{"v":"Keybindings","n":1},"1":{"v":"\nEach rule consists of:\n- a key that describes the pressed keys.\n- a command containing the identifier of the command to execute.\n- an optional when clause containing a boolean expression that will be evaluated depending on the current context.\n\n### Parts of UI\n\"Workbench\" refers to the overall Visual Studio Code UI that encompasses the following UI components:\n- Title Bar\n- Activity Bar\n- Side Bar\n- Panel\n- Editor Group\n- Status Bar\n","n":0.123}}},{"i":61,"$":{"0":{"v":"Debugger","n":1},"1":{"v":"\n## Setup\n1. Create a Javascript Debug Terminal (from command palette)\n2. Run command to start servers (ie. `yarn start`)\n    - command will take longer than normal to run, because vscode is attaching debuggers to all the processes that are about to run.\n3. Set some breakpoints of where we want the code to stop\n4. Manually cause the code to be executed\n    - ex. by visiting a url, clicking a button in UI etc.\n\n## Analysis\n- We can see what each variable evaluates to in the main window\n\n### Debug panel\n- In the debugger left panel, we can see the variables\n\n### Debug Console\n- We can open the Debug Console to get a Debug Console REPL (cmd+shift+y)\n\n* * *\n\nthe debugger will not work for client-side code, since this code is executed in a browser. We would need to use the browser debugging tool to accomplish this.\n- alternatively, we can have a `launch.json` file to allow us to debug\n\n# Chrome Debugger\n- The vscode debugger can connect to Chrome via its Chrome Debugger protocol. Doing this allows us to map files loaded in the browser to the files open in Visual Studio Code.\n    - This enables developers to debug in vscode, by...\n        - setting breakpoints directly in their source code.\n        - setting up variables to watch and see the full call stack when debugging\n","n":0.068}}},{"i":62,"$":{"0":{"v":"Debugger Actions","n":0.707},"1":{"v":"\nActions available on the panel that appears when in debugging mode\n\n### Continue\nMove along to the next debugger point\n\n### Step Over\nCall the function without causing our debugging context to be inside the function call\n- Therefore, if we are at a function and \"step over\", we will not enter the function. However, the function itself will of course still be executed.\n\n### Step Into\nGo into the current function and execute each line within 1 by 1\n\n### Step Out\nExit the current function debugger context.\n","n":0.112}}},{"i":63,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n#### Show intellisense popup\n`cmd+k cmd+i`\n\n#### Delete all console.logs\n`cmd+shift+d`\n\n#### search node modules exclusively\n`cmd+k` + `cmd+n`","n":0.267}}},{"i":64,"$":{"0":{"v":"Nav","n":1},"1":{"v":"\nChange vscode workspace:\n`<C-r>`\n\nToggle Intellisense popup\n`Cmd-k Cmd-i`\n\nswitch tab\n`<C-j/k>`\n\ntoggle between terminal\n`<Cmd-j>`\n","n":0.333}}},{"i":65,"$":{"0":{"v":"Virtual Machine","n":0.707},"1":{"v":"\nA VM is made up of a Host VM and a Guest VM\n\n### Host VM\n- the server component of a VM, which provides resources to a Guest VM, such as processing power, memory, disk, network I/O etc.\n\n### Guest VM\n- can exist on a single phsyical machine, but is usually distributed across multiple hosts for load balancing.\n- The guest VM is not even aware that it is a guest VM, and therefore is not aware of any other physical resources that are allocated to other guest VMs.\n\n### Hypervisor\n- a piece of software in a computer that will create and run virtual machines\n- the hypervisor intermediates between the host and guest VM, which isolates individual guest VMs. This allows a host VM to support multiple guests running on different operating systems.\n\n* * *\n\n### Swap Space\n- Swap space in Linux is used when the amount of physical memory (RAM) is full\n\n### Memory Page (a.k.a. Virtual Page)\n- a fixed length contiguous block of virtual memory.\n- each page is described by an record in a page table\n","n":0.076}}},{"i":66,"$":{"0":{"v":"Vim","n":1},"1":{"v":"\n# Operators and Motions\n- Operator --> Number --> Motion\n\n## operator (Verb)\n- what we are doing\n- ***ex.*** - `d`, `c`, `y`, `f`, `t`\n    - `f#`/`F#` - (*mn.* find)\n        - inclusively go to first occurrence of `#` in the line\n        - `;` to cycle through, `,` to go backwards\n    - `t#`/`T#` - (*mn.* til)\n        - exclusively go to first occurrence of `#` in the line\n\n## Modifier\n- more specific information about the modifier\n- ***ex.*** - `i` (inside), `a` (around)\n\nCombine with numbers to further specify text objects\n- ***ex.*** - `d2f:` - delete until the second `:` character\n- ***ex.*** - `d2/:<cr>` - delete until the second `:` character\n\n## motion (noun) - what we are operating on\n- ***ex.*** - `$`, `0`, `w`, `e`, `b`\n\n* * *\n\n<!-- # strat submodules\nStructure the vim module from the perspective of a user who is trying to accomplish a certain task in vim.\n- Are they trying to substitute all occurrences in a file?\n- are they trying to modify text in some way (surround, uppercase, )\n\n- nav (moving the cursor around)\n    - line\n        - search\n        - find\n    - buffer\n        - search\n    - windows\n    - tree\n        - netrw\n        - MRU\n        - <C-6>\n- modify\n    - surround\n    - substitute\n        - greplace\n    - uppercase\n- feat\n    - marks\n    - macro\n    - shell (execute shell commands)\n- plug\n    - fzf\n    - fugitive\n    - guter (git-gutter)\n- repeating -->\n","n":0.068}}},{"i":67,"$":{"0":{"v":"Substitution","n":1},"1":{"v":"\nRepeat last substitute\n- `:&`\n    - `:&&` command repeats the last substitution with the same flags.\n        - You can supply the additional range(s) to it (and concatenate as many as you like):\n\nreplace multiple search terms at once\n- `:%s/Kang\\|Kodos/alien/gc`\n\n`:6,10s/<search_string>/<replace_string>/g | 14,18&&`\n\n## within Visually Selected Area\n1. visually select the area you want substitutions to take place in, then `<ESC>`\n2. `:%s/\\%Vfoo/bar/g` to replace all `foo` with `bar`\n\nAlternatively, the vis.vim plugin can be used\n\n### Upper/Lower Case\n- `guw`/`gUw` - make word lowercase/uppercase\n    - `guu`/`gUU` - make line uppercase/lowercase\n- `g~` - swap case\n\n## Flags\n### c\ny/n/a/q substitute (within single file)\n- `:%s/search/replace/gc`\n\n#### Values\n`l` - last (substitute this match, then quit)\n`a` - all (this, and all remaining)\n    - note: `l` and `a` are opposites, since `l` will treat the current match as the final one in our substitution process, whereas `a`\n`<C-e>`/`<C-d>` - scroll\n","n":0.087}}},{"i":68,"$":{"0":{"v":"Settings","n":1},"1":{"v":"\n### Syntax highlighting\nVim uses syntax definitions to highlight source code.\n- Syntax definitions simply declare where a function name starts, which pieces are commented out and what are keywords.\n- To color them, Vim uses colorschemes. You can load custom color schemes by placing them in .vim/colors, then load them using the colorscheme command\n\n### Pasting\nwhen you paste text into your terminal-based Vim with a right mouse click, Vim cannot know it is coming from a paste. To Vim, it looks like text entered by someone who can type incredibly fast :) Since Vim thinks this is regular key strokes, it applies all auto-indenting and auto-expansion of defined abbreviations to the input, resulting in often cascading indents of paragraphs.\n\nThere is an easy option to prevent this, however. You can temporarily switch to “paste mode”, simply by setting the following option:\n`set pastetoggle=<F2>`\n\nThen, when in insert mode, ready to paste, if you press `<F2>`, Vim will switch to paste mode, disabling all kinds of smartness and just pasting a whole buffer of text. Then, you can disable paste mode again with another press of `<F2>`. Nice and simple.\n","n":0.074}}},{"i":69,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get the value of a setting\n`:set expandtab?`\n\n\n#### Get the value of setting that is a variable\nto find out what the current syntax highlighting mode is (encoded in a variable, not an option):\n`echo b:current_syntax`\n\n#### See where a variable was set\n`verbose set expandtab?`\n","n":0.154}}},{"i":70,"$":{"0":{"v":"Search","n":1},"1":{"v":"\n#### Partial Word Search\n- When you're typing and you enter a partial word, you can cause Vim to search for a completion by using the `<C-p>` (search for previous marching word) and `<C-n>` (search for next match).\n\n### Where you land\nSearches in Vim put the cursor on the first character of the matched string by default\n- if you search for Debian, it would put the cursor on the D.\n\nWe can offset the cursor placement in a few ways:\n- To land on the last character in the matched string, rather than the beginning, add an /e to your search:\n    - ex. `/Debian/e`\n- To land 2 lines above/below, add `-2`/`+2` to the end\n    - ex. `/Debian/-2`\n- To offset 2 chars from the beginning of the string, add `b+3`\n- To offset 2 chars from the end of the string, add `e-3`\n\n\n## Tips\n- use search with an operator to find the location easier\n    - ex. Yank til `})` \n        - `y/})<Enter>`\n","n":0.08}}},{"i":71,"$":{"0":{"v":"Special Chars","n":0.707},"1":{"v":"\n### `.`\nmatches every single character.\n\n","n":0.447}}},{"i":72,"$":{"0":{"v":"Search Options","n":0.707},"1":{"v":"\nTo remove these options, prepend the command with `no`(`:set nohls`)\n- `:set ic` - ignore case\n- `:set hls` - highlight search\n- `:set is` - incsearch\n","n":0.204}}},{"i":73,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Re-execute the last search pattern (`/`)\n`gn`\n- This puts us in v-mode. If we type `gn` again, it will highlight until the next occurrence of that pattern\n- `gn` is a text-object representing the last searched word\n\t- leverage this with `cgn` to replace the next occurrence of the search pattern\n","n":0.143}}},{"i":74,"$":{"0":{"v":"Regex","n":1},"1":{"v":"\nRegex in Vim largely follows Basic Regex (BRE) (POSIX)\n- see `:help perl-patterns` for differences between Perl Regex and Vim's\n\nTo get Regex to behave more like traditional Regex, use Vim \"very magic\" mode by prepending with `\\v`\n- See `:help /\\v`\n\n","n":0.16}}},{"i":75,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\nfind empty lines (only whitespace)\n- `^\\s.*$`\n\nfind lowercase/non-lowercase\n- `\\l`/`\\L`\n\nfind uppercase/non-uppercase\n- `\\l`/`\\L`\n\nfind word boundary\n- `\\<term\\>`\n\nfind both \"dog\" and \"Dog\"\n- `/[Dd]og`\n\n### Greedy vs Non-greedy Quantifiers\nVim allows you to specify how many of something you wish to match by adding quantifiers to the term.\n- \"greedy quantifiers\" are those that match as many of the specified characters as possible.\n    - `*` matches 0 or more\n        - ex. `/abc*` will match abc, abby, and absolutely\n    - `\\+` matches 1 or more\n        - ex. `/abc\\+` will match abc, but not abby or absolutely\n    - `\\{5,}` matches 5 or more characters\n- \"non-greedy quantifiers\" are those that match only the specified number, or as few as possible.\n    - `\\=` matches 0 or 1\n        - ex. `/abc\\=` will match abc, abby, and absolutely.\n    - `\\{0,10}` matches between 0 and 10 of a character\n    - `\\{5}` matches 5 characters\n    - `\\{,5}` matches 5 characters or less\n    - `\\{-}` matches as little as possible\n","n":0.081}}},{"i":76,"$":{"0":{"v":"Plug","n":1},"1":{"v":"\n`localrc`: lets you load specific Vim settings for any file in the same directory (or a subdirectory thereof). Comes in super handy for project-wide settings.\n","n":0.2}}},{"i":77,"$":{"0":{"v":"Vim GitGutter","n":0.707},"1":{"v":"\n- `[c`/`]c` - jump between hunks\n- `<leader>hs`/`<leader>hu` - stage/unstage individual hunk (when cursor is\n    within it)\n  - can get more granular with visual mode selection\n- `<leader>hp` - preview hunk\n","n":0.186}}},{"i":78,"$":{"0":{"v":"Greplace","n":1},"1":{"v":"\n### Usage\n1. run `:Gsearch <term>`\n2. specify directories to search\n3. make the substitution in the resulting buffer\n4. run `:Greplace`, and respond: yes/no/all/quit\n","n":0.218}}},{"i":79,"$":{"0":{"v":"Nav","n":1}}},{"i":80,"$":{"0":{"v":"Split","n":1},"1":{"v":"\n###### Split vertically\n`<C-w>v`\n\n###### Split horizontally\n`<C-w>-s`\n","n":0.447}}},{"i":81,"$":{"0":{"v":"Line","n":1},"1":{"v":"\n- go to column 15 of current line - `15|`\n- `3f:` - find the 3rd occurrence of `:` in the line\n","n":0.218}}},{"i":82,"$":{"0":{"v":"Directory Navigation","n":0.707},"1":{"v":"\n# \nopen file under cursor\n- `gf` \n  - If doesn't work, try visually selecting first.\nopen file under cursor in new h-split (`<C-w>L` for v-split)\n- `<C-w>f`\n","n":0.2}}},{"i":83,"$":{"0":{"v":"Buffer","n":1},"1":{"v":"\n# Buffer Navigation\n## \nmove cursor up/down full screen\n- `ctrl + b`/`ctrl + f` \n\nscroll page up/down\n- `ctrl + e`/`ctrl + y`\n\njump between empty lines \n- `{`/`}`\n\ngo back 3 sentences \n- `3(`\n  - tip: trigger in *i n mode*\n\nmove to top/bottom\n- `zt`/`zb` \n\nmove current line to top of screen \n- `z<CR>`\n\nreplace a word with a yanked one \n- `vep`\n\ngo to top/mid/end line of current screen (home) \n- `H`/`M`/`L`\n    - go to 50% of page - `50%`\n\n## Returning to previous locations\ngo to last place you inserted text \n- `gi`\n\ngo to last place that you were \n- ***``***\n\ngo back and forth through list of positions you were in \n- `g;`/`g`,\n\ngo to start/end of previously changed or yanked text \n- `'[`/`']`\n","n":0.092}}},{"i":84,"$":{"0":{"v":"Search","n":1},"1":{"v":"\n### Search\n- `/`/`?` to find forward/backward\n    - `n` to show next, `N` to show previous\n    - `gn` will visually select the match\n- `/<CR>` - search for last searched pattern \n- You can search with word boundary by doing `/\\<word\\>`.\n\n### Symbol find\n- `*` to find forward for the word that the cursor is under\n    - prepend with `g` so find without word delimiters\n- `#` to find backward for the word that the cursor is under\n\n- `<C-o>`/`<C-i>` to go back/forward\n","n":0.113}}},{"i":85,"$":{"0":{"v":"Marks","n":1},"1":{"v":"\n`m[a-zA-Z]` marks the current cursor location with the designated letter\n- lowercase marks are local to the buffer, while uppercase are globally accessible\n    - therefore, if we have 5 buffers open, each buffer can have mark \"a\", but only one can have mark \"A\"\n\n- **`a** will jump to the exact spot of *mark \"a\"*\n    - *mn.* - more precise, just like js template literals (which use back ticks)\n- **'a** will jump to the **line** of *mark \"a\"*\n    - more useful in the context of an *ex command*\n- **`** and **'** will both jump to marks. **'** will take you to the line, and **`** will shoot you to the extact spot.\n\n## Automatic marks\n- vim automatically sets up some marks for us:\n\n| Keystroke | Buffer Contents                               |\n| --------- | --------------------------------------------- |\n| \"         | Position before last jump within current file |\n| 0         | Position of cursor when the file was last closed|\n| [         | Start of last change/yank                     |\n| ]         | End of last change/yank                       |\n| <         | Start of last visual selection                |\n| >         | End of last visual selection                  |\n| 1, 2..    | latest position of cursor in last file opened |\n","n":0.072}}},{"i":86,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Delete til mark\n1. Mark the part you want to delete until (`a` used here; of course any can be used) - `ma`\n2. Go to the part you want to delete from\n3. Run `d` `'` `a`\n","n":0.167}}},{"i":87,"$":{"0":{"v":"Modes","n":1}}},{"i":88,"$":{"0":{"v":"Visual Mode","n":0.707},"1":{"v":"\nreselect last v-mode selection\n- `gv`\n\ngo to the *other* end of the v-mode selection\n- `o`\n\nwhen using *dot command* to repeat a v-mode command, it acts on the same amount of text as was marked by the recent visual selection\n\n## Tips\n- use visual block mode to create multiple cursors to edit multiple places at once\n    - ***ex.*** - to append `;` to end of three lines (hence the `jj`), `<C-v>jj$A;`\n- when visually selecting by motion (ex. `vi(`), you can increase the level of enclosing parens that you navigate\n    - for example, if we have nested parens, we can start off with the above command, then say `i(` to go an additional layer\n- When using visual block to edit multiple places, you must use a command that puts you in i-mode(eg. `c`, `I`, `A`)\n    - note the capitalization of `I` and `A`. This shows that in visual block mode, the commands to \"insert at start\" and \"insert at end\" consider the demarcations to be what is within the visual block (instead of the start/end of the entire line, as is normally the case)\n","n":0.075}}},{"i":89,"$":{"0":{"v":"Insert Mode","n":0.707},"1":{"v":"\n- vim records keystrokes until we leave *i mode*\n- moving around in *i mode* resets the change, meaning once we exit *i mode* and undo with `u`, it will undo all changes made in that *i mode*.\n\n## Insert-Normal Mode\n- use *insert normal mode* from within insert mode for a one shot command within normal mode (then return to i mode)\n","n":0.129}}},{"i":90,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n## Modifying text from Insert-mode\n- CTRL-h (delete one character left)\n- CTRL-w (delete word left)\n- CTRL-u (clear line)\n","n":0.243}}},{"i":91,"$":{"0":{"v":"Cmdline","n":1},"1":{"v":"\n### Command line Completion\nWhen executing a command line command, use `<C-d>` to see suggestions based on the text you've already typed\n- doing this will give us a list of all the commands containing the string we've already typed\n- ex. `:help CTRL-W`, then type `<C-d>` to see all commands with `CTRL-W`\nAlternatively we can `<TAB>` to get the completion window\n\n* * *\n\n## Executing Shell commands\n### Running commands without using output\nPrecede any shell command with `!` in the command prompt to have it interpreted as a shell command.\n\n### Backtick expansion\nIf we wrap text in backticks, vim will run the enclosed command using our configured shell, and use its stdout as the argument for the given vim command\n    `` :args `cat files.txt` ``\n- if files.txt is just a list of files, then vim will add all those files to the argument list\n\n## Getting pwd/filename\n- `%` refers to filepath of current buffer\n    - if our current buffer was 3 levels deeper than our pwd, we could open a sibling (same level) file with `:e path/to/file`, or we could just say `:e %:h<Tab>`, which would auto complete the folder of the current buffer\n        - `:h` here removes the filename from the path\n        - `%:h` has been aliased to `%%`\n\n## Functions\nwe can call functions like so:\n- `:call MyFunction()`\n","n":0.069}}},{"i":92,"$":{"0":{"v":"Global","n":1},"1":{"v":"\nGlobal allows us to perform some Ex command on a range (default whole buffer)\n- ex. delete all lines that have the word \"require\" in it\n`:g/require/d`\n\nnote: an Ex command is one starting with a colon such as `:d` for delete\n\n# UE Resources\n[Some use-cases for global command](https://dev.to/iggredible/discovering-vim-global-command-49ad)\n","n":0.149}}},{"i":93,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Perform operation on whole file\nDelete all content of file `:%d`\n\n#### Perform operation on whole line\nDelete all content of line `:.d`\n\n#### Perform operation in a range\nDelete from line 10 to EOF `:10,$d`\n\n#### Perform operation from current line (`.`)\n`:.,+2d`\n","n":0.162}}},{"i":94,"$":{"0":{"v":"Inner","n":1},"1":{"v":"\n### viminfo file\nThe viminfo is like a cache, to store cut buffers persistently\n- If you exit Vim and later start it again, you would normally lose a lot of\ninformation.  The viminfo file can be used to remember that information, which\nenables you to continue where you left off.\n- Vim writes this file for us.\n\nThe viminfo file is used to store:\n- The command line history.\n- The search string history.\n- The input-line history.\n- Contents of non-empty registers.\n- Marks for several files.\n- File marks, pointing to locations in files.\n- Last search/substitute pattern (for 'n' and '&').\n- The buffer list.\n- Global variables.\n","n":0.102}}},{"i":95,"$":{"0":{"v":"Text Object","n":0.707},"1":{"v":"\nOne of the keys to using Vim effectively is trying to edit by text object\n\nA text object is a defined region of text that is defined by the motions, and modifier of that motion (eg. `vaw`, or `v2aw`)\n    - The definition of the text's scope depends on what vim commands we are using\n        - ex. When we say `vi\"` we are defining a text object, whose definition is \"everything inside `\"\"`\". Text objects define regions of text by some sort of structure.\n- text objects are good because we don't have to worry about where in the word the cursor is located. as a result, they work well with `.` \n\n## Types\n- Word (`w`)\n- Sentence (`s`)\n\n## Extended Text Objects (Plugins)\n### CamelCaseMotion\nDefines a text object that is a single word in a larger camel-case word\n- Delete inside the camel-case word\n    - `di,w`\n\n### VimTextObject\nDefines a text object that is a function argument/parameter\n- Delete inside the argument\n    - `dia`\n\n# UE Resources\n[info](https://blog.carbonfive.com/2011/10/17/vim-text-objects-the-definitive-guide/)\n","n":0.08}}},{"i":96,"$":{"0":{"v":"Register","n":1},"1":{"v":"\n([info](https://www.brianstorti.com/vim-registers/))\n- a space in memory with an identifier that vim uses to store text\n    - analogous to a clipboard\n- access register `a` - `\"a`\n    - access register in *i mode* - `<C-r>` (instead of `\"`)\n- vim has a default register, which is where any deleted or yanked text will be\n    - denoted `\"\"` - so `p` is shorthand for `\"\"p`\n    - this is more like copy/paste\n- when writing to a register, lowercase letters will overwrite the register, while uppercase will append to the register\n- Sequential register\n    - `\"1` is most recent (or `\"\"`), `\"2` is second most recent, and so on\n    - this is only for `dd` and `yy` operation\n- Yank register\n    - `\"0`\n- Blackhole register\n     - `\"_` - prepending a command with this will prevent anything from entering a register.\n- Clipboard register\n    - `\"+`/`\"*` - gives access to the system clipboard\n- Expression register\n    - `\"=`\n    - when fetching contents from the register, we enter command-line moe\n    - an exception to the idea that \"registers are containers that hold a block of text\"\n- use `:reg` to see all registers\n    - `:reg a b c` will show registers `a b c`\n- copy text into register `r`, then paste it - `\"ry`, `\"rp`\n- *Yank Register* - when yanking, the copied text is placed into the yank register (`\"0`), as well as the default register.\n- *Delete/yank Register* - `\"-`\n    - only small delete (ie. no `dd`)\n- when using visual mode to make a selection of text that we will replace with `p`, the highlighted text will go into the register as the *pasted* text exits\n","n":0.062}}},{"i":97,"$":{"0":{"v":"Buffer","n":1},"1":{"v":"\na part of vim's memory that can hold text\n- This is normally in the form of:\n    - an actual file\n    - text stored temporarily (yank, paste etc)\n- when opening an existing file, the text of that file is copied and put into a new buffer. When we save that text, the original file is replaced by writing the buffer to disk\n- buffer list vs argument list\n    - argument list is subset of buffer list\n    - buffer list can be seen as more random access, while argument list could have more organization to it\n        - if you are working with only a few files in your current micro session, you'd have just those ones in the arg list, while having many more in the buffer list\n    - idea is that we might have lots of buffers open, and want to execute a macro across many files, but not all buffers.\n        - one solution is to prune the buffer list, but the most practical solution is to populate the argument list with just the files we are interested in. \n","n":0.075}}},{"i":98,"$":{"0":{"v":"Help","n":1},"1":{"v":"\n# Overview\nThe Vim documentation consists of two parts:\n1. The User manual\n   Task oriented explanations, from simple to complex.  Reads from start to\n   end like a book.\n2. The Reference manual\n   Precise description of how everything in Vim works.\n\nIf you only have a generic keyword to search for, use `:helpgrep` and open the quickfix window:\n```\n:helpgrep quickfix\n:copen\n```\n\n# How to...\n\n## Navigate\nJump to a subject\n- put cursor on a subject tag (blue text), and hit `<C-]>`\n    - In fact, this works on any word\n- return with `<C-o>`, or `<C-t>`\n\n## Get specific help\nwe can prepend a binding with the mode to see what it does, and get help on it\n- ex. `:help v_u` to get help on what `u` command does in visual mode.\n- ex. `:help CTRL-W_CTRL-I`\n- ex. `:help i_CTRL-G_<Down>`\n- ex. `:help i^W`\n\nprefixes:\nvisual mode - `v_`\ninsert mode - `i_`,\nEx mode - `:`\n- normal mode has no prefix\n\n\n# UE Resources\n[Guide to using vim help](https://vi.stackexchange.com/questions/2136/how-do-i-navigate-to-topics-in-vims-documentation)\n","n":0.082}}},{"i":99,"$":{"0":{"v":"Vim-Fugitive","n":1},"1":{"v":"\n## Gstatus\nor simply `:G`\n`<C-n>`/`<C-p>` - navigate files\n`-` stage/unstage the file that you are hovering over\n- can use visual mode\n\n`p` - run `git add --patch` for the file under cursor\n- splits file into hunks of changes and allows you to pick which parts to stage for commit\n- not as useful, since we can just use `diffget` as described below.\n\n`cc` - commit\n`D` - open file under cursor as a diff\n`U` - unstage a file (\"I've staged a file and want to remove it from the index\")\n`X` - untrack a file (\"I've changed a file in the working tree and want to discard all changes\")\n  - `git checkout -- <filename>`\n\n`]c`/`[c` - navigate between hunks when in vimdiff window\n\n## Gblame\n`A` resizes to end of author column\n`C` resizes to end of commit column\n`D` resizes to end of date/time column\n`<CR>` opens the patch that was applied by that commit\n\n## Reconciling differences between working copy and index version\n- `:Gw[rite]` - write (save) the current file and stage it\n- `:Gr[ead]` - read the index version of the file into the current buffer\n\t- good for seeing what changes were made (that haven't been staged yet)\n\n- `Gwrite` and `Gread` are opposites. Running `Gread` in the working file will do the exact same thing as running `Gwrite` in the index file +vv (both cause the working copy to revert to the index version)\n  - Both of these commands reconcile differences between the working copy and\n      the index version of a file by overwriting the contents of one buffer.\n- We can be more granular with our reconciliation by using vim built-in `diffget`/`diffput` (alias `do`/`dp`, respectively. These aliases also run :diffupdate afterwards, to fix coloring issues)\n  - While running `:Gdiff`, if we are in the index window and position the\n     cursor over a hunk of text and run `diffget`, it will pull that hunk into\n     the working copy of the file.\n  - If there are changes in the working copy (right side) that you'd like to move to the index, you can visually select those lines and run `diffput`\n    - this opens a new buffer with just the hunks you've added. we need to save this buffer before we `\n  - `diffget` makes changes in the currently active buffer, while `diffput` makes changes in the inactive buffer (by drawing in differences from the active buffer)\n![](/assets/images/2021-03-08-21-36-16.png)\n  - run `:diffupdate` if colors get messed up as a result of doing this\n- `:Gedit :0` - open index version of current file (remember: staging a file\n    updates the index version of the file (?))\n- `:Gremove` - wipe out the buffer and run `git remove` on the current file\n- `:Gmove` - rename the current file\n  - does a whole bunch more than `git move`, so it's not a direct replacement\n  - ex. `:Gmove new-file-name` - (relative to the path of the current file)\n- jump between conflicting regions with `[c`/`]c`\n\n## Merge conflicts\n- open a file with merge conflicts and run `:Gdiff`\n\t- might need to run `Gdiffsplit!` to get 3 windows\n- The windows shown are:\n\t1. Target Branch - The branch we are merging into (normally master)\n\t2. Working Tree - The file as it currently is (with changes visible from both target branch and merge branch)\n\t3. Merge Branch - The branch we are introducing\n- if using `diffget` and want to accept changes from the target branch (likely master), run `:diffget //2`. If we wan to accept changes from merge branch (likely feature-branch), run `:diffget //3`\n\n# Diff\nLeft window is the commit we are comparing against, and right window is the current working copy\n\n### Colors\nred - lines were deleted\ngreen - lines were added\nteal - lines were modified\n\n- knowing colors, we can easily notice if something has actually been removed, or if it was just moved/has a new structure\n  - In this case, you'd find the green code in the left hand side, and try and match it up with a green chunk on the righthand side. If we find a match, then we know the code format just changed, and that no actual code was deleted.\n\n## Seeing diff between 2 versions of a file\n`:Gdiff <sha>`\n\n`:Gdiff :0`\n- diff between current file and staged version\n\n`:Gdiff !~3`\n- diff between current file and 3 commits ago\n\n\n### UE Resources\n- http://vimcasts.org/episodes/fugitive-vim-resolving-merge-conflicts-with-vimdiff/\n- [cheatsheet](https://gist.github.com/mikaelz/38600d22b716b39b031165cd6d201a67)\n","n":0.038}}},{"i":100,"$":{"0":{"v":"Feat","n":1}}},{"i":101,"$":{"0":{"v":"Macros","n":1},"1":{"v":"\nAllow us to record and number of keystrokes into a register\n- ideal for repeating changes over a set of similar lines, paragraphs and files.\n- macros allow us to easily append commands to the end of an existing one\n    - for complex ones, we can paste the macro into a document, edit it, then yank it back into the register\n- golden rule - ensure every command in a macro is repeatable\n- before starting to record a macro, ask: where am I, where did I come from, where am I going?\n    -  before doing anything make sure the cursor is positioned to the next command does as we'd expect, and where we'd expect\n- any motions that fail cause the macro to fail.\n    - ex. - pressing `k` on line 1 does nothing, so it would fail if we tried.\n- `@@` - repeat last macro\n\nWhen you are making a macro, imagine you are looping a piece of paper back onto itself. The edge you start at must touch the edge you finish at. This is the same concept as making macros: After identifying a repetitive action, define the start point of a single action, start recording and do the stuff, then finish the macro when you are at the start of the next action.\n\n## Tips\n\nWhen using macros to perform a repetitive action on multiple lines, it's a good idea to make the first action to move our cursor to the starting position (ie. the edge in the paper loop analogy)\n- ex. we can start with `0` to go to the start of the line\n","n":0.062}}},{"i":102,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### Run a macro on all lines that match a pattern\n","n":0.302}}},{"i":103,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n- `qa` - start recording into register `a`\n- `q` - stop recording\n- `@a` - execute macro stored in register `a`\n- `@@` - execute the most recently executed macro\n- execute macro on each line in selection - `:'<,'>normal @q`\n","n":0.162}}},{"i":104,"$":{"0":{"v":"Jump List","n":0.707},"1":{"v":"\nVim remembers the locations you have recently visited (where you jumped from). Each position (file name, column number, line number) is recorded in a jump list, and each window has a separate jump list that records the last 100 positions where a jump occurred.\n- The commands that are regarded as \"jumps\" include searching, substitute and marks. Scrolling through a file is not regarded as jumping.\n\nLike a web browser, you can go back, then forward:\n- Press Ctrl-O to jump back to the previous (older) location.\n- Press Ctrl-I (same as Tab) to jump forward to the next (newer) location.\n\nuse `:jump` to see the jumplist\n","n":0.099}}},{"i":105,"$":{"0":{"v":"Change List","n":0.707},"1":{"v":"\nVim remembers the locations where changes occurred. \n- Each position (column number, line number) is recorded in a change list, and each buffer has a separate change list that records the last 100 positions where an undo-able change occurred.\n- if you make a change at a certain position, then make another change nearby, only the location of the later change is recorded \n    - \"nearby\" means on the same line, within a certain number of bytes.\n\n## Usage\n- Type g; to jump back to the position of the previous (older) change.\n- Type g, to jump to the position of the next (newer) change.\n\nType `:changes` to view the list\n","n":0.097}}},{"i":106,"$":{"0":{"v":"Config","n":1}}},{"i":107,"$":{"0":{"v":"Viminfo","n":1},"1":{"v":"\n```\nset viminfo=%,<800,'10,/50,:100,h,f0,n~/.vim/cache/.viminfo\n\"           | |    |   |   |    | |  + viminfo file path\n\"           | |    |   |   |    | + file marks 0-9,A-Z 0=NOT stored\n\"           | |    |   |   |    + disable 'hlsearch' loading viminfo\n\"           | |    |   |   + command-line history saved\n\"           | |    |   + search history saved\n\"           | |    + files marks saved\n\"           | + lines saved each register (old name for <, vi6.2)\n\"           + save/restore buffer list\n```\n`set viminfo=xxx` should come after `set nocompatible`\n\nif we set `'` parameter, then marks and other things set during the buffer editing will persist even after we close vim.\n","n":0.101}}},{"i":108,"$":{"0":{"v":"Runtime Path","n":0.707},"1":{"v":"\n# Runtime path\n- vim looks for scripts in various directories. The directories that vim will look in is known as the `runtime path`\n- to see it, type `:set runtimepath?`\n\n# Runtime directory\nAssuming that you're using some flavor of Unix, your personal runtime directory is ~/.vim. This is where you should put any plugin used only by you.\nYou should not install any plugins into the $VIMRUNTIME directory. That directory is intended for plugins distributed with Vim.\n","n":0.116}}},{"i":109,"$":{"0":{"v":"Mapping","n":1},"1":{"v":"\n- `:map` - recursive version of mapping command\n- `:noremap` - non-recursive version of mapping command\n    - \"Recursive\" means that the mapping is expanded to a result, then the result is expanded to another result, and so on.\n- below, `j` will be mapped to `gg`. `Q` will also be mapped to `gg`, because `j` will be expanded for the recursive mapping. `W` will be  mapped to `j` (and not to `gg`) because `j` will not be expanded for the non-recursive mapping.\n    - The expansion stops when one of these is true:\n        1. the result is no longer mapped to anything else.\n        2. a non-recursive mapping has been applied (i.e. the \"noremap\" [or one of its ilk] is the final expansion).\n```vim\n:map j gg\n:map Q j\n:noremap W j\n```\n- since VIM is a **modal editor**, there are corresponding mapping commands for each mode\n    - ***ex.*** - `:imap`, `:nmap`, `:map!` (*i mode* and command line)\n\n- Recursive map - the rhs says to the lhs \"if you get triggered, when I get triggered. It stops with me, and nothing past me gets triggered\"\n- Non-recursive map (nore)- the rhs says \"When you get triggered, I may server as the lhs of another mapping that will trigger something else.\"\n- map is the \"root\" of all recursive mapping commands. \n    - The root form applies to \"normal\", \"visual+select\", and \"operator-pending\" modes. \n- \"Recursive\" means that the mapping is expanded to a result, then the result is expanded to another result, and so on.\n- The expansion stops when one of these is true:\n    1. the result is no longer mapped to anything else.\n    2. a non-recursive mapping has been applied (i.e. the \"noremap\" [or one of its ilk] is the final expansion).\n- At that point, Vim's default \"meaning\" of the final result is applied/executed.\n- \"Non-recursive\" means the mapping is only expanded once, and that result is applied/executed.\n    - Example:\n```vim\nnmap K H\nnnoremap H G\nnnoremap G gg\n```\n- The above causes K to expand to H, then H to expand to G and stop. It stops because of the nnoremap, which expands and stops immediately. The meaning of G will be executed (i.e. \"jump to last line\"). At most one non-recursive mapping will ever be applied in an expansion chain (it would be the last expansion to happen).\n- The mapping of G to gg only applies if you press G, but not if you press K. This mapping doesn't affect pressing K regardless of whether G was mapped recursively or not, since it's line 2 that causes the expansion of K to stop, so line 3 wouldn't be used.\n","n":0.048}}},{"i":110,"$":{"0":{"v":"Autocmd","n":1},"1":{"v":"\nAutocommands are a way to tell Vim to run certain commands whenever certain events happen.\n","n":0.258}}},{"i":111,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n- when an operator command is invoked twice, it means it is operating on the current line\n    - `dd`, `yy`, `>>`\n","n":0.218}}},{"i":112,"$":{"0":{"v":"Surround","n":1},"1":{"v":"\nsurround in visual mode with `()` \n- `Sb`\n\nchange surrounding `''` with `\"\"` \n- `cs'\"`\n\nchange `<p>` with `<b>` \n- `cst<b>`\n\ndelete `\"\"` surroundings \n- `ds\"`\n\ndelete tags \n- `dst`\n\nsurround parentheses content with `{}` \n- `ysib{`\n    - *mn.* - \"you surround!\"\n\nsurround line with `\"\"` \n- `yss`\n\nsurround 3 words with `\"\"` \n- `v3eS\"`\n\ndelete the second outer pair of brackets \n- `2ds}` \n","n":0.132}}},{"i":113,"$":{"0":{"v":"Usenet","n":1},"1":{"v":"\nWith usenet, you are downloading a file directly from a server. The speed of your download, assuming a good provider, will almost certainly be the maximum your ISP allows.\n\nAnother point to mention is that with torrents, access to certain files can be limited to the specific torrent sites you are using. With usenet, when a file gets uploaded to a group, every indexer that is indexing that group has access to the same file. Whether a particular file gets posted or not, and whether certain obfuscation can be read, is up to the automation process of the indexer.\n\nBefore you are able to download anything with Usenet, you will need 3 things:\n1. Provider\n2. NZB Indexer\n3. A Usenet client\n\n### Providers\nThe provider grabs the files you need from your search by granting you access to the Usenet servers that host the files.\n- [List of providers](https://www.reddit.com/r/usenet/wiki/providers/)\n\n- ex. SABnzbd\n\n### Indexers\nThink of these as your search engines. These are the sites that will tell you where all the files for whatever thing you are looking for are (e.g NZB files)\n- ex. NZBGeek, DrunkenSlug, and NinjaCentral \n\nThe indexer information gets input into Sonarr/Radarr\n\n### Usenet client\nOnce the NZB file is obtained via the Indexer, it is loaded into the usenet client. The client then connects to the Usenet server, downloads the required files, and may also handle the decoding of files that are often split into multiple parts or encoded for transmission efficiency.\n\n- ex. NZBGet\n\nProvider information gets put into the download client\n\n### NZB Files\nInstead of browsing Usenet directly, users often rely on NZB files. An NZB file is a small XML format file that contains information about the files available for download on Usenet","n":0.06}}},{"i":114,"$":{"0":{"v":"Unix","n":1},"1":{"v":"\n## Philosophy\nIn Unix philosophy, \"everything is a file\"\n- ex. sockets (both have functions to read, write, open, close etc)\n\n## Unix Domain Socket\n- unix domain sockets allow us to exchange data between processes on the same host machine.\n- a.k.a IPC socket\n","n":0.158}}},{"i":115,"$":{"0":{"v":"Symlink","n":1},"1":{"v":"\nSoft symlinking a directory will not link the contents as well;\n\nA symbolic or soft link is an actual link to the original file, whereas a hard link is a mirror copy of the original file. If you delete the original file, the soft link has no value, because it points to a non-existent file.\n\nBut in the case of hard link, it is entirely opposite. Even if you delete the original file, the hard link will still have the data of the original file, because hard links act as a mirror copy of the original file.\n\nIn a nutshell, a soft link...\n- can cross the file system,\n- allows you to link between directories,\n- has different inode number and file permissions than original file,\n- permissions will not be updated,\n- has only the path of the original file, not the contents.\nA soft link is something like a shortcut in Windows. It is an indirect pointer to a file or directory. Unlike a hard link, a symbolic link can point to a file or a directory on a different filesystem or partition.\n\nA hard Link...\n- can't cross the file system boundaries (i.e. A hardlink can only work on the same filesystem),\n- can't link directories,\n- has the same inode number and permissions of original file,\n- permissions will be updated if we change the permissions of source file,\n- has the actual contents of original file, so that you still can view the contents, even if the original file moved or removed.\nYou can think a hard link as an additional name for an existing file. Hard links are associating two or more file names with the same inode . You can create one or more hard links for a single file. Hard links cannot be created for directories and files on a different filesystem or partition.\n","n":0.058}}},{"i":116,"$":{"0":{"v":"Streams","n":1},"1":{"v":"\n## What is it?\n- Streams are used to read or write input into output sequentially\n- What makes streams unique, is that instead of a program reading a file into memory all at once like in the traditional way, streams read chunks of data piece by piece, processing its content without keeping it all in memory.\n\t- This makes streams really powerful when working with large amounts of data, for example, a file size can be larger than your free memory space, making it impossible to read the whole file into the memory in order to process it. That’s where streams come to the rescue!\n- Streams are a way to handle reading/writing files, network communications, or any kind of end-to-end information exchange in an efficient way\n- streams are not only about working with media (ie. streaming movies/music) or big data. They also give us the power of ‘composability’ in our code\n\t- it’s possible to compose powerful pieces of code by piping data to and from other smaller pieces of code, using streams.\n\nit is best to think of a Stream as an established connection to some input, some output, or both. A lot of things can muddy this definition of stream, but that is the fundamental idea... A connection to some input or some output or some I/O endpoint that can handle both input and output.\n- ex. one might have a readable stream to a file that one wants to get the contents of. In this case the stream is an input connection to the file. The input stream doesn't hold the contents of the file, but rather holds the connection to the file. It is up to the program to then read and store the contents (assuming that is the desired operation).\n\t- One place a stream might store the content is in a [[Buffer|memory.buffer]] object (if we were using Node).\n\n## Why use streams over other data handling methods?\n1. Memory efficiency: you don’t need to load large amounts of data in memory before you are able to process it\n2. Time efficiency: it takes significantly less time to start processing data as soon as you have it, rather than having to wait with processing until the entire payload has been transmitted\n\n## Standard Streams\n- communication channels between a program and its environment (ie. the host that executes it).\n- stdin and stdout are inherited from the parent process, though it can be redirected.\n- anal: The Services menu, as implemented on Mac OSX, is analogous to standard streams. On this operating systems, graphical applications can provide functionality through a systemwide menu that operates on the current selection in the GUI, no matter in what application.\n- The correct way to think about redirection is that one redirects output to a file (`>`), and one redirects a file's contents to stdout (`<`).\n\n### File Descriptor (FD)\n- A FD is a number that uniquely identifies an open file in a computer's operating system\n\t- A file can be thought of simply as a resource. In other words, something that provides data. A file as we know it fulfills this definiion, but also consider that streams do as well. Therefore, a file in this sense is more general, and simply stands for \"a resource that provides data\".\n- A FD gives us the means to access a file, or perhaps more interestingly, a standard stream\n\t- therefore an FD is a reference to the standard stream\n- ex. The file descriptor for standard input has id = 0\n- Part of what makes Unix so well designed is that the common interface between so many commands is a simple file descriptor. If we expect the output of one program to become the input to another program, then those programs must use the same data format (to get a compatible interface). In Unix, that interface is a file descriptor, which is just a sequence of bytes.\n\t- ex. `awk`, `sort`, `uniq`, and `head` all treat their input file as a list of records separated by `\\n`\n- Because the file descriptor is such a simple interface, so many different things can be represented with that interface.\n\t- an actual file on the filesystem\n\t- a communication channel to another process (Unix socket, stdin, stdout)\n\t- a device driver (e.g. `/dev/audio`, `/dev/lp0`)\n\t- a socket representing a TCP connection\n\nA file descriptor is associated with an input/output resource, which would include regular files, network sockets, and pipes\n<!-- - except for redirections with `<&`, any time we do a redirection, -->\n\n### STDIN\nStandard input is a stream from which a program reads its input data.\n- the program requests this data transfer by using the *read* operation\n- ex. the keyboard is the stdin to a text editor, and also to an interactive shell.\n\nWhen we redirect with `<`, implicitly we are running `1<`\n\n### STDOUT\nStandard output is a stream to which a program writes its output data.\n- the program requests this data transfer by using the *write* operation\n\n### STDERR\n- having 2 different output streams is analogous to functions that return a pair of values\n- The terminal that executes the program is the default stderr destination.\n\t- sometimes stdout is redirected, so the fact that stdout and stderr are different streams allows stderr to continue outputting its streams elsewhere than stdout.\n\t\t- ex. when we pipe the output of one program into another. Imagine we were piping data between different hosts. If there was a single stream, it would get passed along the chain. Now with stderr, we are able to see stderr in the executing terminal and trace down the location of the logs to find the offending program.\n\n#### 2>&1\n- we can append `2>&1` on any unix command to redirect stderr to the same destination as stdout\n\t- often, we see `2>/dev/null`, which means \"redirect stderr to `/dev/null`, a blackhole\"\n\t- ex. the **find** utility displays both stdout and stderr to the terminal. If we append the command with blackhole redirection, then only the stdout will be shown.\n- placement of `2>&1` is critical to determining its meaning. Consider the following variants of the same command:\n\n- `|&` is shorthand for `2>&1 |`\n- `echo hello > stuff.txt` === `echo hello 1> stuff.txt`.\n\t- ie. the `1` (stdout) is implicit. we are saying \"redirect the stdout of the echo command to `stuff.txt`\"\n- `0` means stdin, `1` means stdout, `2` means stderr\n- in the context of redirections, `&` means \"the following is a file descriptor (ie. the type of standard stream) and not a filename\". Without `&`, we would be writing to a file named `1`\n```\n// stderr goes to terminal, and nothing gets written to outfile\ncommand_does_not_exist 2>&1 > outfile\n\n// stderr goes to stdout, and gets written into outfile\ncommand_does_not_exist > outfile 2>&1\n```\nIn the first example, when we encountered `2>&1`, we were saying \"pipe all stderr into the same stream as stdout\". At this point in the command, stdout was simply the terminal (spec: since that is the default stdout of a command). Since stdout was the terminal at this point, we also declared that stderr should point to the terminal. In this way, `2>&1` doesn't cause stderr to equal stdout. It merely points it the same way stdout is facing. Nothing is stopping stdout from changing direction later on, which is exactly what happens. When we write `> outfile`, we are declaring that stdout should be `outfile`.\n\nIn the second example, we see that by the time we evaluate `2>&1`, stdout is `outfile`. This causes stderr to also point at `outfile`, hence is why in this example, the file is populated with `command not found: command_does_not_exist`\n- the `2>&1` operator points file descriptor 2 to the same target file descriptor 1 is pointing at.\n\n## Special File\n- spec: In Unix, the most comfortable means for programs to communicate is via files. It is easy to set up standard streams with files, so it is an appropriate level of abstraction to communicate with the Unix system. For this reason, Unix has this concept of a *special file*, which is a file that represents something that is not a file at all.\n- ex. consider a partition of a hard drive that \"exists\" in the FS at `/dev/sda3`. In reality, this is just how the filesystem \"knows of\" the partition. The `sda3` file enables the partition to communicate with the Unix system via standard streams.\n","n":0.027}}},{"i":117,"$":{"0":{"v":"Pipe","n":1},"1":{"v":"\nWe're able to pipe the output of one command into the input of another command precisely because these commands all implement the same interface: a file descriptor.\n\n### Tips\nYou can end the pipeline at any point, pipe the output into less, and look at it to see if it has the expected form. This ability to inspect is great for debugging.\n\nYou can write the output of one pipeline stage to a file and use that file as input to the next stage. This allows you to restart the later stage without rerunning the entire pipeline.","n":0.103}}},{"i":118,"$":{"0":{"v":"Netrc","n":1},"1":{"v":"\nThe `.netrc` file contains configuration and autologin information for the ftp client \n- It resides in the user's home directory\n\n`.netrc` file takes the form:\n> `remote-machine` `name`\n\nThe auto-login process searches the `.netrc` file for a machine token that matches the remote machine specified on the ftp command line\n- Once a match is made, the subsequent `.netrc` tokens are processed, stopping when the end of file is reached or another machine or a default token is encountered\n\nThe following three lines must be included in the file. The lines must be separated by either white space (spaces, tabs, or newlines) or commas:\n\n```\nmachine <remote-instance-of-labkey-server>\nlogin <user-email>\npassword <user-password>\n```\nAn example:\n```\nmachine mymachine.labkey.org\nlogin user@labkey.org\npassword mypassword\n```\nor:\n```\nmachine mymachine.labkey.org login user@labkey.org password mypassword\n```\n\n### Using API Keys\nWhen API Keys are enabled on your server, you can generate a specific token representing your login credentials on that server and use it in the netrc file. The \"login\" name used is \"apikey\" (instead of your email address) and the unique API key generated is used as the password\n\n* * *\n\nNote: the `.netrc` file only deals with connections at the machine level and should not include a port or protocol designation, meaning both \"mymachine.labkey.org:8888\" and \"https://mymachine.labkey.org\" are incorrect.\n","n":0.072}}},{"i":119,"$":{"0":{"v":"Unix Man Pages","n":0.577},"1":{"v":"\n## Formatting a command\n- anything in `[]` brackets indicates optional\n- `--` - signify end of command options. Only positional params are accepted after this point, such as which file will be targeted for the operation.\n    - ex. say we want to grep for the string `-v`. If we just executed `grep -v file.txt`, the `-v` would be interpreted as an argument on grep. If we execute `grep -- -v file.txt`, `--` tells us \"ok, that's it. No more arguments accepted\". Since the section after the args section is the pattern section, `-v` gets interpreted as a pattern.\n\n### Valid command form\n- angle brackets for required parameters: `ping <hostname>`\n- square brackets for optional parameters: `mkdir [-p] <dirname>`\n- ellipses for repeated items: `cp <source1> [source2…] <dest>`\n- vertical bars for choice of items: `netstat {-t|-u}`\n\n### Unix built-ins \nuse the `manbash` command to see man pages for built-ins (e.g. `cd`, `eval`)\n- [source](https://unix.stackexchange.com/questions/18087/can-i-get-individual-man-pages-for-the-bash-builtin-commands)","n":0.082}}},{"i":120,"$":{"0":{"v":"Unix Filesystem","n":0.707},"1":{"v":"\nThe UNIX filesystem unifies all physical hard drives and partitions into a single directory structure. It all starts at the top–the root (`/`) directory. All other directories and their subdirectories are located under the single Linux root directory. This means that there is only one single directory tree in which to search for files and programs.\n\n## Types of files\n- ordinary file\n- directory\n- special (device) file - represents a physical device (ex. printer).\n\t- special files are used for sending outputs to the device, and receiving inputs\n- pipe - a temporary file that gets created when we  commands into each other. Pipes are the mechanism through with programs chain together.\n- socket - a stream of data very similar to network stream, but all the transactions are local to the filesystem.\n- symbolic link\n\n## Filesystem metadata\n- The general file system model, to which any implemented file system needs to be reduced, consists of several well-defined metadata entities: `superblock`, `inode`, `file`, and `dentry`\n\t- they are metadata about the filesystem\n- each entity here interacts using the VFS, and each entity is treated as an object.\n- each entity has its own data structure, and a pointer to a table of methods \n\n## Mount Point\n- a mount point is a special directory within the unix filesystem. It is special because when an external filesystem is mounted (ex. HDD, SSD, USB, SD), its contents are dumped into the mount point, which is accessible from the root folder (`/`)\n- ex. `/media`\n- so when we run `mount /dev/hda2 /media/photos`, we are saying \"hey, I want you take all of the contents of `/dev/hda2` (partition 2 of HDD), and make it all accessible through `/media/photos`\".\n\t- spec:think of it like a map, where it will take the external file system on the USB (in FAT or NTFS format) and it will make all of the contents visible within the UNIX filesystem\n\t- to carry out this \"mapping\", unix has filesystem drivers \n\n## Directory structure\n`/bin` - binaries generally needed by all users of the system.\n`/lib` - contains system libraries\n`/usr` - contains executables, libraries, and shared resources that are not system critical (ex. X Window)\n\t- stands for Unix System Resources (not *user*, as is commonly thought)\n`/usr/bin` - stores binaries distributed with the OS, that aren't in `/bin` \n`/usr/lib` - stores libraries for programs in `/usr`\n`/dev` - contains file representations of peripheral devices. Therefore, to view files, the external device needs to be mounted (see Mount Point below) \n`/media` - default mount point for removable devices\n`/mnt` - default mount point for temporary filesystems\n`/etc` - contains system-wide config files/system databases.\n`/proc` - contains all running processes displayed as their own files.\n`/tmp` - files that get cleaned frequently. Often, this directory gets cleared on reboot. \n`/var` - contains files that get changed often. usually where files go that are not managed by the system-wide package manager\n`/var/mail` - contains all incoming mail\n`/var/www/html` - default root folder of the web server (e.g. Apache)\n\n### Subdirectories\n`sys/` - operating system kernel, handling memory management, process scheduling, system calls etc.\n","n":0.045}}},{"i":121,"$":{"0":{"v":"File Permissions","n":0.707},"1":{"v":"\nThere are 3 permission groups: Owner, Group, Other\n- each permission group has 3 permissions, called a permission set\n\t- ex. `rw-` is a single set\n- each file/directory has 3 permission sets— one for each permission group.\n- rwx mean different things if we are referring to a directory or a file\n\t- `r` \n\t\t- on file means we can read the contents \n\t\t- on directory means we can run `ls`\n\t- `w` \n\t\t- on file means we can modify file contents\n\t\t- on directory means we can add/delte files \n\t- `x`\n\t\t- on a file means we can run it\n\t\t- on a directory means we can `cd` into it \n\n\n### File ownership\n- every file is owned by a specific user (`UID`) and a specific group (`GID`)\n\t- `chown` is used to change both\n\t\t- ex. `chown <user>:<group> test.txt`\n- each member can belong to many groups (`/etc/group`), though a user can only have one primary group (`/etc/passwd`).\n\t- run `$ id` to see the groups the current user belongs to\n\t- When a user creates a file, the file will be owned by the primary group\n- similar to how we need to source the `.zshrc` before changes are live, we need to log out and log back in before group membership is \"activated\"\n\n### Settings Permissions\nThe numerical method is quite easy. For example, we can just replace each `rwx` set by it's binary positional value (from RTL: 1, 2, 4, 8, 16, 32...) and add the the numbers in each set.\n```\n-(rw-)(rw-)(r--)\n-(42-)(42-)(4--)\n664\n```\n- By this, we can define a `7` as `rwx`, a `5` as `r-x`, and so on\n\n* * *\n\nSet new files/directories in a subdirectory to follow the group ownership of the specified directory\n- `chmod g+s /var/www/my-project`\n\n[Unix permissions guide](https://support.plex.tv/articles/200288596-linux-permissions-guide/)\n","n":0.06}}},{"i":122,"$":{"0":{"v":"inode","n":1},"1":{"v":"\nAn inode is a data structure that describes a FS object (like a file/directory)\n- an inode is a [[virtual file-system|fs.virtual]] entity\n- the inode stores metadata about the object, such as its disk block location, file size, file type \n- it also stores owner/permission data about the object\n\nIn reality, a directory is just a list of names that are each assigned to an inode\n- A directory contains an entry for itself, its parent, and each of its children.\n\n- on a UNIX system, files are user facing (ie. we work with them directly). The structure of a file exists only as a virtual FS entity in memory (there is no phsyical correspondent of it)\n- From the point of view of the underlying filesystem, the inode abstracts away the files that the user would directly interact with. From the point of view of the user, the file abstracts away the inode.\n- stands for *index node*\n\n## Dentry\nThe dentry (directory entry) associates an inode with a file name\n","n":0.078}}},{"i":123,"$":{"0":{"v":"Environments","n":1},"1":{"v":"\nAn environment is created every time a shell is initialized\nan environment is just a map of key-value pairs\n- Each command is executed in its own environment, which includes (but not limited to):\n\t1. files that have been sourced\n\t2. current working directory\n\t3. functions defined during execution, or inherited from shell's parent in the environment\n- When a non-builtin command is executed, it is invoked in a separate execution environment.\n\n## Environment Variables\nSince every instance of a shell is a separate process, we have a different set of environment variables in each shell\n- they can be seen by running `env`\n\t- That isn’t all the variables that are set in your shell, though. It’s just the environment variables that are exported to processes that you start in the shell.\n- `compgen -v` allows us to see *all* variables available in shell\n- `export` allows us to add parameters and functions to the environment\n- When a non-builtin command is executed, it is invoked in a separate execution environment.\n\n### Export\n- Exported variables get passed on to child processes, not-exported variables do not.\n\t- When we use `export` in bash, we are adding the variable onto the shell's list of all env variables. This list is exclusive to the shell. When this shell creates a child process, all of these env variables are made available to it.\n\t- This means that if we only need the variable in the current environment, then we don't need to use `export`\n\nthe environment variables that an application can see are based on how the application was launched (from the dock, from the commandline, etc)\n","n":0.062}}},{"i":124,"$":{"0":{"v":"Daemons","n":1}}},{"i":125,"$":{"0":{"v":"inetd","n":1},"1":{"v":"\n### Internet Daemon (inetd)\n- To preserve system resources, UNIX handles many of its services through the internet daemon (inetd), as opposed to a constantly running daemon.\n- inetd is a super server that listens to the various ports and handles connection requests as it receives them by initiating a new copy of the appropriate daemon (program). The new copy of the program then takes it from there and works with the client, and instead goes back to listening to the server ports waiting for new client requests to handle. Once the request is processed and the communication is over, the daemon exits.\n","n":0.1}}},{"i":126,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n# Misc\nIn Unix `<C-d>` on command line specifies an EOF character to end the input.\n","n":0.258}}},{"i":127,"$":{"0":{"v":"Zip","n":1},"1":{"v":"\n### File Compression\n- The core idea of file compression is: the content within a file has redundancies. Why not list data once, then simply back to it when it occurs again?\n\t- ex. the JFK quote: *\"Ask not what your country can do for you -- ask what you can do for your country.\"* has 17 words, made up of 61 letters, 16 spaces, 1 `-` and 1 `.`. If each character was 1 unit of memory, that would be 79 units. If instead of storing each instance of a word in memory, we just refered the repeated words back to the first occurrence, we would cut the phrase in half.\n- In reality, the compression program doesn't look for words, but looks for patterns. Therefore, the larger the file we are compression, the more compression there will be.\n\t- This is why binary files like mp3 don't compress well — there is hardly any repetition of patterns.\n","n":0.08}}},{"i":128,"$":{"0":{"v":"Xargs","n":1},"1":{"v":"\n`xargs` reads items from stdin, and runs a command (default `echo`) on each one\n- each item may either be delimited by:\n    1. blanks (which can be protected with double or single quotes or a backslash))\n    2. newlines\n\n#### Handling blanks and newlines\nBecause Unix filenames can contain blanks and newlines, this default behaviour is often problematic; filenames containing blanks and/or newlines are incorrectly processed by `xargs`.\n- in these situations pass `-0` option, which would prevent these problems, though we need to ensure that the program which produces the input for `xargs` also uses a null character as a separator\n","n":0.102}}},{"i":129,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Pass in grep results to a command\nImagine we wanted to run a command each file in the stdout of `eslint`:\n```\n/Users/kyletycholiz/programming/projects/project-zero/modules/client-ui-components/src/atoms/Radio/FontFaceRadio.js\n  62:50  warning  'onClick' is missing in props validation  react/prop-types\n\n/Users/kyletycholiz/programming/projects/project-zero/modules/client-ui-components/src/atoms/Radio/TextAlignRadio.js\n  38:36  warning  'children' is missing in props validation  react/prop-types\n```\nWe could use grep to get just the filenames, then run a command like `yarn lint | grep /Users/kyletycholiz | xargs npx react-proptypes-generate fix` to pipe those filenames into a command that takes an arbitrary number of args\n- this would be like running: `npx react-proptypes-generate fix FontFaceRadio.js TextAlignRadio.js`\n","n":0.108}}},{"i":130,"$":{"0":{"v":"Tar","n":1},"1":{"v":"\n`tar` stands for tape archive\n\n### Compress a folder into a tarball\n`tar -czvf output.tar.gz input optional_input2`\n\n### Uncompress a tarball using a gzip compressor\n`tar xvzf file.tar.gz`\n\n### Send to different directory than current\n`tar xvzf file.tar.gz -C ~/Downloads`\n\nspec: it seems that we have to cd into the directory that we will create the tar from. If we don't, then the resulting tar will take the whole absolute directory along with it, and when we uncompress it, we will get something like `/Users/kyletycholiz/tarred-file`\n- The same can definitely be said for `zip` utility\n\n### Copy a whole directory from one place to another\n`tar cf - . | (cd $DEST; tar xvf -) `","n":0.097}}},{"i":131,"$":{"0":{"v":"SSH","n":1},"1":{"v":"\nEach SSH key pair has 2 components: public key and private key\n- They are both created with `ssh-keygen` \n\n### Public key\n- Public key is copied to the SSH server so it knows it can trust the user with the corresponding private key when it tries to connect via SSH\n- Anyone that has the public key is able to encrypt the data in such a way that only the bearer of the private key can decrypt it. \n- Once an SSH server receives the public key from a user, it marks the key as trustworthy and is moved to the `authorized_keys` file.\n- `ssh-copy-id` allows us to copy an SSH key from our local machine to a server to be used as an authorized key, allowing s the SSH client to login without password\n\t- What it does is it assembles a list of one or more fingerprints, and tries to log in with each key. It then makes a list of all the keys that failed login, and enables logins with those keys on the remote server (by adding them to the remote's `authorized_keys` file).\n- Public keys are stored on SSH clients \n\n### Private key\n- Considered proof of the user's identity, and only if the private key corresponds to the public key will the user be authenticated\n- Because they are used for authentication, they are called identity keys\n- The private key is stored on the SSH server\n- When we open the private key, what we see is its encrypted form. To decrypt it, we need to enter the password that we created with `ssh-keygen`.\n\t- Therefore, without the password, the private key is useless.\n\n* * *\n\n## SSH Agent\nssh-agent stores unencrypted private keys in memory, allowing us to login without entering our password each time.\n- Therefore, ssh-agent is a form of Single-Sign on (SSO)\n- Without ssh-agent, upon entering the password, the decrypted key is stored in program memory (which is associated with a process (PID)). However, the SSH client process cannot be used to store the decrypted key, since the process is terminated once the remote login session has ended. This is why we use `ssh-agent`\n\n`ssh-agent` works by creating a socket then checks for connections from ssh.\n- anyone who is able to connect to this socket can also connect to the ssh-agent.\n- upon starting, the agent creates a new directory in `/tmp` that determines permissions.\n\nOn most Linux systems, ssh-agent is automatically configured and run at login. \n\nOn the server side, `PubkeyAuthentication` must be enabled in the `ssh_config` file to allow these key-based logins. \n\nwe can use `ssh-add` tool to add identities to the agent.\n- running `ssh-add` without args will add the default private keys to the agent (`~/.ssh/id_rsa`, `~/.ssh/id_dsa` etc.)\n- `ssh-add -l` will list out the private keys accessible to the agent\n\n### Agent Forwarding\nAgent forwarding is a mechanism that gives an SSH server access to the SSH client's `ssh-agent`, allowing it to use it as if it were local. \nTo use agent forwarding, the `ForwardAgent` option must be set to yes on the client \n- This allows us to execute commands on the server we SSHed into and have access to keys stored in `ssh-agent`. \n\t- ex. if we are using keys to interact with Github (instead of entering password each time), we would be able to access them from the server. \n\t- However, there is an implication of this, which is that we would not be able to run these commands as cron jobs on the server. Once our SSH connection ends, the the server no longer has access to `ssh-agent`, so it cannot use the private key.\n\n## Debugging\n- check client logs with `ssh -vvv <user>@<host>`\n\n- check server logs at `/var/log/auth.log`\n\n- on server, try editing `/etc/ssh/sshd_config`, changing: \n\t1. `PasswordAuthentication yes`\n\t2. `PermitRootLogin yes`\n\t- restart sshd `service sshd restart`\n\t- on client, run `ssh-copy-id -i <remote-user>@<remote-ip>`\n\t- on host, revert the `sshd_config` options to their prior state\n\n* * *\n\n## Tunneling\n- In order to tunnel, we need to set up a protocol. SSH is an example of such a protocol, where the receiver of the SSH command must have an SSH server running so that it may intercept the SSH message, and interpret it and enable the SSH connection. \n\n* * *\n\nan OS can be preconfigured to enable SSH, so that it can be SHHed into the first time it is turned on.\n\n* * *\n\n## Config \nClient config (~/.ssh/config)\n[good resource with lots of info](https://gravitational.com/blog/ssh-config/)\n","n":0.037}}},{"i":132,"$":{"0":{"v":"Sed","n":1},"1":{"v":"\nSed is a steam editor\n- here a steam can be thought of as a body of text\n- sed works by reading text line-by-line into a [[buffer|memory.buffer]], and performing some pre-defined instructions (if such instructions are provided).\n- Sed helps us automate the same sort of tasks that we'd accomplish manually by opening a textfile and making manual, predictable edits.\n- like Vim, if we substitute without the `g` flag, then only the first occurrence on the line will substituted.\n- though `/` is the most common delimiter, we can use (almost) anything, like `|` or `:`. This is helpful if the `/` is part of the pattern we want to substitute.\n\nWord-Boundary Expression\n- use `\\b` to disallow partial-word matches\n\t- ex. we want to replace `foo` with `kyle`, but want to leave `foobar` alone: `sed -i '' 's|\\bfoo\\b|kyle|g file.txt'`\n\n![[dendron://code/unix.cli.sed.formulas]]\n\n### UE Resources\nhttps://www.brianstorti.com/enough-sed-to-be-useful/\nhttps://linuxize.com/post/how-to-use-sed-to-find-and-replace-string-in-files/\n","n":0.086}}},{"i":133,"$":{"0":{"v":"Formulas","n":1},"1":{"v":"\nFind and Replace a pattern in all files of a tree\n- find and replace all occurrences of `foo` with `bar` within a directory tree\n\t- `grep -rl 'foo' . | xargs sed -i .bk 's|foo|bar|g'`\n\t- the first part gets a list of all files that have the pattern `foo` in it, then it pipes that list into the second part, which runs the sed substitution on each file\n\t- if we don't want to create a backup, then replace .bk with ''\n\nFind and Replace a pattern in certain files\n- find and replace all occurrences of `foo` with `bar` in all .js files\n\t- `find . -name \"*.js\" -exec sed -i '' s/foo/bar/g {} +`\n","n":0.095}}},{"i":134,"$":{"0":{"v":"Less","n":1},"1":{"v":"\n- `-S<CR>` - enable horizontal scrolling\n","n":0.408}}},{"i":135,"$":{"0":{"v":"Kill","n":1},"1":{"v":"\n- Generally, you should use kill (short for kill -s TERM, or on most systems kill -15) before kill -9 (kill -s KILL) to give the target process a chance to clean up fter itself. (Processes can't catch or ignore SIGKILL, but they can and often do catch SIGTERM.) If you don't give the process a chance to finish what it's doing and clean up, it may leave corrupted files (or other state) around that it won't be able to understand once restarted.\n\n- Generally, you should use kill (short for kill -s TERM, or on most systems kill -15) before kill -9 (kill -s KILL) to give the target process a chance to clean up fter itself. (Processes can't catch or ignore SIGKILL, but they can and often do catch SIGTERM.) If you don't give the process a chance to finish what it's doing and clean up, it may leave corrupted files (or other state) around that it won't be able to understand once restarted.\n\t- strace/truss, ltrace and gdb are generally good ideas for looking at why a stuck process is stuck. (truss -u on Solaris is particularly helpful; I find ltrace too often presents arguments to library calls in an unusable format.) Solaris also has useful /proc-based tools, some of which have been ported to Linux. (pstack is often helpful).\n\n# SIGTERM vs SIGKILL\n- SIGKILL is a higher process killing a lower one, such as a kernel shutting down one of the applications that is currently running\n- SIGTERM is a request to the runner of the application to shut itself down.\n- analogy: we humans are the running of the application, and there is boss above us, giving us commands. SIGTERM would be the boss telling us to shut it down, which we then carry out. This is better, because I have information about what I'm currently working on that will allow me to shut down my process gracefully (ie. without losing data). Sometimes the runner of the application is not in a state where they can shut themselves down. SIGKILL on the other hand is like the boss coming by and forcefully shutting down my computer. The SIGTERM never knew about it, so it's possible something got screwed up while the runner (us) was in the middle of working on something.\n","n":0.051}}},{"i":136,"$":{"0":{"v":"Grep","n":1},"1":{"v":"\n`-l` - only print out the filenames that have the pattern\n`-r` - recurse, allowing us to run grep on directories\n\n- grep for lines with *pattern1* while filtering out lines with *pattern2* from *file.txt*\n    - `grep pattern1 file.txt | grep -v pattern2`\n","n":0.156}}},{"i":137,"$":{"0":{"v":"FZF","n":1},"1":{"v":"\nFZF afts like an interactive filter. The responsibility of the program is not to get us the list of occurrences of our search pattern; that is the job of the search tool we use (eg. grep, ack, ag, find). FZF's only job is to take the list of occurrences as input, and perform the fuzzy searching logic on that list.\n- FZF's default search program is `find`, but it can be changed with the `FZF_DEFAULT_COMMAND` env variable.\n- Silver Searcher is good, because it respects the `.gitignore` and `.ignore` files.\n","n":0.107}}},{"i":138,"$":{"0":{"v":"Curl","n":1},"1":{"v":"\n## HTTP Requests\n\n### Post\n```\ncurl -X POST [options] [URL]\n```\n\nexample\n```sh\ncurl -X POST -H \"Content-Type: application/json\" \\\n    -d '{\"email\": \"linuxize@example.com\", \"password\": \"123\"}' \\\n    localhost:5678/login\n```\n\n### Put\n```\ncurl -X PUT -d arg=val -d arg2=val2 localhost:8080\n```\n\n## Flags\n- `-I` - make sure that cURL outputs the response\n\n* * *\n\n## Cook\n### Setting return value of curl to variable\n```\nhttp_code=$(curl -s -o /dev/null -w \"%{http_code}\" https://www.google.com)\n```\n","n":0.135}}},{"i":139,"$":{"0":{"v":"Cron","n":1},"1":{"v":"\nCron assumes that the machine is running continuously.\n- Therefore, it can't reliably be used on machines that aren't running 24 hours a day.\n- If this is an issue, then try [anacron](https://linux.die.net/man/8/anacron) (linux only), which removes this limitation\n    - Might be able to get a similar effect with [[launchd|mac.os.launchd]] on OS X\n\nSee all active Crontabs:\n`crontab -e`\n","n":0.135}}},{"i":140,"$":{"0":{"v":"Chmod","n":1},"1":{"v":"\n`chmod` allows us to change the access permissions of file system objects (files and directories) sometimes known as modes\n- The access permissions (ie. execute, read, write) of a file system object is known as a \"mode\"\n\nOrdered by:\n- User\n- Group\n- Global\n","n":0.158}}},{"i":141,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### Give user write access\n`chmod u+w /usr/local/share/zsh`\n\n##### Multiple users/groups\n`chmod g+w,o-rw,a+x ~/example-files/`\n\n##### Apply recursively to all children files/directories\n`chmod -R ...`\n\n##### Apply permissions set `rwxr-xr-x`\n`chmod 755 test.txt`","n":0.2}}},{"i":142,"$":{"0":{"v":"Ag","n":1},"1":{"v":"\n## Patterns\n- `ack <search-string> ../src`\n\n## Flags\n- `-i` - ignore case\n- `-w` - only match whole words\n- `-1` - stop searching after 1 match found\n- `-Q` - treat all characters as literal\n    - so if we use `/w` in the pattern, it is taken literally and will not mean \"match word only\"\n\n## Types\n- `ack --react <PATTERN>` - search all files with type react (js, jsx)\n- `--help-types` - see all types defined\n- `ack 'my pattern' ./src` - search for *my pattern* in *src* directory\n\n## Regex (Ag)\n- Ag is magic by default, meaning if we want to match literal characters, we need to escape them. all letters will be literal, and will need to be escaped to get their special meaning (ex. `\\d`, `/s`)\n    - ex. ag '.*\\ddog' - will match `sdfhj5dog`\n        - anything any number of time, followed by a number, followed by 'dog'\n- see `man pcre2pattern` for regex flavors\n\nAll devDependencies should be at the root\neverything we need to build the project is at the root\n","n":0.078}}},{"i":143,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### search for pattern in a specific filename\n- ex. search for pattern 'react' within all package.json\n```sh\nfind . -name 'package.json' | ack -x 'react'\n```\n\nThis has a flaw though, where it will only ack the first result from find.\n","n":0.164}}},{"i":144,"$":{"0":{"v":"Binaries","n":1},"1":{"v":"\nnormally installed in `/sbin`, `/usr/sbin`, `/usr/local/bin` or `~` as a hidden directory\n","n":0.289}}},{"i":145,"$":{"0":{"v":"Unity","n":1},"1":{"v":"\nto allow enemies to walk into a new room, you must add to the walkable area by *baking* it\n\nmisallocation of GameObjects is a major factor in poor performance.\n\n## UI\n![](/assets/images/2021-08-15-16-09-42.png)\n- (B) The Hierarchy window is a hierarchical text representation of every GameObject in the Scene\n- (C) The Game view simulates what your final rendered game will look like through your Scene Cameras\n- (D) The Scene view allows you to visually navigate and edit your Scene.\n- (E) The Inspector Window allows you to view and edit all the properties of the currently selected GameObject.\n- (F) The Project window displays your library of Assets that are available to use in your Project. When you import Assets into your Project, they appear here.\n\n# Organizing files\nWe can use a folder structure similar to MVC architectures:\n\n### Model\nRaw data classes\n- World class, Tile class, Map class etc.\n\n### View\n[[Game Objects|unity.go]]\n\n### Controller\nScripts attached to the GameObjects\n\n# Resources\n[High-quality tutorials on fundamentals of graphics in Unity (fractals, shaders, abstract shapes etc)](https://catlikecoding.com/unity/tutorials/basics/)\n- this has a basis in programming and C# scripts\n\n[Socketweaver multiplayer sdk for unity](https://www.socketweaver.com/)\n- also look into Photon or Mirror\n","n":0.075}}},{"i":146,"$":{"0":{"v":"Terminology","n":1},"1":{"v":"\n*Project* - whole game you are building. Contains all scenes and all assets\n- you never \"save your project\", the only thing you save is the currently open scene. Scenes are containers which can contain one level or many levels, it's up to you to decide how you want to break things up.\n*Scene* - a level in the project. A scene holds related assets. A scene may also be used for the welcome screen, a config screen, intro and other movies, and a credits.\n \nGame view - the view that a player sees as they are playing the game. This is determined by the Main Camera GameObject\nScene view - the perspective of the developer building levels\n\n### Renderer \n- A renderer is what makes an object appear on the screen. \nThis class can be used to access the renderer of any object, mesh or Particle System.\n\n\n### Layers\nLayers allow us to define some functionality across different GameObjects\n\nA layer has no inherent functionality, and we must give it by specifying it on the affected GameObject (e.g. a camera)\nex. we can specify an object to be ignored by the camera with the `Culling Mask` attribute on the Camera.\n\nLayer is set at the top of the Inspector of a GameObject\n","n":0.07}}},{"i":147,"$":{"0":{"v":"Game Object","n":0.707},"1":{"v":"\nthe inspector window displays the properties of a single GameObject\n\nthe inspector is divided into *components*\n","n":0.258}}},{"i":148,"$":{"0":{"v":"Tag","n":1},"1":{"v":"\nTags are a way of identifying GameObjects.\n- ex. Imagine we have orcs, trolls and Urokhai in the enemy army. We can tag all 3 as `Enemy`, and in our code be able to easily check for all GameObjects with that tag.\n\nTags can be added by selecting a GameObject and using the dropdown at the top of the *inspector*\n","n":0.131}}},{"i":149,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get reference to GameObjects that have a Tag\n```cs\nenemy = GameObject.FindWithTag(\"Enemy\");\n// or find multiple\nenemy = GameObject.FindGameObjectsWithTag(\"Enemy\");\n```\n\n#### Check to see if the current gameObject has a particular tag\n```cs\nif (transform.position.x < tippingPoint && gameObject.CompareTag(\"Obstacle\")) {\n    Destroy(gameObject);\n}\n```\n","n":0.171}}},{"i":150,"$":{"0":{"v":"Particle","n":1},"1":{"v":"\nA particle is a GameObject, like smoke appearing behind tires screeching on the pavement.\n\n","n":0.267}}},{"i":151,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Attach particle to another GameObject's action (e.g. Projectile GameObject)\n1. set the particle GameObject to be a child GameObject of the Projectile\n2. get the particle from within the Projectile's script: \n    - `public ParticleSystem explosionParticle;`\n3. call `Play()` on the object: `explosionParticle.Play();` from within the condition that would trigger it. (e.g. on collision of projectile)\n4. attach the particle GameObject to the Projectile for the `Explosion Particle` param.\n```cs\n// ...\npublic ParticleSystem explosionParticle;\n// ...\nprivate void OnCollisionEnter(Collision collision) {\n    if (collision.gameObject.CompareTag(\"Ground\")) {\n        explosionParticle.Play();\n    }\n}\n```\n","n":0.113}}},{"i":152,"$":{"0":{"v":"Light","n":1},"1":{"v":"\n### Directional Light\nex. A simulation of the sun\n- Changing the position of the light will not have an influence on the scene but changing the rotation will change the angle of the light in the Scene.\n\n### Spot light\nex. A simulation of a torch\n\n# Resources\n[Unity: Guide on light](https://docs.unity3d.com/Manual/Lighting.html)\n","n":0.146}}},{"i":153,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Access public variable from different GameObject class\n```cs\n// assuming we have a PlayerController.cs script that is attached to a Player GameObject\nprivate PlayerController playerControllerScript;\n\nvoid Start() {\n    playerControllerScript = GameObject.Find(\"Player\").GetComponent<PlayerController>(); \n}\n\nvoid Update() {\n    // access gameOver variable from the PlayerController instance\n    if (!playerControllerScript.gameOver) {\n        transform.Translate(Vector3.left * Time.deltaTime * speed);\n    }\n}\n```\n","n":0.144}}},{"i":154,"$":{"0":{"v":"Component","n":1},"1":{"v":"\nComponents add behaviour and functionality to an object.\n\"component\" in this sense is like \"component of a GameObject\". Another good word would be \"aspect\", as in \"these are aspects of the object that give it *these* characteristics\n\nspec: Components would be implemented as methods on an object. A component is a kind of capability of the object. For instance a spatial object has a component of `transform`. That is, it can be positioned, rotated, and scaled.\n\n* * *\n\n### C# Script\n`create empty GameObject > Add component > new script`\n","n":0.108}}},{"i":155,"$":{"0":{"v":"Physical","n":1},"1":{"v":"\n### RigidBody\n`RigidBody` gives a GameObject physical properties so that it can interact with gravity, air density, and other GameObjects. \nRigidBody can be used to determine if an object should be frozen in 2D/3D space or not\n\n### Materials\n`Materials` are components that define the surface characteristics of objects and how those surfaces interact with light. In each new 3D Scene, directional light is included to simulate the sun.\n\nYou can drag a PNG into the assets folder of a Unity project, and by doing so, it will become a material (subfolder of `assets/`) that can be dragged onto an object of the scene\n\n#### Physic Materials\nWith Physic Materials, you can make an object bounce and change its friction and drag properties.These properties take effect when the object is under the effects of gravity.\n- in project window, right-click for dropdown and `create > physic material`\n- Collider components\n\n### Colliders\nThe sense of collision, in that you cannot pass through it.\nA collider has a material property, giving us the ability to drag one of our materials on top to give characteristics to the collision of an object\n- ex. if we have a physic material that gives the quality of bounciness, then we can apply that to the collider, which makes the underlying GameObject bouncy. \n\nObjects like spheres and cubes by default have collision\n\nColliders require a RigidBody to detect collision in our physics\n\n","n":0.067}}},{"i":156,"$":{"0":{"v":"Graphical","n":1},"1":{"v":"\n### Mesh\nA mesh is the main graphics primitive in Unity, and make up a large part of the 3D world\n- meshes turn into pixels that look like real objects\n- meshes can be made from either triangular or quadrangular polygons\nA mesh consists of triangles arranged in 3D space to create the impression of a solid object.\n- A triangle is defined by its three corner points or vertices (stored in a single array of integers; understood as each group of 3 belonging to a single vertex) <- so elements 0, 1 and 2 define the first triangle, 3, 4 and 5 define the second, and so on.\n\n### Skybox\nSkyboxes are rendered around the whole scene in order to give the impression of complex scenery at the horizon.\nInternally skyboxes are rendered after all opaque objects\n\n[How to make a Skybox](https://docs.unity3d.com/2019.2/Documentation/Manual/HOWTO-UseSkybox.html)\n","n":0.086}}},{"i":157,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get component of a GameObj within a script\n```cs\nprivate Rigidbody playerRb;\n// ...\n// can also pass any component, like `Collider`\nvoid Start() {\n    playerRb = GetComponent<Rigidbody>();\n}\nvoid Update() {\n    // cause player to jump (obvs we need a Spacebar click-handler here)\n    playerRb.AddForce(Vector3.up * jumpForce, ForceMode.Impulse);\n}\n```\n\n","n":0.154}}},{"i":158,"$":{"0":{"v":"Audio","n":1},"1":{"v":"\n### Audio source\n#### spatial blend \nThis property allows us to increase the volume of objects as a character gets nearer to them\n\n#### rolloff \nthis property defines its range in 3D space, and the rate at which it fades at greater distances and becomes inaudible. You can set the rolloff of your sounds to simulate the ways that different sounds carry.\n\n#### 3D sound settings \nthis property is the quality of audio that it changes orientation based on where the player is.\n- accomplished with an Audio Listener component (attached to MainCamera)\n\n- the the parameters of the 3D Sound Settings property control how volume and pitch can change based on the positions of the Audio Source and the Audio Listener.  \n![](/assets/images/2021-08-16-17-07-11.png)\n\n#### min/max distance\nIf a character is closer to a source of sound than the `minimum distance` (ie. character distance < minimum distance), then the audio plays at max volume\nThe `max distance` will define the most quiet that this object can possibly be\n- min and max denoted by the thin blue spheres around the red pot in the centre\n![](/assets/images/2021-08-16-17-14-38.png)\n\n","n":0.076}}},{"i":159,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Add sound effects to a Player/Unit\n1. add a variable of type `AudioClip` in a script attached to the GameObject to emit sounds (like `PlayerController`)\n2. attach the asset to the script (ie. pass it in as parameter in the Inspector)\n3. add an `Audio Source` component to the Player/Unit\n4. in the script file, get ahold of the component and run `audio.PlayOneShot` during the event.\n```cs\n// ...\n    private AudioSource playerAudio;\n    public AudioClip jumpSound;\n    public AudioClip crashSound;\n\n    void Start() {\n        playerAudio = GetComponent<AudioSource>();\n    }\n\n    void Update() {\n        // Jump\n        if (Input.GetKeyDown(KeyCode.Space)) {\n            // ...\n            playerAudio.PlayOneShot(jumpSound, 1.0f);\n        }\n    }\n```\n","n":0.103}}},{"i":160,"$":{"0":{"v":"Animator","n":1},"1":{"v":"\nThe animator is a [[state machine|general.patterns.state-machine]]\n\n### Animation Layers\nLayers serve to help us organize the complex state machine that is our Animator Component\nWe can use *Layers* to manage the state machine for a specific body part, or specific action\n- ex. Head, Body\n- ex. walking, jumping, running, dying, crouching\n\nWe can also use Layers in such a way where we have a layer for lower-body, and one for upper-body\n\nOn each layer, you can specify the mask (the part of the animated model on which the animation would be applied), and the Blending type. Override means information from other layers will be ignored, while Additive means that the animation will be added on top of previous layers.\n\n#### Mask\nThe Mask property is there to specify the mask used on this layer. For example if you wanted to play a throwing animation on just the upper body of your model, while having your character also able to walk, run or stand still at the same time, you would use a mask on the layer which plays the throwing animation where the upper body sections are defined, like this:\n![](/assets/images/2021-08-26-11-07-48.png)\nNote: the `M` symbol denotes that the layer has a mask applied\n","n":0.072}}},{"i":161,"$":{"0":{"v":"Script","n":1},"1":{"v":"\n### Set animator param value\n```cs\npublic class PlayerController : MonoBehaviour\n{\n    private Rigidbody playerRb;\n    private Animator playerAnim;\n\n    void Update() {\n        if (Input.GetKeyDown(KeyCode.Space) && isOnGround) {\n            playerRb.AddForce(Vector3.up * jumpForce, ForceMode.Impulse);\n            isOnGround = false;\n            playerAnim.SetTrigger(\"Jump_trig\");\n        }\n    }\n}\n```\n\nAlso: `SetFloat`, `SetBool` etc.\n","n":0.167}}},{"i":162,"$":{"0":{"v":"Camera","n":1},"1":{"v":"\n### Follow player\nIn a C# script (attached to the Main Camera), we can specify a new public variable of type `GameObject`. Then we can modify that variable's position attribute in the `update()` method. Then, as a final step we can drag an actual GameObject that we have defined (such as our Player) onto the script, as a way to pass the GameObject to the script.\n- this would be getting a reference to the Player GameObject\n```cs\n    public GameObject player;\n    // Update is called once per frame\n    void Update()\n    {\n        transform.position = player.transform.position;\n    }\n```\n","n":0.104}}},{"i":163,"$":{"0":{"v":"Editor","n":1},"1":{"v":"\n## Local vs Global\n(setting found at the top, next to the QWERTY tools)\nThis refers to `local` or `global` coordinates. If we are set to use `local`, then the directional arrows (x,y,z) of the GameObject will change with the moving object. If we are set to use `global`, then they will stay the same, even if the GameObject moves/rotates.\n- Imagine we have a ball rolling down a slope. If set to `local`, then the y-axis will move with the ball, since that is consistent with the coordinate system of the GameObject itself (ie. the local coordinates). If however we set it to `global`, then the y-axis would always point upward.\n\n* * *\n\n#### Importing `.unitypackage` files\nFrom the top menu in Unity, \nselect `Assets > Import Package > Custom Package`, then find the `.unitypackage` file in the file explorer.\n","n":0.086}}},{"i":164,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n## Tools (QWERTY)\nQ: Hand tool (move camera)\nW: Move tool (to select and change position)\nE: Rotate tool (select + rotate)\nR: Scale tool\nT:Rect Transform tool, to scale in 2D\n- combines move, rect and scale tools; only really useful for 2D\nY: Transform tool, to move, scale, and rotate with one Gizmo\n\n## Actions\nFocus (zoom into object) - `f`\nMove camera view to be same position as scene view - `<cmd+shift+f>`\n\n## Windows\nOpen console - `<cmd+shift+c>`\n\n## Camera movement\n- Hold right-click + WASD to fly to the vehicle, then try to rotate around it\n- With the vehicle selected and your mouse in the Scene view, Press F to focus on it\n- then use the scroll wheel to zoom in and out and hold the scroll wheel to pan\n- Hold alt+left-click to rotate around the focal point or hold alt+right-click to zoom in and out\n\n#### Align view to selected\nselect a game object, select `GameObject` in menubar, Align view to selected\n\n## Resources\n[Asset store](https://assetstore.unity.com/)\n","n":0.081}}},{"i":165,"$":{"0":{"v":"Assets","n":1}}},{"i":166,"$":{"0":{"v":"Sprites","n":1},"1":{"v":"\n### Sorting\nspec: SortingLayer should be used in favor of z-index.\n\nthe render order of Renderers through their `Render Queue`.\n\nThere are 2 types of Render Queue:\n1. Opaque queue\n2. Transparent queue (the main one); this contains the `Sprite Renderer` , `Tilemap Renderer`, and `Sprite Shape Renderer` types.\n\nThe Camera component sorts Renderers based on its Projection setting, of which there are 2 options:\n1. Perspective - the sorting distance of a Renderer = the direct distance of the Renderer from the Camera's position.\n2. Orthographic - The sorting distance of a Renderer = the distance between the position of the Renderer and the Camera along the Camera’s view direction. \n\nsprites that belong together can be put into a `Sorting Group`, which share the same `Sorting Layer`, `Order in Layer` and `Distance to Camera`\n","n":0.089}}},{"i":167,"$":{"0":{"v":"Script","n":1},"1":{"v":"\nDevelopment for Unity is for a large part procedural in nature, not object-oriented. Unity is component-based. You're working with collections of data with behavior attached. This sounds like objects, but encapsulation and identity protection are not at all part of the architecture. Trying to force Unity to always adhere to a strict OOP standard is not practical. In fact, Unity Technologies is shifting focus to data-driven-design with ECS (entity component system) and DOTS (data oriented technology stack).\n- Data-oriented design puts the emphasis of coding on solving problems by prioritizing and organizing your data to make access to it, in memory, as efficient as possible. This contradicts the object-oriented principle that the design of code should be led by the model of the world you are creating.\n\nThe typical object-oriented workflow is to:\n1. Create a GameObject\n2. Add components to it\n3. Write `MonoBehaviour` scripts that change the properties of these components\n\n## Update\n`update()` runs on every frame. One device might get 20 fps, and another might get 60 fps, so the function will be called a disproportionate and unpredictable amount of times. Therefore we need to factor a time-frame into our calculations for code inside `update()`\n- For instance, if we want to move a vehicle forward (the higher the device's fps, the lower the `Time.deltaTime` value will be)\n    - If you multiply a value by `Time.deltaTime`, it will change it from 1x/frame to 1x/second\n\n```cs\n// move ~20 meters/second as long as the user is pressing up arrow\ntransform.Translate(Vector3.forward * Time.deltaTime * 20 * horizontalInput)\n```\nVector3.forward is shorthand for `(0,0,1)`, so when we multiply a vector, each value of the vector is multiplied. Therefore, a vector multiplied by an integer results in a vector\n- ie. `Vector3.forward * 20` results in `(0,0,20)`\n\n## Working with GameObjects\nYou can find a GameObject in the scene like this:\n```cs\nprivate GameObject playerObj = null;\n\nprivate void start() {\n    if (playerObj == null) {\n        playerObj = GameObject.Find(\"Player\");\n    }\n}\n\n```\n\nThen you can get access to things like position:\n```cs\nif (playerObj.transform.position.x > leftBoundary);\n```\n\n`MonoBehavior` is a class that is meant to be added to a component on a GameObject\n- Objects descended from `MonoBehavior` get instantiated through background serialization taking place in Unity\n\n## Misc\nScript template path:\n`/Applications/Unity/Hub/Editor/2020.3.16f1/Unity.app/Contents/Resources/ScriptTemplates`\n\n[gitignore template](https://github.com/github/gitignore/blob/master/Unity.gitignore)\n[.gitattributes file (for git-lfs)](https://gist.github.com/Tycholiz/d9d11c2e8cc8c0c898addc80631c5294)\n","n":0.053}}},{"i":168,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get user keyboard/controller input\n```cs\npublic float verticalInput;\npublic float horizontalInput;\n\nvoid Update()\n{\n    verticalInput = Input.GetAxis(\"Vertical\");\n    horizontalInput  = Input.GetAxis(\"Horizontal\");\n\n    // move the plane forward at a constant rate\n    transform.Translate(Vector3.forward * speed);\n\n    // tilt the plane up/down based on up/down arrow keys\n    transform.Rotate(Vector3.right * rotationSpeed * Time.deltaTime * verticalInput);\n\n    // steer the plane left/right based on arrow keys\n    transform.Rotate(Vector3.up * rotationSpeed * Time.deltaTime * horizontalInput);\n}\n\n```\n\n#### Get input based on key\n```cs\nif (Input.GetKeyUp(KeyCode.Space)) {\n    // instantiate a GameObject\n    Instantiate(projectilePrefab, transform.position, projectilePrefab.transform.rotation);\n}\n```\n\n#### Run a function every 2 seconds, starting after 1 second\n`InvokeRepeating(\"methodName\", 1, 2)`\n\n#### Get/Set(?) property of a component\n```cs\nprivate float repeatWidth;\nvoid Start() {\n    // get the x position value of the box collider component of the GameObject that this script it attached to\n    repeatWidth = GetComponent<BoxCollider>().size.x / 2;\n}\n```\n","n":0.091}}},{"i":169,"$":{"0":{"v":"Components","n":1}}},{"i":170,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\nNaturally, components can be controlled from scripts.\n\n#### Apply force to a GameObject\nFor instance, cause a player to jump, cause an enemy to fly backwards when hit etc.\n```cs\nprivate RigidBody playerRigidBody\nvoid Update() {\n    if (Input.GetKeyDown(KeyCode.Space)) {\n        playerRb = GetComponent<Rigidbody>();\n        playerRb.AddForce(Vector3.up * 1000);\n    }\n}\n```\n\n#### Detect a collision with the GameObject\nDefine a new method on your script's main class\n```cs\nprivate void OnCollisionEnter(Collision collision) {\n    // do the thing when there is a collision\n    if (collision.gameObject.CompareTag(\"Ground\")) {\n        isOnGround = true;\n    } else if (collision.gameObject.CompareTag(\"Obstacle\")) {\n        Debug.Log(\"Game over, bro\");\n        gameOver = true;\n    }\n}\n```\n","n":0.108}}},{"i":171,"$":{"0":{"v":"Prefab","n":1},"1":{"v":"\nPrefabs are equivalent to the concept of components in React.\n\nPrefabs are a special type of component that allows fully configured GameObjects to be saved in the Project for reuse among scenes.\n- A major benefit to Prefabs is that they are essentially linked copies of the asset that exist in the Project window, so changes made to the original prefab will propagate to the rest.\n    - furthermore, you can edit a single instance of a prefab, but it will not necessarily propagate to others \n        - the `revert` button allows us to go back to the master prefab\n        - the `apply` button allows us to apply the changes of the current prefab to the master, and therefore to all instances of it as well.\n\nPrefabs are created automatically when an object is dragged from the Hierarchy into the Project window. \n- prefabs look similar to other objects that appear in the Project window. When selected, you'll see that they are of filetype `.prefab`\n- When prefabs are present in the Hierarchy, they’re represented with blue text and a blue cube.\n\nWhen you convert an object to a Prefab, a new set of buttons is added above the Transforms values:\n![](/assets/images/2021-08-19-21-21-55.png)\n\n#### Creating from existing GameObject\nDrag GameObject from Hierarchy window into `prefab/` directory and choose to create `Original Prefab`\n\n#### Making changes to an instance and propagate changes to Prefab\nAfter making the changes, select a GameObject and in the top right of the Inspector window select `Overrides` and `Apply All`\n","n":0.064}}},{"i":172,"$":{"0":{"v":"Unified Modeling Language (UML)","n":0.5},"1":{"v":"\nhttps://sourcemaking.com/uml","n":1}}},{"i":173,"$":{"0":{"v":"TypeGraphql","n":1},"1":{"v":"\nThe goal is to have a single source of truth in Graphql+Typescript projects\n- When we work with Graphql and Typescript, there is a large amount of duplication of data needed. First, we need to define the Graphql types. Then we need to make a variation of that same data for Typescript (interfaces and types). Then we potentially need to update ORMs and Schemas. All from a single alteration of an object model.\n- The idea of the library is to automatically create GraphQL schema definition from TypeScript's classes\n\nType-graphql provides a `buildSchema` function which takes in all the resolvers we've made and spits out a Graphql schema.\n\nTo avoid the need of schema definition files and interfaces describing the schema, Type-graphql use a bit of reflection magic and decorators.\n\n# API\n## Types and Fields\n### `@ObjectType()`\n- Mark the class that follows as the type known from GraphQL SDL\n```ts\n@ObjectType()\nclass Recipe {\n  @Field()\n  id: string;\n}\n```\n\n- [source](https://typegraphql.com/docs/0.17.0/types-and-fields.html)\n\n### `@Field()`\n- Map the class property that follows as a field on the Graphql type\n## Resolvers\nspec: distinct from [[Graphql Resolvers|graphql.server.resolver]]\n\nResolvers enable us to perform CRUD actions (queries and mutations).\n- It could be thought of as a Controller from classic REST frameworks.\n\n### `@Resolver()`\n\n### `@Query()`\n`@Query(returns => [Recipe])` means that the query resolves to an array of `Recipe` objects\n","n":0.07}}},{"i":174,"$":{"0":{"v":"Middleware","n":1},"1":{"v":"\nDoesn't work like [[express middleware|express.middleware]]\n- instead, inspired by koa.js\n\nA middleware function takes 2 args:\n- resolver data (ie. `root`, `args`, `context`, `info`)\n- the `next` function, which is used to control execution of the next middleware in the stack, as well as the resolver to which the middleware is attached.\n  - the way this differs from Express middleware is that this `next` function returns a Promise that resolves to the value returned by the subsequent middleware and resolver in the stack.\n\nBecause of how `next` works, it is simple to perform actions before and/or after resolver execution:\n```ts\nexport const ResolveTime: MiddlewareFn = async ({ info }, next) => {\n  // before\n  const start = Date.now();\n  await next();\n  // after\n  const resolveTime = Date.now() - start;\n  console.log(`${info.parentType.name}.${info.fieldName} [${resolveTime} ms]`);\n};\n```\n\nIf we only want to do something before an action, like log the access to the resolver, we can just place the return next() statement at the end of our middleware:\n```ts\nconst LogAccess: MiddlewareFn<TContext> = ({ context, info }, next) => {\n  const username: string = context.username || \"guest\";\n  console.log(`Logging access: ${username} -> ${info.parentType.name}.${info.fieldName}`);\n  return next();\n};\n```\n\n### Guards\nGuarding is a technique to programmatically break the middleware stack (which is done by purposefully not calling `next()`)\n- in doing this, the result returned from the middleware will be used instead of calling the resolver and returning its result.\n- We can also throw an error in the middleware if the execution must be terminated and an error returned to the user, e.g. when resolver arguments are incorrect.\n\nIt's called a *guard* because it prevents execution of the resolver and prevents any data return.\n","n":0.062}}},{"i":175,"$":{"0":{"v":"Typescript","n":1},"1":{"v":"\nThe whole value of Typescript in being able to define code that is type-safe, and have any non-type safe code to be uncovered *at* compile time. This inevitably saves us a large amount of effort, since we don't have to wait until runtime to be able to catch these bugs. It makes our code tighter, easier to reason about and more robust.\n- Generics is one way to implement this when writing functions\n\nInstead of running our javascript files directly as we would in a Javascript project, we run our js files through the Typescript compiler, and we run the output it produces.\n\nTypeScript’s type system is very powerful because it allows expressing types in terms of other types. We can accomplish this via:\n- [[generics|ts.lang.generic]]\n- [[typeof operator|ts.lang.op.typeof]]\n- [[keyof operator|ts.lang.op.keyof]]\n- indexed access types\n- conditional types\n- mapped types\n- template literal types\n\nNaturally, any code that does not exist as far as Javascript is concerned (e.g. Generics, Interfaces) does not add to the runtime cost of the code.\n\n#### non-null assertion\nNon-null assertion operator shouldn't be used unless you have a very good reason to do so\n- Non-null assertion operators are simply a way of telling the compiler, \"trust me, I’m absolutely sure that this optional value is in fact never empty.\"\n- While you might think that a value is always non-empty, you could be wrong. Or worse, you could be correct at the moment, but it will not be the case in the future, because some other developer will call your function with a null or undefined.\n```ts\nfunction sayHello(person: Person | undefined) {\n    person!.hello(); // no errors!\n}\n```\n\nThe operator could come in handy, but generally our feeling for it should be that if we are reaching for it, there is probably a more correct way to do it.\n- sometimes the overhead of the type system is not justified, and we just want to create a quick escape hatch that makes sense. We have to address these case by case.\n\nAnother situation in which it might make sense to use this operator is when you want to enable strict null checks on a large, existing codebase. Enabling the flag in such a case might result in hundreds or thousands of errors. It’s a huge effort to go through them all at once. On the other hand, you cannot merge your changes to the master if the whole project doesn’t compile successfully. In this case, the non-null assertion operator might come in handy, as it will allow you to quickly suppress the errors that you don’t have time to fix properly.\n\n### Compiler\nWe can use ESBuild to compile Typescript on server side\n","n":0.048}}},{"i":176,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n# Concepts\n### Type assignability\nWe say that type A is assignable to type B if you can use a value of type A in all places where a value of type B is required. For example:\n- assigning a value to a variable\n- passing a function argument\n- returning a value from a function that has an explicit return type\n\n### Optionality\nconsider that by default, `null` and `undefined` are part of the [[domain|dendron://thoughts-on/math.domain]] of virtually every type. That is, a string can be `null`, an object property can be `null`, and so on. When we enable strict null checking, we are removing `null` from the domains of these types\n![](/assets/images/2021-10-21-15-22-40.png)\n- `strictNullChecks` forces you to explicitly distinguish between values that can be `null` or `undefined` and those that cannot be.\n\nwe could explicitly extend the type to once again include `null` as part of the type's domain:\n```ts\nfunction bar(person: Person | null) {\n```\n\nBut this may not be preferable. Instead we can use optional params (in functions and interfaces) to determine if something can be optional or not:\n- also beneficial to use [[optional chaining|js.lang.op.optional-chaining]] to achieve the effect of optionality, (spec:)as opposed to type guards\n```ts\ninterface Person {}\n\nfunction hello(person?: Person) {}\n\ninterface Employee {\n  id: number;\n  name: string;\n  supervisorId?: number;\n}\n\nhello();\nconst employee: Employee = {\n  id: 1,\n  name: 'John',\n};\n/* * * * */\ninterface Person { hello(): void }\n\nfunction sayHello(person: Person | undefined) {\n  person.hello(); // 🔴 Error!\n\n    // this is a type guard, and it allows the Typescript Type checker to \n    // deduce that the type of person inside the if statement is narrowed to just `Person`\n    if (person) {\n        person.hello(); // OK\n    }\n}\n```\n\nConsider that a `getUser` function might not return a user for a given ID. Therefore, we should set the return value of the function as a union between User and undefined.\n```ts\ngetUser(id: number): User | undefined {\n  return this.users[id];\n}\n```\n\n* * *\n\nspec: If we don't want to specify a new type, we can just do this:\n```ts\ntype TodoPrevious = Pick<Todo, \"title\" | \"completed\">\n```\n\nHere, `\"title\" | \"completed\"` resolves to a type. We could have written it like this:\n```ts\ntype ValueKeys = \"tite\" | \"completed\"\ntype TodoPrevious = Pick<Todo, ValueKeys>\n```\n","n":0.054}}},{"i":177,"$":{"0":{"v":"Type","n":1}}},{"i":178,"$":{"0":{"v":"Util","n":1},"1":{"v":"\nA utility type is also known as a Type Constructor\n\nType constructor is a function (in the mathematical sense) that takes a type and returns a new type (based on the original type).\n- Notice that a generic interface with only one type argument is exactly that\n\nUtility types allow us to construct a new type, taking an initial type, and modifying it some way, determined by which utility type we use.\n\nThink of a utility type as a function that takes in 1+ types, and returns a new type\n\n## Examples\n- `Partial<Type>` - construct a new type with all properties of `Type`, but those properties are all optional.\n- `Required<Type>` - construct a new type with all properties of `Type`, but those properties are required. \n    - therefore opposite of Partial\n- `Record<Keys, Value>` - contruct a new object type, where the keys are of type `Keys` and the values are of type `Value`\n","n":0.082}}},{"i":179,"$":{"0":{"v":"Record","n":1},"1":{"v":"\n`Record<string, number>`\n\nconstructs an object type, whose keys are of type `string`, and properties are of type `number`\n\n```ts\nconst peopleAge: Record<string, number> = {\n    'James': 22,\n    'Frank': 45\n}\n```\n\nOf course, we can pass custom types as well:\n```ts\ninterface CatInfo {\n  age: number;\n  breed: string;\n}\n \ntype CatName = \"miffy\" | \"boris\" | \"mordred\";\n \nconst cats: Record<CatName, CatInfo> = {\n  miffy: { age: 10, breed: \"Persian\" },\n  boris: { age: 5, breed: \"Maine Coon\" },\n  mordred: { age: 16, breed: \"British Shorthair\" },\n};\n```\n\nWhat if we wanted to write a function that takes in an array of objects, and those objects could have any shape *as long as they have an id field*?\n```ts\nfunction getIds<TElement extends Record<'id', string>>(elements: TElement[]) {\n    return elements.map(element => element.id);\n}\n```\n","n":0.093}}},{"i":180,"$":{"0":{"v":"Pick","n":1},"1":{"v":"\n`Pick<Type, Keys>`\n- Constructs a type by picking the set of properties `Keys` (string literal or union of string literals) from `Type`.\n\nIn the below example, we are creating a new type `TypePreview` based on a subset of fields available on the `Todo` type.\n```ts\ninterface Todo {\n  title: string;\n  description: string;\n  completed: boolean;\n}\n\ntype TodoPreview = Pick<Todo, \"title\" | \"completed\">;\n\nconst todo: TodoPreview = {\n  title: \"Clean room\",\n  completed: false,\n};\n```\n","n":0.124}}},{"i":181,"$":{"0":{"v":"Partial","n":1},"1":{"v":"\nA Partial type is derived from an existing type. The only difference between the Partial type and the original type is that the Partial type has all properties set to optional.\n- note: if you want a type that only includes some of the fields from another type, then you're looking for [[ts.type.util.pick]]\n\nExample:\n```ts\ninterface Todo {\n\timportance: number;\n\ttext: string;\n}\n\nfunction updateTodo(todo: Todo, fieldsToUpdate: Partial<Todo>) {\n  return {\n\t\t...todo,\n\t\t...fieldsToUpdate,\n\t};\n}\n```\n\nAbove, the resulting Partial type would be like this:\n```ts\ninterface PartialTodo {\n\timportance?: number,\n\ttext?: string,\n}\n","n":0.115}}},{"i":182,"$":{"0":{"v":"Omit","n":1},"1":{"v":"\n## `Omit<Type, Keys>`\n`Omit` will construct a new type by taking the `Type` and removing the `Keys` (if multiple, represented as a union of string literals)\n\n### Example:\nMake a new type `QueryCommandInput`, which consists of the `__QueryCommandInput` without the `KeyConditions` or `QueryFilter` keys.\n\n`type QueryCommandInput = Omit<__QueryCommandInput, \"KeyConditions\" | \"QueryFilter\">`\n","n":0.146}}},{"i":183,"$":{"0":{"v":"Unknown","n":1},"1":{"v":"\nThis is similar to the `any` type, but is safer because it’s not legal to do anything with an `unknown` value\n\nIf a type is of type `unknown`, then it means the variable can be assigned to any type. At the moment of initializing an `unknown` variable, we are saying \"we don't know the type of this one. If you know it, feel tree to coerce it into its proper type. You can change it however you want.\"\n\nOn the other hand, it also means that we can't set the `unknown` value to a different type.\n```ts\n// This is ok\nlet x: unknown;\n\nx = 123; // no error\n/* * * * * * * * * * */\n// This is not\nfunction foo2(bar: unknown) {\n  const a: string = bar // error: Type 'unknown' is not assignable to type 'string'\n  //...\n}\n```\n\nBoth `any` and `unknown` represent an unknown type, but there is a key difference:\n- as demonstrated in the code above, all types are assignable to the `unknown` type, but `unknown` is not assignable to any type\n    - therefore `unknown` is a type-safe alternative to `any`\n\n![](/assets/images/2021-10-21-08-39-26.png)\n\nThis distinction between `any` and `unknown` is important, because it makes our code stricter. \n- If you have a value of `unknown` type, you need to cast it to some other type before you can do anything useful with it. The consequence of this is that `unknown` doesn’t propagate like any does, which is much safer.\n\n## Use-cases\nWhen you are working with external values (e.g. APIs), there is a good chance you won't know what the return type is. In such cases, it’s a good idea to type such value as `unknown` instead of `any`.\n```ts\ninterface Article {\n    title: string;\n    body: string;\n}\n\nfetch(\"http://example.com\")\n    .then(response => response.json())\n    .then((body: unknown) => {\n        const article = body as Article;\n        // we need to cast, otherwise we'd get an error\n        console.log(article.title);\n    });\n```\n\nIn this example, we use type assertion to tell TypeScript that we know the type of body. It’s still not type-safe because we could be wrong, but at least it’s explicit. The proper solution here would be to perform a runtime check to make sure that body is indeed an Article. We’ll look into such a solution in the lesson dedicated to user-defined type guards.\n","n":0.052}}},{"i":184,"$":{"0":{"v":"Union","n":1},"1":{"v":"\nThe pipe (`|`) in a union can be properly thought of as \"or\", since it means that \"this variable can be of one type or the other\"\n\nA Union is a way to achieve type composition (as are [[intersections|ts.type.intersection]])\n- composition being a way to create new types from existing types\n\nThe rule for union types is that we only allow an operation if it would be legal to do on each member of the union\n\n```ts\nlet ambiguouslyEmptyAlice: Person | null | undefined;\n```\n\nTake the following union:\n```ts\ninterface Foo {\n    foo: string;\n    xyz: string;\n}\n\ninterface Bar {\n    bar: string;\n    xyz: string;\n}\n\nconst sayHello = (obj: Foo | Bar) => { /* ... */ };\n```\nHere, we are saying \"`Foo | Bar` is a type that has either all required properties of `Foo` OR all required properties of `Bar`.\"\n- In the absence of a guard, we'd only be able to access `obj.xyz`, since that is the only property that exists on either type.\n\n\nUnion type is very often used with either null or undefined.\n```ts\nconst sayHello = (name: string | undefined) => { /* ... */ };\n```\n\n### Relation to Unions in statistics\nWhen we consider [[unions|math.set-theory.op.union]] in a statistical sense, it may seem counter-intuitive how Unions and Intersections are implemented in Typescript.\n- If have have a union type `string | number`, it means the type has to be either string or number.\n- If we have an intersection type, it means the type must have both.\n\nThe lack of clarity lies in the perspective we are taking. We are meant to take the perspective of what is accepted as the union type.\n- we say that the set of things that can be set to `string | number` is the union of the string set and the number set.\n    - ex. imagine there was a big list of all strings and all numbers that were loaded into memory. The union of `string | number` would be the set of all of them\n- we say that the set of things that can be set to `string & number` is zero, because there is zero overlap between `string & number`.\n\nPut another way, the incorrect way to see it is: \"a union is a set operator on the field\". The correct way is \"a union is a set operator of the sets of what each type represents.\"\n\n[ref](https://www.reddit.com/r/typescript/comments/9qduq3/why_is_type_intersection_called_like_that_its/)\n","n":0.051}}},{"i":185,"$":{"0":{"v":"Discriminated Union","n":0.707},"1":{"v":"\nDiscriminated unions are a concept closely related to [[state machines|general.patterns.state-machine]].\n\nDiscriminated unions allow us to compose types and enforce some business logic rules *at compile-time*.\nFor example, we can implement the following logic:\n> Customer needs to provide either email or phone number.\nAn obvious (and naive) solution to this is to just make both the `email` and `phone` properties optional; but then we end up with the possibility that neither is provided.\n\nInstead, we can create a new type `Contact` which is the union of 2 interfaces: 1 has a `phone` property, and the other has a `email` property:\n```ts\ninterface EmailContact {\n    kind: 'email';\n    email: string;\n}\n\ninterface PhoneContact {\n    kind: 'phone';\n    phone: number;\n}\n  \ntype Contact = EmailContact | PhoneContact;\n\ninterface Customer {\n    name: string;\n    contact: Contact;\n}\n\nfunction printCustomerContact({ contact }: Customer) {\n    // Here comes the power of discriminated unions. TypeScript analyses the code and sees an if statement that checks whether contact.kind is equal to \"email\". If that’s true, it can be certain that the type of contact is EmailContact, so it narrows the type inside the first if branch. The only other option for contact is to be a PhoneContact, so the type is narrowed to PhoneContact in the second if branch.\n    if (contact.kind === 'email') {\n        // Type of `contact` is `EmailContact`!\n        console.log(contact.email);\n    } else {\n        // Type of `contact` is `PhoneContact`!\n        console.log(contact.phone);\n    }\n}\n```\n\nEach union member follows a special convention of having a `kind` property. This property is called a *discriminator*, and it encodes type information into the object so that it is available at runtime. Now, TypeScript can figure out which union member you’re dealing with by looking at the `kind` property.\n- `kind` is just a convention, and it can be any word.\n\n\n### Example\nUsers place orders for products. Users have contact information, email or postal addresses, and at least one is required. Orders should include price, product name, quantity, payment date, paid amount, sending date, and delivery date.\n```ts\ntype Customer = {\n  name: string;\n  contactInfo: ContactInfo;\n};\n\ntype ContactInfo =\n  | { kind: \"emailOnly\"; email: string }\n  | { kind: \"postalOnly\"; address: string }\n  | { kind: \"emailAndPostal\"; email: string; address: string };\n\ntype PaidOrderData = { paymentDate: Date; amount: number };\ntype SentOrderData = { sendingDate: Date };\ntype DeliveredOrderData = { deliveryDate: Date };\n\ntype OrderState =\n  | { kind: \"new\" }\n  | { kind: \"paid\"; paidData: PaidOrderData }\n  | { kind: \"sent\"; paidData: PaidOrderData; sentData: SentOrderData }\n  | {\n      kind: \"delivered\";\n      data: PaidOrderData;\n      sentData: SentOrderData;\n      deliveredData: DeliveredOrderData;\n    };\n\ntype Order = {\n  customer: Customer;\n  state: OrderState;\n  productName: string;\n  price: number;\n  quantity: number;\n};\n```\n\n### Multi-value Discriminator\na discriminator does not have to be a single property. A group of literal properties can also act as a discriminator! In such a case, every combination of values marks a different member of the union type.\n```ts\ntype Foo =\n  | { kind: 'A', type: 'X', abc: string }\n  | { kind: 'A', type: 'Y', xyz: string }\n  | { kind: 'B', type: 'X', rty: string }\n\ndeclare const foo: Foo;\n\nif (foo.kind === 'A' && foo.type === 'X') {\n    // here, the intellisense on `foo` would show how it's type is narrowed to the only type it could be, namely the first one shown in `Foo`\n  console.log(foo.abc);\n}\n```\n\n","n":0.044}}},{"i":186,"$":{"0":{"v":"Intersection","n":1},"1":{"v":"\nAn Intersection is a way to achieve type composition (as are [[unions|ts.type.union]])\n\nConsider the following intersection:\n```ts\nconst sayHello = (obj: Foo & Bar) => { /* ... */ };\n```\n\nHere, we are saying that `obj` must have *all* properties that are included on both `Foo` and `Bar`. \n","n":0.149}}},{"i":187,"$":{"0":{"v":"Interface","n":1},"1":{"v":"\nInterfaces support overriding (correct term?), allowing us to have multiple methods by the same name, so long as their signature differs:\n```ts\nexport interface Cache {\n  set<T>(key: string, value: T, options?: CachingConfig): Promise<T>;\n  set<T>(key: string, value: T, ttl: number): Promise<T>;\n}\n```\n\n```ts\ninterface SquareConfig {\n  color?: string;\n  width?: number;\n}\n```\n\n","n":0.151}}},{"i":188,"$":{"0":{"v":"Classes","n":1},"1":{"v":"\n```ts\nclass HeaderComponent {\n    // with `strictPropertyInitialization` enabled, we must set properties in the constructor\n    constructor(private header: string) {}\n\n    render() {\n        return `<h1>${this.header.toUpperCase()}</h1>`;\n    \n    constructor(private header: string) {}\n\n    render() {\n        return `<h1>${this.header.toUpperCase()}</h1>`;\n    }\n}\n```\n","n":0.177}}},{"i":189,"$":{"0":{"v":"Assertion","n":1},"1":{"v":"\nType assertions helps you to force types when you are not in control of them. For e.g. -\n1. you are processing user data (with unreliable types)\n2. working with data that has changed shape over years (employee code was numeric, now it is alphanumeric :))\n3. receiving data from an external program\n\nType assertions let the Typescript compiler know that a given variable should be treated as belonging to a certain type. There are no “exceptions” or data restructuring associated with assertions, except minimal validations (we refer this behaviour as “validations that are applied statically”).\n\nThere are two ways to do type assertions.\n1. Bracket syntax. e.g. `let length: number = (<string>lengthField);`\n2. Use as. e.g. `let length: number = lengthField as string;`\nThere is no difference between the two ways and the end result is always the same. Note that if you are using JSX you have to use `as` syntax.\n\n## Cook\n### Assert as a subtype on an Interface/Type\n```ts\ninterface Data {\n  events: {\n    id: string;\n  }[];\n}\nconst copiedEvents = [...data.events] as Data['events']\n```","n":0.078}}},{"i":190,"$":{"0":{"v":"TS Config","n":0.707},"1":{"v":"\n\n`tsconfig.json`\n```json\n{\n  // some other settings\n  \"compilerOptions\": {\n    // some other options\n    \"strictNullChecks\": true,\n    \"strictPropertyInitialization\": true,\n    \"noImplicitAny\": true,\n  }\n}\n```\n\n- `strictNullChecks` - see [[ts.types#optionality]]\n- `target` - \n- `module` - This is the version of Javascript that our code will get compiled to\n  - ex. if we set `module` to `commonjs`, then the Typescript will be compiled into Javascript using `require` and `module.exports` syntax.\n  - server-side project that uses Node.js should probably use CJS, while front-end applications should probably use ESM\n- `target` - This setting changes which JS features are downleveled and which are left intact.\n  - ex. `() => this` will be turned into an equivalent function expression if target is ES5 or lower.\n  - `target` also changes the default value of `lib`.\n\n# Resources\n- [tsconfig options](https://www.typescriptlang.org/docs/handbook/compiler-options.html)\n- [tsconfig bases](https://github.com/tsconfig/bases#centralized-recommendations-for-tsconfig-bases)\n","n":0.089}}},{"i":191,"$":{"0":{"v":"Patterns","n":1},"1":{"v":"\n### Use an existing type as a base, but remove some keys and add some others\n```ts\ntype GetCommandInput = Omit<__GetItemCommandInput, \"Key\"> & {\n  Key: { [key: string]: NativeAttributeValue } | undefined\n}\n```","n":0.183}}},{"i":192,"$":{"0":{"v":"Overloading","n":1},"1":{"v":"\nTypescript supports [[overloading|paradigm.oop.overloading]]\n\nIn TS, we can overload a function by having multiple call signatures, though it is still the same physical function at runtime (of course, since TS doesn't exist at runtime)\n- From the point of view of the caller, this is virtually the same as languages that properly have overloading.\n\nThe point of overload signatures is that it allows you to express certain function contracts that can't be safely implemented in TS.\n- So almost any overloaded function is going to have some casting, or `any`, or some such 'type unsafeness', otherwise you wouldn't need the overloads\n\nWhen implementing overloads, you're making things stricter for the caller at the expense of some safety inside the function\n\nIn the presence of overloads, the implementation signature is invisible to callers\n\nExample:\nStripe has 2 different versions of `stripe.paymentIntents.list`. The first version looks like this:\n```\n(params?: PaymentIntentListParams, options?: RequestOptions): ApiListPromise<PaymentIntent>\n```\n\nThe second looks like this:\n```\n(options?: RequestOptions): ApiListPromise<PaymentIntent>\n```\n\nThe benefit to doing this is that we can call the same function, even if we don't want to pass in a `params` object. When we call `stripe.paymentIntents.list`, the TS compiler will figure out which version of the function it should call, based on the arguments that are passed to the function call. This process is what is known as Method overloading.\n- If we call `stripe.paymentIntents.list` and our arguments don't line up with the first function signature (ex. because one of the args we pass doesn't line up with `PaymentIntentListParams` interface), then TS will attempt to use the second version of the function. This will result in Intellisense spitting both errors at us, even though only the first one is what we should pay attention to\n","n":0.061}}},{"i":193,"$":{"0":{"v":"Modules","n":1},"1":{"v":"\n## Module resolution\nTypescript can be configured to resolve modules in 2 ways: classic or node.\n\n### Node resolution\nWith the node-style of resolution, [[Node's module resolution process|js.node.modules#how-node-resolves-modules,1:#*]] is mimicked. Typescript will overlay the ts source-file extensions (ie. `.ts`, `.tsx`, `.d.ts`) over Node's resolution logic.\n- TypeScript will also use a field in `package.json` named types to mirror the purpose of \"main\" - the compiler will use it to find the “main” definition file to consult.\n\nWhen we `import myFunction from 'moduleB'`, the `moduleB` module will be searched for in a `node_modules` directory at the current level. It will search:\n1. `/root/src/node_modules/moduleB.ts`\n2. `/root/src/node_modules/moduleB.tsx`\n3. `/root/src/node_modules/moduleB.d.ts`\n4. `/root/src/node_modules/moduleB/package.json` (if the package.json specifies a *types* property)\n5. `/root/src/node_modules/@types/moduleB.d.ts`\n6. `/root/src/node_modules/moduleB/index.ts`\n7. `/root/src/node_modules/moduleB/index.tsx`\n8. `/root/src/node_modules/moduleB/index.d.ts`\n\nIf not found, it will go up one level and repeat the process until the module is found or there are none left to check. \n\n### Debugging module resolution\nWe can debug the module resolution process by compiling with the `traceResolution` flag:\n- `tsc --traceResolution`\n    - tip: pipe the output into grep: `tsc --traceResolution | grep module-that-wont-resolve`\n\nKeep an eye out for:\n- Name and location of the import\n    - `======== Resolving module ‘typescript' from ‘src/app.ts'. ========`\n- The strategy the compiler is following\n    - `Module resolution kind is not specified, using ‘NodeJs'.`\n- Loading of types from npm packages\n    - `'package.json' has 'types' field './lib/typescript.d.ts' that references 'node_modules/typescript/lib/typescript.d.ts'.`\n- Final result\n    - `======== Module name ‘typescript' was successfully resolved to ‘node_modules/typescript/lib/typescript.d.ts'. ========`\n\n### Module resolution flags (tsconfig settings)\nSometimes the layout of the source files does not match the layout of the output. That is, the directory structure (which includes the names of files and where they are located) may differ before and after we compile the Typescript.\n- for this reason, the Typescript compiler has a set of additional flags to inform the compiler of transformations that are expected to happen to the sources to generate the final output. That is, these fields will help guide the process of resolving a module import to its definition file.\n\n#### `baseUrl`\nLets you set a base directory to resolve non-absolute module names. All module imports with non-relative names are assumed to be relative to the baseUrl.\n- This negates our need to import with `./` and `../`, and instead we can just start from the baseUrl that we specify\n\nIf we set `baseUrl: ./`, then with the following directory structure...\n```\nbaseUrl\n├── ex.ts\n├── hello\n│   └── world.ts\n└── tsconfig.json\n```\n\nHere, we can just `import { helloWorld } from 'hello/world'`\n\nNaturally, `baseUrl` only applies to non-relative imports.\n\n#### `paths` (path mapping)\nWhen using third party modules, they are not directly located under `baseUrl`.\n- using `import $ from 'jquery'` would be translated at runtime to `\"node_modules/jquery/dist/jquery.slim.min.js\"` \n    - note: this is able to be figured out because Loaders use a mapping configuration to map module names to files at run-time.\n\nThe Typescript compiler lets us declare mappings like this with the `paths` property.\n- for instance, we can specify the above like this;\n```json\n\"paths\": {\n    \"jquery\": [\"node_modules/jquery/dist/jquery\"] // This mapping is relative to \"baseUrl\"\n}\n```\n\nBecause the value is an array, we can specify multiple places for the Typescript compiler to look for those modules.\n- imagine the following directory structure, where some modules are stored in `generated/`, and the rest in another directory. The build step would put them all in the same place.\n```\nprojectRoot\n├── folder1\n│   ├── file1.ts (imports 'folder1/file2' and 'folder2/file3')\n│   └── file2.ts\n├── generated\n│   ├── folder1\n│   └── folder2\n│       └── file3.ts\n└── tsconfig.json\n```\n\n```json\n\"paths\": {\n    \"*\": [\"*\", \"generated/*\"]\n}\n```\n\nThis tells the compiler for any module import that matches the pattern \"*\" (i.e. all values), to look in two locations:\n1. \"*\": meaning the same name unchanged, so map `<moduleName>` => `<baseUrl>/<moduleName>`\n2. \"generated/*\" meaning the module name with an appended prefix `generated`, so map `<moduleName>` => `<baseUrl>/generated/<moduleName>`\n\nFollowing this logic, the compiler will attempt to resolve the two imports as such:\n\n`import 'folder1/file2'`:\n1. pattern '*' is matched and wildcard captures the whole module name\n2. try first substitution in the list: '*' -> folder1/file2\n3. result of substitution is non-relative name - combine it with baseUrl -> projectRoot/folder1/file2.ts.\n4. File exists. Done.\n\n`import 'folder2/file3'`:\n1. pattern '*' is matched and wildcard captures the whole module name\n2. try first substitution in the list: '*' -> folder2/file3\n3. result of substitution is non-relative name - combine it with baseUrl -> projectRoot/folder2/file3.ts.\n4. File does not exist, move to the second substitution\n5. second substitution 'generated/*' -> generated/folder2/file3\n6. result of substitution is non-relative name - combine it with baseUrl -> projectRoot/generated/folder2/file3.ts.\n7. File exists. Done.\n\n* * *\n\nOften, in Typescript files we need to do this:\n```ts\n- import _ from 'lodash';\n+ import * as _ from 'lodash';\n```\n\nThis is due to the fact that there is no default export present in lodash definitions.\n\nTo make imports do this by default and keep `import _ from 'lodash';` syntax in TypeScript, set \"allowSyntheticDefaultImports\" : true and \"esModuleInterop\" : true in your tsconfig.json file.","n":0.036}}},{"i":194,"$":{"0":{"v":"Lang","n":1}}},{"i":195,"$":{"0":{"v":"Keyword","n":1}}},{"i":196,"$":{"0":{"v":"Namespace","n":1},"1":{"v":"\nNamespaces provide a mechanism for organizing code and declarations in hierarchies of named containers.\n\nNamespaces have named members that each denote a value, a type, or a namespace, or some combination thereof, and those members may be local or exported.\n\nThe body of a namespace corresponds to a function that is executed once, thereby providing a mechanism for maintaining local state with assured isolation. \n- Namespaces can be thought of as a formalization of an [[IIFE|js.lang.functions.IIFE]]\n\nNamespaces are either instantiated or non-instantiated. \n- A non-instantiated namespace is a namespace containing only interface types, type aliases, and other non-instantiated namespace. \n- An instantiated namespace is a namespace that doesn't meet this definition. \n  - In intuitive terms, an instantiated namespace is one for which a namespace instance is created, whereas a non-instantiated namespace is one for which no code is generated.\n\nA namespace identifier can be referenced in one of two ways:\n- as a NamespaceName, in which it denotes a container of namespace and type names\n- as a PrimaryExpression, in which it denotes the singleton namespace instance\n\n```ts\nnamespace M {  \n  export interface P { x: number; y: number; }  \n  export var a = 1;  \n}\n\nvar p: M.P;             // M used as NamespaceName  \nvar m = M;              // M used as PrimaryExpression  \nvar x1 = M.a;           // M used as PrimaryExpression  \nvar x2 = m.a;           // Same as M.a  \nvar q: m.P;             // Error\n```\n\nAbove, when `M` is used as a PrimaryExpression it denotes an object instance with a single member `a` and when `M` is used as a NamespaceName it denotes a container with a single type member `P`. The final line in the example is an error because `m` is a variable which cannot be referenced in a type name.\n- If the declaration of `M` above had excluded the exported variable `a`, `M` would be a non-instantiated namespace and it would be an error to reference `M` as a PrimaryExpression.\n","n":0.056}}},{"i":197,"$":{"0":{"v":"Declare","n":1},"1":{"v":"\n### Ambient Declaration\nThe `declare` keyword is used to tell Typescript \"this thing (usually a variable) exists already, and can therefore be referenced by other code\".\n- Also, it tells the Typescript compiler that there is no need to compile this statement into Javascript\n\nImagine our project calls some third party JS script which returns an object. Since Javascript has no typings, we would have no typing information on this object. `declare` gives us a mechanism through which we can say \"trust me, this variable exists and has *this* type\". The compiler will use this statement to statically check other code but will not trans-compile it into any JavaScript in the output.\n- this enables our Typescript to be able to call, say, a method on that returned object, since we've typed it with the `declare` keyword.","n":0.087}}},{"i":198,"$":{"0":{"v":"Op","n":1}}},{"i":199,"$":{"0":{"v":"Typeof","n":1},"1":{"v":"\nWhile Javascript already has `typeof` that can be used in an expression context, Typescript adds a `typeof` that can be used in a type context.\n- that is, we can use it to refer to the type of a property/variable.\n- it’s only legal to use `typeof` on identifiers (i.e. variable names) or their properties\n\nLike `keyof`, `typeof` is used to create new types\n\n```ts\nlet s = \"hello\";\nlet n: typeof s;\n```\n\ntypeof can be used to conveniently express many patterns.\n- imagine we want to grab the return type of function `f()` for usage elsewhere:\n```ts\nfunction f() {\n  return { x: 10, y: 3 };\n}\n// note: ReturnType is a built-in, and gives us the return type of a given function\ntype P = ReturnType<typeof f>;\n```\n\n`typeof` lets us achieve narrowing and type guarding in Typescript\n- https://www.typescriptlang.org/docs/handbook/2/narrowing.html","n":0.089}}},{"i":200,"$":{"0":{"v":"Keyof","n":1},"1":{"v":"\nThe `keyof` operator takes an object type and produces a union of its keys\n\nLike `typeof`, `keyof` is used to create new types\n\nFor any type `T`, `keyof T` is the union of public property names of `T`\n```ts\ninterface Person {\n  age: number;\n  name: string;\n}\n\ntype PersonKeys = keyof Person; // \"age\" | \"name\"\n```\n\nThe type `PersonKeys` here is the same as:\n```ts\ntype PersonKeys = \"age\" | \"name\"\n```\n\nIf the type uses a string or number as the index, then `keyof` will return those types instead:\n```ts\ninterface Arrayish = {\n    [n: number]: unknown\n}\ntype A = keyof Arrayish\n// A = number\n```\n","n":0.104}}},{"i":201,"$":{"0":{"v":"Is Keyword","n":0.707},"1":{"v":"\n`is` is a keyword that is used in compile-time to tell the developers the code will have a chance to have a runtime error.\n- `is` is a type guard that lets us model the behaviour that \"this variable will be of *this* type within any guarded blocks of code\"\n\n```ts\n// we say \"test is string\", meaning that inside any blocks that are guarded by this function, the value must be a string\nfunction isString(test: any): test is string{\n    return typeof test === \"string\";\n}\n\nfunction example(foo: any){\n    // because of the \"test is string\" part above, we give a guarantee to typescript that \"if isString\" is returning true, then we guarantee that the foo value will be a string\n    if(isString(foo)){\n        console.log(\"it is a string\" + foo);\n        console.log(foo.length); // string function\n    }\n}\nexample(\"hello world\");\n```\n\nAnother example:\n```ts\nexport function isBook(x: Item): x is Book {\n  return x.itemType === 'book'\n}\n\n// imagine we have an array of objects with an `itemType` field\nconst filtered = items.find(isBook)\n```\n\n## E Resources\nhttps://stackoverflow.com/questions/40081332/what-does-the-is-keyword-do-in-typescript\n","n":0.08}}},{"i":202,"$":{"0":{"v":"Generic","n":1},"1":{"v":"\nTS takes a more parametric approach to generics \n- ie. the type is a variable being passed in just like the values, and the type is unknown while writing the function, which necessarily limits what you can do with it\n\nWe must pass in the type parameters (e.g. `<T>`), because otherwise Typescript wouldn't know if `T` is a type argument or an actual type.\n\nIt's critical to remember that everything in TS happens at compile-time. Therefore, against what your intuition might suggest, generic types are not checked when the function is called (ie. run-time). This is the beauty of Typescript, because every type must be known in advance, even though the code has not even been run yet. Therefore, we need to be strict in our typing, which means allowing for a given variable to be any type that it may be *depending* on the conditions happening during runtime.\n- ex. consider that while an API is *expected* to return some data of a certain structure, that may not always be the case. Perhaps the network request fails, and the variable you are trying to set remains `undefined`. Therefore your type system must be smart enough to recognize this possibility. You must therefore build that possibility into the system.\n\n## Generic constraints (ie. `extend`ing generics)\n### Constraining type parameters by interfaces\nImagine we have a function that returns the length of the argument\n```ts\nfunction getLength<T>(arg: T): number {\n    return arg.length\n}\n```\n\nWritten like this, only string and array would be able to work, and everything else would throw an error. To solve this, we can use an interface and extend the generic:\n```ts\ninterface Lengthwise {\n  length: number;\n}\nfunction getLength<T extends Lengthwise>(arg: T): number {\n// ...\n```\n### Constraining type parameters by other type parameters\nThis generic constraint basically says \"`Key` can only be a public property in `Type`\".\n- It has nothing to do with extending a type or inheritance, contrary to extending interfaces.\n```ts\nfunction getProperty<Type, Key extends keyof Type>(obj: Type, key: Key) {\n  return obj[key];\n}\n\nconst person: Person = {\n  age: 22,\n  name: \"Tobias\",\n};\n\n// name is a property of person\n// --> no error\nconst name = getProperty(person, \"name\");\n\n// gender is not a property of person\n// --> error\nconst gender = getProperty(person, \"gender\");\n```\n\n* * *\n\n### Default generic parameter\nIn Javascript, we can set a default parameter if one isn't passed:\n```js\nfunction registerName(fullName = 'John Doe') {\n```\n\nIn Typescript, we can set a default generic type, if the type isn't passed:\n- this means that if `Data` type isn't passed (which must extend to `DataType`), then the generic will be `DataType`\n```ts\nexport interface FormProps<Data extends DataType = DataType> { }\n```\n\n#### Example: Implementing HTML\n- type `T` of the following signature defaults to a `div` element. If `create` is called with its first argument, then it will extend an `HTMLElement` type.\n- type `U` defaults to an array of type `T`. If `create` is called with its second argument, `U` becomes the type of whatever was passed in\n    - ex. if a `<p>` was passed as a child of the main element being created, then `U` = `HTMLElement`. If an array of `<p>` was passed, then `U` = `HTMLElement[]`\n```ts\ndeclare function create<\n    T extends HTMLElement = HTMLDivElement, \n    U = T[]\n>(element?: T, children?: U): Container<T, U>;\n```\n\nA generic parameter default follows the following rules:\n- A type parameter is deemed optional if it has a default.\n- Required type parameters must not follow optional type parameters.\n- Default types for a type parameter must satisfy the constraint for the type parameter, if it exists.\n- When specifying type arguments, you are only required to specify type arguments for the required type parameters. Unspecified type parameters will resolve to their default types.\n- If a default type is specified and inference cannot chose a candidate, the default type is inferred.\n- A class or interface declaration that merges with an existing class or interface declaration may introduce a default for an existing type parameter.\n- A class or interface declaration that merges with an existing class or interface declaration may introduce a new type parameter as long as it specifies a default.\n\n* * *\n\n### Unbound Type Variable\nTake the following example:\n```ts\nfunction foo<T>(): T {}\nlet x = foo();\n// what type is x? the world may never know...\n```\n\n`foo` can use `T` inside the implementation, but it has to treat it as basically `unknown`, because it could be anything at all\n- put another way, `T` is `unknown` during compile-time, and only known during run-time.\nfrom the caller's perspective, if you haven't given it something to infer `T` from, it will just default to its constraint. And if there's no constraint, that means `unknown`\n\nHere's some examples that help illustrate the issue:\n```ts\ndeclare function reduce <T, Acc>(inputArr: T[], reducerMethod: (accumulator: Acc, currVal: T) => Acc, initialValue?: Acc): Acc;\n\nreduce([\"string\"], (acc, val) => acc.concat(val), [] as string[])\n// reduce<string, string[]>, 😀\nreduce([\"string\"], (acc, val) => acc.concat(val))\n//                               ^^^\n// Object is of type 'unknown'.\n// reduce<string, unknown>, errors with a somewhat mysterious error 🤔\nreduce<string, string[]>([\"string\"], (acc, val) => acc.concat(val));\n// reduce<string, string[]>, compiles 😭\n```\n","n":0.035}}},{"i":203,"$":{"0":{"v":"Generic Interface","n":0.707},"1":{"v":"\nGeneric interfaces are very much like functions that transform types.\n\nA generic interface lets us define a function signature, and enforce that signature on a future value that we define. Put another way, we are describing a generic function\n```ts\ninterface GenericIdentityFn {\n  <Type>(arg: Type): Type;\n}\n \nfunction identity<Type>(arg: Type): Type {\n  return arg;\n}\n \nlet myIdentity: GenericIdentityFn = identity;\n```\n\nIn this case, when we use `GenericIdentityFn` we now will also need to specify the corresponding type argument (here: `number`), effectively locking in what the underlying call signature will use. This changes its use markedly from the previous implementation.\n```ts\ninterface GenericIdentityFn<Type> {\n  (arg: Type): Type;\n}\n \nfunction identity<Type>(arg: Type): Type {\n  return arg;\n}\n \nlet myIdentity: GenericIdentityFn<number> = identity;\n```\n\nThis changes it slightly. Now we have a non-generic function signature that is a part of the generic type\n- Understanding when to put the type parameter directly on the call signature and when to put it on the interface itself will be helpful in describing what aspects of a type are generic.\n\nIt’s not possible to refer to a generic interface without providing its type argument.\n\nFor example, if you want to write a function that accepts an instance of a generic interface, you have two options:\n1. Write a specialized function that accepts a very specific instance of the interface (e.g. `FormField<string>`)\n2. Write a generic function\n```ts\ninterface FormField<T> {\n    value?: T;\n    defaultValue: T;\n    isValid: boolean;\n}\n\n// Very specialized. Only works with `FormField<string>`.\nfunction getStringFieldValue(field: FormField<string>): string {\n    if (!field.isValid || field.value === undefined) {\n        // Thanks to the specialization, the compiler knows the exact type of `field.defaultValue`.\n        return field.defaultValue.toLowerCase();\n    }\n    return field.value;\n}\n\n// Generic. Can be called with any `FormField`.\n// It’s important to understand that here, T is the type argument of the function, not of the FormField interface. It gets passed only to FormField like any regular type does.\nfunction getFieldValue<T>(field: FormField<T>): T {\n    if (!field.isValid || field.value === undefined) {\n        // On the other hand, we don't know anything about the type of `field.defaultValue`.\n        return field.defaultValue;\n    }\n    return field.value;\n}\n```\n\n### Generic constraints on interfaces (Type argument constraint)\nyou might decide to enforce the fact that `FormField` should only contain string, number, or boolean values.\n```ts\ninterface FormField<T extends string | number | boolean> {\n    value?: T;\n    defaultValue: T;\n    isValid: boolean;\n}\n\n// Type 'T' does not satisfy the constraint 'string | number | boolean'\nfunction getFieldValue<T>(field: FormField<T>): T { /* ... */ }\n```\n\nThe reason for this error is that the `T` type argument of the function has no restrictions. You’re trying to pass it to `FormField` which only accepts types that extend string, number or boolean. Therefore, you get a compile error. To get rid of the error, you need to put the same or stricter restrictions on the type argument `T`.\n\n```ts\ninterface FormField<T extends string | number | boolean> {\n    value?: T;\n    defaultValue: T;\n    isValid: boolean;\n}\n\n// Type 'T' does not satisfy the constraint 'string | number | boolean'\nfunction getFieldValue<T extends string | number>(field: FormField<T>): T {\n    return field.value ?? field.defaultValue;\n}\n```\n","n":0.046}}},{"i":204,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Object where keys are integers as string\n```ts\ninterface IDictionary<TValue> {\n    [id: string]: TValue;\n}\n\n// later\nconst myObject: IDictionary<string> = { \"0\": \"teleport\" }\n```\n\n### Object where keys are not known ahead of time\n[source](https://stackoverflow.com/questions/12710905/how-do-i-dynamically-assign-properties-to-an-object-in-typescript#answer-44441178)\n```ts\nconst constants: Record<string, any> = {}\n\nconstants.dendronTechUrl = 'https://tycholiz.github.io/Digital-Garden/'\n\nexport default constants\n```\n\n#### Create a type that can evaluate to a certain interface, or a function that returns that same interface\n\nHere, `initValueType` will accept as `T` either a type or function that returns that type\n```ts\ntype initValueType<T> = T | (() => T)\n// and use like:\ninitValueType<Response>\n```","n":0.111}}},{"i":205,"$":{"0":{"v":"Function","n":1},"1":{"v":"\n\nThe following function uses closure to remember the pair\n```ts\nfunction makePair<F, S>() {\n  let pair: { first: F; second: S }\n  function getPair() {\n    return pair\n  }\n  function setPair(x: F, y: S) {\n    pair = {\n      first: x,\n      second: y\n    }\n  }\n  return { getPair, setPair }\n}\n\nconst { getPair, setPair } = makePair<string, number>()\nsetPair(5, 3)\n// error: Argument of type 'number' is not assignable to parameter of type 'string'.\n\nsetPair(\"age\", 3)\n// all good!\n```\n","n":0.12}}},{"i":206,"$":{"0":{"v":"Examples","n":1},"1":{"v":"\n### Reduce Function with double type coercion and overloading\n```ts\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn, currVal: TElement) => TReturn\n): TReturn\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn, currVal: TElement) => TReturn, \n    initialValue: TReturn\n): TReturn\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn, currVal: TElement) => TReturn, \n    initialValue?: TReturn\n): TReturn {\n    let acc: TReturn\n    let currentIndex: number\n\n    if (initialValue === undefined) {\n        acc = inputArr[0] as any as TReturn\n        currentIndex = 1\n    } else {\n        acc = initialValue\n        currentIndex = 0\n    }\n\n    while (currentIndex < inputArr.length) {\n        const currentVal = inputArr[currentIndex]\n        acc = reducerMethod(acc, currentVal)\n        currentIndex++\n    }\n    return acc\n}\n```\n\n### Reduce function with unions\n```ts\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn | TElement, currVal: TElement) => TReturn\n): TReturn\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn | TElement, currVal: TElement) => TReturn, \n    initialValue: TReturn\n): TReturn\nfunction reduce <TElement, TReturn>(\n    inputArr: TElement[], \n    reducerMethod: (accumulator: TReturn | TElement, currVal: TElement) => TReturn, \n    initialValue?: TReturn\n): TReturn | TElement {\n    let acc: TReturn | TElement\n    let currentIndex: number\n\n    if (initialValue === undefined) {\n        acc = inputArr[0]\n        currentIndex = 1\n    } else {\n        acc = initialValue\n        currentIndex = 0\n    }\n\n    while (currentIndex < inputArr.length) {\n        const currentVal = inputArr[currentIndex]\n        acc = reducerMethod(acc, currentVal)\n        currentIndex++\n    }\n    return acc\n}\n```\n","n":0.069}}},{"i":207,"$":{"0":{"v":"Class","n":1},"1":{"v":"\n```ts\nclass GenericNumber<NumType> {\n  zeroValue: NumType;\n  add: (x: NumType, y: NumType) => NumType;\n}\n \nlet myGenericNumber = new GenericNumber<number>();\nmyGenericNumber.zeroValue = 0;\nmyGenericNumber.add = function (x, y) {\n  return x + y;\n};\n```\n\n","n":0.189}}},{"i":208,"$":{"0":{"v":"Examples","n":1},"1":{"v":"\n### Making optional code more strict\n\nBefore\n```ts\ninterface Address {\n  street: string;\n  city?: string;\n}\n\ninterface User {\n  name: string;\n  address: Address;\n  meta: Record<string, string>;\n}\n\ninterface SuperUser extends User {\n  permissions: string[];\n}\n\nclass UserRepository {\n  private users: User[];\n\n  constructor() {\n    this.users = [\n      // Do not change the data. Let's assume it comes from the backend.\n      {\n        name: \"John\",\n        address: undefined,\n        meta: { created: \"2019/01/03\" }\n      },\n      {\n        name: \"Anne\",\n        address: { street: \"Warsaw\" },\n        meta: {\n          created: \"2019/01/05\",\n          modified: \"2019/04/02\"\n        }\n      }\n    ];\n  }\n\n  getUser(id: number): User | undefined {\n    return this.users[id];\n  }\n\n  getCities() {\n    return this.users\n      .filter(user => user.address?.city)\n      .map(user => user.address.city);\n  }\n\n  forEachUser(action: (user: User) => void) {\n    this.users.forEach(user => action(user));\n  }\n}\n\nconst userRepository = new UserRepository();\n\nconsole.log(userRepository.getUser(1).address.city.toLowerCase());\n\nconsole.log(\n  userRepository\n    .getCities()\n    .map(city => city.toUpperCase())\n    .join(\", \")\n);\n\nconsole.log(new Date(userRepository.getUser(0).meta.modfified).getFullYear());\n```\n\nAfter\n```ts\ninterface Address {\n  street: string;\n  city?: string;\n}\n\ninterface User {\n  name: string;\n  address: Address | undefined; /* 1 */\n  meta: Record<string, string>;\n}\n\ninterface SuperUser extends User {\n  permissions: string[];\n}\n\nclass SafeUserRepository {\n  private users: User[];\n\n    /* 2 */\n  constructor() {\n    this.users = [\n      // Do not change the data. Let's assume it comes from the backend.\n      {\n        name: \"John\",\n        address: undefined,\n        meta: { created: \"2019/01/03\" }\n      },\n      {\n        name: \"Anne\",\n        address: { street: \"Warsaw\" },\n        meta: {\n          created: \"2019/01/05\",\n          modified: \"2019/04/02\"\n        }\n      }\n    ];\n  }\n\n  // `user` with given `id` might not exist, so marking the return type as possibly undefined\n  getUser(id: number /* 3 */): User | undefined /* 4 */ {\n    return this.users[id];\n  }\n\n  getCities() {\n    return this.users\n        /* 5 */\n      .map(user => user.address?.city)\n      .filter(city => city !== undefined);\n  }\n\n  forEachUser(action: (user: User) => void) {\n    this.users.forEach(user => action(user));\n  }\n}\n\nconst safeUserRepository = new SafeUserRepository();\n\n/* 6 */\nconsole.log(safeUserRepository.getUser(1)?.address?.city?.toLowerCase());\n\nconsole.log(\n  safeUserRepository\n    .getCities()\n    /* 7 */\n    .map(city => city?.toUpperCase())\n    .join(\", \")\n);\n\n/* 8 */\nconsole.log(new Date(safeUserRepository?.getUser(0)?.meta.modfified ?? 0).getFullYear());\n```\n","n":0.06}}},{"i":209,"$":{"0":{"v":"Declaration Merging","n":0.707},"1":{"v":"\n## Overview\nA fundamental concept in TS is the language's ability to describe the shape of javascript objects at the type level.\n- One example of the implementation of this concept is the idea of **Declaration Merging**\n\nIn TS, a declaration creates entities in at least one of three groups: namespace, type, or value.\n- Namespace-creating declarations create a namespace, which contains names that are accessed using a dotted notation.\n- Type-creating declarations do just that: they create a type that is visible with the declared shape and bound to the given name.\n- Value-creating declarations create values that are visible in the output JavaScript.\n![](/assets/images/2021-04-04-13-05-32.png)\n\n## Interface merging\nthe merge mechanically joins the members of both declarations into a single interface with the same name.\n```ts\ninterface Box {\n  height: number;\n  width: number;\n}\ninterface Box {\n  scale: number;\n}\nlet box: Box = { height: 5, width: 6, scale: 10 };\n```\n\nIf 2 objects have a function member with the same name but different signatures, then they will be [[overloaded|paradigm.oop.overloading]], and both functions will appear on the merged object:\n```ts\ninterface Cloner {\n  clone(animal: Animal): Animal;\n}\ninterface Cloner {\n  clone(animal: Sheep): Sheep;\n}\n\n// The two interfaces merged will create a single declaration:\ninterface Cloner {\n  clone(animal: Animal): Animal;\n  clone(animal: Sheep): Sheep;\n}\n","n":0.072}}},{"i":210,"$":{"0":{"v":"Declaration File","n":0.707},"1":{"v":"\nTypeScript is a superset of JavaScript, meaning you can write and use JavaScript libraries from within TypeScript. In such situations how should TypeScript handle the lack of Type information in JavaScript? We can either:\n- Accept the lack of types from the JS file\n- use Declaration files, which give us an ad-hoc way to specify the shape of things in Javascript.\n\n`.d.ts` files are analogous to C++ header files.\n\nWithout considering runtime, Javascript is a very loosely typed language. Our js code can have an object `Person`, call a member that doesn't even exist on it (like `startEngine()`), and no errors will be shown until runtime. This makes it prone to issues.\n- by adding a delcaration module, we can run into fewer issues like this, since we are creating a mapping that declares what types something can and can't take.\n\n## Setting up a declaration file\n1. add a `@types/name-of-third-party-module.d.ts` at the base of your project\n\t- give it contents `declare module \"name-of-third-party-module\"`\n2. add `\"typeRoots\": [ \"./@types\", \"./node_modules/@types\"],` to tsconfig under `compilerOptions`\n\t- this is the directories (in order) that the compiler will look for `.d.ts` files.\n3. in the file you are importing from, add to the top `/// <reference types=\"../../@types/name-of-third-party-module\" />`\n\n## Anatomy\n\nThis [[ESModule|js.lang.imports#es-modules-es6-import,1:#*]] \n```js\nexport const getArrayLength = (arr) => arr.length\n```\n\ncan be typed like this:\n```ts\nexport function getArrayLength(arr: any[]): number;\n```\n\n* * *\n\n### DefinitelyTyped: Incorporating type declarations from third parties\n[DefinitelyTyped](http://definitelytyped.org/) is a project that hosts declaration files for popular JS libraries.\n\nWhen we want to use Javascript 3rd party libraries in our TS project, we can often find declaration modules as packages\n- for example, `@types/lodash` is a module that gives us types out of the box with Lodash.\n- these declaration files get saved in `node_modules/@types/`\n\nWhen a type declaration is included in the `@types` directory, the types are automatically available for us to use in our project, as the TS compiler finds these types and makes them available during compilation time.\n\n### Config\n\"typeRoots\" and \"types\" are the two properties of the tsconfig.json file that can be used to configure the behavior of the type declaration resolution.\n\nThe following config will cause the typescript compiler to look for typings in the following places: first `./@types`, then `./node_modules/@types`\n- note: if we don't set `typeRoots`, `node_modules/@types` is the default place to check. Here we are saying \"let's check locally first, then in `node_modules/@types`\".\n```json\n\"typeRoots\": [ \"./@types\", \"./node_modules/@types\"],\n```\n\n### How to type a third party module\n1. create a `@types` folder at same level as `tsconfig.json`\n2. add a `.d.ts` file using the format `[nameOfTheThirdPartyModule].d.ts`\n3. add the following to top-level of `tsconfig.json`:\n```json\n\"files\": [\n  \"@types/[nameOfTheThirdPartyModule].d.ts\"\n],\n```\n\n\n* * *\n\n# Examples\n### Ajala Example\nImagine we had a Javascript file in a Typescript project:\n```js\n// main.ts\nlet ajala\n\najala = {\n name: \"Ajala the traveller\",\n age: 12,\n getName: function() {\n     return this.name;\n   }\n};\n\najala.lol()\n```\nWhen we compile this file with `tsc main.ts`, the compilation would be successful, and `main.js` is outputted.\n\nIf we then run `node main.js`, we will get a runtime error that `ajala.lol is not a function`. Since no type information was available for it, Typescript could not have warned us about this at compile time.\n\nTo fix this, we can create a file `main.d.ts`:\n```ts\ndeclare module \"MyTypes\" {\n\texport interface Person {\n\t\tname: string;\n\t\tage: number;\n\t\tgetName(): string;\n\t}\n}\n```\n\nAnd then add a triple-slash directive and type annotation in `main.ts`\n- note: this triple-slash directive is possible, but not recommended. Instead, just install npm type declaration modules (below)\n```ts\n/// <reference path=\"Main.d.ts\" />\n\nimport * as MyTypes from \"MyTypes\"\n\nlet ajala: MyTypes.Person\n\najala = {\n\tname: \"Ajala the traveller\",\n\tage: 12,\n\tgetName: function() {\n\t\treturn this.name;\n\t}\n};\n\najala.lol();\n```\n\nWith the declaration file provided and included via the triple-slash directive, the TS compiler now has information about the shape of `Person`, and will throw us an error at compile time.\n\n## E Resources\n[Ajala examples](https://www.geekabyte.io/2017/10/understanding-declaration-files-in.html)\n\n## Resources\n[Declaration files for third-party libraries without `@types` file](https://www.typescriptlang.org/docs/handbook/declaration-files/templates/module-d-ts.html)","n":0.041}}},{"i":211,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Run a typescript file\n```sh\n# the respawn flag will cause the file to be recompiled upon making changes\nnode-dev file.ts --respawn\n```\n\n### Destructure obj/array with types\nIntuitively, we might try and do:\n```ts\nconst { name: string, age: number } = body.value\n```\n\nBut in reality we are renaming the destructed values. The correct way is like this:\n```ts\nconst { name, age }: { name: string; age: number } = body.value\n```\n","n":0.126}}},{"i":212,"$":{"0":{"v":"Compiler","n":1},"1":{"v":"\n## Resources\n[Mini Typescript compiler to help understand its base components](https://github.com/sandersn/mini-typescript)","n":0.316}}},{"i":213,"$":{"0":{"v":"Tmux","n":1}}},{"i":214,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n# Command line commands\nnote: all prefixed with prefix `<C-a>`, except where noted\n- `?` see list of available commands\n\n# Plugins\n###### Fetch new plugins (like PlugInstall)\n`I`\n\n###### Update plugins\n`U`\n\n###### Uninstall plugins not on list\n`opt+u`\n\n# Integration shortcuts\n###### Paste from clipboard\n`p`\n- `P` - choose which buffer to paste from\n\n* * *\n\n## Modes\n### Copy Mode (no prefix)\nenter copy mode\n- prefix + `[`\n\nmove up/down a paragraph\n- `{`/`}`\n\ngo to middle\n- `M`\n\ncopy until end of line\n- `D`\n\n\n### Normal\n- paste from buffer - `]`\n- send the contents of the current buffer to a temp file - `P`\n\t- custom bind\n- open current pane in vim - `v`\n- If you want to switch to a window based on something displayed in it (this also includes window names and titles but not history), (starting with more than one window open) press Ctrl-b f then type the string to search for and press Enter. You will be switched to a window containing that text if it's found. If more than one window matches, you'll see a list to select from.\n\n* * *\n\n### Other\n- edit tmux.conf - `e`\n\n# Resources\nhttp://tmuxp.git-pull.com/en/latest/examples.html\nhttps://www.barbarianmeetscoding.com/blog/2019/12/25/jaimes-guide-to-tmux-the-most-awesome-tool-you-didnt-know-you-needed\n","n":0.076}}},{"i":215,"$":{"0":{"v":"Window Commands","n":0.707},"1":{"v":"\n###### Create new window\n`c`\n\n###### Go to window 1\n`1`\n\n###### Go to last window\n`<C-i>`\n\n###### swap between windows\n`<>`/`<C-l>`\n\n##### Swap window location\n`<C-o>`\n\n###### Close window\n`&`\n\n###### List all windows\n`w`\n\n###### Move window to another session\n`.`\n","n":0.189}}},{"i":216,"$":{"0":{"v":"Session Commands","n":0.707},"1":{"v":"\n###### Detach\n`d`\n\n###### Rename session\n`$`\n\n###### Swap between sessions\n`(`/`)`\n\n###### Attach\n`a`\n- `-t 0` - attach to \"session 0\"\n\n###### New session\n`:new`\n- `-s` - add a name to the session\n`tmux new -s <session-name>`\n\n###### Rename session\n`$`\n\n###### List overview of sessions\n`s`\n\n###### Kill session\n`:kill-ses -t <session-name>`\n\n###### Change default directory of new windows\n`:attach -c /path/to/default/directory`\n\n###### Save session / restore session (tmux-resurrect)\n`<C-s>` / `<C-r>`\n","n":0.136}}},{"i":217,"$":{"0":{"v":"Pane Commands","n":0.707},"1":{"v":"\n###### Split vertically\n`shift + |`\n\n###### Split horizontally\n`shift + _`\n\n###### Close pane\n`x`\n\n###### Last active pane\n`;`\n\n###### Move current pane left/right\n`{`/`}`\n\n###### Pane zoom (cmd+shift+return in iTerm2)\n`z`\n\n###### Swap/rotate pane locations\n`<`/`>`\n\n###### Toggle between horitontal and vertical split\n`<space>`\n\n###### Break current pane out of window\n`!`\n\n###### Resize pane\n`H`/`J`/`K`/`L`\n","n":0.158}}},{"i":218,"$":{"0":{"v":"Third Party","n":0.707}}},{"i":219,"$":{"0":{"v":"Services","n":1},"1":{"v":"\n### Hosting\n- [Render](https://render.com)\n    - *\"If Heroku and Netlify had a baby, it would be Render.com\" -Scott Tolinski*\n- [Begin](https://begin.com)\n- [Netlify: for static apps](https://netlify.com)\n    - plain html or generated with create-react-app or gatsby. It provides a good, fast CDN that is pretty easy to use.\n    - Netlify typically gets used for pure-frontend (it now supports serverless functions too)\n\n- [Vercel](https://vercel.com/)\n    - for dynamic apps like the ones generated with Next.js\n    - with Vercel you can deploy your API next to your frontend (assuming you're using Next.js).\n\n### Diagram sketching\n- tags: wireframing, wireframe\n- [Excalidraw](https://excalidraw.com/)\n- [Lucid](https://lucid.app.com)","n":0.105}}},{"i":220,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\nThe different strategies of testing, whether it's [[unit testing|testing.method.unit]], [[integration testing|testing.method.integration]], [[E2E testing|testing.e2e]] etc., have a common concept of a **System Under Test (SUT)**. In unit and integration(?) tests, it is the actual module being tested. In E2E, it is the whole flow that is being tested.\n\nTesting pyramid is a way to minimize quality risk per investment. Represents an ideal, easier to find issues earlier lower down\n\n## Test Doubles\n![[testing.test-double]]\n\n## Making the test fail at first\nThe goal of doing TDD is to turn the red bar green. The key distinction here is that before we get to the green bar, we have to have the red bar.\n\"if your tests verify your code then what verifies your tests?\". The answer is twofold:\n- your tests verify the code\n- your code verifies the tests\n\nThe purpose of making tests fail first is to improve confidence that we don't have false negatives. That is, we don't want to be in a position where tests are passing for a reason other than the fact that our code is working properly.\n","n":0.076}}},{"i":221,"$":{"0":{"v":"Test Double","n":0.707},"1":{"v":"\nOften, in order to test a module in isolation we need other modules that help the main module perform its action. In cases such as these, we need *test doubles* to stand in for the actual modules.\n\nThe reason why we mock is that we want to replace something we don't control with something that we do control.\n- \"don't control\" can mean a third-party module, or even just a function/class that we wrote that we are using in the current unit we are testing.\n\nThe word “mock” is often used in an informal way to refer to Test Doubles.\n- Semantically speaking, a \"real mock\" refers to the kind described below. In colloquial parlance, it refers indiscriminately to all 5.\n\nThere are 5 types of Test Double:\n- [[Dummy|testing.test-double.dummy]] objects\n- [[Fake|testing.test-double.fake]] objects\n- [[Stubs|testing.test-double.stub]]\n- [[Spies|testing.test-double.spy]]\n- [[Mocks|testing.test-double.mock]] are what we are talking about here: objects pre-programmed with expectations which form a specification of the calls they are expected to receive.\n\nAll Test Doubles can and usually do state verification, but only mocks insist upon behavior verification.\n\nTest Doubles need to make the SUT believe it's talking with its real collaborators.\n\n[[Stubs|testing.test-double.stub]] and [[Spies|testing.test-double.spy]] can probably cover most use-cases for test doubles.\n\n*\"stubs and spies are very easy to write. The IDE makes it trivial. You just point at the interface and tell the IDE to implement it. Voila! It gives you a dummy. Then you just make a simple modification and turn it into a stub or a spy. So you seldom need an actual mocking tool.\"*\n\nIf you have to mock too much then this may indicate a high degree of coupling in your application.\n\nMocked resources add additional coupling between the test and the source code \n- this is because mocks don’t just change the state or what is returned – they assert how an object behaves toward its collaborators. Additionally, since the internals of how the class interacts with its mocked resources are exposed, tests produce encapsulation. Thus, the test would fail if the interaction between a class and the mocked resources is changed when refactoring.\n\nProblems arise when mocks are used to isolate classes. Don't mock internals, privates or adapters.\n\n## Authorizer example\nImagine we had a `System` class, which accepts an `Authorizer` as a parameter. That is, when we instantiate a new `system` object, it is associated with some `authorizer`. Different Test Doubles are used depending on what we want to do in the test.\n```java\npublic class AcceptingAuthorizerStub implements Authorizer {\n  public Boolean login(String username, String password) {\n\t\treturn true;\n\t}\n}\n\npublic class System {\n\tpublic System(Authorizer authorizer) {\n\t\tthis.authorizer = authorizer;\n\t}\n\n\tpublic int loginCount() {\n\t\t//returns number of logged in users.\n\t}\n}\n```\nWhile testing the `System` class, if we:\n- don't even care about the `authorizer` object (since we are not calling any of its methods within the test), we would use a dummy.\n- wanted to simply return `true` for the `authorizer.login()` method, we would use a stub.\n\t- here, we just want to know that we are logged in so we can test the parts of the `System` that require us to be logged in.\n- want to assert that the `login()` method was called by the `authorizer` object, we would use a spy.\n\n* * *\n\n### Mocking vs Stubbing\nA stub is a simple fake object. It just makes sure test runs smoothly. A mock can be thought of as a smarter stub. You verify your test passes through it.\n\nMocks are used to assert and should never return data, stubs are used to return data and should never assert.\n\nA Mock is interaction-based; Stubs are state-based.\n- This means you don't expect from Mock to return some value, but to assume that specific order of method calls are made.\n\nStubs test how your SUT (system under test) handles receiving messages, mocks test how your SUT sends messages.\n\na stub returns answers to questions. A mock also returns answers to questions, but it also verifies that the question was asked\n\nthere may be several stubs in one test, but generally there is only one mock.\n\nA Stub is written with predetermined behavior.\n- The idea is you would have a class that implements the dependency (abstract class or interface most likely) you are faking for testing purposes and the methods would just be stubbed out with set responses. They would not do anything fancy and you would have already written the stubbed code for it outside of your test.\n\nStubs don't care that correct methods have been invoked in mock.\n\nA mock is set up with the expectations.\n- Mocks in a way are determined at runtime since the code that sets the expectations has to run before they do anything.\n\nTests written with mocks usually follow an initialize -> set expectations -> exercise -> verify pattern to testing. While the pre-written stub would follow an initialize -> exercise -> verify.\n\nBoth mocks and stubs testing give an answer for the question: What is the result?\nTesting with mocks are also interested in: How the result has been achieved?\n\n# E Resources\n[A conversation about \"Mocking\"](https://blog.cleancoder.com/uncle-bob/2014/05/14/TheLittleMocker.html)\n","n":0.035}}},{"i":222,"$":{"0":{"v":"Stub","n":1},"1":{"v":"\nStubs provide canned answers to calls made during the test, usually not responding at all to anything outside what's programmed in for the test.\n\nStubs are a kind of [[dummy|testing.test-double.dummy]]\n\nImagine you were testing a part of your system that required you to be logged in. You could simply log in to carry out actions in the test, but why test that functionality again, when we have (presumably) already tested it elsewhere?\n\nInstead, let's just extend the regular `Authorizer` class to override `login` to just be a method that returns `true`.\n- then, when we want to test the part of the system that expects an unauthorized user, we can just return `false` from the stub.\n\n```java\n@Test\npublic void newlyCreatedSystem_hasNoLoggedInUsers() {\n\tSystem system = new System(new AcceptingAuthorizerStub());\n\tsystem.authorizer.login('dude@dude.com', 'securePassword')\n\tassertThat(system.loginCount(), is(1));\n}\n```\n\n* * *\n\nExample: Your test class depends on a method `Calculate()` taking 5 minutes to complete. Rather than wait for 5 minutes you can replace its real implementation with a stub that returns hard-coded values; taking only a small fraction of the time.\n","n":0.078}}},{"i":223,"$":{"0":{"v":"Spy","n":1},"1":{"v":"Spies are [[stubs|testing.test-double.stub]] that also record some information based on how they were called.\n- ex. One form of this might be an email service that records how many messages it sent.\n\nFor example, you’d use this when you wanted to be sure that the `login` method was called by your system.\n\nIn your test you’d inject it like a stub, but then at the end of the test you’d check the `loginWasCalled` variable to make sure the system actually called `login`.\n- therefore, a Spy spies on the caller.\n\nA Spy could be used to...\n- count the number of invocations.\n- see inside the workings of the algorithms you are testing.\n\nThe more you spy, the tighter you couple your tests to the implementation of your system, leading to fragile tests.\n- spec: this is because we are spying on specific implementations of modules that are not within the SUT. Therefore, these units are subject to change without the SUT even being aware. If their implementation changes, we would start getting failing tests. In this case, the spy would have to be updated to use the new implementation.\n\n```java\npublic class AcceptingAuthorizerSpy implements Authorizer {\n  public boolean authorizeWasCalled = false;\n\n  public Boolean login(String username, String password) {\n\tloginWasCalled = true;\n\treturn true;\n  }\n}\n```\n","n":0.07}}},{"i":224,"$":{"0":{"v":"Mock","n":1},"1":{"v":"\nMocking is a technique to isolate test subjects by replacing dependencies with objects that you can control and inspect. It allows us to test the links between code by erasing the actual implementation.\n- we can introspect how many times a mocked function/method was called (along with the args passed), and assert that it matches what we'd have expected to happen.\n- The goal for mocking is to replace something we don’t control with something we do, so it’s important that what we replace it with has all the features we need.\n\nMocks encourage testing based on behavior verification\n- The mock is not so interested in the return values of functions. It’s more interested in what function were called, with what arguments, when, and how often.\n  - therefore a mock is always a spy.\n\nif an object returns or throws based on data passed in then it's a fake, not a mock.\n\nWe mock dependencies because a) we don't care about it in this unit test, and b) we want a predictable result from the external module, so that we can test the main unit under those different conditions\n- ex. if we are mocking a call to Postgres, we want to subject our main unit to testing that will allow it to yield expected results when it is hammered with different inputs provided by an external module. If the postgres dependency returns a populated array, how does the main unit respond? how about if the array is empty? how about if it returns 404?\n\nImagine we have a function that does some computation, then sticks the result in a new file. Since we are unit testing, we aren't testing whether or not the file gets created— that can be safely assumed (since we will be unit testing that separately). Instead, we can safely mock the creation of the file.\n- If we didn't mock this, we'd have to clean up after ourselves each time we run a test. It would result in a slower testing suite.\n\n### Examples\n#### forEach\nIf we were writing a unit test for `forEach`, we would just mock the callback.\n```js\nfunction forEach(items, callback) {\n  for (let index = 0; index < items.length; index++) {\n    callback(items[index]);\n  }\n}\n```\n\nImagine we had a mock function:\n```js\nconst mockCallback = jest.fn(x => 42 + x);\nforEach([0, 1], mockCallback);\n```\n\nThere are a few things we care about when it comes to unit testing this function:\n- was the callback called twice?\n- was the first argument of the first call `0`?\n- was the return value of the first call `42` (42 + 0)?\n\n#### Difference between mock and stub\nA mock is like a stub but the test will also verify that the object under test calls the mock as expected\nex. You can stub a database by implementing a simple in-memory structure for storing records. The object under test can then read and write records to the database stub to allow it to execute the test. This could test some behavior of the object not related to the database and the database stub would be included just to let the test run. If you instead want to verify that the object under test writes some specific data to the database you will have to mock the database. Your test would then incorporate assertions about what was written to the database mock.\n","n":0.043}}},{"i":225,"$":{"0":{"v":"Fake","n":1},"1":{"v":"\nFakes actually have working implementations, but usually take some shortcut which makes them not suitable for production (an in memory database is a good example).\n\nFakes are not written very often, and can probably be avoided outright.\n\na Fake has business behavior. You can drive a fake to behave in different ways by giving it different data.\n- it can therefore be thought of as a simulator.\n\nFakes aren’t stubs because fakes have real business behavior; stubs do not.\n\nWe can say that a Mock is a kind of spy, a spy is a kind of stub, and a stub is a kind of dummy. But a fake isn’t a kind of any of them. It’s a completely different kind of test double.\n\nThey can so complicated that they need unit tests of their own. At the extremes the fake becomes the real system.\n","n":0.085}}},{"i":226,"$":{"0":{"v":"Dummy","n":1},"1":{"v":"\nWe pass a dummy object to something when we don't care how the dummy is used\n\nDummy objects are passed around but never actually used. Usually they are just used to fill parameter lists.\n- We use dummies when we don't actually care about the parameters that are passed to the SUT.\n\n### Example: System Class\nimagine we are testing a `System` object. When we create an instance of `System`, we must specify who the authorizer is. However, for the particular test we are writing, maybe we don't even care about the authorizer, and that parameter is immaterial to the test. A case such as this would be a test where there is not even a user logging in. In this case we would pass the argument, but it would never even get used. In this case we would just use a dummy authorizer, and move on with our day:\n```java\npublic class System {\n\tpublic System(Authorizer authorizer) {\n\t\tthis.authorizer = authorizer;\n\t}\n\n\tpublic int loginCount() {\n\t\t//returns number of logged in users.\n\t}\n}\n\npublic class DummyAuthorizer implements Authorizer {\n  public Boolean authorize(String username, String password) {\n\t\treturn null;\n\t}\n}\n\n@Test\npublic void newlyCreatedSystem_hasNoLoggedInUsers() {\n\tSystem system = new System(new DummyAuthorizer());\n\tassertThat(system.loginCount(), is(0));\n}\n```\n","n":0.074}}},{"i":227,"$":{"0":{"v":"Testing Methods","n":0.707},"1":{"v":"\n![](/assets/images/2021-12-15-13-20-51.png)\n","n":1}}},{"i":228,"$":{"0":{"v":"Unit Testing","n":0.707},"1":{"v":"\nThe biggest misconception engineers have about unit testing is that they think it is about finding bugs.\n- Unit testing is a design tool to help you find leaky, verbose interfaces and implementations that are highly coupled and hard to maintain.\n- When you write unit tests early, you will write different code than if you test late or not at all. Unit tests guide you to clean decoupled designs with minimal interfaces.\n\nThe following sequence works well:\n1.\t⁠Quickly sketch in prototype code that mostly works\n2.\t⁠Comment it all out\n3.\t⁠Write unit tests that force you to comment back in code one small bit at a time.\n4.\t⁠Refactor the code when the unit tests get unwieldy.\n\nIf a test is a pain to write, your code is wrong.\n\n### Setup\nspec: this is what a [[harness|testing.harness]] is.\nBefore running tests, it is often useful to prepare the grounds so that the tests can do their work.\n- We can draw parallels this with the world of manual testing, where a tester would need to seed their database before being able to effectively perform their tests.\n\nex. if we are running tests against the database, then we might want to get some preliminary data in place. We might also want to establish a `pgPool` for us to query with.\n\n### Teardown\nAfter we are done running the tests, we might want to end the `pgPool` connection, or maybe we want to delete all test users from the db\n\n## Snapshot testing\nsnapshots can capture any serializable value and should be used anytime the goal is testing whether the output is correct.\n\nYour tests should be deterministic. Running the same tests multiple times on a component that has not changed should produce the same results every time.\n- if you have a `Clock` component that uses `Date.now()`, the snapshot generated each time the test is run will be different. To fix this, mock the `Date.now()` method.\n\nIn some scenarios, snapshot testing can potentially remove the need for unit testing for a particular set of functionalities (e.g. React components), but they can work together as well.\n\n* * *\n\n#### Common pitfall: not separating interface from implementation\nBecause some classes may have references to other classes, testing a class can frequently spill over into testing another class.\n\nA common example of this is with classes that depend on a database: in order to test the class, the tester often writes code that interacts with the database. This is a mistake, because a unit test should not usually go outside of its own class boundary, and should especially not cross process/network boundaries because doing so can introduce unacceptable performance problems to the unit test-suite. Crossing such unit boundaries turns unit tests into integration tests, so when test cases fail, it is much less clear which component is causing the failure.\n\nInstead, the software developer should create an abstract interface around the database queries, and then implement that interface with their own mock object. By abstracting this necessary attachment from the code (temporarily reducing the effective net coupling), the independent unit can be more thoroughly tested than may have been previously possible. This results in a higher quality unit that is also more maintainable.\n","n":0.044}}},{"i":229,"$":{"0":{"v":"Smoke Testing","n":0.707},"1":{"v":"\nA smoke test is just a [[load test|testing.method.load]] configured for minimal load.\n\na smoke test may address basic questions like \"does the program run?\", \"does the user interface open?\", or \"does clicking the main button do anything?\"\n- \"smoke tests broadly cover product features in a limited time. If key features don't work or if key bugs haven't yet been fixed, your team won't waste further time installing or testing\"\n\nThe phrase smoke test comes from electronic hardware testing. You plug in a new board and turn on the power. If you see smoke coming from the board, turn off the power. You don't have to do any more testing.\n\nThe process of smoke testing aims to determine whether the application is so badly broken as to make further immediate testing unnecessary.\n- Therefore, smoke tests should run quickly, giving benefits of faster feedback\n\nA daily build and smoke test is among industry best practices\n\nSmoke tests can be [[functional tests|testing.method.functional]] or [[unit tests|testing.method.unit]].\n","n":0.08}}},{"i":230,"$":{"0":{"v":"Sanity Testing","n":0.707},"1":{"v":"\nA sanity test (or sanity check) is a basic test to quickly evaluate whether a claim (or the result of a calculation) can possibly be true.\n- it offers \"quick, broad, and shallow testing\".\n\nThe point of a sanity test is to rule out certain classes of obviously false results, not to catch every possible error.\n\nSanity tests may sometimes be used interchangeably with [[smoke tests|testing.method.smoke]] insofar as both terms denote tests which determine whether it is possible and reasonable to continue testing further. However, there is a distinction:\n- a sanity test determines whether the intended result of a code change works correctly\n- a smoke test ensures that nothing else important was broken in the process.\n\nSanity tests are normally followed by more rigorous forms of testing.\n\n### Example: Hello World\nA \"Hello, World!\" program is often used as a sanity test for a development environment in a similar fashion. Rather than a complicated script running a set of unit tests, if this simple program fails to compile or execute, it proves that the supporting environment likely has a configuration problem that will prevent any code from compiling or executing\n\n### Example: Multiplying by 9 divisibility rule\nIn arithmetic, we have the following divisibility rule for the number 9:\n> To find out if a number is divisible by 9, sum the digits. If the result is divisible by 9, then the original number is too.\n\nIn practice,\n> 2880: 2 + 8 + 8 + 0 = 18\n\nTesting to make sure that this logic is fulfilled is a sanity test. It's not covering everything that could go wrong, but it's just giving us a reasonable baseline of expectation for our function.\n","n":0.061}}},{"i":231,"$":{"0":{"v":"Load Testing","n":0.707},"1":{"v":"\nLoad testing is the practice of modeling the expected usage of a software program by simulating multiple users accessing the program concurrently.\n- it can be thought of as a unit test, but instead of testing the logic of our system, we are testing the performance of our system.\n\nFor example we can load test a multi-user system on a client-server architecture. But we can also load test programs like Microsoft Word or Sketch by forcing them to read very large documents. We could test a personal finance software by forcing it to generate a report based on several years' worth of data\n\nWe should load test a website whenever you are concerned about the availability and scalability of your website.\n\nFor example, a load test simulating hundreds of concurrent users could validate that:\n- Your server doesn't respond with errors.\n- The response time for 95% of your users should be below 400ms.\n- The response time of your images must always be below 600ms.\n\nThe most accurate load testing simulates actual use, as opposed to testing using theoretical or analytical modeling.\n\nLoad testing tools analyze the entire [[OSI|network.osi-model]] protocol stack\n- contrast with most regression testing tools, which focus on GUI performance\n- ex. a regression testing tool will record and playback a mouse click on a button on a web browser, but a load testing tool will send out hypertext the web browser sends after the user clicks the button.\n\t- In a multiple-user environment, load testing tools can send out hypertext for multiple users with each user having a unique login ID, password, etc.\n\nload testing tools provide insight into the causes for slow performance, whether it's caused by...\n- Application server(s) or software\n- Database server(s)\n- Network – latency, congestion, etc.\n- Client-side processing\n- Load balancing between multiple servers\n\nWhen the load placed on the system is raised beyond normal usage patterns to test the system's response at unusually high or peak loads, it is known as stress testing.\n- in which case the load is usually so great that error conditions are the expected result, but there is no clear boundary when an activity ceases to be a load test and becomes a stress test.\n\nThe term \"load testing\" is often used synonymously with concurrency testing, software performance testing, reliability testing, and volume testing for specific scenarios.\n- All of these are types of non-functional testing that are not part of functionality testing used to validate suitability for use of any given software.\n\n### What a Load Testing framework/tool does\nThe following procedure happens across all load testing tools and frameworks:\n\n1. When customers visit your website, a script recorder records the communication and then creates related interaction scripts.\n2. A load generator tries to replay the recorded scripts, which could possibly be modified with different test parameters before replay.\n3. In the replay procedure, both the hardware and software statistics will be monitored and collected by the conductor.\n\t- these statistics include the CPU, memory, disk IO of the physical servers and the response time, the throughput of the system under test (SUT), etc.\n4. At last, all these statistics will be analyzed and a load testing report will be generated.\n\n### Shopping cart example\nA website with shopping cart capability is required to support 100 concurrent users broken out into the following activities:\n- 25 virtual users log in, browse through items and then log off\n- 25 virtual users log in, add items to their shopping cart, check out and then log off\n- 25 virtual users log in, return items previously purchased and then log off\n- 25 virtual users just log in without any subsequent activity\n\nA test analyst can use various load testing tools to create these VUsers and their activities. Once the test has started and reached a steady-state, the application is being tested at the 100 VUser loads as described above. The application's performance can then be monitored and captured.\n\n### Analysis\nThe *n*th percentile is important because it is usually how you set up your \"failure\" threshold. If you decide that you want your service to respond within 500ms \"most of the time\" then you would set your toleration for the 90% threshold to be 500ms. This lets you push your load as far as it can go within this toleration limit.\n- ex. you have a service `WidgetService`. You set up your toleration to be 500ms. Then you start to bombard `WidgetService`. At 80 requests per second, `WidgetService` starts to have 88% of its requests take less than 500ms, but 12% take more than that. This would be considered a fail.\n\t- By analyzing what your % spread looks like at various numbers of requests per second, you can determine how many instances of `WidgetService` you need. This is why the nth percentile is important.\n\t- If it turns out that you have to guarantee < 200ms requests and 10% of your requests in an interval fail this at 20 requests per second, then that means 2 times per second you are just \"not good enough.\" If you optimize your balancing / routing and never have a sub-par request, then you've done your job... but you can't adequately do your job if you have no data.\n\n## Tools\n- [[k6]]\n- [Locust](https://locust.io/)\n","n":0.034}}},{"i":232,"$":{"0":{"v":"Integration Testing","n":0.707},"1":{"v":"\nWith integration tests, we are testing relationships between services\n- A naive approach might be to get all of the dependent services up and running for the testing environment. But this is unnecessary, and creates a lot of potential failure points from services that our outside of our control.\n\t- Instead, we could narrow it down by writing a few service integration tests using mocks and stubs.\n\nIntegration tests use live deployed applications.\n\nIn integration testing, the rules are different from unit tests. Here, you should only test the implementation and functionality that you have the control to edit. Mocks and stubs could be used for this purpose.\n\nIn integration or [[E2E|testing.method.e2e]] testing, the first step to uncovering what tests to write is to understand what could go wrong.\n\nImagine we were making a banking app with the following 5 modules:\n![](/assets/images/2021-12-06-12-39-40.png)\n\nEven if we had all modules done except for the 5th, we could still test the integration. We would do this by stubbing the 5th service.\n\nIntegration tests are notoriously more difficult to write when using a [[Service-oriented architecture|general.arch.SOA]]\n\n### Downsides\nIntegration tests give us confidence to release, but...\n- introduce dependencies\n- give slow feedback\n- break easily\n- require lots of maintenance","n":0.072}}},{"i":233,"$":{"0":{"v":"Functional Testing","n":0.707},"1":{"v":"\nFunctional tests exercise the complete program with various inputs.\n\nFunctional tests may comprise a scripted series of program inputs, possibly even with an automated mechanism for controlling mouse movements.\n\nFunctional tests differ from [[contract testing|testing.method.contract]], since...\n- contract tests focus on the interaction points (ie. messages that flow between a consumer and provider)\n- while functional tests also ensure that the correct side effects have occured.","n":0.127}}},{"i":234,"$":{"0":{"v":"E2E","n":1},"1":{"v":"\nend-to-end testing – testing the full end-to-end flow of your project. This includes external HTTP calls and complete flows within your project.\n\nIn integration or E2E testing, the first step to uncovering what tests to write is to understand what could go wrong.\n\n## Cypress vs Playwright\nPlaywright use the Chrome DevTools Protocol (CDP) which allows automation directly in the browser and is supported by all the modern ones. This means, it doesn't run in the execution loop of the browser and as such needs an external process (Node, ...) to drive it. It's a bit like Selenium, which by default uses its own protocol+implementation to drive the browser (the webdrivers). Note that from the version 4,Selenium is now able to use the CDP protocol and its features.\n\nCypress is an Electron App (Native JS app on desktop) which embeds the browser window in it. The library and your code is directly injected in the execution loop of the browser and needs to modify its default behaviour to achieve this. It uses its own implementation for this purpose although a smoother integration with CDP is in progress, but it's not exposed via its API. Node is also used but the main part of the control is delegated to the custom injected library (in the same process, I guess). PW (as Puppeteer) delegates this control to the CDP implementation of the browser which is standardised and maintained by the browser providers.\n\nWhat are some consequences coming from these differences ?\n- Cypress is limited to Javascript and transpiled languages as it runs in the browser. Playwright is language independant and gets by default JS/TS, Python, C# and Java support. But you can easily use it with other frameworks/languages like RobotFramework for instance.\n- Cypress needs some adaptation for each type of browser and currently Safari is not supported. Playwright is supported on all the modern browsers and even gets experimental support for native mobile testing.\n- CP doesn't support tabs. PW does.\n- Support of Iframes is possible with CP but has sometimes an erratic behaviour and is harder to use than with PW in our experience.\n- As it runs directly in the browser, you can setup some shortcuts in your test/fixtures for more efficiency with CP. It means that you can also implement Component/Unit tests with it and it's a promising feature. PW is focused on E2E/System tests and you need another framework to handle the Unit/Component testing part (Jest, Karma/Jasmine ...). You have the choice to use a single framework or 2 different with CP. On the opposite your tests are more close to the real end user's experience with PW as with CP which modifies the browser's behaviour.\n- Although the integration with some high level (Acceptance/BDD) frameworks is possible, it's not as easy and more limited than with PW. For instance, when we integrate CP with CucumberJS the test runner is provided by CP and the structure of your scenarios and your implemented steps is more constraining. PW not only offers an integration with CucumberJS, RobotFramework, Gauge, but CodeceptJS provides also an official and excellent support for it and CP is not on their roadmap.\n- On the CI aspect. If you don't master the building agents configuration, you probably need a containerisation solution (Docker) to deploy it. We just need npm install for PW.\n- Another point is about test parallelisation. PW does provide it out of the box for free. CP doesn't. You'll have to subscribe to their SaaS offer to run your tests in parallel, or find some workarounds which are not perfect.\n- Remote browsing concern: Playwright is compliant with an internal/self hosted Selenium grid (https://playwright.dev/docs/selenium-grid). Both are supported on SaaS solutions (BrowserStack, ...)\n- On the UI/API aspect, they are relatively close. Both provide a runner, API mocking and fixtures, advanced locators, .... PW uses a classical pattern to manage asynchronicity (async/await) and CP has chosen a kind of promise answer (dot notation, which is in definitive not totally async) to simplify the code with autocompletion. To structure and reuse your code, CP also has a different approach with custom commands to extend the cy keyword which is somewhat similar to the user centric approach of TestingLibrary or Codecept (the I prefix).\n\n## Frameworks\n- [Playwright](https://playwright.dev/)\n- Cypress\n- Selenium\n","n":0.038}}},{"i":235,"$":{"0":{"v":"Contract Testing","n":0.707},"1":{"v":"\n## Why do it?\nContract-based unit tests tell us if API endpoint connections are active and working properly.\n\nWith good contract tests in place, we should be reasonably assured that our code doesn't break upstream/downstream dependencies because of other teams unexpectedly changing their APIs.\n- ex. a good reason to adopt contract tests would be if you had a service that served data to many different teams in your software organization. Say we had 5 different teams that consumed data from our interal API. It would be hard for us to know and keep track of which data each client expects. With contract tests, we can be kept aware of what each client needs, and know immediately if changes to our API will affect anyone who depends on it. This will save us in situations where say, we upgrade our API to v2 and break a few consumers (out of potentially dozens), because for instance the shape of the data changed.\n\nA good contract test for a deployed service gives us confidence that:\n- The API under test understands the request made of it\n- The API under test can send the response expected of it\n- The endpoints are accessible\n- The provider and the consumer have a working connection (assuming we are testing against a deployed service)\n- An expected integration is working\n\nThe whole point of contract testing is to enable independent releases of components and [[continuous delivery|deploy.CD]].\n- Coupling this way prevents this and scales poorly as you add more teams and components.\n\n## What is it?\nContract testing is an alternative to e2e integrated testing.\n\nContract testing is a methodology for ensuring that two separate systems (such as two microservices) are compatible and are able to communicate with one other\n- interactions exchanged between each service are captured, storing them in a contract, which can then be used to verify that both parties adhere to it.\n\t- to engage in contract testing is to interrogate a deployed (or mocked) service endpoint to get information back and compare to the expected values.\n- contract testing requires both parties to come to a consensus on the allowed set of interactions, and allowing for evolution over time.\n\nThe difference between contract testing and other methods that aim to achieve the same thing is...\n- each system is able to be tested independently from the other \n- the contract is generated by the code itself, meaning the contract is always kept up to date with reality.\n\nContract testing is more relevant with a [[microservice|general.arch.microservice]] architecture, given its distributed nature.\n\nContract tests exist to help with [[integration testing|testing.method.integration]].\n- In a distributed system, integration testing is a process that helps us validate that the various moving parts that communicate remotely - things like microservices, web applications and mobile applications - all work together cohesively.\n\n\n## How does it work?\nWhen writing contract tests, you have to ask yourself: am I the consumer or the provider?\n- if consumer, then you need to look at only the endpoints you consume\n- if provider, then you need to look at *all* your endpoints\n\nThe contract must be available to all parties, which serves as our API documentation.\n\n1. The consumer (client) makes a request to a mock provider, and in turn receives a response. The request and response are both placed into a contract:\n\t```\n\trequest: GET /users/123\n\tresponse: 200 OK {\"name\": \"Mary\"}\n\t```\n2. A simulated consumer (provided by a tool like Pact) then replays each request against the real provider and compares the actual and expected responses. If they match, we've verified that simulated applications should behave as real applications.\n![](/assets/images/2021-12-15-13-18-27.png)\n\n### Consumer/Provider\nThe consumer initiates the HTTP request.\n\nThe consumer side bargains to keep the [[mock|testing.test-double]] provider aligned with the contract, while the producer does everything to follow the contract.\n- because of this, integration-like tests can be run without requiring the actual service provider to be available.\n\nFor applications that use...\n- HTTP \n\t- the consumer is always the application that initiates the HTTP request (eg. the web front end), regardless of the direction of data flow.\n\t- the provider is the application that returns the response.\n- Queues \n\t- the consumer is the application that reads the message from the queue.\n\t- the provider (also called producer) is the application that writes the messages to the queue.\n\nThe provider is often called the `service`\n\n## Implementation\nPact tests exist as unit tests in the source code of the consumer.\n- ex. if we are testing an `OrderApiClient` class, whose responsibility is to make an http request, then this would be the unit to apply the contract test to.\n\nIn the unit test, we set up a few things:\n- The [[mock|testing.test-double]] provider service, specifying a port at which it will run.\n\t- The mock service will respond to the client's queries over HTTP as if it were the real `OrderApi`.\n- The mock provider object, which allows us to set expectations.\n\nExample provider:\n\n```js\n// Setup Pact\nconst provider = new Pact({\n  port: 1234,\n  log: path.resolve(process.cwd(), \"logs\", \"pact.log\"),\n  dir: path.resolve(process.cwd(), \"pacts\"),\n  consumer: \"OrderWeb\",\n  provider: \"OrderApi\"\n});\n\n// Start the mock service!\nawait provider.setup()\n```\n\n## Value of contract testing\nContract tests generally have the opposite properties to E2E-integrated tests:\n- They run fast, because they don't need to talk to multiple systems.\n- They are are easier to maintain: you don't need to understand the entire ecosystem to write your tests.\n- They are easy to debug and fix, because the problem is only ever in the component you're testing - so you generally get a line number or a specific API endpoint that is failing.\n- They are repeatable\n- They scale. Because each component can be independently tested, build pipelines don't increase linearly / exponentially in time\n- They uncover bugs locally. Contract tests can and should run on developer machines prior to pushing code.\n- Contract testing keeps the API producers in sync with the consumers.\n\nMostly used for:\n- Detecting irregularities in a consumer workflow\n- Detecting any service configuration defects\n- Keeping the connections safe even when the producer changes any service configuration\n\nIn contract testing for message queues, for the purpose of verifying a contract, the underlying message queue is mostly irrelevant. As long as you can ensure that the message producer is generating a correctly formatted message, and the consumer is able to consume it, you don’t need an actual message queue.\n\n## Contract testing types\n### Consumer-driven\nThis means the consumer oversees contract creation. \n\nWhy would a consumer take charge of contract creation?\n- In most cases, consumers only care about a certain subset of data. They use a certain set of fields, they expect certain status codes etc. All of these details gets put into a contract that is then agreed to by the provider. Now, the provider has a good idea of what its consumers actually need, and now the provider can be free to develop its API without fear of breaking things for its consumer, since it now needs to make sure that it still satisfies the contracts laid out by the consumers.\n\n### Producer-driven\nRarely used.\n\n## Testing Broker\nOnce we have our contract tests in place, we need somewhere to put them. They don't do much good packaged alongside the repo whose code they are testing. Instead, we need it somewhere where it can be used to:\n- Share and collaborate on contracts across teams\n- Manage contracts across code branches and environments\n- Orchestrate builds to know when it is safe to deploy\n- Integrate into your processes and tooling\n\nPact Broker is an example of a testing broker, and it can be run both managed and self-hosted (e.g. as a [[Docker container|docker.containers]])\n\n# E Resources\n- [Pact: Contract testing tool]\n- https://dius.com.au/2017/08/22/contract-testing-serverless-and-asynchronous-applications/\n","n":0.029}}},{"i":236,"$":{"0":{"v":"Pact","n":1},"1":{"v":"\nThere are 3 components to implementing a [[contract test|testing.method.contract]] with Pact:\n- Consumer - initiates the HTTP request.\n- Pact Mock Provider\n- Interaction\n\nWith Pact testing, a service is never talking directly to another service. Instead, we only ever test one application at a time, and we're only going to capture its view of the integration point.\n- the consumer will talk to a mock of the provider (supplied by Pact)\n- Pact pulls all of the contracts (from all consumers) from the Pact Broker and replays the requests against the provider (contract validation)\n\nPact tests will only fail if the specific subset of the data that the consumer cares about is changed on the producer side. If we are making breaking changes to the provider, then we'll know it as soon as we run the provider contract tests (can even be run locally).\n- ex. if an API endpoint offers 3 fields (`make`, `model`, `year`) but our consumer only uses the first 2, if the provider changes its API to remove the `year` field, the contract test will not fail.\n\n## How it works\n### Consumer-side\n1. For each integration point with a given provider, we write the interaction in the consumer (commonly as a unit test), specifying the expected request and response.\n    - therefore, each route will have a unit test\n\nRunning the unit tests in CI/CD...\n\n2. The consumer fires a real request to a mock provider (provided via Pact framework).\n3. The mock provider checks to see if the request matches the expected request from the interaction.\n4. Assuming it was successful, the consumer confirms that the response was correctly understood.\n5. A file is generated that records all of the interactions between the consumer and the provider. This record is called a pact, and it is published to the Pact Broker (*pact publication*).\n    - a new *consumer version* will be generated any time the consumer changes\n      - Multiple consumer versions can all refer to the same pact version (this happens when changes to the consumer don't change the pact).\n      - A specific version of the consumer (ie. consumer version) will always only have one pact version for each provider.\n    - a new *pact version* will be generated only when the contents of the pact change (ie. the request/response shape)\n\n![](/assets/images/2022-11-07-12-40-45.png)\n\n6. (can-i-deploy; optional) this part of the Pact system answers the question \"is it safe to deploy this consumer to the specified environment?\"\n    - the first time, the answer will be NO, because the Provider has never verified the contract. In other words, there is no Provider in production that satisfies this contract.\n\n### Provider-side\nOnce the consumer-side is done (ie. once we've created pacts for the consumer), we need to validate the pacts against the provider.\n\n1. We run our provider server locally (e.g. if our provider is an express server, then we start the server)\n\nRunning the unit tests in CI/CD...\n\n2. We write a unit test that fetches all the pacts for the provider and checks that the provider can satisfy them by running them against the URL for the provider (which we supply in the provider unit tests).\n    - `return new Verifier(opts).verifyProvider()`, which starts the mock server.\n3. The result of the testing will then be sent back to the Pact Broker to record which *provider version* can/cannot satisfy which *pact verions*.\n4. (can-i-deploy; optional) answers the question \"is it safe to release this *provider version* to production?\"\n5. Deploy: since the first time around, the consumer is not in production (step 6 above), so it will be safe to move this provider version to production.\n6. Check the deployed pact broker, and the contract should be green (verified)\n7. Re-run the consumer pipeline. This time the can-i-deploy step will pass and the consumer will be deployed to the specified environment.\n\nFor each interaction in a pact file, the order of execution is as follows:\n- `BeforeEach` -> `StateHandler` -> `RequestFilter (pre)` -> `Execute Provider Test` -> `RequestFilter (post)` -> `AfterEach`\n\n### Broker process\nOnce we have the interactions generated from the consumer-side unit Pact tests, we move to the provider side. Pact takes that list of interactions and replays them onto your server (ie. Pact makes HTTP requests to your live server)\n- Running the consumer side of the pact contract tests generates the contract itself, which is then used by the Pact broker to send the same requests to your actual provider. \n    - This contract (pact) will be stored in the pact broker.\n- The provider then pulls the contract from the pact broker and replays this request against their local environment, by verifying the request and response match with the consumer requirements.\n\nThere are a number of choices that can be made here, but usually these are the choices:\n- Invoke just the controller layer (in an MVC app, or the \"Adapter\" in our diagram) and stub out layers beneath.\n- Execute the code up to he service/business logic layer, but mock out the data persistence layer\n- Choosing a real vs mocked out database\n- Choosing to hit mock HTTP servers or mocks for external services\n\nGenerally speaking, it's easiest to test the entire service and mock out external services such as downstream APIs (which would need their own set of Pact tests)\n- this is what gives us some of the benefit offered by [[integration tests|testing.method.integration]] without the high cost of maintenance.\n\nIf the database can be easily set up/torn down locally, then the real database is probably easiest to use. If necessary, you can just stub out the data persistence layer.\n\n* * *\n\nPact will ensure that the provider returned the expected object, but we must test that our code receives the right object.\n- this is often the same as the object that was in the network response.\n\nThe Pact Framework is of course called from within the client's testing framework (e.g. [[jest|jest]]).\n\nThe Pact Framework offers a Pact Mock Provider and a Pact Mock Consumer.\n\n## Interactions\nEach pact is a collection of interactions. An interaction is a request-response pair.\n- The first step in writing a pact test is to describe this interaction.\n\nEach interaction is considered to be independent. This means that each test only tests one interaction\n\nPact tests operate on each interaction. They ask the question, \"assuming the provider returns the expected response for this request, does the consumer code correctly generate the request and handle the expected response?\"\n\nUsually, the interaction definition and consumer test are written together\n- the interaction says \"upon receiving a POST request to /event, with an `event` object in the body, we will respond with 200 OK\"\n- the test triggers the client code to generate the request and receive the response\n\nIf we pair the consumer test and provider verification process for each interaction, the contract between the consumer and provider is fully tested without having to spin up the services together.\n\nInteractions for HTTP and Messages look different:\n- For HTTP:\n    - An expected request - describing what the consumer is expected to send to the provider\n    - A minimal expected response - describing the parts of the response the consumer wants the provider to return.\n- For messages:\n    - The minimal expected message - describing the parts of the message that the consumer wants to use.\n\n### The pact describes a series of interactions\nOnce all of the interactions have been tested on the consumer side, the Pact framework generates a pact file, which describes each interaction:\n\n![](/assets/images/2022-01-19-15-36-29.png)\n\n### Provider state\nIf you need to describe interactions that depend on pre-existing state, you can use provider states to do it. Provider states allow you describe the preconditions on the provider required to generate the expected response - for example, the existence of specific user data. \n- ex. Instead of writing a test that says “create user 123, then log in”, you would write two separate interactions - one that says “create user 123”, and one with provider state “user 123 exists” that says “log in as user 123”.\n\nThe provider should be in a particular state when a given request is replayed against it\n- e.g. “when user John Doe exists” or “when user John Doe has a bank account”. \n- These allow the same endpoint to be tested under different scenarios.\n\nA provider state name is specified when writing the consumer specs, then, when the pact verification is set up in the provider the same name will be used to identify the set up code block that should be run before the request is executed.\n\n## Provider Verification\nVerification ultimately tells us if a particular pact version will work with a particular version of the provider.\n- when we run the verification, Pact fetches the contracts and replays each request against the provider and ensures that the provider's response matches the consumer's expectations as recorded in the pact. If the two match, then we know the consumer and provider are compatible.\n\nTo verify a pact we must:\n1. configure the URL of the pact (a path of the pact broker URL)\n2. set up data for the provider states\n    - provider states allow us to set up data on the provider by injecting it straight into the data source right before the interaction is run. In this way, it can make a response that matches what the consumer expects.\n3. configure and run the provider app so that we can run the requests against it.\n\nEvery time we run a verification in CI/CD, the results are published to the Pact Broker (known as *verification result*).\n\nWhenever the provider changes, it is considered as a new *provider version* (identified at a minimum by git commit SHA + git branch)\n- as with a consumer, you should associate a particular version of the provider with 1+ branch.\n\nWith provider verification, each request is sent to the provider, and the actual response it generates is compared with the minimal expected response described in the consumer test.\n- Provider verification passes if each request generates a response that contains at least the data described in the minimal expected response.\n\nIn many cases, your provider will need to be in a particular state (such as \"user 123 is logged in\", or \"customer 456 has an invoice #678\")\n- Pact enables us to do this: it allows us to set the provider state before the interaction is replayed.\n\nUnlike Consumer testing, provider verification is entirely driven by the Pact framework\n\n* * *\n\n### Random Data\nAvoid random data. If you are using a Pact Broker to exchange pacts, then avoid using random data in your pacts. If a new pact is published that is exactly the same as a previous version that has already been verified, the existing verification results will be applied to the new pact publication. This means that you don't have to wait for the provider verification to run before deploying your consumer - you can go straight to prod. Random data makes it look like the contract has changed, and therefore you lose this optimisation.\n\n* * *\n\n### Artifacts\n#### Contract\nPact is another word for a contract.\n\nA Pact defines:\n- the consumer name\n- the provider name\n- a collection of interactions\n- the pact specification version\n\n##### Pact verification\nThe contract must be verified, so the requests contained in a Pact file are replayed against the provider code, and the responses returned are checked to ensure they match those expected in the Pact file.\n\n# Resources\n- [Example](https://github.com/YOU54F/jest-pact-typescript/blob/master/src/pact/client/requestPathMatching.pacttest.ts)\n- [Another Example](https://github.com/alessioerosferri/pact-demo)\n# UE Resources\n- [non-HTTP testing](https://docs.pact.io/getting_started/how_pact_works/#non-http-testing-message-pact)","n":0.023}}},{"i":237,"$":{"0":{"v":"Pact API","n":0.707},"1":{"v":"\n## Matching\ntldr; be strict with what we request but relaxed with what we expect.\n- see [[Postel's Law|general.principles#robustness-principle-postels-law,1]]\n- But Pact breaks Postel's law when it comes to request headers\n\n### Request Matching\nAs a rule of thumb, you generally want to use exact matching when you're setting up the expectations for a request (`upon_receiving(...).with(...)`) because you're under control of the data at this stage\n- Note that the request matching does not allow \"unexpected\" values to be present in JSON request bodies or query strings. (It does however allow extra headers, because it was found that writing expectations that included the headers added by the various frameworks that might be used resulted in tests that were very fiddly to maintain.)\n\n### Response Matching\nYou want to be as loose as possible with the matching for the response\n- This stops the tests being brittle on the provider side. \n- Generally speaking, it doesn't matter what value the provider actually returns during verification, as long as the types match. \n    - When you need certain formats in the values (eg. URLS), you can use `terms`\n\nBe selective with adding matchers in the response.\n- ex. the provider might be currently returning a GUID, but would anything in your consumer really break if they returned a different format of string ID? (If it did, that's a nasty code smell!)\n\n### `Pact.term`\n- `matcher` will verify that the value matches the given pattern.\n    - Sometimes we want to hit an endpoint that uses an id. Since we can't explicitly put an id in the interaction, we can use a *Matcher*, which may take the form of a [[Regex|regex]].\n- `generate` allows us to specify a value that will be replayed against the provider as an example of what the real value would look like.\n    - this means provider states can still use known values to set up their data, but your Consumer tests can generate data on the fly. \n\n`Pact.term` can be used for request and response header values, the request query, and inside request and response bodies.\n\n### `Pact.like`\nThis is for when we don't care what the exact value is at a particular path, you just care that a value is present and that it is of the expected type.\n- can wrap a whole object, or a single value:\n```js\nbody: Pact.like(\n      name: \"Mary\",\n      age: 73\n)\n// or\nbody: {\n      name: \"Mary\",\n      age: Pact.like(73)\n}\n```\n","n":0.051}}},{"i":238,"$":{"0":{"v":"Component Testing","n":0.707},"1":{"v":"\nCould have been called a \"microservice test\", since it involves testing the microservice as a whole.\n\nUnit testing is a way to test behaviour within a specific module, integration testing is a way to test multiple modules as a group, and Component testing is a way to verify that the (micro)service works as a whole to satisfy the business requirements. \n- therefore it is about testing all of the modules that work together toward some business requirement.\n- naturally, there may be some modules of a service that don't get tested during a component test, if they do not directly contribute to the service's business requirement.\n\nyou can think of component testing as end-to-end testing an entire service that is isolated from external dependencies and any collaborating microservices with [[test doubles|testing.test-double]].\n- By writing tests at this granularity, the contract of the API is driven through tests from the perspective of a consumer. Isolation of the service is achieved by replacing external collaborators with test doubles and by using internal API endpoints to probe or configure the service. \n","n":0.076}}},{"i":239,"$":{"0":{"v":"Test Harness","n":0.707},"1":{"v":"\nA Test Harness is the collection of all the items needed to test software at the unit, module, application or system level, and provides the mechanism to execute the tests\n- Every item such as input data, test parameters, test case, test script, expected output data, test tool, and test result report is part of the test harness.\n\nA test harness (a.k.a *automated test framework*) is a combination of:\n1. The code to be tested\n2. The test data\n    - because we need to test different circumstances, there will be multiple sets of test data\n\nTest harnesses allow for the automation of tests\n- They can call functions with supplied parameters and print out and compare the results to the desired value.\n\nA test harness should allow specific tests to run (this helps in optimizing), orchestrate a runtime environment, and provide a capability to analyse results.\n\nThe typical objectives of a test harness are to:\n- Automate the testing process.\n- Execute test suites of test cases.\n- Generate associated test reports.\n\nThink of a Test Harness as an 'enabler' that actually does all the work of (1)executing tests using a (2)test library and (3)generating reports. It would require that your test scripts are designed to handle different (4)test data and (5)test scenarios. Essentially, when the test harness is in place and prerequisite data is prepared (aka data prep) someone should be able to click a button or run one command to execute all your tests and generate reports.\n","n":0.065}}},{"i":240,"$":{"0":{"v":"Test-Driven Development","n":0.707},"1":{"v":"\nThe objective of TDD is to test behaviours in the system.\n- this is accomplished by writing tests first that describe behaviour of the software, then writing code that passes those tests.\n\nWhile TDD is accomplished with [[testing.method.unit]], the *unit* does not necessarily correpond to a class/module. Instead, the system under test can be thought of as an API that implements some behaviour. As a result, a unit may encompass multiple classes, so long as they all achieve some higher level behaviour of the system.\n\n## Behaviour nature of TDD\nTDD could be thought of as *behavioural testing*, if that term wasn't already appropriated by [[development-process.agile.BDD]]. The idea is that we view isolation as modules of behaviour, rather than as individual classes. We define our units as modules that don't cross a port (such as making a network call, or accessing the filesystem). In other words, we test at the level of our *units*, which perform some self-isolated action that we can test for. Therefore, a unit may involve more than one class. The implementation details are immaterial. What matters is that we can test the inputs and outputs of this unit (the surface area). What this means is that we can structure our unit tests in such a way where we are testing for behaviour. Then, once all the tests are written, we can start to implement the code. Subsequent refactorings of the code shouldn't involve changes to the tests, since tests don't worry about implementation details. The implementation could be one big function, or it could be several.\n- here, isolation is about not crossing ports, rather than sticking to a single class.\n\nTypically, the trigger to following a TDD cycle is the necessity of a new method on a class. People think \"I need to add a method that does this and that, so I will first write a test to cover that implementation, then write the method to make all tests pass\". This is the wrong way to go about it.\n- instead, the trigger should be that the system has a new requirement. Therefore, \"I want the software to do this new thing, so I will start by writing tests that test that this system does that thing\".\n- this shows how testing should be declarative, not imperative.\n  - we don't test that `foo()` gets called 2 times when `bar()` is called. This is a flaky test, since if our implementation changes, it's likely the test will change too.\n- Thought of another way, we want to test the API of our software, not the implementation details.\n\nWhen we think of individual classes as our units, we tend to mock more heavily. This leads to overspecified tests, whereby we know all the implementation details of our classes via our mocks and how they are called.\n- an overspecified test is a test that makes too many assumptions about how the system under test is implemented, rather than focusing on the system's surface (API). These assumptions lead to tests that break when implementation details change.\n\nBecause of the focus on software behaviour, TDD forces us to think in terms of what the consumer of that API would find useful. What results are tests that match the way that consumers actually use the code.\n\n### Red, Green, Refactor\nCommonly thought to mean *\"write a test, make sure it fails. Write some code to pass the test. Tidy up a bit\"*, but actually means:\n1. Write a test that represents the behaviour that is needed from the system. It compiles, but the test fails.\n  - this represents a requirement for the program\n  - we want to see our test fail because there are no tests for tests.\n2. Write some minimal code to make the test pass. This is as quick-n-dirty as it needs to be, and involves no patterns, no designs, and no structure. Just stick code in a method that might not even belong in that class. As long as it gets the job done, it's good\n  - we do this because once we've made the test pass, we've understood how to implement the requirements. \n    - at this point we can move fast. We can feel more free to go to Stackoverflow and copy some code. \n  - another reason we do this is because we are focusing on one thing at a time: get the behaviour of the code right, then engineer it well. It is far more difficult to do both at the same time, and what often results is either an overengineered solution, or analysis paralysis.\n3. Add design: Extract methods, implement a design pattern, create additional classes etc. Clean up the quick-n-dirty\n  - refactoring is a process of making safe moves that let us change the design of the code without changing the behaviour. Therefore, no new tests are written at this point.\n  - refactoring is about changing implementation details, not about how that interface has changed. If we think of this in terms of [[OOP|paradigm.oop]] it can be thought of as *\"refactoring doesn't happen in your public properties, only your private ones\"*. This of course is not absolute, and is more of a guideline. The idea is that we don't want to change our public API, because others depend on it. Instead, our refactoring should focus on how something has been implemented.\n\nWhen we approach testing this way, we move away from creating [[test doubles|testing.test-double]] for each class of our classes to depend on, and instead start to create test classes only to replace dependencies at the port (e.g. at the level of network call, file access etc.) \n\nIf we isolate testing at the level of classes (as is more typically done), what happens is that we find ourselves having to modify the tests whenever the implementations themselves get refactored. Most of the time the culprit is [[mocks|testing.test-double.mock]] that need to be changed.\n- modifying tests becomes more problematic down the road when we forget how our tests work (since tests are typically more imperative than the implementations, they can be more difficult to understand)\n\n* * *\n\nWe may also take a bit of an inverse approach with TDD, and write the code first. In this case, the idea is to write some production code - enough to pass a test or two - and then write those tests. Then, to determine how much code you need to pass a test or two, think of a couple of tests to pass, then write the code that would pass those tests. Then write the tests.\n- In order to work in small chunks, you have to imagine the tests that you'll be writing; so that you can write code that is testable.\n- The core insight of TDD is the size of the cycle, not so much whether or not you write the test first. The reason we write the tests first is that it encourages us to keep the cycles really short, but we would get pretty much the same benefit if we took a code-first approach, but in similar size cycles.\n- [source](https://blog.cleancoder.com/uncle-bob/2016/11/10/TDD-Doesnt-work.html)\n\n## E Resources\n- https://www.stevefenton.co.uk/2013/05/my-unit-testing-epiphany/\n- https://www.youtube.com/watch?v=EZ05e7EMOLM","n":0.029}}},{"i":241,"$":{"0":{"v":"Test Anything Protocol","n":0.577},"1":{"v":"\nTAP is a protocol to allow communication between unit tests and a [[test harness|testing.harness]]\n\nWith TAP, each test (ie. a TAP producer) can communicate its results to the testing harness.\n- This communication is done in a language-agnostic way\n","n":0.164}}},{"i":242,"$":{"0":{"v":"Terraform","n":1},"1":{"v":"\n## What is it?\nTerraform is a cloud-agnostic [[IaC|deploy.IaC]] solution.\n\nTerraform is split into two parts:\n- One part is the Terraform engine, which understands... \n    - how to read state from a provider\n    - read HCL code\n    - how to get from the state your infrastructure is currently into the state you want your infrastructure to be in.\n- The other part is the provider, which talks to the infrastructure to find out the current state and make changes using the infrastructure’s API.\n\n## Provisioning Workflow\nThere are 3 main CLI commands that involve creating, modifying and destroying infrastructure: `plan`, `apply` and `destroy`.\n\n[[terraform.cli]]\n\n## Terminology\n### Provider\n![[terraform.provider#summary,1:#*]]\n\n### Resource\n![[terraform.resource#summary,1:#*]]\n\n### Module\n![[terraform.module#summary,1:#*]]\n\n### Data source\n![[terraform.data-source#summary,1:#*]]\n\n### Local\nA `local` is Terraform's representation of a variable.\n- note: not to be confused with Terraform variables.\n\n### Variable\n![[terraform.variables]]\n\n### State\n![[terraform.state]]\n\n### Workspaces\nWorkspaces solve the problem \"how do we create multiple environments using the same code?\"\n\n`terraform.workspace` is a special variable that resolves to the current workspace we are running in.\n\nUnless we explicitly specify, we are running in the `default` workspace.\n\nLocal workspaces are stored in `terraform.tfstate.d/`\n- each workspace has its own state\n\n#### CLI\n- List workspaces -  `terraform workspaces list`\n- Create new workspace - `terraform workspace new development`\n- Switch workspaces - `terraform workspace select development`\n\n### Terraform Cloud\nTerraform cloud provides us with a method to change our input variables at the top level, meaning each set of infra (for each environment) can have its own set of variables.\n\nWith it, we:\n1. create a workspace \n2. point it at a source control repo containing your Terraform code\n3. set the variables for that workspace\n\n### Lifecycle\nEach resource has a special attribute block called `lifecycle` that gives us extra control.\n\nIt allows us to:\n- `create_before_destroy`, to ensure a new resource is created prior to deleting the old one\n- `prevent_destroy`, to prevent Terraform from ever deleting the resource, so long as the property exists\n\n```tf\nlifecycle {\n  prevent_destroy = true\n}\n```\n\n### Provisioner\nProvisioners allow us to run a script (remotely or locally) after a resource has been created.\n- provisioners allow us to step in and solve problems ourselves when they are not solved out of the box by the provider we are using.\n- because provisioners are imperative, they are seen as a last resort approach to solving our problem.\n\n* * *\n\n## Misc\n### Multi-line string\nMulti-line strings are declared between `<<ANYWORD` and `ANYWORD`:\n```tf\nresource \"aws_iam_policy\" \"my_bucket_policy\" {\n    name = \"my-bucket-policy\"\n\n    policy = <<POLICY\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Action\": [\n                    \"s3:ListBucket\"\n                ],\n                \"Effect\": \"Allow\",\n                \"Resource\": [\n                    \"${data.aws_s3_bucket.bucket.arn}\"\n                ]\n            }\n        ]\n    }\n    POLICY\n}\n```\n\nString interpolation (`${interpolated_value}`) can be used inside a multi-line string.\n- only needed when inside quotes (`\"\"`)\n\n### Outputting to console (stdout)\n```tf\noutput \"message\" {\n    value = aws_s3_bucket.my_bucket.id\n}\n```\n\nor we can print all attributes exported by a resource:\n```tf\noutput \"all\" {\n    aws_s3_bucket.my_bucket\n}\n```\n\n### Folder structure\nAll Terraform files should be in a single directory (the Terraform project) at the top level. Any files within subdirectories will be ignored. Conceptually, when we run Terraform commands, everything will be appended into a single file anyway.\n- child directories are used to set up Modules\n\nBy convention,\n- set up providers in `main.tf`.\n- resources named after their type (e.g. `sqs.tf`, `api-gateway.tf`)\n- variables in `variables.tf`\n\n* * *\n\n## Tools\n- [Atlantis](https://www.runatlantis.io/) - Pull Request automation for Terraform\n    - purpose is to have improved code review for infra changes.\n- Terratest - a unit testing framework for Terraform\n\n## Alternatives\n- Chef/Puppet - these are configuration management tools. They are designed to configure and manage the already existing infrastructure, while Terraform is designed to set up the infrastructre itself.\n    - In other words, Puppet and Chef would be used to configure servers, while Terraform would be used to create the server itself.\n- Pulumi - This IaC tool uses a programming language (like Typescript) instead of a configuration language.","n":0.041}}},{"i":243,"$":{"0":{"v":"Variables","n":1},"1":{"v":"\nA variable is set at runtime, allowing us to vary Terraform's behaviour. \n- Therefore, if Terraform were a function, a variable would be an input to the function.\n- note: not to be confused with `locals`, which themselves are actually more like variables as used in general programming\n\n```hcl\nvariable \"bucket_name\" {\n    type = string\n    # describe what this variable is used for\n    description = \"the name of the bucket we are creating\"\n    default = \"default_bucket_name\"\n}\n\nresource \"aws_s3_bucket\" \"bucket\" {\n    bucket = var.bucket_name\n}\n```\n\nVariables can be more complex too:\n```tfvars\ninstance_map = {\n    dev = \"t3.small\"\n    test = \"t3.medium\"\n    prod = \"t3.large\"\n}\n\nenvironment_type = \"dev\"\n```\n\nAnd referenced like:\n```hcl\nvariable \"instance_map\" {\n    type = map(string)\n}\nvariable \"environment_type\" {}\n\noutput \"selected_instance\" {\n    value = var.instance_map[var.environment_type]\n}\n```\n\n#### Types\n- `string`\n- `bool`\n- `number`\n- `list(<TYPE>)`\n- `set(<TYPE>)`\n    - each value is unique\n- `map(<TYPE>)`\n- `object()`\n    - like a map, but values can be different types\n- `tuple([<TYPE>, …])`\n    - number of values and order is preserved\n- `any`\n    - unlike `any` type from [[Typescript|ts]]; this `any` allows Terraform to infer based on the actual value.\n\n#### Providing variables (4 ways)\n1. When we run `terraform init` and `terraform apply`, we will be prompted to provide a value for the variable(s).\n\n2. pass the value with:\n```sh\nterraform apply -var bucket_name=my_bucket\n```\n\n3. `export` environment variables in the terminal prefixed with `TF_VAR_`:\n```sh\nexport TF_VAR_bucket_name=my_bucket\n```\n\n4. create a `terraform.tfvars` file (or `<ANYNAME>.auto.tfvars`):\n```tfvars\nbucket_name = \"my_bucket\"\n```\n","n":0.069}}},{"i":244,"$":{"0":{"v":"Terragrunt","n":1},"1":{"v":"\nTerragrunt is a tool on top of [[terraform]] for orchestrating multiple [[modules|terraform.module]] at a time\n- Terragrunt helps organize [[state files|terraform.state]], help reduce code duplication, manage different AWS accounts gracefully.\n\n- ex. We have an [[IAM Role|aws.terms.IAM]] for a Lambda function to run, but also have an S3 bucket for code to be deployed into. These 2 things aren't really related to each other, so we can put them into their own modules and then use Terragrunt to orchestrate all those modules and then deploy them all at once. \n    - Terragrunt sees all the modules it has to deploy (the terraform code) and figures out how to deploy it.\n\nThis is how we would implement the same `example_bucket` in both Terraform and Terragrunt.\n\nTerraform:\n```tf\nresource \"aws_s3_bucket\" \"example_bucket\" {\n    bucket = \"my-test-bucket\"\n    acl = \"private\"\n    tags = {\n        Name = \"My bucket\"\n        Environment = \"Dev\"\n    }\n}\n```\n\nTerragrunt:\n```hcl\nlocals {\n    env = \"development\"\n    short_env = \"dev\"\n    region = \"us-east-1\"\n}\n\nterraform {\n    # This references a predefined S3 module. In this case, it is a repo called\n    # S3 in the terraform-modules project group. The benefit is that there is a\n    # lot of S3 code that would be the same across different S3 buckets. We can\n    # define it once here, then reference it each time we create a new bucket.\n    source = \"git::https://gitlab.com/tycholiz/infra/terraform-modules/s3.git//?ref=0.3.0\"\n}\n\n# These are inputs that are used by the source S3 bucket referenced above\n# (terraform-modules/s3)\ninputs = {\n    bucket_name = \"my-test-bucket-${locals.short_env}-${locals.region}-serverless\"\n    environment = locals.env\n}\n```\n\n## UE Resources\n- https://transcend.io/blog/why-we-use-terragrunt/\n","n":0.065}}},{"i":245,"$":{"0":{"v":"State","n":1},"1":{"v":"\nState is the place where Terraform stores of all of the resources (and their metadata) it has created.\n- run `terraform state list` to see all resources existing in state.\n\nThis state is used by Terraform to work out how changes need to be made.\n\nState is stored in `terraform.tfstate`\n\nIf we want to move resource creation from one project to another, state needs to be manipulated directly \n- this can be handled by (example uses a AWS VPC resource)\n    1. running `terraform state rm aws_vpc.my_vpc` command, which will remove the resource from state (so Terraform is no longer managing it), but will not delete the resource in the cloud.\n    2. in the new project, copy+paste over the resource and run `terraform import aws_vpc.my_pc <VPC_ID>`\n    3. run `terraform apply`\n\n- some resources do not support `import`. In this case, use `terraform state mv`\n\n### Remote state\nMultiple people working on the same Terraform project can introduce a lot of complexity, since a local state file is used to store a record of what has been created. If we run terraform commands on a second machine, it will try to create double the resources.\n- to get around this issue, we can store state in a remote location (e.g. in an S3 bucket)\n\nWe specify the remote state location using the `backend` keyword. Here we are using an S3 bucket:\n```tf\n# state.tf\nbackend \"s3\" {\n    bucket = \"<bucket-name>\"\n    key = \"my-project.state\"\n    region = \"us-west-1\"\n}\n```\n\nThe remote state backend needs to support \"locking\", which prevents changes to the state while Terraform commands are running.\n\nA good idea is to use S3 bucket versioning so we can time travel through different Terraform states.\n","n":0.061}}},{"i":246,"$":{"0":{"v":"Resource","n":1},"1":{"v":"\n## Summary\nA resource represents a piece of real world infrastructure \n\nExamples:\n- an S3 bucket\n- an EKS (Elastic Kubernetes) cluster\n- a Postgres [[role|pg.lang.roles]]\n\n### Example: Declaring an S3 bucket\nThis is how we might declare an [[S3 bucket|aws.svc.S3]]. You can reference this in different components, and then everything will deploy together when running `terraform apply`.\n```tf\nresource \"aws_s3_bucket\" \"my_bucket\" {\n    bucket = \"my-test-bucket\"\n    acl = \"private\"\n    tags = {\n        Name = \"My bucket\"\n        Environment = \"Dev\"\n    }\n}\n```","n":0.118}}},{"i":247,"$":{"0":{"v":"Provider","n":1},"1":{"v":"\n## Summary\nA provider is a connection that allows Terraform to manage infrastructure using an interface (e.g. AWS API)\n\n## How it works\nThe added abstraction means the provider is completely separate from the Terraform engine\n\nWe can have multiple instances of the same provider in our project. To more easily distinguish between the two, we can use an `alias` property.\n- ex. if we want an aws provider in `us-east-1` and another in `us-west-2`\n\nBecause of the Terraform engine, anyone can write a provider to connect to anything that has a programmable way to talk to it.\n- therefore, Providers are not part of the main Terraform source code. They are separate binaries that exist in their own repos.\n    - Hashicorp hosts a registry that contains the most popular providers, like AWS and Azure\n\nProviders in your project can be found in the `.terraform/` directory.\n- each provider binary is in the format `terraform-provider-<NAME>_vX.Y.Z`\n    - when you run `terraform init`, binaries of this format are automatically searched for so it can determine if it actually needs to download it or not. \n\nTerraform uses lockfiles so that the same versions will be used across different machines (just like npm/yarn lockfiles.)\n\nAll the author of a provider has to do for each resource they want Terraform to control is to define how to create, read and destroy it.\n","n":0.068}}},{"i":248,"$":{"0":{"v":"Module","n":1},"1":{"v":"\n## Summary\nA Terraform module is a set of Terraform configuration files in a single directory that can be considered its own standalone Terraform project.\n- it can therefore...\n    - contain its own resources, data sources, locals, etc.\n    - take variables (ie. inputs on a per-module basis)\n\n### What are they for?\nModules are useful as they allow us to define a reusable block of Terraform code of which we can have many instances in our main Terraform project.\n- even a single directory with a single `.tf` file is considered a module.\n\nA convention is to use a common prefix for resources from a single module\n\nA Terraform module is similar to the concept of a package (e.g. npm package), providing many of the same benefits.\n- As a result, any non-trivial real-world Terraform configuration would use modules.\n\n\nExample:\n```tf\nmodule \"work_queue\" {\n    source = \"./directory-of-module\"\n    variable_name = \"passed_in_value\"\n}\n```\n\n### Returning values from modules (`output`)\nMost modules return values to make them easier to use.\n- to reference those values, use `module.<module_identifier>.<output_name>`\n\nIt is possible to return a whole resource from a module. \n- This allows us to return all of the fields from the resource that we created\n\n### Remote modules\nModules can be specified with a URL pointing to a remote git repo, designating them as *remote modulues*.\n- these modules get cloned to `.terraform/modules/`, which is similar to `node_modules/` in [[js.node]]\n\nRemote modules should be specified with a [[git tag|git.cli.tag]] to peg a specific version\n- ex. `github.com/Tycholiz/my-terraform-module?ref=0.0.1`","n":0.066}}},{"i":249,"$":{"0":{"v":"Data Source","n":0.707},"1":{"v":"\n## Summary\nA data source is used to fetch data from a resource that is not managed by the current Terraform project.\n- think of it as a read-only resource that already exists\n\n### How is it used?\nThe following data source is used to look up an [[aws.svc.S3]] bucket.\n```hcl\ndata \"aws_s3_bucket\" \"bucket_identifier\" {\n    bucket = \"my_s3_bucket\"\n}\n```\n\nWe could use a *data source expression* to access values of the data source, such as the [[arn|aws.terms#arn-amazon-resource-name]] of the above bucket, like `data.aws_s3_bucket.bucket_identifier.arn`\n- ex. we could then use this in our [[IAM|aws.terms.IAM]] policy\n\nThe bigger the project, the more valuable data sources are, since we can split up our terraform project into multiple projects and reference them via data sources.\n- the benefit is two-fold, since not only do we *not* have to hard-code things like ARNs, but if the resource that we are referencing no longer exists, then the Terraform will properly fail on us.\n","n":0.083}}},{"i":250,"$":{"0":{"v":"Terraform CLI","n":0.707},"1":{"v":"\n### Plan\n`plan` will read the terraform HCL file and determine the desired state of the infrastructure, and compare that to the current infra.\n- Once this \"diff\" is determined, `terraform plan` presents a description of the changes necessary to achieve that desired state (it does *not* actually make any changes).\n\nIf we want, we can save the plan as a runnable artifact that can be fed into the `terraform apply` command.\n\nThe purpose of the plan is to validate configuration changes and confirm that the resulting actions are as expected.\n\n#### Flags\n- `-out <filename>` - specify an output file for the diff\n\n#### Markers in the plan diff\n- `+` parts that will be added\n- `-` parts that will be deleted\n- `~` parts that will be updated in place (ie. it won't delete and recreate)\n- `-/+` destroy-then-create\n    - this erases the state and results in downtime. We can overcome this downtime by using a resource lifecycle\n\n### Apply\n`apply` will perform a plan just like `terraform plan` command, but then it will actually carry out the planned changes to each resource using the infrastructure provder's API (e.g. AWS API).\n- therefore, it can be thought of as \"committing\" the plan.\n\n### Destroy\n`destroy` behaves exactly like deleting every resource from the configuration and then running an apply, except that it doesn't require editing the configuration\n- Therefore, this is more convenient if you intend to provision similar resources at a later date.\n\n* * *\n\n### `import`\nThe `import` command is how we get Terraform to begin managing a resource that wasn't created in Terraform.\n- What's happpening is that Terraform is fetching its data, then including it as part of its state.\n\n```sh\nterraform import <resource_type>.<resource_identifier> <value>\n```","n":0.061}}},{"i":251,"$":{"0":{"v":"Synology","n":1},"1":{"v":"\n### Storage Pools\nThink of a storage pools as a group of your HDDs that you decide to group\n\nStorage pools are mostly about RAID levels. If you want one RAID for all disc - use one pool for all of them. But maybe you want to have large HDD for storage and fast SSD for docker? Obviously you don't want to put HDD and SSD in one RAID. So, you'll have two storage pools.\n\nyou cannot reduce an existing Storage Pool's number of drives or replace its drives with smaller capacity ones, since data are distributed across the drives in most RAID types\n- if you must, follow the [guide](https://kb.synology.com/en-in/DSM/tutorial/Reduce_RAID_drives) to do so\n\n### Volumes\nA volume is a collection of HDDs (or even just one)\n\nVolumes provide the basic storage space on your Synology NAS. All of your stuff—such as shared folders, documents, and package data—is stored on volumes. Therefore, before you start storing anything on your Synology NAS, you will need to create at least one volume.\n\nThink of a volume as a Partition, if you think of the RAID group as one HDD.\n\nYour volume is assigned to a Storage Pool and does not know what is the underlying hardware structure of that pool.\n\nmostly about filesystem: ext4 vs btrfs. \n- Unless you have specific reason and you know what you're doing, - just use btrfs and do one volume per storage pool.\n\nYou may want to create multiple volumes if you previously created multiple pools with the same or different type of RAID and/or disk numbers\n\n#### Example usage\n- volume 1 is 6 bays and holds my data, movies, personal photos etc\n- volume 2 is a single drive that records surveillance and therefore is very heavy use. I don't want all my disks working that hard. I figure it would be a slim coincidence for it to go when I do need to review its data.\n- volume 3 is a single SSD for my dockers including plex config for speed purposes. backup up daily, so no big deal if I lose a day.\n\n### Shares\nFolders inside the Volume.\n- Under any volume you will create \"Shares\" which are actually folders and this is how you manage your file tree.\n\n### Shared Folder\nA Shared Folder is the basic directory for you to store files and folders on your Synology NAS. \n- Therefore, you need to create at least one shared folder before you can start storing data.\n\n### LUN (Logical Unit Number)\nA LUN is loosley the equivalent of a Partition\n\n\n* * *\n\n### `homes/` and `home/` directories\nEach user has their own `home/` directory that lives within the `homes/` directory\n- `homes/` is owned by admin\n- home folder should only be used for personal files and not for sharing with others.","n":0.047}}},{"i":252,"$":{"0":{"v":"Transcoding","n":1},"1":{"v":"\nTranscoding, encoding, and decoding are various terms used when a media file needs to be transformed to be better suited for a specific device, internet connection, or hardware\n\nThe two main factors in streaming is the CPU (GPU) and internet bandwidth\n\n### Transcoding\nImagine we had a movie on our NAS in the HEVC format, and we wanted to play it back on our iPhone which doesn't support that format.\n- in this case, the Plex Media Server (on the NAS) will try to change the file to a more suitable version on the fly. This is transcoding.\n    - this work is done by the NAS CPU, and takes a heavy toll on the CPU\n- In streaming apps like Plex, we can enable an option to allow the CPU to run to its full potential (`Transcoder Quality`: `Make My CPU Hurt` option in Plex)\n\nLocally, transcoding probably isn't necessary.\n- when accessing media from remote locations with limited bandwidth, weaker internet connection, or smaller devices, you may want to access the media in lower quality, which is where transcoding (or real-time encoding), comes in.\n\nCPU needs for transcoding:\n- In Intel or AMD Based Based CPU that is 64bit (x86) in Architecture\n- Higher than 1.6Ghz in Frequency\n- More than 2 Cores\n\n* * *\n\n### Bitrate\nBitrate refers to the quantity of data that is processed per unit of time\n- ex. megabits per second (Mbps) for video \n- ex. kilobits per second (kbps) for audio.\n\nThe higher the bitrate, the better the quality and resolution of the media tends to be.","n":0.063}}},{"i":253,"$":{"0":{"v":"Photos","n":1},"1":{"v":"\nPhotos supports two kinds of spaces, personal, located under /home/Photos (/homes/username/Photos) and the shared space located under the shared folder /photo. These folders can not be changed, but can be located on different volumes.","n":0.171}}},{"i":254,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Ignore directory to be synced with Synology Drive Client\n1. open `~/Library/Application Support/SynologyDrive/data/session/{1,2,3...}/conf/blacklist.filter`\n2. add\n```\n[Directory]\nblack_name = \"node_modules\"\n```\n\n### Rsync\n- [guide](https://www.truetoad.com/Blog/copy-data-between-two-synology-nas)\n- enable Rsync (both devices) - Control Panel -> File Services -> Rsync -> Enable Sync Service\n- On source NAS, Control Panel -> File Services -> Advanced -> Task List\n\n### Download logs\nSupport Centre -> Support Services -> Log Generation\n- this will download a `.dat` file. Unzip it in a new directory to get access to the contents","n":0.116}}},{"i":255,"$":{"0":{"v":"Svg","n":1},"1":{"v":"\n### Accessibility\nTo make an `<img>` accessible, we must add alt text. However, to make svgs accessible, we must:\n1. Add a `<title>` as the first child of it's parent element\n2. Give the `<title>` a unique ID\n3. Add role=\"img\" and aria-labelledby={uniqueId}to the `<svg>`\n\n### strokeOffset property\nSvg property to animate svg. This is used any time you see an svg being drawn on. You can also apply css animations to it\n\n# Tools\nhttps://www.svgator.com/\n","n":0.121}}},{"i":256,"$":{"0":{"v":"Svelte","n":1},"1":{"v":"\nUnlike the traditional frameworks (React and Vue) which carry out the bulk of their work in the browser, Svelte shifts that work into a compile step that happens when an app is built.\n\n# Features of the framework\n## Updates to the DOM\nWhen you update component state in Svelte, it doesn't update the DOM immediately. Instead, it waits until the next microtask to see if there are any other changes that need to be applied, including in other components. Doing so avoids unnecessary work and allows the browser to batch things more effectively.\n- sometimes this behavior is undesirable, and for that we can turn to `tick()` lifecycle method\n\n## Reactivity in Svelte\nSvelte's reactivity is triggered by assignments (ie. setting a new variable, or changing the assignment of an existing variable)\n- therefore `map`, `filter` and `reduce` all trigger re-renders, because those methods return a new array. `push` would not trigger a re-render, unless we reassign the same variable to equal itself:\n```js\nfunction addNumber() {\n\tnumbers.push(numbers.length + 1);\n\tnumbers = numbers;\n}\n```\nThe idiomatic way of course is just to use non-mutative methods.\n\nUpdating properties of an object will also cause the re-render, since assigning new values to keys of an object is considered to be assignment.\n\n### Reactive declarations\nSome parts of a component's state need to be computed from other state , and therefore need to be computed when its dependent values change \n- eg. fullname computed from firstname+lastname. If firstname changes, then fullname needs to be recomputed.\n\nFor these circumstances, we can use reactive declarations, which look like:\n```js\nlet firstname = 'joe';\nlet lastname = 'schmidt';\n$: fullname = firstname + lastname;\n```\n\n`$: ` basically says, \"do this thing (e.g. set fullname equal to firstname + lastname) whenever the dependent variables involved are updated (ie. whenever firstname or lastname change)\". \n- In other words, this symbol marks a statement as reactive.\n\nThis symbol is known as the \"destiny operator\" in reactive programming.\n- A destiny operator ensures a variable is updated whenever values that it's computed from are changed)\n\nReactive values become particularly valuable when you need to reference them multiple times, or you have values that depend on other reactive values.\n\nWe can also just run arbitrary code blocks that are executed any time a dependent variable changes:\n- ex. here, any time firstname or lastname changes, the codeblock is run\n```js\n$: {\n    console.log(`Nice to meet you!`)\n    console.log(`my fullname is ${firstname} ${lastname}`)\n}\n```\n\nWe can even use conditionals:\n```js\n$: if (count >= 10) {\n\talert(`count is dangerously high!`);\n\tcount = 9;\n}\n```\n\n## Props\nDeclare props in the `<script>` tag:\n```js\n<script>\n\texport let answer;\n    export let name = 'Kyle'; // defaultValue\n</script>\n```\n\nProps are passed (almost) identically to React\n```js\n<Nested answer={42}/>\n```\n\n## DOM Logic\n### Conditional rendering\n```js\n{#if user.loggedIn}\n\t<button on:click={toggle}>\n\t\tLog out\n\t</button>\n{:else}\n\t<button on:click={toggle}>\n\t\tLog in\n\t</button>\n{/if}\n```\n\n### Loop\n```js\n<ul>\n\t{#each cats as { id, name }} // destructuring here\n\t\t<li><a target=\"_blank\" href=\"https://www.youtube.com/watch?v={cat.id}\">\n            ID: {id}\n\t\t\tName: {name}\n\t\t</a></li>\n\t{/each}\n</ul>\n```\n\n### Data fetching\nSvelte makes it easy to await the value of  directly in your markup:\n- Only the most recent promise is considered, meaning you don't need to worry about race conditions.\n\n```js\n{#await promise}\n\t<p>...waiting</p>\n{:then number}\n\t<p>The number is {number}</p>\n{:catch error}\n\t<p style=\"color: red\">{error.message}</p>\n{/await}\n```\n\n## Events\nMore or less in the same manner as React, any event can be listened to with the `on` directive:\n```html\n<div on:mousemove={handleMousemove}>\n\tThe mouse position is {m.x} x {m.y}\n</div>\n```\n\n### Event modifiers\nDOM event handlers can have modifiers that alter their behaviour. \n- ex. a handler with a once modifier will only run a single time:\n```html\n<button on:click|once={handleClick}>\n\tClick me\n</button>\n```\n\nNotable modifiers:\n- `preventDefault`\n- `once` - remove the handler after first time it runs\n- `self` - only trigger handler if `event.target` is the element itself\n\n### Event dispatching\nA component can be set to dispatch events by creating an event dispatcher and calling them via a handler. The parent component (here `App`) can listen to messages dispatched from a child component via the `on:message` directive (where `message` is the event name we are dispatching).\n- Without this `on:message` attribute, messages would still be dispatched, but the App would not react to it.\n\ninner.svelte:\n```html\n<script>\n\timport { createEventDispatcher } from 'svelte';\n\n\tconst dispatch = createEventDispatcher();\n\n\tfunction sayHello() {\n\t\tdispatch('message', {\n\t\t\ttext: 'Hello!'\n\t\t});\n\t}\n</script>\n\n<button on:click={sayHello}>\n\tClick to say hello\n</button>\n```\n\nApp.svelte:\n```html\n<script>\n\timport Inner from './Inner.svelte';\n\n\tfunction handleMessage(event) {\n\t\talert(event.detail.text);\n\t}\n</script>\n\n<Inner on:message={handleMessage}/>\n```\n\n### Event forwarding\nUnlike DOM events, component events don't [[bubble|js.event-loop.event-bubbling]]. If you want to listen to an event on a deeply nested component, the intermediate components must *forward* the event.\n\nThis would be the mid-layer, with a dispatched event called in the `Inner` component. Here, `on:message` is what bubbles the event up to the parent\n```html\n<script>\n\timport Inner from './Inner.svelte';\n</script>\n\n<Inner on:message/>\n```\n\n### Binding\nThough Svelte takes a top-down data flow approach like React, it can be useful to break that paradigm, and can be done so with bindings.\n\nwe can use the bind:value directive:\n```html\n<input bind:value={name}>\n```\n\nThis means that not only will changes to the value of `name` update the input value, but changes to the input value will update `name`.\n\nCheckbox:\n```html\n<input type=checkbox bind:checked={yes}>\n```\n\nRadio:\n```html\n// `scoops` is an array of choices\n<input type=radio bind:group={scoops} name=\"scoops\" value={1}>\n```\n\n## Lifecycles\nLike React, Svelte has a familiar (albeit simpler) list of lifecycle methods.\n`onMount`, `onDestroy`, `onUpdate`, `beforeUpdate`, `afterUpdate`\n\n### tick\n`tick` is a lifecycle method distinct from the familiar ones. It's different in that it can be called any time. It returns a promise that resolves as soon as pending state changes have been applied to the DOM.\n\n## Stores\nGlobal state solution of Svelte. \nA store is simply an object with `subscribe` method, allowing interested parties to be notified whenever the value changes.\n```js\n// stores.js\nimport { writable } from 'svelte/store';\n\n// this is a writable store\nexport const count = writable(0);\n\n/* * * * * * * * * * * * * * * * * * * */\n// Incrementor.svelte\nimport { count } from './stores.js';\n\nfunction decrement() {\n    count.update(n => n - 1);\n}\n```\n\n## Motions, Animations and Transitions\nSvelte provides lots of motion, animation and transition support out-of-the-box:\n\nExamples: \n- https://svelte.dev/tutorial/tweened\n- https://svelte.dev/tutorial/spring\n- https://svelte.dev/tutorial/transition\n\n## Actions\nActions are essentially element-level lifecycle functions. They're useful for things like:\n\n- interfacing with third-party libraries\n- lazy-loaded images\n- tooltips\n- adding custom event handlers\n\nSvelte actions allows you to build code in response to the lifecycle of DOM elements\n\n[Make an on-screen object pannable](https://svelte.dev/tutorial/actions)\n\n## Context API\nThe context API provides a mechanism for components to 'talk' to each other without passing around data and functions as props, or dispatching lots of events.\n\n### Parts to it\n[source](https://svelte.dev/tutorial/context-api)\nThere are 2 halves to the API: `setContext` and `getContext`.\n- if a component calls `setContext(key, context)`, then any child component can retrieve that context with `getContext(key)`\n\nUnlike React Context, you do not need to import and wrap your subtree with the Provider in order to get access to the context. Simply, the parent component calls `setContext` and that context is available to all children to want access to it.\n\n* * *\n\n## CSS Class shorthand\n```html\n<button\n\tclass=\"{current === 'foo' ? 'selected' : ''}\"\n\ton:click=\"{() => current = 'foo'}\"\n>foo</button>\n```\n\nCan be shortened to:\n```html\n<button\n\tclass:selected=\"{current === 'foo'}\"\n\ton:click=\"{() => current = 'foo'}\"\n>foo</button>\n```\n\n## Component children\nWe can render children of a component like React by passing content between a component's tags.\n\nIn React, it looks like this:\n```js\nconst childComponent = ({ children }) => {\n    return (\n        <div>\n            {children}\n        </div>\n    )\n}\n```\n\nIn Svelte, it looks like this:\n```html\n<div>\n\t<slot></slot>\n</div>\n```\n\nFallbacks can be specified by placing data between the `<slot>` tags. Think of this like a default value to the children \"prop\". This is called a *default slot*d\n- [named slots](https://svelte.dev/tutorial/named-slots) can also be used for more control over placement.\n","n":0.03}}},{"i":257,"$":{"0":{"v":"Sveltekit","n":1},"1":{"v":"\n\nThere are two basic concepts to Sveltekit:\n- Each page of your app is a Svelte component\n- You create pages by adding files to the `src/routes` directory of your project. These will be server-rendered so that a user's first visit to your app is as fast as possible, then a client-side app takes over\n\n## Router\nAt the heart of SvelteKit is a filesystem-based router. \n\nThere are two types of route — pages and endpoints.\n\n* * *\n\n## Misc\nThe value of Sveltekit is similar to the value of Next, but even extending beyond that. Sveltekit takes care of the modern best-practices surrounding web development and does it for you, including build optimizations, so that you load only the minimal required code; offline support; prefetching pages before the user initiates navigation; and configurable rendering that allows you to generate HTML on the server or in the browser at runtime or at build-time.\n\nA filename that has a segment with a leading underscore, such as src/routes/foo/_Private.svelte or src/routes/bar/_utils/cool-util.js, is hidden from the router, but can be imported by files that are not.\n\n# Resources\n[Realworld Sveltekit app (Medium clone)](https://github.com/sveltejs/realworld)\n","n":0.075}}},{"i":258,"$":{"0":{"v":"Routes","n":1}}},{"i":259,"$":{"0":{"v":"Pages","n":1},"1":{"v":"\nPages typically generate HTML to display to the user (as well as any CSS and JavaScript needed for the page).\nBy default, pages are rendered on both the client and server, though this behaviour is configurable.\n\nSince pages are Svelte components, they are of filetype `.svelte`\n\nBy default, when a user first visits the application, they will be served a server-rendered version of the page in question, plus some JavaScript that 'hydrates' the page and initialises a client-side router. From that point forward, navigating to other pages is handled entirely on the client for a fast, app-like feel where the common portions in the layout do not need to be rerendered.\n\nThe filename determines the route. \n\nDynamic parameters are encoded using `[brackets]`. For example, a blog post might be defined by `src/routes/blog/[slug].svelte`\n- that parameter can then be accessed in 2 ways: a load function, or the page store\n\nA good structure is to have minimal information in your page `.svelte` file, and import the entirety of the visual html from a hidden file (`_Editor.svelte`), like this:\n```js\n<script context=\"module\">\n\texport function load({ session }) {\n\t\tif (!session.user) {\n\t\t\treturn {\n\t\t\t\tstatus: 302,\n\t\t\t\tredirect: `/login`\n\t\t\t};\n\t\t}\n\n\t\treturn {};\n\t}\n</script>\n\n<script>\n\timport Editor from './_Editor.svelte';\n\tlet article = { title: '', description: '', body: '', tagList: [] };\n</script>\n\n<Editor {article}/>\n```\n","n":0.071}}},{"i":260,"$":{"0":{"v":"load Function","n":0.707},"1":{"v":"\nPages and layouts can export a `load()` function that runs before the component is created.\n- This function runs both during server-side rendering and in the client\n- An example of its usage would be to allow us to get page data without showing a loading spinner and fetching data in `onMount`.\n- `load` is similar to `getStaticProps` or `getServerSideProps` in Next.js, except that it runs on both the server and the client.\n\nIf load returns nothing, SvelteKit will fall through to other routes until something responds, or will respond with a generic 404.\n\nSvelteKit's load receives an implementation of fetch, which has the following special properties:\n- it has access to cookies on the server\n- it can make requests against the app's own endpoints without issuing an HTTP call\n- it makes a copy of the response when you use it, and then sends it embedded in the initial page load for hydration\n\ncode inside `load` blocks should never reference `window`, `document`, or any browser-specific objects\n\ncode inside `load` blocks should not directly reference any API keys or secrets, which will be exposed to the client, but instead call an endpoint that uses any required secrets\n\nspec: If we want to make HTTP calls in our `load` function, we will have to import a created API endpoint (`export async function get()...`) and use it in `load`. \n\nThe `load` function is reactive, and will re-run when its parameters change, but only if they are used in the function.\n- Specifically, if page.query, page.path, session, or stuff are used in the function, they will be re-run whenever their value changes. \n- Note that destructuring parameters in the function declaration is enough to count as using them. \n\nIf you return a `Promise` from load, SvelteKit will delay rendering until the promise resolves.\n\n* * *\n\n### Signature of `load()`\nThe `load` function receives an object containing four fields — `page`, `fetch`, `session` and `stuff`\n\n#### page\n```ts\ninterface page: {\n    host: string;\n    path: string;\n    params: PageParams;\n    query: URLSearchParams;\n};\n```\nSo if the example above was `src/routes/blog/[slug].svelte` and the URL was `https://example.com/blog/some-post?foo=bar&baz&bizz=a&bizz=b`, the following would be true:\n- `page.host === 'example.com'`\n- `page.path === '/blog/some-post'`\n- `page.params.slug === 'some-post'`\n- `page.query.get('foo') === 'bar'`\n- `page.query.has('baz')`\n- `page.query.getAll('bizz') === ['a', 'b'`]\n\n#### stuff\n`stuff` is passed from layout components to child layouts and page components and can be filled with anything else you need to make available. For the root `__layout`.svelte component, it is equal to `{}`, but if that component's load function returns an object with a `stuff` property, it will be available to subsequent load functions.\n\n```ts\nexport interface LoadInput<\n\tPageParams extends Record<string, string> = Record<string, string>,\n\tStuff extends Record<string, any> = Record<string, any>,\n\tSession = any\n> {\n\tpage: {\n\t\thost: string;\n\t\tpath: string;\n\t\tparams: PageParams;\n\t\tquery: URLSearchParams;\n\t};\n\tfetch(info: RequestInfo, init?: RequestInit): Promise<Response>;\n\tsession: Session;\n\tstuff: Stuff;\n}\n```\n\n```ts\nexport interface LoadOutput<\n\tProps extends Record<string, any> = Record<string, any>,\n\tStuff extends Record<string, any> = Record<string, any>\n> {\n\tstatus?: number;\n\terror?: string | Error;\n\tredirect?: string;\n\tprops?: Props;\n\tstuff?: Stuff;\n\tmaxage?: number;\n}\n```\n","n":0.047}}},{"i":261,"$":{"0":{"v":"Layouts","n":1},"1":{"v":"\nLayouts are useful when there are elements that should be visible on every page, such as top-level navigation or a footer.\n- The only requirement is that the component includes a `<slot>` for the page content.\n\n### Nested Layouts\nSuppose we don't just have a single /settings page, but instead have nested pages like /settings/profile and /settings/notifications with a shared submenu (for a real-life example, see [github.com/settings](github.com/settings)).\n\nWe can create a layout that only applies to pages below /settings (while inheriting the root layout with the top-level nav):\n\n```html\n<!-- src/routes/settings/__layout.svelte -->\n<h1>Settings</h1>\n\n<div class=\"submenu\">\n\t<a href=\"/settings/profile\">Profile</a>\n\t<a href=\"/settings/notifications\">Notifications</a>\n</div>\n\n<slot></slot>\n```\n\n#### Resetting the layout stack\nTo reset the layout stack, create a __layout.reset.svelte file instead of a __layout.svelte file. For example, if you want your `/admin/*` pages to not inherit the root layout, create a file called `src/routes/admin/__layout.reset.svelte`.\n\n### Error pages\nSveltekit renders a default error page, but if we supply a `__error.svelte` alongside the component, then that will be shown to the user.\n","n":0.082}}},{"i":262,"$":{"0":{"v":"Endpoints","n":1},"1":{"v":"\nEndpoints run only on the server\n- This means it's the place to do things like access databases or APIs that require private credentials or return data that lives on a machine in your production network. Pages can request data from endpoints. Endpoints return JSON by default\n\nEndpoints are written in `.js` (or `.ts`) files.\n\nEndpoints export functions that correspond to HTTP methods.\n- `get`, `post`, `del`, `put`, `patch`\n\n> We don't interact with the req/res objects you might be familiar with from Node's http module or frameworks like Express, because they're only available on certain platforms. Instead, SvelteKit translates the returned object into whatever's required by the platform you're deploying your app to.\n\nheaders (including cookies) can be set in the return value of the HTTP method.\n\nif we had a `routes/blog/post1.svelte` page, we could request data from `routes/blog/post1.json`\n- both could be represented in the file structure as `blog/[slug].svelte` and `blog/[slug].json`, respectively.\n","n":0.083}}},{"i":263,"$":{"0":{"v":"Styled Components","n":0.707}}},{"i":264,"$":{"0":{"v":"TypeScript","n":1},"1":{"v":"\n\n### Pass props to StyledComponent\n```ts\nconst Icon = styled.Image<{ width: number, height: number }>`\n  width: ${p => p.width};\n  height: ${p => p.height};\n`;\n\n// later\n<Icon width={50} height={100}>\n```\n\nand if you want to be more precise and ignore the `onPress`, effectively giving us a subset of `Props`:\n```ts\nconst Icon = styled.Image<Pick<Props, 'src' | 'width' | 'height'>>`\n  width: ${p => p.width};\n  height: ${p => p.height};\n`;\n```\n\n# UE Resources\n- https://blog.agney.dev/styled-components-%26-typescript/","n":0.128}}},{"i":265,"$":{"0":{"v":"Styled Components API","n":0.577},"1":{"v":"\nA styled component can take any prop. It passes it on to the HTML node if it's a valid attribute, otherwise it only passes it into interpolated functions.\n\n### `attrs`\nA chainable method called on a styled component, with the purpose of attaching some props to the styled component.\n- The fact that it's chainable shows that calling `.attrs` on a styled component returns another styled component.\n\nThe first and only argument is an object that will be merged into the rest of the component's props\n\nThe *attributes* refer to those found on the component/HTML node in question.\n\n#### Purpose\n`attrs` is useful for defining default attributes, and defining dynamic props\n- ex. you want a `<Button />` component with default attribute of `type=\"button\"` (instead of the normal `type=\"submit\"`)\n- ex. also, if you want a default size of small, but to be able to override that.\n\n```js\nconst Button = styled.button.attrs(props => ({\n  // Every <Button /> will now have type=\"button\" as default\n  type: props.type || 'button'\n  // small is the default size, but this can be overridden\n  size: props.size || 'small'\n}))`\n  display: block;\n  ...\n`\n```\n\n#### Example\nInstead of:\n```js\nconst PitchSlider = ({ className }) => <PitchSliderContainer\n    type=\"range\"\n    min=\"0.9\"\n    max=\"1.1\"\n    step=\"any\"\n    onChange={this.props.onChange}\n    className={className}\n/>\n\nexport default styled(PitchSlider)`\n  text-align: right;\n  float: right;\n`;\n```\n\nWe can do:\n```js\nconst PitchSlider = styled.input.attrs({\n  type: 'range',\n  min: '0.9'\n  max: '1.1'\n  step: 'any'\n  onChange: this.props.onChange\n})`\n  text-align: right;\n  float: right;\n`;\n```\n\n\n### `as`\nkeep all the styling you've applied to a component but just switch out what's being ultimately rendered\n- ex. Keep all the stylings of your `Button` styled component, but actually render it as a `<input />` instead of a `<button />`\n\nThis is done at runtime.\n\nThis is known as a \"polymorphic prop\"\n","n":0.061}}},{"i":266,"$":{"0":{"v":"Stripe","n":1},"1":{"v":"\n## Misc\nwith (spec) stripe elements, we can use the `stripe_code` on the client as a way to get the credit card information (presaved) into the UI.\n\nBy default, anyone can POST data to our webhook. This is why we have the webhook secret to protect us, and only react to the POST request once we have verified that it is coming from Stripe\n- each webhook secret is associated with a single webhook endpoint\n\n### Credit Cards Internationally\nRegions like Europe and India require our application to handle requests from banks to authenticate a purchase (3DS or OTP)\n- If we are using webhooks, then the authentication is already handled and we don't have to worry about this.\n","n":0.094}}},{"i":267,"$":{"0":{"v":"Integrating Stripe","n":0.707},"1":{"v":"\nThere are different ways to integrate Stripe into your application.\n\n### Only Client-side handling\nStripe allows you to integrate a Checkout mechanism, where you only need to implement client code to perform payments.\n\nHere is a reference for one time payments with only client-side code: https://stripe.com/docs/payments/checkout/client\n\n### Client-side and Server-side\nBut there are sometimes usecase where client and server need to communicate with each other before/while a payment process (https://stripe.com/docs/payments/integration-builder). For example if you don't manage your inventory with Stripe you will need some own logic on the server. One reason would be that you just want to offer Stripe just as a payment gateway for credit cards besides other payment methods like PayPal.\n\n### Webhooks\nWell webhooks can be used in both cases. Webhooks allow you to let Stripe communicate with your backend server to inform you about succeeded payment, failed payments, customer updates, orders, billings and so on. By defining a webhook URL you can specify which events you want to receive from Stripe. You can then use events to update specified data in your database.\n","n":0.076}}},{"i":268,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n### `listen`\n- from the CLI, we can listen for events that happen on the stripe API, and have those events forwarded to us.\n- by default, `listen` will listen to the events happening on our live configuration (endpoint), found in Stripe dashboard. If we want to test, then we can listen for events happening at a different endpoint, with `stripe listen --forward-to localhost:5000/webhook`\n\n### `trigger`\n- from the CLI, we can fake an event happening (ie. fake a call to Stripe's API)\n- if we are listening in another terminal, then we should see the fakes command being listened to.\n\n## payment intents\n### `create`\n```sh\nstripe payment_intents create \\\n  --amount=2000 \\\n  --currency=cad \\\n  -d \"payment_method_types[]\"=card\n```\n[ref](https://stripe.com/docs/api/payment_intents/create)\n","n":0.096}}},{"i":269,"$":{"0":{"v":"API Keys","n":0.707},"1":{"v":"\n## Keys\nSecret key is used on server side code and publishable key is to be used on the client side to associate with the Stripe account.\n","n":0.196}}},{"i":270,"$":{"0":{"v":"Setup Intent","n":0.707},"1":{"v":"\n- A SetupIntent guides you through the process of setting up and saving a customer's payment credentials for future payments.\n- If the SetupIntent is used with a Customer, upon success, it will automatically attach the resulting payment method to that Customer.\n- if we wanted to save a credit card at the time of purchase, then we can add the `setup_future_intent` at the time of creating a PaymentIntent. This negates our need to use the SetupIntent. spec: Therefore, the SetupIntent is needed only when saving a payment for later, but not making a purchase at the time (like when someone click \"add payment method\".\n- Creating a SetupIntent will generate a PaymentMethod to be attached to a customer, which can then be used to create a PaymentIntent when you are ready to charge them.\n","n":0.087}}},{"i":271,"$":{"0":{"v":"Price","n":1},"1":{"v":"\nEach Stripe Product can have multiple prices. In other words, Products/Prices have a 1:many relationship\n- ex., your premium monthly subscription (the product) may have 3 different prices attached to it:\n\t1. $9/month\n\t2. $100/year\n\t3. $500 one-time-purchase\n","n":0.171}}},{"i":272,"$":{"0":{"v":"Payment Method","n":0.707},"1":{"v":"\nrepresent the customer's payment instruments\n- they can be either:\n\t1. included as part of the payment intent when we are creating (so we know which credit card to use)\n\t2. saved to a stripe customer object for future payments\n\n### Types\n- *CARDS*\t- Cards are linked to a debit or credit account at a bank. To complete a payment online, customers enter their card information at checkout.\n- *WALLETS*\t- Wallets are linked to a card or bank account, but can also store monetary value. Wallets typically require customer verification (e.g., biometrics, SMS, passcode) to complete a payment.\n\t- ex. Apple pay, Google pay\n- *BANK DEBITS* -\tBank debits pull funds directly from your customer’s bank account. Customers provide their bank account information and typically agree to a mandate for you to debit their account.\n- *BANK REDIRECTS* - Bank redirects add a layer of verification to complete a bank debit payment. Instead of entering their bank account information, customers are redirected to provide their online banking credentials to authorize the payment.\tNo, but Stripe supports recurring for some methods by converting to direct debit\n- *BANK CREDIT TRANSFERS* -\tCredit transfers allow customers to push funds from their bank account to yours. You provide customers with the bank account information they should send funds to.\n- *BUY NOW, PAY LATER* - Buy now, pay later is a growing category of payment methods that offers customers immediate financing for online payments, typically repaid in fixed installments over time.\n- *CASH-BASED VOUCHERS* -\tWith cash-based vouchers, customers receive a scannable voucher with a transaction reference number that they can then bring to an ATM, bank, convenience store, or supermarket to complete the payment in cash.\n","n":0.061}}},{"i":273,"$":{"0":{"v":"Payment Intent","n":0.707},"1":{"v":"\nThere should be exactly one payment_intent for each order or customer session so we can reference the PaymentIntent later to see the history of payment attempts for a particular session.\n- The PaymentIntent API tracks a payment, from initial creation through the entire checkout process, and triggers additional authentication steps when required. Therefore, a payment intent has different states during its lifetime.\n\nWhen you use a PaymentIntent to collect payment from a customer, Stripe creates a charge behind the scenes\n\n- The best practice is to create a PaymentIntent as soon as the purchase amount is known\n\t- ex. this might be when the customer begins the checkout process. This would be fine, since we can always update the amount if the user backs out, adds another item to their cart, then begins checkout again\n- We should store the `PaymentIntentID` in our database, to be associated with the customer's shopping cart (order). If this isn't feasible, then we can store it on their session.\n\t- This would allow us to track and failed payment attempts for a given cart (or session)\n- We should provide an `idempotency key` to the PaymentIntent\n\t- Doing this will help us avoid creating duplicate PaymentIntents for the same purchase\n\t- this key would be an ID associated with the cart/session, which we store in the database\n- The PaymentIntent contains a client secret key which is unique to the individual PaymentIntent. In other words, a client secret uniquely identifies a payment intent.\n\t- This client secret is used as a parameter when calling `stripe.confirmCardPayment` on the client.\n\t- We retrieve the client secret from the PaymentIntent on our server, which we then can pass to the client. There are different approaches to getting the client secret to the client side.\n- The client secret of a paymentintent is dependent on the paymentintentid.\n\t- An example of the secret is `pi_1IR0JiKQXfStDK4vk3nikCNH_secret_GAWuc4MSiWoezicYMqMe5qgWj`, showing that the client secret would change if the payment intent was different.\n- If the customer leaves the CheckoutSession incomplete, it will expire and cancel the PaymentIntent automatically after 24 hours.\n\n## Refunds\n- We can provide an `amount` parameter to the Refunds API. Otherwise, the default amount will be the exact amount paid in the initial payment intent\n","n":0.053}}},{"i":274,"$":{"0":{"v":"Storage","n":1},"1":{"v":"\nAlmost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away\n- Generally the fast volatile technologies (which lose data when off power) are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".\n\n## Levels\n### Registers (L0)\n- registers hold words from cache memory\n### On-chip cache (L1)\n- L1 cache holds cache lines retrieved from the L2 cache\n- ex. SRAM\n### Off-chip cache (L2)\n- L2 cache holds cache lines retrieved from memory\n- ex. SRAM\n### Main memory (L3)\n- Main memory holds disk blocks retrieved from local disks\n- ex. DRAM\n### Local secondary storage (L4)\n- local disks hold files retrieved from disks on remote network servers\n- ex. local disk\n### Remote secondary storage (L5)\n- ex. distributed filesystems, web servers\n\n* * *\n\n### Storage on a CD\n- the data on a CD is stored in an outward spiral pattern. The CD port on the computer has motor which rotates the CD. It also has a laser arm which follows the spiral, as the disk spins.\n- the spiral track has little bumps along it. The bumps are used to encode the binary value. The laser arm in the CD port has a sensor attached to it, which \"listens for\" the reflection of the laser. If the sensor receives the light, an electrical signal is generated and sent to the microprocessor, which interprets it as a `1`\n![](/assets/images/2021-03-09-21-22-35.png)\n","n":0.065}}},{"i":275,"$":{"0":{"v":"Personal Storage","n":0.707},"1":{"v":"\nThere has to be a clear distinction between your different layers of storage. For instance, you may have storage to sync config settings, warm storage for things that you want to be able to retrieve relatively easily (photos 3 years ago), cold storage for bulletproof backups (ex. all your pictures ever taken).\n\nMaking this distinction clear helps us realize if our current backup methods are good enough. For instance, as in the first example, what if we are using Dropbox to sync our dot-files across multiple devices. Since this file is actively used, it is no longer stable as a backup solution. All it takes is for one device to delete all of the contents on the folder for your entire \"backup solution\" to fall over. Therefore, we need another solution. Because dot-files get updated often, we will need to write to this often. Therefore, a warm storage solution is probably sufficient. Following the rule of 3-2-1,\n\nIf something is built into a software workflow, then it is not a backup\n- ex. Joplin sync does not count as backup, nor does Mackup. The reason is that we interact with these tools in a dynmaic way. We don't treat them as immutable snapshots. A backup is only a backup if it's immutable (or treated as such).\n\nIf you haven't yet tried to restore all of your data via your backup file, then you don't have a backup system in place.\n\n#### 3-2-1 rule of backups\nFollow the 3-2-1 rule of backups - 3 copies, on 2 different media (e.g. disk, usb), with at least one copy offsite.\n","n":0.062}}},{"i":276,"$":{"0":{"v":"Formats","n":1},"1":{"v":"\n## Storage Formats \nThere are 3 main ways (formats) that data can be stored digitally: \n1. file storage \n2. block storage \n3. object storage\n\n### File storage\n- organizes and represents data as a hierarchy of files in folders\n- ex. Unix, NAS\n- Since a system must be the thing that implements a filesystem, data stored in this way is not easily portable \n\t- ex. using Windows FS and trying to open it on Mac will prove difficult\n\n### Block storage\n- chunks data into arbitrarily organized, evenly sized blocks. Each block is given an ID, making it easily retrievable from storage.\n\t- it takes several blocks to make up one file.\n- Data can be retrieved fast because it doesn't rely on a single path to find the data.\n- the more data you need to store, the better off you’ll be with block storage.\n- ex. Storage Area Network (SAN)\n\n### Object storage\n- manages data and links it to associated metadata.\n","n":0.081}}},{"i":277,"$":{"0":{"v":"Deduping","n":1},"1":{"v":"\nDeduping (or, de-duplicating) is the idea that many data files or records may contain duplicate information that would be inefficient to store multiple times. Instead, you store the duplicated data sequence just once and then for every file or record that contains that same data sequence, you just insert a short reference that points to the data sequence (i.e. basically saying 'insert data sequence X here' rather than storing the whole X data sequence multiple times).\n\nSometimes you also have cases where you have multiple copies of the exact same file (as opposed to just chunks or sequences of data within files that are sometimes duplicated). In this case, where files are duplicated exactly, you can perform a hash function against files and compare the resulting values to test whether their data contents are identical. If so, the file only needs to be stored once, and every other duplicate file record can just reference the original file by its hash.\n\nIn a [[log|general.lang.data-structs.log]], we want each transaction to appear exactly once. If we failed at this, it could result in the master and its slave replications becoming inconsistent.\n\n[[Set|js.lang.type.set]] is a great way to deduplicate an array, since each entry in a `Set` must be unique.","n":0.07}}},{"i":278,"$":{"0":{"v":"Storage Methods","n":0.707},"1":{"v":"\n## RAID (Redundant Array of Independent Disks) \nA RAID is simply multiple physical hard drives acting as a single virtual hard drive\n- RAID provides redundancy across several disks attached to the same machine\n\nThe main goals of RAID are to safeguard your data with some sort of backup/failsafe, or to speed up reading/writing.\n- When data is read, it is read from both hard drives and checked to make sure they agree.\n\nRAID bases its storage creation on the smallest drive in the storage pool. As shown in the below image, if the smallest drive in a classic RAID storage pool is 500 GB, all other drives in the storage pool can each only contribute 500 GB. As a result, the storage pool ends up with 2 TB of usable storage and 4.5 TB of wasted storage.\n- note: this does not apply to SHR (Synology Hybrid Raid)\n![](/assets/images/2023-02-26-07-09-40.png)\n\nThere are 3 main types (with example of storing a song):\n1. *RAID-0* - the song is stored across multiple hard drives. Therefore, if one fails then the file is corrupted \n\t- Least safe\n2. *RAID-1* - all hard drives have the same data. Therefore, if one fails, the redundancy provided by the second will allow us to retain all of the data.\n\t- RAID-1 is where disk mirroring is most commonly used\n3. *RAID-5* - the song is stored across multiple hard drives (like RAID-0), but each hard drive will have its own backup. If one hard drive fails, the backup is restored so we don't get a corrupted file. \n\t- This is the safest, but also more expensive and a bit slower.\n","n":0.062}}},{"i":279,"$":{"0":{"v":"Sqlite","n":1},"1":{"v":"\nSQLite is a self-contained C library that keeps the entire database in a single file\n\n- show tables - `.tables`\n- show databases - `.databases`\n- open database - `.open ~/.config/joplin/database.sqlite`\n- show signature of table - `.shema <tablename>`\n\t- more readable form - `pragma table_info('<tablename>')`\n\n### Migrations\nThe  package includes a database migrations feature\n\n## Tools\n- [Litestream: Fully replicated SQLite](https://litestream.io/)","n":0.137}}},{"i":280,"$":{"0":{"v":"SQL","n":1},"1":{"v":"\nSQL is a declarative language— we declare the result we want to obtain in terms of a data processing pipeline that is executed against a known database model and a dataset.\n\nEvery single sql query contains some level of business logic\n\nThe SQL language is statically typed: every query defines a new relation that must be fully understood by the system before executing it. This is the reason why sometimes cast expressions are needed in your Queries\n\nIn SQL, you need to explain your problem, unlike most programming languages where you need to focus on a solution you think is going to solve your problem. When struggling to write an sql query, First write down a single sentence in English that perfectly describes what you’re trying to achieve.\n\n## Parts of SQL\nAll SQL can be broken down into either DDL or DML\n- both DDL and DML have their own CRUD operations\n\n### DDL (data definition language)\n- used to define data structures (working with the structure of the data)\n    - associated with creating tables, defining relationships (primary, foreign and composite keys, for example) and data types, constraints, etc, and also for modifying them.\n- ex. `CREATE`, `ALTER`, `DROP`\n\n### DML (data manipulation language)\n- used to manipulate data (working with the data itself)\n- ex. `INSERT`, `UPDATE`, `DELETE`, `SELECT`\n\n# E Resource\n- [quality tutorial (current progress)](https://pgexercises.com/questions/joins/self2.html)\n- [quality tips](https://blog.jooq.org/2016/03/17/10-easy-steps-to-a-complete-understanding-of-sql/)\n","n":0.068}}},{"i":281,"$":{"0":{"v":"View","n":1},"1":{"v":"\nA view is like a derived table, but instead of it only living within the context of a single query, it exists more in perpetuity\n- in other words, it is the result set of a stored query\n- users of the database can query the view just as they would query any other table\n\nA view is a table-like object whose contents are the results of some query.\n\nWhile a view is a virtualized table that represents the result of a particular db query, when a query is made against a view, that query is converted into a query that can be made against the base table (in other words, the underlying base table(s) that the view is based off of)\n","n":0.092}}},{"i":282,"$":{"0":{"v":"Materialized View","n":0.707},"1":{"v":"\nIn contrast to the way a view is queried, a materialized view actually takes up storage space, making it a proper table (and not just a conceptual table). When you query a materialized view, the result set is [[cached|general.arch.cache]] and stored (or, materialized) as a sort of pseudo table\n- can be thought of as a snapshot of a query set that is updated periodically (or on-demand) to keep the data up-to-date.\n- makes queries more efficient at the expense of space\n\nThe difference is that a materialized view is an actual copy of the query results, written to disk, whereas a virtual view is just a shortcut for writing queries.\n\nWhen the underlying data changes, a materialized view needs to be updated, because it is a denormalized copy of the data.\n- The database can do that automatically, but such updates make writes more expensive, which is why materialized views are not often used in [[OLTP|db.olap-oltp]] databases.\n\n### How does it work?\n1. identify frequently executed and resource-intensive SQL queries in your application and create materialized views for these queries. These are the queries that can benefit from caching.\n2. Once the materialized views are created, you can store them in the cache server\n3. When a user or application requests data, the cache server checks if the requested data is available in the materialized view cache. If the data is found, the cache server can quickly serve the result without executing the SQL query against the database.","n":0.065}}},{"i":283,"$":{"0":{"v":"Terms","n":1},"1":{"v":"\n# SQL Term Map\n- relation -> table\n- attribute -> column\n- tuple -> row\n![](/assets/images/2021-03-09-17-13-13.png)\n","n":0.277}}},{"i":284,"$":{"0":{"v":"Relation","n":1},"1":{"v":"\n### What is relational?\nA relation is a set of tuples (fixed length array)\n- in other words, a single row in a single database is a relation between all the values that represent that row\n- in SQL, a relation is represented by a table, with each row being a tuple\n- a tuple is described as a function, since it maps names to values.\n\t- ex. for row 1, name -> \"Kyle\"\n- each element (cell) of the tuple is termed an `attribute value`\n\t- an attribute is the combination of the column name and its type\n\t\t- ex. age int\n\t- an attribute value is the cross-section of an attribute and a tuple (ie. it is a single piece of data)\n- each SQL clause will transform one or several relations in order to produce new relations.\n- Data is not relational. Rather, data has relationships.\n- *relational* is the concept that we have a set of elements that all share the same properties (aka attribute domains)\n\t- ex. if we have a table `foo` with 2 columns: id: int and name: text, then every single record in this database will look exactly like that.\n","n":0.074}}},{"i":285,"$":{"0":{"v":"Heading","n":1},"1":{"v":"\nA heading is a set of attributes in which column names are unique amongst each other\n- In SQL, the Heading is simply the set of all column names in a table\n- This logically follows from the earlier definition, that each tuple has a corresponding (and unique) Heading\n","n":0.146}}},{"i":286,"$":{"0":{"v":"Body","n":1},"1":{"v":"\nBody - the set of tuples that correspond to a single Heading is called a *Body* \n- In SQL, the Body is simple all rows in the table\n- A relation is thus a heading paired with a body\n\t- in other words, a table\n","n":0.152}}},{"i":287,"$":{"0":{"v":"Tables","n":1},"1":{"v":"\n*Replicated tables* are typically static data that does not change very often. Replicating them allows for read scalability.","n":0.236}}},{"i":288,"$":{"0":{"v":"Junction Tables","n":0.707},"1":{"v":"\nMany:Many relationships are achieved by junction tables (ie. associative tables)\n- A junction table maps two or more tables together by referencing the primary keys of each data table. In effect, it contains a number of foreign keys, each in a many-to-one relationship from the junction table to the individual data tables. The PK of the associative table is typically composed of the FK columns themselves.\n","n":0.124}}},{"i":289,"$":{"0":{"v":"Design","n":1},"1":{"v":"\n## Should this be a table or not?\n- can you imagine the current data being *expanded* in the future?\n\t- ex. imagine we had a Book table, which included 3 properties: genre1, genre2, genre3. Now if we decided that each book should have 4 potential genres, we would have to change the structure. The solution would be to have a Books table and a Genres table\n- Is the data being held in a table \"genericable\"? If you are tempted to combine different concepts into something that has a common thread, it might be a sign that they should be different tables. Framed another way, if you find yourself in a position where it would be difficult to extend a table, it should give us pause.\n\t- ex. Imagine we have to keep track of `InvoiceStatus`, `BackOrderStatus`, `ShipViaCarrier`, `CreditStatus`, and `CustomerStatusType`. Each of these concepts has a common thread, which is that aside from their respective PKs, they only have a single text field. We might therefore think it appropriate to combine all 5 concepts into a single generic table. However, this becomes problematic when we start to query information. What results is a necessity to JOIN ON many different fields, and to use aliases to distinguish multiple \"versions\" of the same table:\n- if we were to look at 5 rows of a single table and noticed that 3/5 of the rows have an identical value for one of the columns, that would be a sign that the column should be its own table\n\t- ex. we have a table called `shoes` and there is a field to describe what type of shoe it is (business, casual etc). If we were to query some records, we would find that 'business' would show up a lot in the 'type' column. This is a sign that we should create a \"shoe type\" table.\n\n```sql\nSELECT *\n\tFROM Customer\n\tJOIN GenericDomain as CustomerType\n\tON Customer.CustomerTypeId = CustomerType.GenericDomainId\n\t  and CustomerType.RelatedToTable = \"Customer\"\n\t  and  CustomerType.RelatedToColumn = \"CustomerTypeId\"\n\tJOIN GenericDomain as CreditStatus\n\tON Customer.CreditStatusId = CreditStatus.GenericDomainId\n\t  and CreditStatus.RelatedToTable = \"Customer\"\n\t  and CreditStatus.RelatedToColumn = \"CreditStatusId\"\n```\n\t- on the other hand, if we separate them all out as seperate tables, then it greatly simplifies our query:\n```sql\nSELECT * FROM Customer  \n\tJOIN CustomerType \n\t\tON Customer.CustomerTypeId = CustomerType.CustomerTypeId  \n\tJOIN CreditStatus    \n\t\tON Customer.CreditStatusId = CreditStatus.CreditStatusId\n```\n","n":0.052}}},{"i":290,"$":{"0":{"v":"Derived Tables","n":0.707},"1":{"v":"\nImagine we were in a position where we needed to make complex queries that involved some aggregation functions. What we could do to fulfill this, is create a temporary table, fill it with data (using INSERT), and use a *cursor* to loop through the temporary data, making updates as we go. Finally, we could execute a query against the temporary table, allowing us to obtain the final results. \n- An alternative approach for some data problems is to use *derived tables* (also known as inline views)\n\nA derived table is a virtual table that is created within the scope of a query\n- The table is created with a SELECT statement of its own and is given an alias using the AS clause\n- the contents of this table can then be used within the query.\n- the result is similar to the above naive solution, and doesn't carry the overhead of having to actually CREATE/DELETE tables and run INSERTs on them\n\t- Therefore, derived tables eliminate the requirement to use cursors and temporary tables\n\nyou can think of a derived table as a temporary variable that points to a table that we just defined \n\n## Using Derived Tables\n- Because derived tables are used in place of an actual table, the query to build the derived table goes in the same spot as we normally have our FROM clause. All we need to do is replace the table name with parentheses, and define an alias with AS afterward\n- In the below example, all our derived table is really doing is making a new table that we can query against, where instead of having 3 columns: `first_name`, `last_name` and `value`, we have 2 columns: `salesperson` and `value`. This is a contrived example to show that a derived table allows us to transform how the original table appears, which we can then take advantage of to support our more complex queries of the data. \n```sql\nSELECT * FROM\n(\n\tSELECT first_name || ' ' || last_name AS salesperson, value_of_sales FROM sales_table\n) as derived_sales_table\nWHERE value_of_sales > 100 \n```\n- The data from this inner query can be used in the column list, in joins, and any other area where table data would be acceptable.\n\nIn Postgres, we can use the WITH clause to clean up the logic of derived tables:\n```sql\nSELECT first_name, last_name, age\nFROM (\n  SELECT first_name, last_name, current_date - date_of_birth age\n  FROM author\n)\n```\nbecomes\n```sql\nWITH a AS (\n  SELECT first_name, last_name, current_date - date_of_birth age\n  FROM author\n)\nSELECT *\nFROM a\nWHERE age > 10000\n```\n- with the above example, if we were anticipating the need to query the data set resulting from that subquery, we could just make it into a view so we wouldn't have to repeat that logic with each query we make\n\n[tutorial](http://www.blackwasp.co.uk/SQLDerivedTables.aspx)\n","n":0.047}}},{"i":291,"$":{"0":{"v":"Stored Procedure","n":0.707},"1":{"v":"\nProvided that all data required by a transaction is in memory, the stored procedure can execute very fast, without waiting for any network or disk I/O.\n![](/assets/images/2022-04-02-20-22-19.png)\n\nA database is often much more performance-sensitive than an application server, because a single database instance is often shared by many application servers. A badly written stored procedure (e.g., using a lot of memory or CPU time) in a database can cause much more trouble than equivalent badly written code in an application server.","n":0.113}}},{"i":292,"$":{"0":{"v":"Query","n":1}}},{"i":293,"$":{"0":{"v":"Subquery","n":1},"1":{"v":"\nThink of subqueries as fluent syntax that's chained by nesting rather than step by step after each other.\n- similar to functional programming and how you read from the inside-out, using the output of the inner function as input for the next outer function \n\n- subqueries return a subset of data from a larger set of data (which arose from a different query)\n\t- ie. perform a query to get some data, then immediately perform another query on that data.\n- subqueries can be used almost anywhere in an SQL query.\n- if a subquery is slow, try and convert it to a derived table, as those are much faster\n- Subqueries can usually be executed independently, allowing us to see what each one is doing so we can mentally make substitutions as we are trying to understand an SQL query\n\nUse subqueries any time you want to first query a set of data, which you will then query again on\n- note: in reality it is not as inefficient as it sounds. This is just a simplification\n\nex. below, if `dr_recommended` is a table with a column called `activity`, then this query will return all activities in both tables, since they will be \"doctor recommended\" \nselect the `type` column go in the `dr_recommended` table\n- you can use the WITH clause in place of sub-queries\n\n```sql\nselect * \nfrom exercise_logs \nwhere activity in (\n\tselect type from dr_recommended\n);\n```\nex. below, we first query for the highest joindate, then out of the subset of data, we query for the joindate, firstname and surname\n```sql\nselect joindate, firstname, surname\nfrom cd.members\nwhere joindate = (select\nmax(joindate)\nfrom cd.members)\n```\n","n":0.062}}},{"i":294,"$":{"0":{"v":"Keys","n":1},"1":{"v":"\n### Primary Key\n- a foreign key of a table can also be made into its PK\n\t- ex. Imagine we had a `public.user` table that held more public info about a user like `username`, `interests`, and a `private.user_account` which held sensitive info like `password` and `email`. Since it is logical to think of the `user_account` table as just an extension of the `user` table, we can simply use the already existing `id` from the `user` table, and make `user_id` the primary key of `user_account`, which references the `user` table.\n- PK is a type of candidate key.\n\n#### Surrogate Key\n- A surrogate key is a (likely automatically generated) primary key that is chosen because there is nothing else that uniquely identifies a row. Put another way, the only reason it exists is for the purposes of our database.\n\t- By contrast, a key that actually means something outside our database (ex. an item SKU)\n- The danger in just relying on surrogate keys is if we have a situation where there are other columns that should uniquely identify the row, then it leaves us open to developer error\n\t- ex. what if we have an `items` table, and 2 rows are `id` and `SKU`. There's nothing stopping us from inputting the same SKU twice, which result in a duplicated row with different ids.\n\n### Foreign Key\n- for foreign keys, we definitely want one column to uniquely identify a row (likely a primary key), otherwise we would have to reference multiple columns in the other table.\n- foreign keys can reference tables or columns\n- the foreign key is on the many side of a 1:many relationship\n- foreign keys are not unique\n- foreign keys can be null\n\n### Candidate key\n- a column or combination of columns that uniquely identifies a row.\n\t- ex. if the rules of our application dictated that emails in the db must be unique, then email could qualify as a candidate key \n- Any column or column combination that can guarantee uniqueness is referred to as a *candidate key*\n\t- if this uniqueness is achieved by 2 or more columns, then it is referred to as a composite key. Therefore, composite key is a type of candidate key.\n\n#### Differences with PK\n- a table can have multiple candidate keys.\n- candidate key column(s) can have null values\n\n#### Composite key\n- If a table does not have a single column that qualifies for a Candidate key, then you have to select 2 or more columns to make a row unique\n\t- ex. if no EmployeeID or SocialSecurityNumber that could qualify as a candidate key, then you can make FullName + DoB as composite primary key (though there is small risk of duplication still)\n- Using composite keys could save us from having to perform lookups before making insertions.\n\n### Super key\n- if you add any column to a PK, then it becomes a super key\n\t- ex. EmployeeID + FullName\n\n## Constraining tables\n- foreign key constraints ensure that there is another table in existence that has the same column-value\n\t- ex. if we have an attribute `product_no` in an Order table, then we should expect that attribute to reference an `id` from a Product table\n\t- maintains the referential integrity between two related tables\n- when we create a foreign key, we essentially are saying \"a row cannot be entered into this table, unless we have an established reference to the other table\"\n\t- in other words, if the id in the other table (the *referenced table*) doesn't exist, then neither can the row in the *referencing table* (the table with the FK) \n\n## Constraining columns\n- below, we reference columns `c1` and `c2` from `table2`, calling the `b` and `c` in our table (`table1`)\n```sql\nCREATE TABLE table` (\n  a integer PRIMARY KEY,\n  b integer,\n  c integer,\n  FOREIGN KEY (b, c) REFERENCES table2 (c1, c2)\n);\n```\n\n1:1 and 1:many relationships are achieved by the use of key contraints (foreign keys)\n- ex. User table with an `id` (PK) property can be used as the value for the `owner_id` (FK) property on the Nugget table\n\n\n### Shoestore example\nLets start with a  sensible table: `product` (ex. Air, Jordans). Since we are going to keeping inventory in the store, we will need actual instances of those shoes, which we can call the `item` table. For instance, this table will hold the information about the shoe's size, its particular color, its price etc. (All of this information is specific to a shoe instance, and doesn't make sense to be included in the product table). Next, we might be tempted to add a column to the `product` table indicating what type of shoe it is (business, running etc). What we would find when querying data however, is that 'business' would be repeated many times, which is a sign that it should be its own table. So we create a table called `product_type` and relate it to the `product` table with ids. Next, we will have to consider that we will have to store data on the actual sale of the item (called `sales_item`). There is a distinct but important difference between difference between the `item` table and the `sales_item`. `item` is a table that includes the actual instances of the shoe in inventory. `sales_item` is the item that is being sold. Therefore, it is going to be part of another table `sales_order`. In `sales_item`, we will include information like the specific tax rate that was used (since different people are subject to different tax rates), the discount that was applied, the quantity bought etc. Next, in our `sales_order` table, we will include the form of payment (credit, cash), what time the order was placed etc. \n![8da5a5b7c2d3633de044d693884219ee.png](:/7d7f11b898814196afe51643fe2a6f8d)\n","n":0.033}}},{"i":295,"$":{"0":{"v":"Join","n":1},"1":{"v":"\n### Syntax for joining on a key\nMost common is to use JOIN ... ON\n```sql\n...\ninner join T1 on T2.id = T1.id\n```\n\nWe can also use the USING clause, which is a shorthand that allows you to take advantage of the specific situation where both sides of the join use the same name for the joining column(s)\n- For example, the following produces the join condition ON T1.a = T2.a AND T1.b = T2.b.\n```sql\nselect * from T1\ninner join T2 using (a, b)\n```\nDoing it like this also suppresses redundant columns\n- there is no need to print both of the matched columns, since they must have equal values\n\n### Anti-join\nAn anti-join is meant to keep only the rows that fail a test.\n- ex. imagine we have a result set of students having taken an Economics exam. If we want to filter to include only those students who did not finish the exam (denoted by `null` value in the `score` column), we can filter out those who did in fact finish the exam.\n    - note: since WHERE clause is just a filter, it doesn't care which columns are actually returned by the query. This is why we can just say `SELECT 1`. Postgres doesn't even look at what was selected; only that something was returned from the query.\n```sql\nselect *\n    from students\n    inner join results on students.id = results.student_id\nwhere not exists (\n    select 1\n        from results r\n    where score is not null\n)\n```\n","n":0.066}}},{"i":296,"$":{"0":{"v":"Self Join","n":0.707},"1":{"v":"\nA self join allows us to join a table to itself using an inner or outer join\n- this allows you to create a result set that joins the rows with the other rows within the same table.\n\nIn practice, you typically use a self-join to query hierarchical data or to compare rows within the same table.\n- Imagine we need to compare a column of each row with all other rows in that table, and get a result set of only the rows that pass a certain condition\n\t- ex. we have a table `temperature_readings` that include both the high and low for the day. What we conceptually need to do is take row1 and compare it to every other row. Then we take row2 and compare it to every other row, and so on. Conceptually, this is like a nested for loop, where w1 is the outer loop and w2 is the inner loop:\n```sql\nselect\n     w1.city,\n     w1.temp_lo as low,\n     w1.temp_hi as high,\n     w2.city,\n     w2.temp_lo as low,\n     w2.temp_hi as high\n from weather w1, weather w2\n where w1.temp_lo < w2.temp_lo\n and w1.temp_hi > w2.temp_hi;\n```\nWith self joins, we need to use an alias because you cannot refer to the same table more than once in a query, so we assign it a name when it is joining against itself\n- since we are using 2 versions of the same table, we should give both an alias. For example in the below example, give alias `m` for the manager's version and `e` for the employee's version\n- [why use aliases?](https://www.reddit.com/r/SQL/comments/9e5yi9/why_use_table_alias_noob_question/)\n\nTwo reasons for using a self-join:\n1. query parents/child relationship stored in a table\n\t- ex. in `employee` table, demonstrate relationship between `employee_id` and `reports_to` columns. Here, the employee's `reports_to` would match up with the manager's `employee_id`\n\t\t- `INNER JOIN employees m ON m.employeeid = e.reportsto`\n2. obtain running totals\n\n","n":0.058}}},{"i":297,"$":{"0":{"v":"Outer Join","n":0.707},"1":{"v":"\nWhen doing an inner join, if either table is unmatched by the other, then those rows will be excluded from the result set.\n- Outer joins allow us to select rows from one table that may or may not have the corresponding rows in other tables.\n\t- a LEFT JOIN returns only the unmatched rows from the left table (the original table we are joining onto)\n\t\t- \"get me all rows from the left table, even if there are no matching rows in the right table\"\n\t- a RIGHT JOIN returns only the unmatched rows from the right table (the new table we are adding)\n- Making a LEFT JOIN and then putting a column from the outer table in the WHERE clause would give us an error, (spec): since that column would be unselectable.\n\n### Example: songs and artists\nImagine we have songs and artists. Each song has 0 or more artists. This is possible because a song might not have an attributed author.\n\nImagine we query the songs table, then left join artists. This would allow us a result set that includes all the songs without artist. If we had done an inner join, then the songs without an artist would not have ended up in the result set\n\n## Left Outer Join\n- called *left outer* because the table on the left of the join operator will have each of its rows in the output at least once, whereas the table on the right will only have those rows output that match some row of the left table.\n\t- if there is not right-table match, a null value will be substituted in for right-table columns.\n\t- a common way to distinguish the left and right side of the join is to use aliases\n- Left join semantics are to keep the whole result set of the table lexically on the left of the operator, and to fill-in the columns for the table on the right of the left join operator when some data is found that matches the join condition, otherwise using NULL as the column’s value.\n- there are times when we do want to include data where there isn't a match (ex. there is no matching FK for a given PK between 2 tables of a corresponding row). In these cases, we have to use an OUTER JOIN.\n\t- ex. we are doing a left join on \"bucket\", and we have buckets in our db that have no nuggets. this would result in a `null` value for the \"nugget\" column associated with that row.\n\t![2890a1430af9f888455d50224e795554.png](:/2af5c67cf5fb4b898d5323ce1cb0849b)\n- It's the LEFT or RIGHT keyword that makes the JOIN an \"OUTER\" JOIN\n\t- ie, all JOINs designated either LEFT or RIGHT are by definition OUTER JOINs\n\t- We use LEFT, RIGHT or FULL to specify which table should have all of its rows returned, regardless of the condition being met.\n- the first table is the \"left\" table\n- best practice is to avoid Right Outer Joins\n- when joining tables, all rows inclded in the left table will show up on the new table, even if one of the cells is missing, as in prev. example.\n- the left outer join starts selecting data from the left table. it compares the 2 values (specified after `ON`).\n\t- If they are equal, a new row is created that contains columns of both tables and adds the new row to the result set\n\t- if they are not equal, a new row is still created, but the right table's columns will be filled with `null`\n\t- If we want, we can add a WHERE clause to this query to only return rows from the left table that do not having matching rows in the right table\n\t\t- this effectively cuts out the intersection from the venn diagram\n\t\t- ex. `...WHERE tableb_col IS NULL;`\n- if we made a venn diagram of table1 and table2, an outer join would give us the outer parts of the venn diagram Union.\n\t- we will get all of the rows of the left table, but only the \"rows in common\" that the right table has\n\t\t- ex. if the left table has `1`, `2`, `3`, those will always show up no matter what. If the right table has `2`, `3`, `4`, then only `2` and `3` will show up (the rest will be null)\n- Venn diagram illustrates the left join that returns rows from the left table that do not have matching rows from the right table:\n![6457179c137a03db0dd9765c9cd6c988.png](:/69d6642385194490a8d62f93f8cf1bdb)\n\n## Full Outer Join\n- returns a result set that contains all rows from both left and right tables, with the matching rows from both sides if available.\n\t- In case there is no match, the columns of the table will be filled with NULL.\n- similar to the above techniques using `WHERE` clause to cut out the intersection, we can do the same here\n\t- This would give us the rows from one table that do not have the corresponding rows in another table\n![9e04ed55193420bac12aa21a38c81141.png](:/56dbccd24a6d449fa4d6ba8a0427d96a)\n\n### Left vs Right Join\nImagine we have `paymnent_sources`, which has a `payment_method_id` column, and both `coupons` and `credit_cards` has also have a `payment_method_id` column. In other words, a payment_source can reference either a credit_card or a coupon. Let's say we want to query all payment_sources, including all credit_cards and coupons. We would `select from payment_sources`, then `left outer join` the credit_cards table and the coupons table. Therefore, `left join` is the concept of \"I have a baseline group of records, and I want to get all attachments to that baseline, without caring about how each attachment relates to other attachements (e.g. credit_cards to coupons)\n","n":0.033}}},{"i":298,"$":{"0":{"v":"Lateral","n":1},"1":{"v":"\nSubqueries appearing in FROM can be preceded by the key word LATERAL. This allows them to reference columns provided by preceding FROM items. (Without LATERAL, each subquery is evaluated independently and so cannot cross-reference any other FROM item.)\n\nTable functions appearing in FROM can also be preceded by the key word LATERAL, but for functions the key word is optional; the function's arguments can contain references to columns provided by preceding FROM items in any case.\n\nLATERAL JOINs behave more as a correlated subquery, rather than just a plain subquery.\n- expressions to the right of a LATERAL join are evaluated once for each row left of it - just like a correlated subquery - while a plain subquery (table expression) is evaluated once only.\n\nFor returning more than one column, a LATERAL join is typically simpler, cleaner and faster.\nAlso, remember that the equivalent of a correlated subquery is `LEFT JOIN LATERAL ... ON true`\n","n":0.081}}},{"i":299,"$":{"0":{"v":"Cross","n":1},"1":{"v":"\nWhile inner and outer joins involve starting with one table and appending rows onto it, CROSS JOINs involve taking 2+ tables and generating a cartesian coordinate table as a result of it.\n![](/assets/images/2021-05-21-17-12-27.png)\n\nEffectively, this gives us a combination of each row in T1 and T2\n- In the above image, T1 has 3 rows and T2 has 3 rows, so the result set of the CROSS JOIN has 9 (3 * 3) rows\n","n":0.119}}},{"i":300,"$":{"0":{"v":"Constraint","n":1},"1":{"v":"\n### Differences between a column NOT NULL constraint and a CHECK CONSTRAINT not null\na `check constraint` is like a column constraint, except that it's on a table.\n\nAlthough the end result may be the same, there are some differences:\n- check constraints have to be named and belong to the table, while a NOT NULL column is just an option of the latter\n- from a performance point of view, creating an explicit not-null constraint is more efficient.\n\t- only 0.5% performance penalty, so hardly noticeable.\n- you have to remove check constraints before removing the associated columns\n- a `NOT NULL` is written beside the column name when issuing `\\d your_table` on psql, while check constraints are described below a specific session\n","n":0.092}}},{"i":301,"$":{"0":{"v":"Clause","n":1}}},{"i":302,"$":{"0":{"v":"Update","n":1},"1":{"v":"\nwe can update a table while leveraging the commands that help us create a result set.\n- ex. imagine we discover that all temperature readings after Nov 14 are off by 2 degrees:\n```sql\nUPDATE weather\nSET temp = temp - 2\nWHERE date > '2020-11-14'\n```\n- UPDATE always requires a SET token.\n\n[[pg.query.CTE]]\n\n### Joining\n```sql\nupdate app_public.payments as payments\n  set stripe_pi_code = book_orders.stripe_pi_code\n  from app_public.book_orders as book_orders\n  where payments.book_order_id = book_orders.id;\n```\n","n":0.126}}},{"i":303,"$":{"0":{"v":"Select","n":1},"1":{"v":"\nSELECT is known as a \"projection\" in relational algebra.\n- Once you’ve generated your table reference, filtered it, transformed it, you can step to projecting it to another form\n\n- Within the SELECT clause, you can finally operate on columns, creating complex column expressions as parts of the row.\n- SELECT accepts expressions, allowing us to say `select (temp_lo + temp_hi)/2 as temp_avg...`\n- The SELECT clause may be one of the most complex clauses in SQL, even if it appears so simple. All other clauses just “pipe” table references from one to another\n- In order to understand SQL, it is important to understand everything else first, before trying to tackle SELECT\n\n\n### SELECT DISTINCT\nremove duplicate rows *after* the SELECT has been applied (in other words, after the projection)\n\n### SELECT (inside a WITH)\n- purpose is to break down complicated queries into simpler parts\n\n# Combining result set of 2+ SELECT statements\nThe queries that involve UNION, INTERSECT, or EXCEPT need to follow these rules:\n1. The number of columns and their orders must be the same in the two queries.\n2. The data types of the respective columns must be compatible.\n\n* * *\n\n### Don't use SELECT *\n- By manually listing out each column we want to project, we show the intention of the code. Listing out each column allows for declaring our thinking as developers\n- When we implicitly select all columns, if the columns on a table change (eg. we remove a column), then the implementations where we call `SELECT *` may not be happy with the new dataset returned.\n\t- Maybe the column we just removed was a dependency of that module. If we had manually listed out each column, then the error would have been more clear.\n- It isn't efficient to retrieve all bytes of data, when we don't even need them all.\n","n":0.058}}},{"i":304,"$":{"0":{"v":"Order by","n":0.707},"1":{"v":"\norder by the column name.\n- SQL doesn't guarantee any ordering of the result set of any query except when you use ORDER BY\n- you can use `order by 1` to sort the view or table by 1st column of query's result.\n\n## Implementing kNN (*k* nearest neighbors)\nSuppose we want to order a list of cities by their distance to Paris:\n- note: `<->` operator is \"distance between\"\n```sql\nselect name, location, country\n    from circuits\norder by point(lng,lat) <-> point(2.349014, 48.864716)\n   limit 10;\n```\n","n":0.114}}},{"i":305,"$":{"0":{"v":"From","n":1},"1":{"v":"\nThe result of the FROM list is an intermediate virtual table that can then be subject to transformations by the WHERE, GROUP BY, and HAVING clauses\n","n":0.196}}},{"i":306,"$":{"0":{"v":"Object Relational Mapper","n":0.577},"1":{"v":"\nAn ORM is for translating SQL records into application language objects (Javascript, Python etc.).\n- an ORM doesn't prevent your need to write SQL or manage indexes\n\nIf our server has an ORM built-in (like the one Django has), it is not necessarily a bad idea to use an ORM. Consider that:\n```py\nMyModel.objects.get(id=1)\n```\n\nis equivalent to:\n```sql\nselect mymodel.id, mymodel.other_field, ...\n  from mymodel\n where id=1;\n ```\n\nThe SQL language abstracts away the procedural steps for querying a database, allowing one to merely define what one wants. But certain SQL queries are thousands of times slower than other logically equivalent queries. On an even higher level of abstraction, ORM systems, which isolate object-oriented code from the implementation of object persistence using a relational database, still force the programmer to think in terms of databases, tables, and native SQL queries as soon as performance of ORM-generated queries becomes a concern.\n","n":0.084}}},{"i":307,"$":{"0":{"v":"Spring","n":1},"1":{"v":"\n## Overview\nCentral to the Spring Framework is its [[IoC|general.patterns.IoC]] container, which provides a consistent means of configuring and managing Java objects using reflection.\n- The container is responsible for managing object lifecycles of specific objects: creating these objects, calling their initialization methods, and configuring these objects by wiring them together.\n\nSpring beans are objects that are created, managed and destroyed by the Spring Container\n- The container can be configured by loading XML files or detecting specific Java annotations on configuration classes. \n  - These data sources contain the bean definitions which provide the information required to create the beans.\n\n## Commands\n### Start app\n```sh\n# Gradle\n$ ./gradlew bootRun\n\n# Maven\n$ ./mvnw spring-boot:run\n```\n\n### Controllers\nA key difference between a traditional MVC controller and the RESTful web service controller from Springboot is the way that the HTTP response body is created. \n```java\n@GetMapping(\"/greeting\")\npublic Greeting greeting(@RequestParam(value = \"name\", defaultValue = \"World\") String name) {\n  return new Greeting(counter.incrementAndGet(), String.format(template, name));\n}\n```\n\n- Rather than relying on a View technology to perform server-side rendering of the greeting data to HTML, this RESTful web service controller populates and returns a `Greeting` object. The object data will be written directly to the HTTP response as JSON. To do this, the `Greeting` object must be converted to JSON\n  - Spring’s out-of-the-box HTTP message converter support allows us to do this. This works out of the box because Jackson 2 (the Java JSON package) is in the classpath\n\n#### `@RestController`\nThe `@RestController` annotation marks the class as a controller where every method returns a domain object instead of a view.\n- it is shorthand for including both `@Controller` and `@ResponseBody`.","n":0.062}}},{"i":308,"$":{"0":{"v":"Repository","n":1},"1":{"v":"\nA Repository is basically a mechanism where you let Spring write the database queries for you based on (for example) the method names.\n\nIf you have a `PersonRepository` defined properly, with a method defined like this:\n\n```java\nList<Person> findByFirstName();\n```\n\nUnder the hood Spring will generate code for you that would turn this into a query like:\n\n```sql\nSELECT * FROM person WHERE firstName = :firstName\n```\n\nAnd then also do the mapping of the query results to `Person` objects for you.","n":0.117}}},{"i":309,"$":{"0":{"v":"Sketch","n":1},"1":{"v":"\n# Styles\n- apply styles to things like text to be able to reuse them easily\n    - when selecting text, it is under the `Appearance` headline\n\n# Layout\n## Smart Layout\n- Create a symbol that when modified (ex. text), will retain its ratio\n    - ex. make a button, group it, then make into a symbol. On the naming\n\twindow, select \"horizontal align\". Now, when you replace the text on\n\tthat button, the horizontal size will automatically adjust\n- Use smart layout any time you create a list of horizontal text (ex. a navbar,\n    or tabs).\n    - Each item should be a symbol that has a smart layout of \"horizontal\n\talign\".\n    - The whole navbar itself should also be a symbol with \"horizontal align\",\n\tmaking the spacing between the items to be consistent even with changes\n\tto the text.\n    - On the sidebar, you can choose to override styles in order to remove an\n\titem from the list. Each item will shift horizontally to take its place.\n\n# Reusing\n- Mainly done through `Layer Styles`, `Text styles` and `Symbols`\n\n## Symbols\n- similar to the concept of components in programming\n- ex. create a button from scratch, similar to grouping them together, we can\n    make them into a symbol to be reused throughout the design\n- to make edits to the master symbol, just click on any symbol to be transported\n    to the master symbol's canvas.\n- [guide to reusing colors](https://www.reddit.com/r/sketchapp/comments/gwft1f/if_you_could_just_edit_these_and_for_the_change/)\n\n### Nested Symbols\n- ability to override the symbols\n- Imagine we have a navbar and want to create an active/inactive state for each\n    button.\n- Create the whole navbar as one symbol, then when editing the master navbar symbol, click\n    into each button and convert it to a symbol, naming it `nav/icon{1,2,3}/off`\n- Duplicate each icon and make the ON state by changing color, then renaming to\n    `nav/icon{1,2,3}/on`\n- This allows us to easily insert each state of a symbol easily.\n- We can quickly swap in on/off states now by clicking on the symbol (from the\n    artboard) and overriding the styles on the sidebar\n- If we want to swap the icon for another, we can just go to the navbar master\n    symbol and modify each nested symbol (the icon), and it will be updated\n    across all instances of the navbar symbol \n    \n#### Thought experiment\nLet’s say that a project requires 3 buttons in 3 different colours. To use only one symbol for all of them at once, we need to be able to change the colour of the button without detaching it from a symbol.\nWhen you put a symbol inside another, you can swap the one inside with any other available symbol of the same size. In Sketch, this function is named nested overrides. This way, if you have multiple colour swatches saved as symbols, you can easily replace one swatch with another.\n\n## Overriding symbols\n- Say we have a vertical scrollable list of airbnb houses for rent, each with common styles. What we can\n    do is create 1, make it a symbol, then insert that symbol into the document.\n    On the sidebar, you can see all of the overrides that are available\n    (adjusting text, adjusting colors, images etc)\n\n* * *\n\n## Mask\n- use to chop a hole out of the object (ex. picture) in the shape of the\n    overlying object.\n    - take a square and hover over a picture, then click 'mask', and you have an\n\timagine in the shape of the square.\n### Alpha mask\n- created when using opacity with gradients\n- allows us to place a rectangle with a gradient at 0%-100% over text, and hit\n    'mask' to impose that gradient on the text.\n- the object on top will apply its gradient onto the object below\n\n# Shortcuts\n- `cmd+1`/`cmd+2` - zoom in/out on object\n- `shift+cmd+l` - lock an object from movement\n- `shift+cmd+h` - hide an object \n- `cmd+[`/`cmd+]` - send layer back/fwd\n    - `cmd+opt+[]` - send to bottom/top\n","n":0.04}}},{"i":310,"$":{"0":{"v":"Shell","n":1},"1":{"v":"\n## High-level\n- A shell program is typically an executable binary that takes commands that you type and (once you hit return), translates those commands into (ultimately) system calls to the Operating System API.\n\t- a shell language is a language designed to ‘glue’ together other programs.\n- a shell is simply a macro processor that executes commands.\n\t- The term macro processor means functionality where text and symbols are expanded to create larger expressions.\n\nSince other shells are also programs, they can be run from within one another\n\n*\"Although most users think of the shell as an interactive command interpreter, it is really a programming language in which each statement runs a command. Because it must satisfy both the interactive and programming aspects of command execution, it is a strange language, shaped as much by history as by design.\"*\n- GNU specifies the shell as both a programming language AND a command interpreter. The command interpreter part provides the user interface to the GNU utilities, while the programming language part allows these utilities to be combined.\n\n### Piping\nShell languages are exceptional at one thing: piping\n- The `|`, `&` and `;` operators, plus `()` and ``` form a tidy little language for describing pipelines.\n\t- `a & b` is concurrent\n\t- `a ; b` is sequential\n\t- `a | b` is a pipeline where a feeds b\n\n- Think of `( a & b & c ) | tee capture | analysis` as the kind of thing that's hard to express in Ruby (or Python).\n\n- environment variables exist in the same environment as the code that is executed\n\n### Precedence of Synonyms\n1. Alias\n2. Keyword (ie. syntax of the shell)\n3. Function\n4. Builtin\n\t- commands that are built into the shell. These are executed directly, as opposed to the shell having to load and execute the executable\n\t\t- ex. pwd, cd\n\n* * *\n\n*interactive* means that input is accepted from the command line, while *non-interactive* means that input is accepted from a file.\n\nif the first word of a given command does not correspond to a built-in shell command, then the shell assumed that it is the name of an executable file.\n\nwhen we run `bash -c`, a script, or other executable in the shell, it becomes a child of the current environment.\n- ex. `bash -c 'echo \"child [$var]\";` will only have access to `$var` if it was exported beforehand from the shell that the command is run in.\n\nIn bash utilities, the `--` signifies the end of command options, after which only positional parameters are accepted.\n- often, `--` can signify the separation from the command options, and the following Regex\n\n### Status code 127\nThink of it as \"Error: the program you tried to use was not found\"\n- it's returned by your shell `/bin/bash` when any given command within your bash script or on bash command line is not found in any of the paths defined by PATH system environment variable.\n- to fix, make sure the command we are using is discoverable through `$PATH`\n\n### Subshell: `$()`\nwhen we execute commands within `$(...)`, they are executed in a subshell\n\n## UE Resources\n[GNU High-quality documentation](https://www.gnu.org/software/bash/manual/bash.html)\n","n":0.045}}},{"i":311,"$":{"0":{"v":"Variables","n":1},"1":{"v":"\n### Passing variables to a command\nTo pass a variable to a shell command, just set it before the command:\n- ex. `MY_VAR=123 ./my-script.sh`\n- note: the variables only become available to the command that follows. For instance, the variables will not be available in the `my-second-script` file in this case:\n```sh\nMY_VAR=123 ./my-script.sh && ./my-second-script.sh\n```\n\n### Conventions\nvariables should be **snake_case**\n- kebab-case does not work\n\n","n":0.13}}},{"i":312,"$":{"0":{"v":"Internal","n":1},"1":{"v":"\n`$?`\n- Exit status of a command, function, or the script itself\n\n`$$`\n- Process ID (PID) of the script itself\n    - The `$$` variable often finds use in scripts to construct \"unique\" temp file names\n\n`$_`\n- Special variable set to final argument of previous command executed.\n\n`$!`\n- PID (process ID) of last job run in background\n","n":0.139}}},{"i":313,"$":{"0":{"v":"Test","n":1},"1":{"v":"\n\"Test\" is a conditional check in Bash\n\n`[ -e FILE ]` - True if FILE exists\n`[ -d FILE ]` - True if FILE exists and is a directory.\n`[ -f FILE ]` - True if FILE exists and is a regular file (ie. not a directory).\n`[ -z STRING ]` - True if the length if \"STRING\" is zero.\n`[ -n STRING ]` or `[ STRING ]` - True if the length of string > 0\n- use to check for existence of variable\n\n```sh\nhttp_status=$(curl ___)\nif [ $http_status != \"200\" ]\n    then\n        echo \"command failed\"\nfi\n```\n\n### Test based on exit status of previously executed command\n- The `?` variable holds the exit status of the previously executed command (the most recently completed foreground process).\n```sh\nif [ $? -eq 0 ]\nthen echo 'That was a good job!'\nfi\n```\n\n### Using `&&` and `||` for conditionals\nNever use `x && y || z` when `y` can return a non-zero exit status. Use `if` statements instead\n- [source](http://mywiki.wooledge.org/BashPitfalls#cmd1_.26.26_cmd2_.7C.7C_cmd3)","n":0.081}}},{"i":314,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n## Modifications\n- `!` before the flag to check opposite \n    - ex.  `if [ ! -f \"$FILE\" ]` checks that file does NOT exist\n\n* * *\n\n### Check if a CLI package is installed\n```sh\nif ! aws_bin=\"$(which aws)\" 2>/dev/null; then\n    echo \"aws cli is missing; you can get it from https://aws.amazon.com/cli/\"\n    exit 1\nfi\n```\n\n#### Variation: install package if it's not found\n```sh\n[ $(which ts-node) ] || echo \"ts-node not found. Installing...\" && npm i -g ts-node\n```\n\n### Check if environment variable set\n```sh\nif [ -z $APP_NAME ]; then echo \"Error:'APP_NAME' is not set, it is required for serverless\" && exit 1; fi\n```\n\n\n### Check if X exists\n- file - `-f`\n- directory - `-d`\n\n```\nif [ -d \"$FILE\" ]; then\n    echo \"$FILE is a directory.\"\nfi\n```\n\nOne liner:\n`[ -d /etc/docker ] && echo \"$FILE is a directory.\"`","n":0.089}}},{"i":315,"$":{"0":{"v":"Subshell","n":1},"1":{"v":"\nA subshell starts out as an almost identical copy of the original shell process. Under the hood, the shell calls the fork system call1, which creates a new process whose code and memory are copies2. When the subshell is created, there are very few differences between it and its parent. In particular, they have the same variables\n\nA subshell is different from executing a script. \n- A script is a separate program. This separate program coincidentally might also be a script which is executed by the same interpreter as the parent, but this doesn't mean the separate program has any special visibility on internal data of the parent. This is a mere coincidence.","n":0.094}}},{"i":316,"$":{"0":{"v":"Strings","n":1},"1":{"v":"\n- Single quotes `''` - preserves the literal value of each character within the quotes (don't allow anything to be interpolated inside). Take it exactly as it appears.\n\t- ex. if `$MYVARIABLE` is within the `''`, the string `$MYVARIABLE` will be interpreted.\n- Double quotes `\"\"` - evalutate the functions within, interpolating into the string\n\t- ex. if `$MYVARIABLE` is within the `\"\"`, the value that `$MYVARIABLE` stands for will be interpreted.\n\nnm: double quotes are bigger, so they have more capabilities (the ability to interpret and expand variables)\n\n```sh\n# NO STRING INTERPOLATION\n$ echo '$(echo \"upg\")'\n# $(echo \"upg\")\n\n# js equivalent:\n# console.log(\"$(echo \"upg\")\")\n\n# STRING INTERPOLATION\n$ echo \"$(echo \"upg\")\"\n# upg\n\n# js equivalent\n# `${}`\n```\n- The implication of this is that when we use double quotes, since the inerpolated functions are called when defined, the variable that gets set to the string will only be executed once.\n- ex:\n```sh\nPROMPT=\"$(git_status) $\"\n# defining this variable will cause git_status to run, interpolating the return\n# value of that function. The only way to update this variable, is by sourcing .zshrc\n\nPROMPT='$(git_status) $'\n# here, we literally pass the string '$(git status)' to the shell (?),\n# and let it interpolate (?) on its own, each time it is displayed in the terminal\n```\n\n## Globbing\n- quotes used in a globbing context do not behave as described above. Globs are not expanded when in either single or double quotes\n","n":0.068}}},{"i":317,"$":{"0":{"v":"Script","n":1},"1":{"v":"\n[Using colors effectively in bash scripts](https://github.com/mikebranski/the-art-of-postgresql-docker/blob/master/fetch-f1db-data.sh)\n","n":0.408}}},{"i":318,"$":{"0":{"v":"Redirection","n":1},"1":{"v":"\nthe redirect operator `<`/`>` takes the stdout from the preceding command and redirects it to a file\n- implicity, `>` === `1>`\n\n`<` redirects input\n- `cat < input.txt` says \"take the contents of input.txt and use it as input to the `cat` command.\n\n`>` redirects output\n","n":0.152}}},{"i":319,"$":{"0":{"v":"Pipeline","n":1},"1":{"v":"\n- a pipe takes stdout from one process and passes it as stdin to another\n\t- if the first program fails, then a non-zero status code will be returned.\n- Each command in a pipeline is executed in its own subshell, which is a separate process\n","n":0.151}}},{"i":320,"$":{"0":{"v":"Special Parameters","n":0.707},"1":{"v":"\n- `$1, $2, $3` are the positional parameters.\n- `\"$@\"` is an array-like construct of all positional parameters\n\t- ex. `{$1, $2, $3 ...}`\n- `\"$*\"` is the IFS expansion of all positional parameters\n\t- ex. `$1 $2 $3`\n- `$#` is the number of positional parameters.\n- `$-` current options set for the shell.\n- `$$` pid of the current shell (not subshell).\n- `$_` most recent parameter (or the abs path of the command to start the current shell immediately after startup).\n- `$IFS` is the (input) field separator.\n- `$?` is the most recent foreground pipeline exit status.\n- `$!` is the PID of the most recent background command.\n- `$0` is the name of the shell or shell script.\n","n":0.095}}},{"i":321,"$":{"0":{"v":"Lang","n":1}}},{"i":322,"$":{"0":{"v":"Ternary Operator","n":0.707},"1":{"v":"\n```sh\n(test -d publishable/.next) && (echo 'updating dendron next...' && cd publishable/.next && git reset --hard && git clean -f && git pull) || (echo 'init dendron next' && npx @dendronhq/dendron-cli publish init --wsRoot publishable)\n```\n","n":0.171}}},{"i":323,"$":{"0":{"v":"Loops","n":1},"1":{"v":"\n#### Loop over regular files in directory\n```sh\nfor f in *; do\n  echo \"File -> $f\"\ndone\n```\n\n#### Loop over all files in directory (inc. dotfiles)\n```sh\nfor f in $(ls -a ~/Documents); do\n    echo \"Processing $f file..\"\ndone\n```\n","n":0.174}}},{"i":324,"$":{"0":{"v":"Glob","n":1},"1":{"v":"\n### Glob Patterns\n`*` is not the only globbing primitive. Other globbing primitives are:\n- `?` - matches any single character\n- `[abd]` - matches any character from a, b or d\n- `[a-d]` - matches any character from a, b, c or d\n\n- `.` has no meaning as a glob.\n\n### Globbing vs Regex\n- spec: Globbing is interpreted by the shell, while regex is interpreted by a program (like `rename`)\n- commands surrounded by single-quotes are not interpreted by the shell.\n","n":0.115}}},{"i":325,"$":{"0":{"v":"Expansion","n":1},"1":{"v":"\n## History Expansion\n[source](https://www.thegeekstuff.com/2011/08/bash-history-expansion/)\n- History expansion is performed immediately after a complete line is read, before the shell breaks it into words, and is performed on each line individually\n\nDesignators:\n- Event Designators - refer to a particular command in history\n\t- start with `!`\n- Word Designators - refer to a particular word of a history entry\n\t- often gets combined with Event Designators, which are demarcated by `:`\n\t- ex. `!cp:^` finds the most recent `cp` command in history and grabs the 1st argument\n- Modifiers - modify result of the Event/Word Designator\n\nCommands:\n- `!ls` - execute most recent `ls` command\n\t- ex. if I ran `git log`, then `!git`, bash will say \"oh ok, you want the last executed bash command\", and will run `git log`\n- `!?apache` - execute most recent command that contains the keyword `apache`.\n\t- `!%` will refer to the word that was matched by the previous `!?<pattern>` search\n- `!-3` - execute 3rd most recent command\n\t- `!!` === `!-1` (most recent command)\n- `!$` - reuse last part of the most recent command\n\t- ex. `less ~/myfile` then `vim !$` will run `vim ~/myfile`\n- `!*` - reuse all parts of most recent command\n- `^ls^cat^` - modify the pattern `ls` with `cat` in the previous command\n- `!!:s/ls -l/cat/` - replace `ls -l` of previous command with `cat`\n\t- `!!:gs/...` for global substitution\n- `!cp:^` - get 1st arg of the most recent `cp` command\n- `!cp:$` - get last arg of the most recent `cp` command\n- `!cp:2` - get 2nd arg of the most recent `cp` command\n- `!cp:*` - get all args of the most recent `cp` command\n\n* * *\n\n## Tilde Expansion\n### Tilde prefix\nThe tilde prefix includes all of the characters before the first slash `/`\n\n`~`\n`$HOME`\n- in reality, this is shorthand for `~kyletycholiz` (the current user)\n\n`~+`\n`$PWD`\n\n`~-`\n`$OLDPWD`\n- or, the directory we were in previous to our PWD\n","n":0.058}}},{"i":326,"$":{"0":{"v":"Variable (aka Parameter) Expansion","n":0.5},"1":{"v":"\nThe `$` character introduces parameter expansion, command substitution, or arithmetic expansion.\n- The parameter name or symbol to be expanded can be enclosed in braces, which are optional but serve to protect the variable to be expanded from characters immediately following it which could be interpreted as part of the name.\n\n### Substitution\nwhen we write `${parameter}`, the value of `parameter` is substituted.\n\n# UE Resources\n- [Overview](https://wiki.bash-hackers.org/syntax/pe)\n- [Explanation](https://ss64.com/bash/syntax-expand.html#parameter)\n","n":0.125}}},{"i":327,"$":{"0":{"v":"Brace Expansion","n":0.707},"1":{"v":"\nWith brace expansion, we can generate arbitrary strings\n- similar to filename expansion (ex. `*.env`), except the generated strings actually need to exist.\n\nTo be a proper brace expansion, there must be opening and closing braces, and at least one `,`\n```sh\n$ echo a{d,c,b}e\n# ade ace abe\n```\n\nBrace expansion is performed before any other expansions, and any characters special to other expansions are preserved in the result\n- Bash does not apply any syntactic interpretation to the context of the expansion or the text between the braces.\n\n### Alpha example\n```sh\n$ ls\nalpha.py alphabeta.py\n\n### Brace expansion\n$ mv alpha{,beta}.py ../\n```\n","n":0.105}}},{"i":328,"$":{"0":{"v":"Direnv","n":1},"1":{"v":"\nDirenv is an environment variable manager for your shell\n- ex. maybe you want to have `NPM_CONFIG` variable set to something else depending on what directory you are in.\n- It does this by [[hooking|general.patterns.hook]] into the shell to load/unload env variables, depending on current directory.\n\nDirenv allows us to have project-specific env variables, preventing us from having to clutter a `/.profile` file.\n\nBefore each prompt, existence of `.envrc` file is checked in current and parent directories.\n- If exists, it is loaded into a sub shell and all exported variables and then captured by Direnv and made available to the current shell.\n\nspec: You could also use Direnv to load the correct version of node with `nvm`.\n\n### Procedure\n1. create an `.envrc` file with the contents of what will get run when entering the directory:\n```sh\n# .envrc\nexport NPM_TOKEN=glpat-638g1hnfxn32nf2\n```\n\n2. hook direnv into the shell: https://direnv.net/docs/hook.html\n\n3. run `direnv allow` in the directory to source in the file, and mark it as safe so it will be sourced automatically in the future.\n","n":0.079}}},{"i":329,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### See the command attached to a bash alias\nrun `type <command>`\n\n### check to see if alias exists in shell\nrun `which g`\n\n### Run multiple commands in sequence\n```sh\n$ command1; command2\n```\n\nThis is distinct from `&&`, since commands will be run regardless of exit status\n\n### Run previous command with `sudo` in front\n```sh\n╰─➤ $ rm -rf /var/www\nrm: /var/www: Permission denied\n╰─➤ $ sudo !!\n╰─➤ $ sudo rm -rf /var/www\n```\n\n### Set a timer to report how long an operation took\n```sh\nstart=$SECONDS\n# do some operations\nduration=$(( SECONDS - start ))\necho \"build and bundle completed in ${duration}s\"\n```\n","n":0.108}}},{"i":330,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n### Line Navigation\n- go to start of line - `<C-a>`\n- go to end of line - `<C-e>`\n\n`<C-R>` to search through history. We can start to type a command, then by hitting this shortcut, the history will be searched for that string.\n- `d` - show previous 10 directories visited\n    - press the number beside it to go to that directory\n","n":0.13}}},{"i":331,"$":{"0":{"v":"CLI","n":1}}},{"i":332,"$":{"0":{"v":"Eval","n":1},"1":{"v":"\n`eval` will run the arguments as a command in the current shell\n- variables will be expanded before executing, so we can execute commands saved in shell variables\n\nSometimes we have bash commands that output shell commands (ex. ssh-eval)\nWe could run these outputted shell commands in 2 steps:\n1. run the command\n2. take the output of that command, and run it in the shell\n\nAlternatively, we can just wrap the command in `eval`\n- ex. `eval $(ssh-agent)`\n\n### Say Hello example\nsay-hello.sh\n```sh\necho \"echo hello\"\n```\n\n```sh\n$ ./say-hello.sh\n> echo hello\n\n$ eval $(./say-hello.sh)\n> hello\n```\n","n":0.11}}},{"i":333,"$":{"0":{"v":"Serverless Framework","n":0.707},"1":{"v":"\nThe idea with Serverless Framework is \"let's think of our application in terms of events and functions. We recognize when events happen, and we know which functions to run when they do.\"\n\nServerless is an [[IaC|devops.IaC]] tool\n- the framework is cloud-provider agnostic; all examples just assume AWS\n\nTo accomplish this philosophy, the framework defines a `serverless.yml`\n- handles the logic of which [[lambda|aws.svc.lambda]] to call, and when to call it.\n\t- ex. we may decide to call that function when a particular event enters an [[event bus|aws.svc.event-bridge]]. In this case, all we need to do in the `serverless.yml` is to specify the aws resourceId (ARN) of the event bus, and it will be subscribed to.\n\nWhat the framework does is that it automates the process of creating all the necessary services on the cloud provider. Once you run `sls deploy` the config file (`serverless.yml`) is transformed into a [[CloudFormation|aws.svc.cloud-formation]] template.\n- once the `serverless.yml` has been transformed into a CloudFormation template it gets sent to AWS to create the services you specified; all API Gateway routes, all Lambdas etc.\n- Apart from that, the Serverless framework also packages up all the code you have written and sends it to an [[S3|aws.svc.S3]] bucket.\n\t- This bucket will have all the versions of the code you have ever deployed. Effectively, the Serverless framework grabs the latest version from the bucket and sends it to the Lambda it created.\n\n### `serverless.yml`\nThe yml file is a configuration file that declaratively describes all the necessary services that should be deployed to the cloud provider.\n\n#### `functions`\nEach function in this object basically says: \"when an event comes through with these header properties, run this function.\"\n\n* * *\n\nThe serverless framework is just one of many ways to use serverless architectures. Alternatives include:\n- Managing lambdas yourself in AWS's console (or equivalent for other platforms like Azure, etc)\n- Building & deploying your application with chalice AWS Lambda & Python ONLY\n- deploying your application with apex (AWS ONLY)\n- deploy a traditional Python WSGI app to AWS using Zappa\n- lots of other options.\n\n### `stage`\nusing this to map to environments, resulting in a different environment variables that we have our build in.\n\n### `package`\n\"Packaging\" refers to the process of assembling the code and dependencies required for your serverless functions into a deployable artifact, which can then be easily uploaded to AWS.\n\nBy default, when you deploy a Serverless service, all of your functions share a single deployment package. \n- This means that all functions in your service will be packaged together into a single ZIP file, and this ZIP file is then uploaded to AWS Lambda.\n- We can set `individually: true` to override this behaviour\n\t- with this option, each function is packaged separately into its own ZIP file and uploaded to AWS Lambda.\n\n## Example\nserverless.yml\n```yml\nservice: The name of my app\n\nprovider:\n\tname: aws\n\truntime: nodejs14.x\n\nfunctions:\n\tMyLambda:\n\t\thandler: src/my-lambda.handler\n\t\tevents:\n\t\t\t- http:\n\t\t\t\tpath: hello/world\n\t\t\t\tmethod: get\n```\n\nsrc/my-lambda.ts\n```ts\nimport type { APIGatewayProxyEvent } from 'aws-lambda'\nexport async function handler(event: APIGatewayProxyEvent) {\n\treturn {\n\t\tstatusCode: 200,\n\t\tbody: 'Hello world'\n\t}\n}\n```\n\nThen run `serverless deploy`\n\n* * *\n\nServerless framework supports [[Cloudflare workers|cloudflare.workers]]\n- build serverless applications that are then deployed as Cloudflare Workers.\n- For developers whose applications run code in multiple places, using the Serverless Framework may be more efficient than writing their Workers within the Cloudflare Workers UI.\n\n# Alternatives\n- [[AWS SAM|aws.svc.sam]] & [[AWS CDK|aws.CDK]]\n\t- essentially the same thing, but using a code language instead of Yaml\n","n":0.043}}},{"i":334,"$":{"0":{"v":"Env","n":1},"1":{"v":"\nEnvironment variables can be\n- `self` - defined within the same yml file\n- `env` - defined within an `env` file\n- `opt` - defined on CLI\n  - ex. `sls invoke --stage=\"local\" ...` can be accessed with `${opt:stage}`\n\n## Resources\n- https://www.serverless.com/framework/docs/providers/aws/guide/variables\n- [3 ways to access envronment variables in serverless app](https://dev.to/eoinsha/3-ways-to-read-ssm-parameters-4555)","n":0.147}}},{"i":335,"$":{"0":{"v":"Security","n":1},"1":{"v":"\nSecurity by obscurity is generally not a thing, since DigitalOcean and AWS etc have known IP ranges that all hackers always target; if you don't ban them they'll eventually brute force you\n- Use ssh keys from day 1 and/or install fail2ban (preferably both)\n    - instead of fail2ban, you can whitelist only your CDN's IP addresses (like Cloudflare), through the use of DigitalOcean firewalls (or something similar). That way, you can’t get DoS’d because everything has to go the CDN route.\n","n":0.112}}},{"i":336,"$":{"0":{"v":"Malware","n":1},"1":{"v":"\nMalware is a catch-all phrase that includes any kind of bad software\n- ex. malware can be used to install spyware on a host\n\nA virus is a specific type of malware that requires user interaction to infect the user’s device\n- viruses are typically self-replicating. If you open an email attachment that inadvertently runs some malicious code, it will then send off more spam emails. \n\nA worm is another type of malware, and differs from a virus in that it doesn’t require user interaction. \n- ex. User runs a vulnerable network app which the hacker can send malware to. \n\nA Trojan horse is another type of malware, where the malware is hidden as a part of a useful piece of software\n\nA Botnet is a network of infected (\"mind controlled\") hosts that can be used by a hacker to do malicious things, like send out spam email or launch DDOS attacks\n","n":0.082}}},{"i":337,"$":{"0":{"v":"Firewall","n":1},"1":{"v":"\na firewall is a network security system that monitors and controls incoming and outgoing network traffic based on predetermined security rules.\n- the purpose is to establish a barrier between a trusted and an untrusted network\n\nFirewalls are either network-based (e.g. [[LAN|network.lan]]) or host-based\n- Network-based firewalls are positioned between two or more networks (e.g. between LAN and WAN)\n- Host-based firewalls are deployed directly on the host itself to control network traffic or other computing resources.","n":0.117}}},{"i":338,"$":{"0":{"v":"Federated Identity","n":0.707},"1":{"v":"\n[[Federated|general.terms.federated]] Identity is the idea that we can link a person's identity across multiple distinct identity management systems.\n- ex. we can link identity between Atlassian, Microsoft, and AWS products.\n\nTechnologies used to implement Federated Identity include:\n- [[SAML|security.federated-identity.SAML]]\n- OAuth\n- Security Tokens (Simple Web Tokens, [[JWWTs|auth.tokens.jwt]], and SAML assertions)\n\nFederated identity is related to single sign-on (SSO), in which a user's single [[auth token|auth.tokens]] is trusted across multiple IT systems\n- SSO is a subsets of federated identity management, as it relates only to authentication and is understood on the level of technical interoperability and it would not be possible without some sort of federation.\n","n":0.1}}},{"i":339,"$":{"0":{"v":"Security Assertion Markup Language","n":0.5},"1":{"v":"\nSAML lets us use one set of credentials to log into many different websites.\n","n":0.267}}},{"i":340,"$":{"0":{"v":"DDOS","n":1},"1":{"v":"\nDDOS attacks leverage the internet’s ability to reroute traffic (ie packet switching)\n","n":0.289}}},{"i":341,"$":{"0":{"v":"SDK","n":1},"1":{"v":"\nAn SDK is an interface that is defined by some provider\n- The SDK is usually imported as a library in your code.\n\n### Example AWS SDK\n![[aws.SDK]]\n","n":0.2}}},{"i":342,"$":{"0":{"v":"Sanity","n":1}}},{"i":343,"$":{"0":{"v":"Schema","n":1},"1":{"v":"\nSchemas are what allow us to edit database content in Sanity Studio\n- in order to make something editable in this portal, the type of the document must be `document`\n\nTo use any individual schema (for instance, from within another schema), it must first be imported into `schema.js`","n":0.147}}},{"i":344,"$":{"0":{"v":"Rich Text","n":0.707},"1":{"v":"\nConsider that we can style text when it is in the HTML, such as bolding it, italicizing it, etc. But how does this work with dynamic text? This is not something we can build into our front-end code, because it changes depending on the data\n- ex. Blog posts have their own italicizing, bolding, paragraph breaks etc. built in. That \"formatting\" is coupled with the text itself. Therefore, it must be stored somehow. Sanity accomplishes this by storing content as Rich Text\n\nWhen you query your Sanity project’s API your Rich Text content is returned as Portable Text\n- Other Headless CMSs may store the data in html or markdown.\n","n":0.097}}},{"i":345,"$":{"0":{"v":"Portable Text","n":0.707},"1":{"v":"\nIn its simplest form, Portable Text is an array of objects of `type` with an array of `children`\n\nPortable Text is an agnostic abstraction of Rich Text.\nIt can be serialized into any markup language\n- ex. HTML, Markdown, XML\n\nPortable Text is built on the idea of rich text as an array of blocks, themselves arrays of children `<span>`s.\n- Each block can have a style and a set of mark definitions\n\n\n```json\n{\n  \"style\": \"h1\",\n  \"_type\": \"block\",\n  \"children\": [\n    {\n      \"_type\": \"span\",\n      \"text\": \"This is a heading\"\n    }\n  ]\n}\n```\n\n### Block\nA section of text, such as paragraph or heading\n\nIncludes:\n- `children`\n- `_type`\n- `style`\n- `markDefs`\n- `listItem`\n- `level`\n\n- includes all styling that is part of the section of text\n    - ex. if a paragraph has 2 italicized words in it, then it will result in 5 objects in the `children` array, but it will be part of the same block.\n\n#### `children`\nThe elements of the `children` array are sections of a block. They maintain the styling of the rich text. For instance, if the following were Rich Text, then we'd end up with a `children` array of length 5:\n```md\nI have to tell *you* what is happening! In fact, it was my **uncle** who shot at us.\n```\n\nIt is split up like this;\n```js\nconst children = [\n    { text: \"I have to tell \", marks: []},\n    { text: \"you\", marks: ['em']},\n    { text: \"what is happening! In fact, it was my \", marks: []},\n    { text: \"uncle\", marks: ['strong']},\n    { text: \" who shot at us\", marks: []},\n]\n```\n\n#### `_type`\nAll blocks must be of a type. The type makes it possible for a serializer to parse the contents of the block.\n\n#### `style`\nStyle typically describes a visual property for the whole block (ie. all children).\n\n#### `markDefs`\nMark definitions is an array of objects with a key, type and some data\n\nMark definitions describe data structures distributed on the children.\n- Mark definitions are tied to spans in `children` by adding the referring `_key` in the child's `marks` array.\n```json\n{\n  \"markDefs\": [\n    {\n      \"_key\": \"some-random-key\",\n      \"_type\": \"link\",\n      \"href\": \"https://portabletext.org\"\n    },\n    {\n      \"_key\": \"some-other-random-key\",\n      \"_type\": \"comment\",\n      \"text\": \"Change to https://\",\n      \"author\": \"some-author-id\"\n    }\n  ]\n}\n```\n\n* * *\n\n### Custom blocks\nCustom blocks is typically images, code blocks, tables, video embeds, or any data structure. \n- Custom blocks should be given a `_type`.\n```json\n{\n  \"_type\": \"image\",\n  \"asset\": {\n    \"_ref\": \"some-asset-reference\"\n  }\n}\n```\n```json\n{\n  \"_type\": \"code\",\n  \"language\": \"javascript\",\n  \"code\": \"console.log(\\\"hello world\\\");\"\n}\n```\n\n* * *\n\n## Full example\n```json\n[{\n    \"_type\": \"block\",\n    \"_key\": \"3628734dd519\",\n    \"style\": \"normal\",\n    \"markDefs\": [{\n        \"_type\": \"link\",\n        \"_key\": \"e556761904ba\",\n        \"href\": \"https://www.portabletext.org\"\n    }],\n    \"children\": [{\n            \"_type\": \"span\",\n            \"_key\": \"3628734dd5190\",\n            \"text\": \"This is a paragraph with a \",\n            \"marks\": []\n        },\n        {\n            \"_type\": \"span\",\n            \"_key\": \"3628734dd5191\",\n            \"text\": \"link\",\n            \"marks\": [\n                \"e556761904ba\"\n            ]\n        },\n        {\n            \"_type\": \"span\",\n            \"_key\": \"3628734dd5192\",\n            \"text\": \".\",\n            \"marks\": []\n        }\n    ]\n}]\n```\n","n":0.048}}},{"i":346,"$":{"0":{"v":"Serializing","n":1},"1":{"v":"\nSerializing is the process of taking Portable Text (ie. an array of Blocks) and converting to a format that can be rendered to a page, like HTML, React, Markdown etc.\n\n# E Resources\n- [Codesandbox with some decent illustrations](https://codesandbox.io/s/portable-text-serializer-demo-all-defaults-p8ms8?file=/src/Page.js)\n- [Sanity docs on presenting block text](https://www.sanity.io/docs/presenting-block-text)\n- [Sanity's block-content-to-react library](https://github.com/sanity-io/block-content-to-react)\n","n":0.147}}},{"i":347,"$":{"0":{"v":"Images","n":1},"1":{"v":"\n```ts\nimport imageUrlBuilder from '@sanity/image-url'\n\nfunction urlFor (source: SanityImageSource) {\n  return imageUrlBuilder(client).image(source)\n}\n\nreturn (\n  {authorImage && (\n    <div>\n      <img\n        src={urlFor(authorImage)\n          .width(50)\n          .url()}\n      />\n    </div>\n  )}\n)\n```\n","n":0.213}}},{"i":348,"$":{"0":{"v":"GROQ","n":1},"1":{"v":"\nGROQ (Graph-Relational Object Queries) is a declarative query language that shares a lot of similarities with [[GraphQL|graphql]].\n- It is used by...\n    - describing exactly what information your application needs\n    - potentially joining together information from several sets of documents\n    - then stitching together a very specific response with only the exact fields you need.\n\n```\n*[_type == \"post\" && slug.current == $slug] | order(publishedDate, title) {\n    title,\n    author,\n    publishedDate\n}[0..5]\n```\nMeans:\n`*` \n- select all documents\n\n`[_type == 'post' && slug.current == $slug]` \n- filter the selection down to documents with the type \"post\" and those of them who have the same slug to that we have in the parameters\n\n[0] \n- select the first and only one in that list\n\n## Breakdown\nWe think of GROQ statements as describing a data flow from left to right.\n\n### Filter\nthe first set of `[]` is the filer. We return data the type of data we are seeking, based on what parameters etc.\n- ex. return data that are of either type `Movie` or `Person`, and `popularity > 15`, or `releaseDate > 2016-04-25`\n\n### Slice\nthe second set of `[]`\nAllows us to determine which items we will get back from the query\nex. first 5\n\n### projection\nwhat's found between `{}`\nex. return `title`, `author`, and `publishedDate` from the query for all blog posts.\n\n### Ordering\nwhat's found after the ` | `\nWe can order by secondary properties, if the first property is the same (e.g. in cases where there are many items with the same `year` value)\n\n# Resources\n[Cheat sheet](https://www.sanity.io/docs/query-cheat-sheet)\n","n":0.065}}},{"i":349,"$":{"0":{"v":"Document","n":1},"1":{"v":"\nEverything in the Studio starts with the document. A document is what you create and edit in the studio—all the other types you may define live inside the documents. In the default studio configuration, the document-types are the ones that will be listed in the content-column.\n\nA reference in Sanity is a link from one document to another\n\nStandard references are “hard” meaning when a document references another document, the target document must exist, and is actually prevented from being deleted until the reference is removed.\n- There are also weak-references that do not \"hold on to\" the target. You make them by adding a _weak-key to the reference object like this: `{_ref: \"<document-id>\", _weak: true}`\n\nan object containing a `_ref` key appearing anywhere in the document becomes a hard reference, and must be followed with the dereferencing operator `->` in order to access its values:\n","n":0.084}}},{"i":350,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n### Login\n`sanity login`\n\n### Query all documents by type\n`sanity documents query \"*[_type == 'song']\"`\n\n### Delete document by ID\n`sanity documents delete <UUID>`\n\n### Get Bearer Token\n`sanity debug --secrets`","n":0.2}}},{"i":351,"$":{"0":{"v":"Rxjs","n":1},"1":{"v":"\n# Overview\nThe essential concepts in RxJS which solve async event management are:\n- Observable: represents the idea of an invokable collection of future values or events.\n- Observer\n- Subscription: represents the execution of an Observable, is primarily useful for cancelling the execution.\n- Operators: are pure functions that enable a functional programming style of dealing with collections with operations like map, filter, concat, flatMap, etc.\n- Subject: is the equivalent to an EventEmitter, and the only way of multicasting a value or event to multiple Observers.\n- Schedulers: are centralized dispatchers to control concurrency, allowing us to coordinate when computation happens on e.g. setTimeout or requestAnimationFrame or others.\n\nRxJS introduces Observables, a new Push system for JavaScript. An Observable is a Producer of multiple values, \"pushing\" them to Observers (Consumers).\n- A Function is a lazily evaluated computation that synchronously returns a single value on invocation.\n- A [[generator|js.lang.functions.generator]] is a lazily evaluated computation that synchronously returns zero to (potentially) infinite values on iteration.\n- A [[Promise|js.lang.promises]] is a computation that may (or may not) eventually return a single value.\n- An [[Observable|general.patterns.behavioural.observable]] is a lazily evaluated computation that can synchronously or asynchronously return zero to (potentially) infinite values from the time it's invoked onwards.\n\n*\"Think of RxJS as Lodash for events.\"*\n\nBecause RxJS centers around this concept of function purity. Normally, we create impure functions which leave our state exposed, making it more prone to errors from other pieces of code.\n- ex. if we have a variable `count` and increment it each time a button is clicked, there is nothing stopping some other piece of code mutating that value itself.\n  - alternatively, with observables, we can just create an observable out of the event (`fromEvent`)\n\nMarble diagram:\n![](/assets/images/2022-06-02-10-41-07.png)\n\n### Observers\nAn observer is a collection of 3 callbacks that knows how to listen to values (ie. types of notification) delivered by the Observable.\n- those types of notification are: \n  1. the observer got the next value\n  2. the observer got the last value\n  3. the observer got an error\n\nAn observer may be *partial*\n\n* * *\n\n### Example: event listeners\nNormally we register event listeners by finding an HTML element and calling `addEventListener` on it. With RxJS, we can create an observable that represents a stream of events.\n- ex. we can attach an observable to a `<button />` element. We can then configure observers to `subscribe` to this observable, and will be notified any time the button is clicked:\n```js\nvar button = document.querySelector('button');\nRx.Observable.fromEvent(button, 'click')\n  .subscribe(() => console.log('Clicked!'));\n```\n\n# Questions\n- what is meant by inner/outer observable\n    - [source](https://academind.com/learn/javascript/callbacks-vs-promises-vs-rxjs-vs-async-awaits/)\n\n# Resources\n- [Operators](http://reactivex.io/documentation/operators.html)\n- [thinking in nested streams](https://rangle.io/blog/thinking-in-nested-streams-with-rxjs/)\n","n":0.049}}},{"i":352,"$":{"0":{"v":"React","n":1},"1":{"v":"\nRxJS can be used with React to implement streams as state. That is, we use RxJS streams as our application state.\n- a stream represents events or changing values over time. This can be represented as state in React. \n\nReact is a [[pull-first system|data#push-vs-pull-data-creation,1]], while RxJS is push-first. By making React push-first, we gain several benefits:\n- Performance: only those entities that depend on the value that has changed will update, and it can be done without having to make comparisons or detect changes.\n- state management becomes more declarative, in a way that can be read top-to-bottom.\n\n## Resources\n- [RxJS-React Docs](https://react-rxjs.org/docs/core-concepts)","n":0.101}}},{"i":353,"$":{"0":{"v":"Operators","n":1},"1":{"v":"\nRxJS operators are inspired by array methods (map, filter, reduce, every, etc).\n\nOperators are pure functions that enable a functional programming style of dealing with collections with operations\n\nOperators are the bread and butter of RxJS. They are what allow us to take complex async code and easily compose it in a declarative manner.\n\nThere are 2 types of operator:\n1. pipeable operators\n2. creation operators\n\n## Pipeable Operators\noperators that can be piped to observables using the `.pipe(operator)` syntax.\n- ex. `filter`, `mergeMap`.\n- these operators return a new Observable, whose subscription logic is based on the first Observable.\n- since pipeable operators are just functions, we could technically just use them like ordinary functions: `op()(obs)`, but since normally we have multiple operators combined together, they would very quickly become unreadable. This is the purpose of `pipe()`.\n  - compare `op4()(op3()(op2()(op1()(obs))))` to `obs.pipe(op1(), op2(), op3(), op4())`\n\n## Creation Operators\noperators that can be called as standalone functions to create a new Observable.\n- ex. `of(1, 2, 3)` creates an observable that will emit 1, 2 and 3, one right after another.\n\nTheir purpose is to create an Observable with some common predefined behavior or by joining other Observables.\n- ex. the `interval` function takes a number and produces an Observable.\n\n[creation operator list](https://rxjs.dev/guide/operators#creation-operators-list)","n":0.071}}},{"i":354,"$":{"0":{"v":"API","n":1},"1":{"v":"\n\n### `map`\nanalogous to `Array.map` in that it applies a given *project function* on each value of a collection. \n- while `Array.map` accepts an array and returns an array, `Observable.map` accepts an observable and returns an observable.\n\n### `pipe`\nTakes in observable(s) as input and returns another observable\n- `pipe` is immutable, so previous observables remain unmodified.\n\nBy using `pipe` we decouple the streaming operations (map, filter, reduce...) from the core functionality (subscribing, piping).\n\nEven if there is only one observable, we should still use `pipe`\n\n[[see: pipeable operators|rxjs.op#pipeable-operators,1:#*]]\n\n### `from`\nTurn an array, promise, or iterable into an observable.\n\nSimilar to `of()`, except `from()` emits the items that are inside the argument it receives, while `of()` emits the argument as a whole.\n\n### `fromEvent`\nCreates an observable that emits certain events (e.g. click events) coming from a given event target.\n```js\nconst source = fromEvent(button, \"click\")\nconst subscribe = source.subscribe(val => console.log(val))\n```\n\n### `of`\n`of()` returns an Observable which immediately emits whatever values are supplied to `of()` as parameters, then completes.\n- This is better than returning static values, as it allows you to write subscribers that can handle the Observable type (which works both synchronously and asynchronously), even before implementing your async process.\n\nEmit variable amount of values in a sequence and then emits a complete notification.\n\n`of` is useful for maintaining the Observable data type before implementing an asynchronous interaction \n- ex. an http request to an API\n\nContrary to what the docs might suggest, `of()` is not deprecated. Calling `.of()` on an observable is deprecated, so we must change the way we use it:\n```js\n// before\nObservable.of(1,2,3).map(x => 2 * x);\n\n// after\nof(1,2,3).pipe(map(x => 2 * x));\n```\n\n### `pluck`\n`pluck` is used when we just need to pass single field value to the subscription instead of sending entire JSON object.\n\n### `tap`\n`tap` is a place to perform side-effects. \n- Typically we use it with `map` or `mergeMap`, so the question becomes \"why don't we just perform our side-effects in those methods?\"\n  - Performing side-effects here makes those `map` methods impure, limiting ourselves. For instance, it would prevent us from being able to memoize.\n\n`tap` returns an observable that is identical to the source with the only difference being that if there was an error, it will be emitted from the returned observable.\n\nConveniently, we can also use `tap` for debugging\n- can place a `tap(console.log)` anywhere in your observable pipe, log out the notifications as they are emitted by the source returned by the previous operation.\n\n```js\nof(Math.random()).pipe(\n  tap((value) => console.log(value)),\n  map(n => n > 0.5 ? 'big' : 'small')\n).subscribe(console.log);\n```\n\n### `combineLatest`\nThis operator combines multiple Observables to create an Observable whose values are calculated from the latest values of each of its input Observables.\n- Whenever any input Observable emits a value, it computes a formula using the latest values from all the inputs, then emits the output of that formula\n\n## Higher-order observables\n- ***flatten*** - subscribing inside a subscribe\n- All work mostly in same manner\n  - They map some value to an observable (you are the one in charge of returning an observable value from them, they just map it)\n  - They flatten the observable you return ( they just subscribe to it)\n  - They decide about what to do before / after they flatten (“Flattening Strategy”)\n\n### `concatAll`\nsubscribes to each \"inner\" Observable that comes out of the \"outer\" Observable, and copies all the emitted values until that Observable completes, and goes on to the next one. \n- All of the values are in that way concatenated\n\n### `mergeMap` (`map` + `mergeAll`) - the slacker operator\n- simply keep subscribing to every new observable that we return from the map\n- Other than mapping + flattening the observable, it does nothing else.\n- ex. Imagine Netflix shows up-to-date ratings for each movie, retrieved from IMDB's API. We can `mergeMap` the movie into an http request to IMDB to get this data and enhance our UI.\n- Maps each value to an Observable, then flattens all of these inner Observables using `mergeAll`.\n\n### `switchMap` (`map` + `switch`) - the \"latest and greatest\" operator\n- unsubscribe from the last mapped observable\n- ex. Imagine we are typing in Google and the autocomplete box shows up. Of course, these suggestions change with each key press. If we use `switchMap`, each previous request will be cancelled if a new one happens. If we'd used `mergeMap`, a request for each keystroke would be made\n- `switchMap` projects each source value to an Observable which is merged in the output Observable, emitting values only from the most recently projected Observable\n- ie. it maps each value to an Observable, then flattens all of these Observables in the only output Observable.\n\n### `concatMap` (`map` + `concatAll`) - the \"wait in line\" operator\n- queue up the observables one after the other, and play their events in that order (i.e. subscribe to the next Observable in the queue only when the previous one is completed).\n- Similar to `mergeMap`, except order matters\n  - `concatMap` is `mergeMap` with a concurrency of 1.\n- ex. top 10 list\n\n### `exhaustMap` - the \"do not disturb\" operator\n- waits for the current inner observable to complete (exhaust) so that the next item would be turned into an inner observable (items emmitted while inner observable is running are ignored). ExhaustMap is very similar to concatMap, but drops some values:\n- `concatMap` - will take everything, inner observable n has to finish before n+1 will start\n- `exhaustMap` - will ignore all items which *would* produce new inner observables, if there is an inner observable running\n\n- ex. login button - since we don't want multiple clicks to be registered, we want want to disable mapping while the first http request is on the go, ensuring that we never call the server while the current request is running.\n\n## UE Resources\nhttps://betterprogramming.pub/how-to-create-observables-in-rxjs-aa3bf79b05e0","n":0.033}}},{"i":355,"$":{"0":{"v":"Objects","n":1}}},{"i":356,"$":{"0":{"v":"Subscription","n":1},"1":{"v":"\n***subscription*** - an object that represents the execution of an *observable*\n\nMuch like a promise, we need to unwrap our observable to access the values it contains. The observable unwrapping method is called subscribe. The function passed into subscribe is called every time the observable emits a value (in this case, a message is logged to the console anytime the button is clicked).\n\n```js\nlet myObs$ = clicksOnButton(myButton);\nmyObs$\n.subscribe(clickEvent => console.log('The button was clicked!'));\n```\n\nOne thing to note here is that observables under RxJS are lazy. This means that if there’s no subscribe call on `myObs$`, no click event handler is created. Observables only run when they know someone’s listening to the data they’re emitting.\n\nThe function passed into subscribe is called every time the click event happens(the observable emits a value)\n\n- `.create` accepts a subscribe function\n    - `subscribe` accepts an *observer argument*\n```js\n// This first part is the observable, which emits things\nconst myObservable = Observable.create(function subscribe(observer) {\n    observer.next('hey!') //this is emitting a value\n})\n\n// To grab the value, we define an observer\n// (x is the observer)\nconst observer = myObservable.subscribe((x) {\n    console.log(x) // hey!\n})\n```\n","n":0.075}}},{"i":357,"$":{"0":{"v":"Scheduler","n":1},"1":{"v":"\nA Scheduler lets you define in what execution context will an Observable deliver notifications to its Observer.\n\nA scheduler controls when a subscription starts and when notifications are delivered. It consists of three components.\n1. A Scheduler is a data structure. It knows how to store and queue tasks based on priority or other criteria.\n2. A Scheduler is an execution context. It denotes where and when the task is executed (e.g. immediately, or in another callback mechanism such as setTimeout or process.nextTick, or the animation frame).\n3. A Scheduler has a (virtual) clock. It provides a notion of \"time\" by a getter method now() on the scheduler. Tasks being scheduled on a particular scheduler will adhere only to the time denoted by that clock.\n\n[docs](https://rxjs.dev/guide/scheduler)","n":0.091}}},{"i":358,"$":{"0":{"v":"Observable","n":1},"1":{"v":"\n- An observable is a function that returns a stream\n- `$` is used to indicate that the variable in question is observable\n- an observer is an object with `next`, `error` and `complete` methods\n- Observables are inert (they just sit there until they are *subscribed* to), while observers stay active and listen for events from the producers\n\n**epic** - a collection of observables\n\n```js\nexport default function fetchTeams(action$) {\n  // epic\n  return action$.pipe(\n    ofType('FLASHCARDS_TEAMS_GET'),\n    switchMap(() =>\n      queryGraphQL('flashcards__teams'),\n    ),\n    map(resp => { operator\n      const flattenedData = flattenGraphqlNode(resp).data.teams;\n      return getTeamsSuccess(flattenedData);\n    }),\n    catchError(err => {\n      getError(err);\n    }),\n  );\n}\n```\n\n## Using observables\n- An `observable` is a function that takes in an `observer` (an object with `next`, `error`, and `complete`) and returns cancellation logic (i.e. unsubscribe).\n- `.next()` is called when the observable produces values\n- When an observer subscribes to an observable, the observer will keep receiving values until one of 2 things happens:\n    - there are no more values to be sent (in which case `.complete()` is called)\n    - the observer calls `.unsubscribe()` on the observer\n- fromEvent will turn an event into an observable\n    - `fromEvent(<event to be listened to>, <eventName>)`\n```js\nconst input$ = Rx.Observable.fromEvent(node, 'input')\n  .map(event => event.target.value)\n  .filter(value => value.length >= 2)\n  .subscribe(value => {\n    // use the `value`\n  });\n```\nHere are the steps of this sequence:\n1. Let’s assume the user types the letter \"a\" into our input (node is a variable that has query selected the html input element)\n2. The Observable then reacts to this event, passing the value to the next observer\n3. The value “a” is passed to `.map()`, which is subscribing to our initial observable\n4. `.map()` returns a new Observable of event.target.value and calls `.next()` on it’s observer\n5. The `.next()` call will invoke `.filter()`, which is subscribing to `.map()`, with the resulting value of the .map() call\n6. .filter() will then return another Observable with the filtered results, calling `.next()` with the value if the length is 2 or above\n7. We get the final value through our `.subscribe()` block\n\nEach time a new Observable is returned, a new observer is hooked up to the previous Observable (allowing us to pass values along a stream of observers, which do something we've asked, then call `.next()`, then pass the result to the next observer.\n- Basically, an operator returns a new Observable each time, allowing the stream to continue\n\n## Higher-order Observables\n- [[api|rxjs.op.api#higher-order-observables,1:#*]]\n\nObservables normally emit scalar values, but sometimes we must handle Observables of Observables (so-called higher-order observables)\n- We work with these by flattening, which means to convert it from a higher-order observable into a regular observable.\n\nHigher-order observables have their own set of operators that can be used on them.\n\nSince higher-order observables have an outer observable and 1+ inner observables, it is helpful to think of them in that way.\n\nAn output (e.g. from `tap(console.log)`) of `{_isScalar: false, _subscribe: ƒ}` would imply we are trying to work with a higher order observable as if it were a plain observable.\n- this is similar to how when we forget to `await` a promise, we get `Promise<pending>`.\n\n### Example: Search box\nImagine we have a search box, where each time the text changes, a request is sent to the server.\n- the text changes are an observable (outer)\n- each response is an observable (inner)","n":0.044}}},{"i":359,"$":{"0":{"v":"Rust","n":1},"1":{"v":"\n# UE Resources\n- [Learn Rust: recommendation from Scott Tolinski](https://github.com/rust-lang/rustlings)\n","n":0.333}}},{"i":360,"$":{"0":{"v":"Ruby","n":1},"1":{"v":"\nEverything in Ruby is an object, and therefore has a class that it's derived from.\n\n### Object Types in Ruby\nSpecifying what type any particular object is in Ruby is a bit of a wobbly concept. Since Ruby takes a [[duck typing|general.terms.duck-typing]] approach, it's not so important what object it is, but rather what it can do.\n\nTherefore, it's not common in Ruby-land to want to check the type of a particular object. Instead, it makes more sense to test its ability to respond to certain methods with `object.respond_to?(:to_s)`.\n\n### Data types\n- Numbers\n- Boolean\n- Strings\n- Hashes\n- Arrays\n- Symbols\n\nWe can verify what type the object is with `response.instance_of?(Array)`\n\n### Loose syntax rules of Ruby\nRuby allows you to omit parenthesis `()` and in some cases curly braces `{}`, sometimes making the code harder to read:\n```rb\n# the following\nhas_many :models, dependent: :destroy\n\n# is identical to:\nhas_many(:models, { dependent: :destroy } )\n# has_many takes in 2 args; a symbol and a hashmap\n```\n\n* * *\n\n### Built-in variables\n- `__FILE__` - name of the current file\n- `$0` - name of the file that was originally executed\n\nRuby treats `?` and `!` as actual characters in a method name. respond_to and respond_to? are different. `?` indicates a boolean evaluation (by convention; not strict requirement).\n\n* * *\n\n### Closures\nRuby doesn't have [[first-class functions|general.lang.feat.functions.first-class]], but it does have closures in the form of [[blocks|ruby.lang.block]] , procs and [[lambdas|ruby.lang.lambda]]\n\n# UE Resources\n\n### Book recommendations from Sivers\nhttps://www.manning.com/books/the-well-grounded-rubyist\nLearn to program - Chris Pine\nEloquent Ruby","n":0.066}}},{"i":361,"$":{"0":{"v":"Regex","n":1},"1":{"v":"\n`=~` allows us to do a Regex check in Ruby:\n```rb\n# don't throw if Order equals modified, modified_reverse, added, or added_reverse\nabort 'Unrecognized order type.' unless Order =~ /(modified|added).*/\n```\n","n":0.192}}},{"i":362,"$":{"0":{"v":"Modules","n":1}}},{"i":363,"$":{"0":{"v":"File Module","n":0.707},"1":{"v":"\nnote: this is Ruby code that is more appropriate for scripting tasks\n\n### Get array of files from a folder path\n```rb\ndownloads_folder = Pathname.new(ENV['HOME']).join('Downloads')\n# call .children on a path to get the array\nall_entries_without_dotfiles = downloads_folder.children.reject { [p] p.basename.to_path.start_with?('.') }\n```\n","n":0.164}}},{"i":364,"$":{"0":{"v":"Open3","n":1},"1":{"v":"\nOpen3 grants you access to stdin, stdout, stderr and a thread to wait for the child process when running another program.\n- You can specify various attributes, redirections, current directory, etc., of the program in the same way as for Process.spawn.\n\n#### `capture2`\ncaptures the standard output of a command.\n- `2` refers to stdout\n```rb\nstdout_str, status = Open3.capture2([env,] cmd... [, opts])\n```\n","n":0.132}}},{"i":365,"$":{"0":{"v":"File","n":1},"1":{"v":"\n#### Create new file\n```rb\nFile.new(\"out.txt\", \"r\")\n```\nr - Read only. The file must exist.\nw - Create an empty file for writing.\na - Append to a file.The file is created if it does not exist.\nr+ - Open a file for update both reading and writing. The file must exist.\nw+ - Create an empty file for both reading and writing.\na+ - Open a file for reading and appending. The file is created if it does not exist.\n\n#### Open + Read file\n```rb\nfile = File.open(\"users.txt\")\n```\n\nRead whole file:\n```rb\nfile_data = file.read\n# \"user1\\nuser2\\nuser3\\n\"\n```\n\nIterate over each line\n- `chomp` will remove newline (`\\n`) characters\n```rb\nfile_data = file.readlines.map(&:chomp)\n# [\"user1\", \"user2\", \"user3\"]\n```\n","n":0.101}}},{"i":366,"$":{"0":{"v":"Lang","n":1}}},{"i":367,"$":{"0":{"v":"Symbol","n":1},"1":{"v":"\n[[See|general.lang.feat.symbol]]\n\nSymbols are defined by prepending a variable with `:`.\n\nSymbols have the distinct feature that any two symbols named the same will be identical:\n```rb\n\"foo\".equal? \"foo\"  # false\n:foo.equal? :foo    # true\n```\n\nThis makes comparing two symbols really fast (since only a pointer comparison is involved, as opposed to comparing all the characters like you would in a string)\n\nKeys in a hash are implemented as symbols. Hashes were originally implemented in Ruby like this:\n```rb\nhash = {\n   :key => \"value\",\n   :another_key => 4\n}\n```\n\nBut the syntax was simplified in v1.9:\n```rb\nhash = {\n   key: \"value\",\n   another_key: 4\n}\n```\n\n### Should you use String or Symbol?\nNo clear answer exists, only guidelines:\n- if the text at hand is \"data\", then use a string;if it's \"code\", then use a symbol, especially when used as keys in hashes\n- symbols aren't really text, even though they read well. Instead, they are unique identifiers, like numbers, or bar codes. While strings represent data that can change, symbols represent unique values, which are static.\n- if you use strings that contain the same text in your code multiple times, then a new string object will be created every time.\n   - ex. `puts \"hello\"` 10 times will create 10 string objects.\n","n":0.072}}},{"i":368,"$":{"0":{"v":"Proc","n":1},"1":{"v":"\nA `proc` object holds a code block to be executed, and can be stored in a variable.\n- it is an instance of the `Proc` class.\n\nProc is short for procedure.\n\nWhen calling a proc, the program yields control to the code block in the proc. So, if the proc returns, the current scope returns. If a proc is called inside a function and calls `return`, the outer function immediately returns as well.\n\nWhen using parameters prefixed with ampersands, passing a block to a method results in a proc in the method’s context. Procs behave like blocks, but they can be stored in a variable.\n\nTo create a proc, you call Proc.new and pass it a block:\n```rb\ndef run_proc_with_random_number(proc)\n  proc.call(rand)\nend\n\nproc = Proc.new { |n| puts \"#{n}!\" }\nrun_proc_with_random_number(proc) # 0.8417555636545129!\n```\n\nSince a proc can be stored in a variable, it can also be passed to a method just like a normal argument.\n\nInstead of creating a proc and passing that to the method, you can use Ruby’s ampersand parameter syntax and use a block instead:\n- this will convert a passed block to a proc object and store it in a variable in the method scope.\n```rb\ndef run_proc_with_random_number(&proc)\n  proc.call(random)\nend\n\nrun_proc_with_random_number { |n| puts \"#{n}!\" }\n```\n\nnote: While it’s useful to have the proc in the method in some situations, the conversion of a block to a proc produces a performance hit. Whenever possible, use implicit blocks instead.\n","n":0.067}}},{"i":369,"$":{"0":{"v":"Methods","n":1},"1":{"v":"\nThe last parameter of a method may be preceded by an asterisk(*), which is sometimes called the 'splat' operator. This indicates that more parameters may be passed to the function. Those parameters are collected up and an array is created.\n```rb\ndef by_added(dir, entries)\n  Open3.capture2(\n    'mdls',\n    '-name', 'kMDItemFSName',\n    '-name', 'kMDItemDateAdded',\n    '-raw',\n    *entries.map(&:to_path)\n  )\nend\n```\n","n":0.14}}},{"i":370,"$":{"0":{"v":"Lambda","n":1},"1":{"v":"\nA lambda allows us to assign some functionality to a variable.\n\nIn Javascript, if we wanted, we could extract out a callback (e.g. `double`) to be its own function:\n```js\nconst double = num => num * 2\nconst doubled = numbers.map(double)\n```\n\nIn Ruby, the same way we'd do this is with Lambdas or [[Procs|ruby.lang.proc]]\n```rb\ndouble = lambda do |num|\n  return num * 2\nend\n```\n\nLambdas are essentially [[procs|ruby.lang.proc]] with some distinguishing factors.\n- They are more like “regular” methods in two ways:\n\t1. they enforce the number of arguments (ie. strict arity) passed when they’re called.\n\t2. they treat the `return` keyword in the same way that a method does. They return as methods (instead of in their parent scope).\n\nHere we encapsulate some logging to a variable:\n\n```rb\nsay_something = -> { puts \"This is a lambda\" }\n```\n\nFor the code inside the lambda to actually run, we need to call it:\n```rb\nsay_something.call\n```\n\n### Proc vs Lambda\nThis function will yield control to the proc, so when it returns, the function returns. Calling the function in this example will never print the output and return 10:\n```rb\ndef return_from_proc\n  a = Proc.new { return 10 }.call\n  puts \"This will never be printed.\"\nend\n```\n\nWhen using a lambda, it will be printed. Calling return in the lambda will behave like calling return in a method, so the `a` variable is populated with 10 and the line is printed to the console.\n```rb\ndef return_from_lambda\n  a = lambda { return 10 }.call\n  puts \"The lambda returned #{a}, and this will be printed.\"\nend\n```\n","n":0.065}}},{"i":371,"$":{"0":{"v":"Iterables","n":1},"1":{"v":"\nIn Ruby, iterables are arrays, hashes and ranges.\n\n#### `.map` function\n```rb\narray.map { |string| string.upcase }\n```\n\n#### Iterate over hash map\nHere, we have arguments to the callback(?) for the key and value.\n```\nhash = { bacon: \"protein\", apple: \"fruit\" }\nhash.map { |k,v| [k, v.to_sym] }.to_h\n```\n","n":0.156}}},{"i":372,"$":{"0":{"v":"Class","n":1},"1":{"v":"\n#### Define a class\n```rb\nclass Greeter\n\tdef initialize(name = \"world\")\n\t\t@name = name\n\tend\n\n\tdef say_hi\n\t\tputs \"hello #{@name}\"\n\tend\nend\n\nmyGreeter = Greeter.new(\"Kyle\")\nmyGreeter.say_hi\n```\n\n#### Get instance methods of a class\n```rb\n# See all\nGreeter.instance_methods\n# See non-inherited\nGreeter.instance_methods(false)\n```\n\n#### Check if an object has a particular method on it\n```rb\nmyGreeter.respond_to?('say_hi')\n```\n","n":0.169}}},{"i":373,"$":{"0":{"v":"Attribute Accessors","n":0.707},"1":{"v":"\nAttribute accessors provide us with a streamlined way to define getters and setters for instance variables. Otherwise, they are left private.\n\nLet’s say that you have a class with instance variables and you want to expose them to the outside world:\n\n```rb\nclass Food\n  def initialize(protein)\n\t\t# this instance variable cannot be accessed from outside\n    @protein = protein\n  end\nend\n\nbacon = Food.new(25)\nbacon.protein # NoMethodError: undefined method `protein'\n```\n\nTo solve this, we'd define a getter and setter:\n```rb\nclass Food\n\t# getter\n\tdef protein\n\t\t@protein\n\tend\n\t# setter\n\tdef protein=(value)\n\t\t@protein = value\n\tend\nend\n```\n\nLucky for us, Ruby provides attribute accessors so we don't have to do this by hand.\n\n```rb\nclass Food\n\tattr_accessor :protein, :calories\n\tdef initialize(protein, calories)\n\t\t@protein = protein\n\t\t@calories = calories\n\tend\nend\n```\n\nAbove, we are having 4 new methods defined for us:\n1. `protein`\n2. `protein=`\n3. `calories`\n4. `calories=`\n\n* * *\n\n- `attr_reader` - generate *getter*\n- `attr_writer` - generate *setter*\n- `attr_accessor` - generate *getter* and *setter*\n","n":0.088}}},{"i":374,"$":{"0":{"v":"Block","n":1},"1":{"v":"\nA block is an anonymous code block that can be passed into a method. Therefore, a block is nothing more than a snippet code that can be created now, but executed later. Blocks are passed to methods that `yield` them within with the `do` and `end` keywords.\n- From within that, every time `yield` is called, that code block that was passed in will get executed\n- ex. The block is passed to the each method of an array object.\n\n```rb\ndef someMethod\n  hello = 'hello'\n  yield(hello) if block_given?\nend\n\n# the block is between the curly braces\nsomeMethod { |h| h }\n```\n\nSomething implemented in JS with callbacks would be implemented with blocks in Ruby:\n- Blocks -> Ruby\n- Callbacks -> Javascript\n\nBlocks are essentially methods, since they:\n- can accept arguments\n- returns a value\n\nBut, they:\n- have no name\n- do not belong to an object\n\na block is a [[closure|general.lang.feat.closure]], so it can refer to variables from the scope in which it was declared.\n\nBlocks can only be created by passing them to a method when the method is called.\n\nBlocks are snippets of code that can be created to be executed later.\n\nBlocks are used extensively in Ruby for passing bits of code to functions. By using the `yield` keyword, a block can be implicitly passed without having to convert it to a proc.\n\nIf we are using the `.each` method, we are using blocks. The part between `do` and `end` is the block:\n- the `|name|` is the parameter of the block\n```rb\n@names.each do |name|\n  puts \"Hello #{name}!\"\nend\n```\n\nInternally, the `each` method will essentially call `yield \"Albert\"`, then `yield \"Brenda\"` and then `yield \"Charles\"`, and so on.\n\nConceptually, this is what's happening under the hood:\n```rb\ndef each\n  i = 0\n  while i < size\n    yield at(i)\n    i += 1\n  end\nend\n```\n\nThe real power of blocks is when dealing with things that are more complicated than lists. Beyond handling simple housekeeping details within the method, you can also handle setup, teardown, and errors\n\n## Two ways of calling Blocks\nBlocks can be called in one of 2 ways\n1. by using `yield` (implicit blocks)\n2. by capturing it by specifying the final argument of the method as `&block` and using `block.call` (explicit blocks)\n\n### `yield` and Implicit Blocks\nIn Ruby, methods can take blocks implicitly and explicitly\n- Implicit block passing works by calling the `yield` keyword in a method.\n  - `yield` finds and calls a passed block, so you don’t have to add the block to the list of arguments the method accepts.\n\nBecause Ruby allows implicit block passing, you can call all methods with a block. If it doesn’t call `yield`, the block is ignored. If the called method does `yield`, the passed block is found and called with any arguments that were passed to the `yield` keyword.\n\n### Explicitly accepting blocks\nWe can explicitly accept a block in a method by adding it as an argument using an ampersand parameter (usually called `&block`). Since the block is now explicit, we can use the `#call` method directly on the resulting object instead of relying on `yield`.\n- The `&block` argument is not a proper argument, so calling this method with anything else than a block will produce an `ArgumentError`.\n\n```rb\ndef each_explicit(&block)\n  return to_enum(:each) unless block\n\n  i = 0\n  while i < size\n    block.call at(i)\n    i += 1\n  end\nend\n```\n\nWhen a block is passed like this and stored in a variable, it is automatically converted to a *proc*.\n\nIt's important to understand the different uses of `&` here as a prefix to the last argument of a function:\n- in a function definition, it captures any passed block into that object\n- in a function call, it expands the given callback object into a block\n\n### Blocks allow us to implement [[Inversion of Control|general.patterns.IoC]]\nBy accepting a block from you as a programmer, the method can pass control to you.\n\n### Syntax\nThe following are identical:\n```rb\n5.times do |name|\n  puts \"Oh, hello #{name}!\"\nend\n\n5.times { |name| puts \"Oh, hello #{name}!\" }\n```\n","n":0.04}}},{"i":375,"$":{"0":{"v":"Array","n":1}}},{"i":376,"$":{"0":{"v":"Methods","n":1},"1":{"v":"\n`collect` is an alias of `map`\n`inject` is an alias of `reduce`\n","n":0.302}}},{"i":377,"$":{"0":{"v":"Gems","n":1}}},{"i":378,"$":{"0":{"v":"Pathname","n":1},"1":{"v":"\n#### Append to a pathname\n```rb\ndownloads_dir = Pathname.new(ENV['HOME']).join('Downloads')\n```\n\n#### Get files/folders of a directory as an array\n```rb\nPathname.new(ENV['HOME']).join('Downloads').children\n```\n\n#### Get basename (ie. all leaf nodes of a path)\n```rb\nPathname.new(ENV['HOME']).basename\n```\n","n":0.204}}},{"i":379,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Check if a variable is iterable\nHere, we basically check if there is an `each` method on the `@names` instance variable\n```rb\nif @names.respond_to?(\"each\")\n```\n\n#### Access shell variables\n```rb\nrequire 'pathname'\n\nPathname.new(ENV['HOME'])\n```\n","n":0.196}}},{"i":380,"$":{"0":{"v":"RSS Feed","n":0.707},"1":{"v":"\nAn RSS feed is an application with a data layer that stores URLs that return XML data from a feed.\nex. we store `https://reddit.com/r/investing.rss` in the database, and when I open the RSS feed, a GET request is made and all the posts are returned.\n\n### Make a subreddit RSS feed\nSimply add `.rss` to the subreddit URL\n- ex. `https://www.reddit.com/r/hockey.rss`\n","n":0.132}}},{"i":381,"$":{"0":{"v":"Retool","n":1},"1":{"v":"\nRetool allows us to create internal tools that can hook into our APIs and database. With Retool, we can read/write data in a UI that we don't have to build ourselves.\n- [home](https://retool.com/)\n- [ToolJet: Untested alternative to Retool](https://tooljet.io/)\n","n":0.164}}},{"i":382,"$":{"0":{"v":"Regex","n":1},"1":{"v":"\nRegex is the art of defining a string format. This format could be as simple as a literal string (e.g. the word \"email\"), or it could be as complex as the pattern that an email address follows.\n\n### The Eager Regex engine\nRegex is eager, meaning it will always return the leftmost match even if there is a better match to be found later.\n- put another way, the engine is “eager” to report a match.\n- ex. if we were looking for the word `cat` in the sentence \"He captured a catfish for his cat.\", it would match the substring \"cat\" of the whole word \"catfish\", even though there is a single word \"cat\" at the end of the sentence. This shows how the regex engine returns the leftmost match; not the best match.\n\nImagine we used an alternation to match one of multiple strings: `Get|GetValue|Set|SetValue`. Because\n\n1. The engine walks through the regex, attempting to match the next token (ie. character or character class) in the regex to the next character.\n2. If a match is found, the engine advances through the regex and the subject string.\n3. If a token fails to match, the engine backtracks to a previous position in the regex and the subject string where it can try a different path through the regex.\n\nA regex engine always returns the leftmost match, even if a “better” match could be found later:\n1. When applying a regex to a string, the engine starts at the first character of the string.\n2. It tries all possible permutations of the regular expression at the first character.\n3. Only if all possibilities have been tried and found to fail, does the engine continue with the second character in the text.\n4. Again, it tries all possible permutations of the regex, in exactly the same order. The result is that the regex engine returns the leftmost match.\n\n[source](https://www.regular-expressions.info/engine.html)\n\n### Implementing Business Logic in Regex\nConsider that Regex can be used to implement business logic. For instance, consider the following Regex:\n```\n[0-9]{1,2}(\\.\\d+)? MB\n```\n\nThis Regex will match `25 MB`, `3 MB`, `7.45 MB`, but not `100 MB`. By using `{1,2}`, we've limited our search to matches that are less than 100 MB.\n\n* * *\n\n### Match vs pattern\n- `pattern` - the regex text that we type into the search field\n- `match` - any text in the document that is highlighted as a result of the search\n\n* * *\n\nWhile there are many implementations of regex that differ sometimes slightly, there are basically only two kinds of regular expression engines: text-directed engines, and regex-directed engines, with nearly all modern regex flavors are based on regex-directed engines\n- certain very useful features, such as lazy quantifiers, backreferences, atomic grouping, possessive quantifiers etc., can only be implemented in regex-directed engines.\n\n## Different flavors of Regex\nThe \"normal\" regex is Perl-Compatible Regex (PCRE)\n- may be contrasted with Basic Regular Expressions (BRE)\n- Vim Regex largely follows BRE\n\nThe main difference between the two is that BRE tends to treat more characters as literals - an \"a\" is an \"a\", but also a \"(\" is a \"(\", not a special character - and so involves more backslashes to give them \"special\" meaning.\n\n#### BRE\n- `\\V` Verbatim Switch - only `\\` has special meaning (ie. very nomagic)\n  - prevent regex symbols from taking over - `/\\Va.k.a` (equivalent to `/a\\.k\\.a\\.`)\n  - in regex, `.` means \"match any character\". making a verbatim search removes that functionality\n  - when we use `\\V`, it means for the following search, only `\\` will have a special meaning\n- `\\v` Literal Switch - all characters (except word characters (**[a-zA-Z_]**)) have special meaning (ie. very magic)\n- delimit words - since the word \"the\" appears in \"these\", if we wanted to just search for the word \"the\", we can use delimiters: `/\\v<the>`\n\n- ignore case - `/search\\c`\n- enforce case - `/search\\C`\n\n## Terms\n- **magic** means we don't have to escape a character for it to take on its special meaning. \n- **nomagic** means we have to escape, because otherwise the character will be taken literally\n  - *ex.* with magic, `.` will mean \"stand in for any character\". with nomagic, it will literally mean \"match the `.` character\"\n  - this verbiage makes sense, because with magic, a lot of cool stuff is happening, but we have no idea how it's being done. Without magic, it's just looking for a character\n\n# Useful Regex Patterns\n`'\\{\\s.*ListView.*from\\s.react-native.'` - search for a non-default exported module from a specific module\n","n":0.037}}},{"i":383,"$":{"0":{"v":"Special Characters","n":0.707},"1":{"v":"\na.k.a metacharacters\n\nThere are 12 characters with special meanings:\n- `\\`\n- `^`\n- `$`\n- `.`\n- `|`\n- `?`\n- `*`\n- `+`\n- `(`\n- `)`\n- `[`\n\t- this usually denotes the start of a custom [[character class|regex.character-class]].\n\t- in Javascript regex, if we want to match a literal `]` inside a character class, we need to escape it.\n- `{`\n\t- to type a literal `{`, most flavors of regex don't require us to escape with `\\`, unless it is part of a repetition operator (e.g. `{1, 3}`)\n\nInside character classes, the only special characters are `-`, `^`, `\\`, `]`\n- `^` is only special if it appears at the start of the character class, ie. `[a-z^]` will match a-z and a literal `^`.\n- `[- /.]` matches `-`, ` `, `/` or `.`\n\n### Dot `.`\nMatches any character but newline\n\nIf we wanted to match any character including newlines, we could do `[\\s\\S]`\n\nA negated character class is often more appropriate than the dot.\n\nex. consider the text: \"Type 1\" errors and \"Type 2\" errors\n- If we want to match inside quotes, we can negate `\"` as if to say \"continue matching characters as long as it's not a `\"` character\n- `\"[^\"]*\"`\n","n":0.074}}},{"i":384,"$":{"0":{"v":"Quantifiers","n":1},"1":{"v":"\nQuantifiers help us achieve repetition\n- they will match the whole character class; not just the character that was actually matched\n\t- ex. The regex `[0-9]+` can match 837 as well as 222.\n\t- If you want to repeat the matched character, rather than the class, you need to use [[backreferences|regex.backreferences]]\n\t\t- ex. `([0-9])\\1+` matches 222 but not 837\n\nQuantifiers can be applied to an entire group of character classes with `()`\n- ex. `Nov(ember)?` matches Nov and November.\n\nMatch zero or 1 time: `?`\n- identical to `{0,1}`\n\nMatch zero or more times: `*`\n- identical to `{0,}`\n- ex. match an HTML tag without any attributes: `<[A-Za-z][A-Za-z0-9]*> `\n\nMatch one or more times: `+`\n- identical to `{1,}`\n\n### Limiting repetition\nRepeat the previous character class once (for a total of 2)\n`hel{1}o` matches hello.\n\nRepeat the previous character class between 0 and 3 times\n`hel{0, 3}o` matches helo, hello, helllo...\n\nRepeat the previous character class at most 3 times\n`hel{,1}o` matches helo, hello\n\nRepeat the previous character class at least 1 time\n`hel{1,}o` matches hello, helllo...\n\n### All repetition operators are greedy\nImagine we had the regex `Get(Value)?|Set(Value)?`. If we had a string \"SetValue\", we would match the whole string and not just Set, due to the fact that quantifiers are greedy.\n\nBecause repetition operators are greedy, they (may) have to backtrack to complete the full regex.\n- Backtracking slows down the regex engine. This isn't noticeable if doing a simple text search in a file, but we start to notice a drain on CPU if we have a regex that we need to backtrack through on each iteration of a loop.\n\nMost people new to regular expressions will attempt to use `<.+>`. They will be surprised when they test it on a string like: This is a `<EM>first</EM>` test. You might expect the regex to match `<EM>` and when continuing after that match, `</EM>`.\n\nBut it does not. The regex will match `<EM>first</EM>`. Obviously not what we wanted. The reason is that the plus is greedy. That is, the plus causes the regex engine to repeat the preceding token as often as possible. Only if that causes the entire regex to fail, will the regex engine backtrack. That is, it will go back to the plus, make it give up the last iteration, and proceed with the remainder of the regex.\n\nImagine we had a string `This is a <EM>first</EM> test.`. Not taking into account the greedy nature of repetition operators, a naive approach to match any HTML tag with regex `<.+>`. However, this matches the whole substring `<EM>first</EM>`.\n1. we match the `<` literal\n2. then we match any character 1+ times\n3. when we arrive at the first `>` character, we are still evaluating for the `.`. Since `>` is a part of the `.` set (ie every character), it gets interpreted as such\n4. by virtue of the `.`, we arrive at the end of the sentence, so we have `<EM>first</EM> test.`\n5. now the engine realizes that it is at the end of the string, and it still have more things to match (ie. the `>` character), so it backtracks until it reaches that character in `</EM>`.\n6. `<EM>first</EM>` is matched\n\t- because of the eagerness, the regex engine is eager to return a match. Therefore, it will not backtrack more than it has to. Once it has found a full match, it will return it. Because of the greediness, this is the leftmost longest match.\n\n#### Turning greediness off\nThe operator can be turned lazy (ie. turn greediness off) with `?`.\n- e.g. `+?`, `??`, `*?`\n\t- ex. in the string \"Today is Feb 23rd, 2003\", the regex `Feb 23(rd)?` will match \"Feb 23rd\", and the regex `Feb 23(rd)??` will match \"Feb 23\".\n","n":0.041}}},{"i":385,"$":{"0":{"v":"Lookarounds","n":1},"1":{"v":"\n## Lookaheads/Lookbehinds\n- while the boundaries of a match normally correspond with the start and end of a pattern, we can use `\\zs` and `\\ze` to crop the beginning and end of a match, making the new match a subset of the pattern\n  - ex. search for matches of \"eagle\", but only when it follows the word \"bald\"\n    - `/bald \\zseagle`\nIn this example, \"bald eagle\" still forms part of the pattern, but only the word \"eagle\" is matched\n  - ex. search for everything inside quotes, without the quotes themselves\n    - `/\\v\"\\zs[^\"]+\\ze\"`\n\nLookarounds are zero-length assertions, meaning they don't take place on a character of the string; they take place between characters.\n- the difference between [[quantifiers|regex.quantifiers]] zero-length nature is that lookaround actually matches characters, but then gives up the match, returning only the result: match or no match.\n  - ie. They do not consume characters in the string, but only assert whether a match is possible or not.\n- Lookaround allows you to create regular expressions that are impossible to create without them, or that would get very longwinded without them.\n","n":0.075}}},{"i":386,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Match any hex code\n`#([a-fA-F0-9]{6})`\n\n#### Match any number of words\n`[\\w\\s]+`\n","n":0.316}}},{"i":387,"$":{"0":{"v":"Character Class","n":0.707},"1":{"v":"\nA Character Class (CC) is how we tell the regex engine that we want to accept only a subset of all characters\n- ex. I want all letters from a-d, and f-z\n- ex. I want all alphanumeric characters\n\nA CC stands in for a single character. We can use [[quantifiers|regex.quantifiers]] to match the CC multiple times.\n\n### Built-in Character Classes\nwhitespace char: `\\s`\n- identical to `[ \\t\\r\\n\\f]`\n- e.g. tab, space, newline, carriage return, form feed\ndigit: `\\d`\n- identical to `[0-9]`\nhex-digit: `\\x`\nword character (alphanumeric + underscore): `\\w`\n- identical to `[A-Za-z0-9_]`\nalphabetic char: `\\a`\nlowercase char: `\\l`\nuppercase char: `\\u`\ntab char: `\\t`\ncarriage return: `\\r`\nnew line: `\\n`\nvertical/horizontal whitespace: `\\v`/`\\h`\n- only available in Perl Regex; if available, these are preferable to `\\s`\n\n#### Inverse\nTo [[inverse|dendron://thoughts-on/math.function.inverse]] the match, we just need to uppercase the character\n- ex. `\\s` matches a whitespace character, `\\S` matches a non-whitespace char.\n\n### Custom Character Classes\nLetter from a-d, and f-z\n- `[a-df-z]`\n\n#### Inverse\nTo [[inverse|dendron://thoughts-on/math.function.inverse]] the whole character class, we just need to put `^` in front\n- ex. `[^0-9a-z]` matches any character that isn't numeric or lowercase\n\nnote: `[\\D\\S]` is not the same as `[^\\d\\s]`\n- the first matches any character that is either not a digit, or is not whitespace. Therefore, matches any character; since all digits are non-whitespace, and all whitespace characters are not digits\n- the second matches any character that is neither a digit nor whitespace. It matches `x`, but not `8`\n\n* * *\n\n### Modifications\n\n#### Inverse\nTo [[inverse|dendron://thoughts-on/math.function.inverse]] the match, we just need to uppercase the character\n- ex. `\\s` matches a whitespace character, `\\S` matches a non-whitespace char.\n- ex. `[^0-9a-z]` matches any character that isn't numeric or lowercase\n\n##### Match a word surrounded by whitespace\n`\\sthe\\s`\n","n":0.062}}},{"i":388,"$":{"0":{"v":"Backreferences","n":1},"1":{"v":"\nBackreferences match the same text as previously matched by a capturing group (with are made with parentheses `()`).\n- `\\0` references the whole search, `\\1` references the first capture group, `\\2` the second group, and so on.\n\nuse `%` to not capture the following parentheses\n- imagine we want to find and replace all occurrences of a first and last name, then replace it by the format `LAST, FIRST`. Notice the where we use and omit the `%` in order to control which matches are going to the `\\1` and `\\2` registers. Here, we don't care whether \"Drew\" or \"Andrew\" was matched, so we don't bother registering it.\n\t- `/\\v(%(And|D)rew) (Neil)`\n\t- `:%s//\\2, \\1/g`\n- Another resource says that the way to negate is with `?:` immediately after the opening `(`.\n\t- ex. `color=(?:red|green|blue)`\n\nSurround all instances of a given word with `\"\"` (using regex w/ vim replace)\n- `:s/(dog)/\"\\1\"`\n\n- ex. Suppose you want to match a pair of opening and closing HTML tags, and the text in between. By putting the opening tag into a backreference, we can reuse the name of the tag to let us match the closing tag: `<([A-Z][A-Z0-9]*)\\b[^>]*>.*?</\\1>`\n\t- note: this doesn't seem to work\n\nTo figure out the number of a particular backreference, scan the regular expression from left to right. Count the opening parentheses of all the numbered capturing groups.\n","n":0.068}}},{"i":389,"$":{"0":{"v":"Anchors","n":1},"1":{"v":"\nWhile literal characters, [[character classes|regex.character-class]], and the `.` match a single character, anchors don't match any character at all. Instead, they match a position before, after, or between characters.\n- Anchors are therefore said to be *zero-length*.\n- They are called anchors because they can be used to “anchor” the regex match at a certain position\n\n### `^`\n`^` matches the position before the first character in the string.\n- Applying `^a` to `abc` matches `a`.\n- `^b` does not match `abc` at all\n\n### `$`\n`$` matches right after the last character in the string.\n- `c$` matches `c` in `abc`\n- `a$` does not match at all.\n\n### `\\b`\nmatches at a position that is called a “word boundary”\n- `\\b` allows you to perform a “whole words only” search using a regular expression in the form of `\\bword\\b`\n\n`\\b` is not technically an anchor, but it is very closely related. If we were to imagine a string of text as the space between 2 walls, then we would consider the anchors `^` and `$` to be either wall (ie. the position before the first character, and after the last character). `\\b` differs from this because `\\b` marks the position of a word character `\\w` on one side, and a non-word character `\\W` on the other.\n\nThere are three different positions that qualify as word boundaries:\n1. Before the first character in the string, (if the first character is a word character).\n2. After the last character in the string, (if the last character is a word character).\n3. Between two characters in the string, where one is a word character and the other is not a word character.\n\n![](/assets/images/2021-11-28-14-22-35.png)\n\n### Using `^` and `$` as Start of Line and End of Line Anchors\nCalled “multi-line mode”, and it generally needs to be explicitly enabled.\n\nIf you have a string consisting of multiple lines, like `first line\\nsecond line`, it often makes sense to think in terms of lines, rather than the entire string\n- ex. `^` can match at the start of the string (ie. before the letter `f`), as well as after each line break (between `\\n` and the letter `s`)\n- ex. Likewise, `$` still matches at the end of the string (after the last `e`), and also before every line break (between `e` and `\\n`)\n\n* * *\n\nAnchors are very important when using regex in a programming language to validate user input\n","n":0.051}}},{"i":390,"$":{"0":{"v":"Redux","n":1},"1":{"v":"\nRedux uses a different instance of the store for every request.\nRedux is actually based in part on [[CQRS|general.patterns.architectural.CQRS]] and [[event sourcing|general.patterns.event-sourcing]]\n\nre-render performance in Redux is faster in comparison to Context API because every component that consumes the Context will re-render even if the state relevant to that component hasn’t changed.\n\n# Tools\n- [Redux Toolkit: Opinionated Redux](https://redux-toolkit.js.org/)\n","n":0.135}}},{"i":391,"$":{"0":{"v":"Redux Tookit","n":0.707},"1":{"v":"\n### Slice\nA slice is a collection of Redux reducer logic and actions for a single feature in your app, typically defined together in a single file. The userSlice will have actions and reducers to perform CRUD actions.","n":0.164}}},{"i":392,"$":{"0":{"v":"Selectors","n":1},"1":{"v":"\nIf actions are the entry points, selectors are the exit.\n- After dispatching an action, the application can use selectors to get the resulting data from the store after reducers and sagas have done something with it.\n\n### `useSelector`\n- equivalent to the purpose of `mapStateToProps`, which is provide easy access to the particular parts of the store that you want. \n\t- when a component has multiple instances, the selector needs to be defined inside the component, so that each instance gets its own selector instance.\n\n## Reselect\n- Will memoize the value that is returned by the selected value\n    - Imagine there is a list of questions in the redux store, and we want to retrieve all questions that have an even-numbered `id`. We could write a reselect selector that gets this array, performs a `filter` on it, memoizes (caches) that function, then return it to us. Then, next time we call that selector, the value is already available to us, rather than having to perform that `filter` operation on the array all over again.\n- If we are just getting data without modification (ex. getting `store.title`), then reselect is not necessary. Only when there is expensive computation is reselct beneficial.\n- `createSelector` will take in 1+ selectors (functions), call them, and passes their return values (the selected part of the store) as arguments to the final arg (an anonymous transform function) of `createSelector`. That function will accept 1 parameter for every selector that came before it. \n\t- If one of the input selectors has changed since the last call, the transform function will be called. Otherwise, it will just return the memoized value.\n    - this function is a thunk (returns a function)\n    - ***ex.***\n    ```\n    export const getFeedbackForYou = createSelector(\n        getByRecipient,\n        (byRecipientState) => byRecipientState.myFeedback,\n    );\n    ```\n    - here, `createSelector` is being called and returns a function that takes state as the argument (since that's what thunks do).\n    - conceptually, we can therefore replace `createSelector(...)` with `(state) => \n\n* * *\n\n- `mapStateToProps` is really just a type of selector when you think about it.\n\t- specifically, it is the selector that (potentially) uses other selectors to get data from the store. It is meant to clump them together for easy access to components that get rendered by the container \n- `state` refers to the data that currently exists in the store. `store` refers to the instance\n","n":0.051}}},{"i":393,"$":{"0":{"v":"Reducers","n":1},"1":{"v":"\nReducers are supposed to return the initial state when they are called with undefined as the first argument, no matter the action.\n","n":0.213}}},{"i":394,"$":{"0":{"v":"redux-observable","n":1},"1":{"v":"\n`redux-observable` is a redux middlware allowing us to `map` and `filter` an `action$` (ie. an action as an observable) with RxJS operators.\n- While javascript `map` and `reduce` allows us to transform arrays, these versions allow us to transform *streams of actions*\n\n## Epic\n- ***def*** - a function that takes in a stream of actions, and returns a modified stream of actions.\n  - receive an observable as input\n- epics can be thought of as a description of what additional actions redux-observable should dispatch\n- epics are analogous to sagas from redux-saga\n\nexample:\n```js\nconst pingEpic = action$ => action$.pipe(\n  ofType('PING'), //equivalent to filter(action => action.type === 'PING')\n  mapTo({ type: 'PONG' })\n);\n\n// later...\ndispatch({ type: 'PING' });\n```\n- pingEpic will listen for actions of type PING and map them to a new action, PONG. This example is functionally equivalent to doing this:\n```js\ndispatch({ type: 'PING' });\ndispatch({ type: 'PONG' });\n```\n- Epics run alongside the normal Redux dispatch channel, after the reducers have already received them. When you map an action to another one, you are not preventing the original action from reaching the reducers; that action has already been through them\n\n","n":0.075}}},{"i":395,"$":{"0":{"v":"Middleware","n":1},"1":{"v":"\nMiddlewares only wrap `store.dispatch()`\n","n":0.5}}},{"i":396,"$":{"0":{"v":"Thunk","n":1},"1":{"v":"\nA thunk is a pure function that returns an impure function \n\n## The pattern of providing dispatch to a helper function\n- imagine we wanted to dispatch 2 actions that normally occur together, such as an action to show a notification, and an action to hide it 5 seconds later. The simplest approach is to dispatch an action to show it, then dispatch an action 5 seconds later to hide it. But what if we want to use this combination of 2 actions elsewhere in the app? What we can do is create an external function that accepts `dispatch` as the first arg, and the payload that goes to both dispatches as the rest of the args. \n\t- Not only does this give us code reuse, but it also allows us to have better control over *that* action, and only that action. In our first naive method, we have no idea if the notification related to the action being dispatched 5 seconds later is associated with the notification related to the action dispatched instantly. For instance, if a notification is trigged and then milliseconds later another is triggered, we might end up in a race condition where our app doesn't know how to match up the `show` and `hide` actions with the correct notification. \n\n```js\n//Naive method\nthis.props.dispatch({ type: 'SHOW_NOTIFICATION', text: 'You logged in.' })\nsetTimeout(() => {\n  this.props.dispatch({ type: 'HIDE_NOTIFICATION' })\n}, 5000)\n\n// external function to increase code reuse\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(dispatch, text) {\n  // Assigning IDs to notifications lets reducer ignore HIDE_NOTIFICATION\n  // for the notification that is not currently visible.\n  const id = nextNotificationId++\n  dispatch(showNotification(id, text))\n\n  setTimeout(() => {\n    dispatch(hideNotification(id))\n  }, 5000)\n}\n```\n\n## What are thunks\n- A thunk is a middleware that gives `dispatch` the ability to accept a function. Normally dispatch can only accept an action (usually dispatch accepts the invocation of an action creator, which returns an action). With Thunk middleware (mw being something that gives enhanced functionality), `dispatch` gains the ability to accept functions.  \n\t- When a function is dispatched, Thunk MW will recognize that and pass `dispatch` as the first argument, allowing us to dispatch any action we want within the function (ie within the thunk)\n- The result is a function that is very similar to the external helper function we made before. There are 2 key differences: \n\t- our new function doesn't accept `dispatch` as the first prop\n\t- our new function *returns* a function of its own, and *that* function accepts `dispatch` as it's first argument\n```js\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  return function (dispatch) {\n    const id = nextNotificationId++\n    dispatch(showNotification(id, text))\n\n    setTimeout(() => {\n      dispatch(hideNotification(id))\n    }, 5000)\n  }\n}\n```\n\n## Motivation for Thunks\n- the above solution works but it presents 2 issues:\n\t- we have to pass `dispatch` around everywhere. This means that where we need to call that helper function, we need to have access to `dispatch`, meaning we need to create a container for that component. \n\t- We have to mentally keep track which redux `actions` are just regular action creators, and which are actually more like action helpers (above example. since they do not return an action, they are not action creators)\n\n## Using thunks\nwithout the Thunk MW, we could execute this function like so:\n```\nshowNotificationWithTimeout('You just logged in.')(this.props.dispatch)\n```\nbut since we are using the MW, any time a function is dispatched instead of an action object, thunk takes over and calls that function with `dispatch` as its first argument, giving the function access to `dispatch`\n- therefore, we just do this \n```\nthis.props.dispatch(showNotificationWithTimeout('You just logged in.'))\n```\n\nWith thunks, dispatching an asynchronous action (really, a series of actions) looks no different than dispatching a single action synchronously to the component. Which is good because components shouldn’t care whether something happens synchronously or asynchronously. We just abstracted that away. The component only knows about the thunk, and the thunk deals with the async logic\n\nThunks also receive as the second arg the application store. This is useful for if we want to bail out of an API call \n- ex. user has notifications turned off:\n```js\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  return function (dispatch, getState) {\n    // Unlike in a regular action creator, we can exit early in a thunk\n    // Redux doesn’t care about its return value (or lack of it)\n    if (!getState().areNotificationsEnabled) {\n      return\n    }\n\n    const id = nextNotificationId++\n    dispatch(showNotification(id, text))\n\n    setTimeout(() => {\n      dispatch(hideNotification(id))\n    }, 5000)\n  }\n}\n```\n- note:  If you use `getState()` only to conditionally dispatch different actions, consider putting the business logic into the reducers instead.\n\nThunks may also return promises. In fact, Redux doesn’t care what you return from a thunk, but it gives you its return value from `dispatch()`\n- This is why you can return a Promise from a thunk and wait for it to complete by calling `dispatch(someThunkReturningPromise()).then(...)`\n- You may also split complex thunk action creators into several smaller thunk action creators. The dispatch method provided by thunks can accept thunks itself, so you can apply the pattern recursively. Again, this works best with Promises because you can implement asynchronous control flow on top of that.\n\n## When thunks aren't enough, and to reach for Sagas\nFor some apps, you may find yourself in a situation where your asynchronous control flow requirements are too complex to be expressed with thunks.\n- ex. retrying failed requests, reauthorization flow with tokens, or a step-by-step onboarding\n* * *\n[source](https://stackoverflow.com/questions/35411423/how-to-dispatch-a-redux-action-with-a-timeout/35415559#35415559)\n","n":0.034}}},{"i":397,"$":{"0":{"v":"Saga","n":1},"1":{"v":"\n## What is it?\n- a saga is a daemon that lets us define long-running processes that take [[actions|redux.actions]] as they come, and transform or perform requests before outputting actions. \n- This moves the logic from action creators into sagas\n\nA saga is a separate thread in the app that's just for side-effects\n- while thunks utilize callbacks, a saga thread can be started, paused (`yield`) and cancelled by dispatching actions within [[generator functions|js.lang.functions.generator]]\n\nIn synchronous Redux, a dispatched action is assigned to a [[reducer|redux.reducers]]. In Async redux, a dispatched action is assigned to a saga. \n- The saga does its side effect (e.g. `resourceListReadRequest`), and takes the returned data, and dispatches another action (e.g. `resourceListReadSuccess`) which is then picked up by the reducer.\n\n### Reconciling generator functions and Saga\n- Imagine a saga as a thread that constantly calls `next()` and tries to execute the `yield` lines as soon as it can\n- spec: like a promise, it will wait on that value to \"return\" before it calls `next()` and goes to the next `yield`\n- In Sagas, we are \"yielding\" to the redux-saga middleware\n\t- The MW suspends the saga until the yielded side effect resolves. At this point, the MW calls `next()`\n- When we say `yield call(___)`, we are only describing what we want to happen. We aren't describing the actual outcome. In this sense, it is declarative.\n\n## Types of Saga\n### Worker Saga\nA worker saga is a saga that performs side effects and dispatches other actions asynchronously.\n\n### Watcher Saga\nA watcher saga is a saga that listens for dispatched actions and calls worker sagas in response.\n\n### Root Saga\nA root saga is a saga that runs all watcher sagas in parallel\n\n## Example process\n1. An action `resourceListReadRequest` is dispatched somewhere in the code\n2. A *Watcher Saga* that is designed to listen for `resourceListReadRequest` picks up on the fact that it was dispatched, and notifys the *Worker Saga*\n\n## API Reference\n### Effects\n*def* - an object containing instructions to be fulfilled by middleware. When the MW retrieves an effect yielded by a saga (ie. when a saga executes put, call, take etc), the saga pauses until the effect is done.\n- `call` - call the fn (1st arg) with args (rest args)\n- `put` - dispatch action\n- `take` - block execution of the saga until the provided action is dispatched\n\t- therefore, this is used in a watcher saga\n- `fork` - useful when a saga needs to start a non-blocking task\n\t- Non-blocking means: the caller starts the task and continues executing without waiting for it to complete\n\t- Situations:\n\t\t1. grouping sagas by logical domain\n\t\t2. keeping a reference to a task in order to be able to cancel/join it\n- `takeEvery` - each time a particular action is dispatched, spawn a saga\n\t- `takeEvery` uses `take` and `fork` under the hood\n- `takeLatest` - spawn a saga only for the latest dispatched action of a given type\n\t- ex.  imagine a user is mashing the login button. with Thunk, an API call would be made with each button press. with redux-saga, we get to just take the latest one and ignore the rest\n\n# UE Resources\n[redux-saga primer](https://flaviocopes.com/redux-saga/)\n- [also](https://medium.com/appsflyer/dont-call-me-i-ll-call-you-side-effects-management-with-redux-saga-part-2-cd16f6bcdbcd)\n","n":0.044}}},{"i":398,"$":{"0":{"v":"Actions","n":1},"1":{"v":"\nAn action creator is found in the /actions directory. When they are called, an action is created. However, it doesn't do anything, it just floats there. The action becomes mobile only once it is dispatched. When store.dispatch is called with the action as its argument, the action is able to reduce the payload into the state\n","n":0.134}}},{"i":399,"$":{"0":{"v":"Redis","n":1},"1":{"v":"\nThe name Redis means *Remote Dictionary Server*\n\nWith Redis, the data is (ideally) all in memory, making the lookups much faster.\n- anything slower than 10ms is considered slow, and is indicative of an issue.\n\nRedis is a good fit in situations where you want to share some transient, approximate, fast-changing data between servers, and where it’s not a big deal if you occasionally lose that data for whatever reason\nex. a good use case is maintaining request counters per IP address (for rate limiting purposes) and sets of distinct IP addresses per user ID (for abuse detection).\nex. Real time Analytics - you can use Redis to store user identities and their transaction details when implementing a real-time fraud detection service.\nex. You can use Redis to queue application (FIFO) tasks that take a long time to complete\n\n- used as a database, cache, message broker, and message queue.\n- All Redis data resides in-memory RAM\n- Redis delivers response times of less than 1ms, enabling millions of requests per second\n- Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps.\n- Redis processes commands using a single thread (i.e. while a command is being executed all others need to await until executing one ends).\n\nYou can run atomic operations on the contents of the redis cache.\n\nRedis supports [asynchronous replication](https://redis.io/topics/replication)\n\nBy default, Redis writes data to a file system at least every 2 seconds, meaning in event of system failure, only the last few seconds of data would be lost.\n\n### Data storage and access\nGenerally, data in your cache will be stored in a highly [[normalized|db.design.normalization]] way, often only one layer deep.\n\nWhen desigining your keys, think chiefly about how that data will be accessed. Consider the access pattern that your app uses to its main database. What type of data are you querying on? For instance, if your app's business logic dictates an important access pattern as `albumsByArtist`, then our cache key structure should follow a similar pattern. Put another way, we want to be able to access our cache data with a similar mindset to how we are accessing our main database's data.\n\n### Cache invalidation\nOne of the ways to achieve cache invalidation in Redis is through a built-in Pub/Sub system. It can be usd to send invalidation messages to clients listening, so that the client knows it should fetch again if it wants up to date data.\n- This can be made to work but is tricky and costly from the point of view of the bandwidth used, because often such patterns involve sending the invalidation messages to every client in the application, even if certain clients may not have any copy of the invalidated data\n\n## Client-side Caching (called Tracking)\nThere are 2 modes to client-side caching in Redis\n- *default* - server remembers which clients it sends data to. When the data is modified, it sends an invalidation message to the client, letting it know that its data is no longer up to date. This costs memory in the server side, since the server must keep the list of clients in memory.\n- *broadcast* - server does not attempt to remember what keys a given client accessed. Instead clients subscribe to key prefixes such as `object:` or `user:,` and will receive a notification message every time a key matching such prefix is touched.\n    - does not consume any memory on the server side, but instead sends more invalidation messages to clients.\n    - The server does not store anything in the invalidation table. Instead it only uses a different Prefixes Table, where each prefix is associated to a list of clients.\n\nThis is an example of the protocol:\n```\nClient 1 -> Server: CLIENT TRACKING ON\nClient 1 -> Server: GET foo\n(The server remembers that Client 1 may have the key \"foo\" cached)\n(Client 1 may remember the value of \"foo\" inside its local memory)\nClient 2 -> Server: SET foo SomeOtherValue\nServer -> Client 1: INVALIDATE \"foo\"\n```\n\nThis looks great superficially, but if you think at 10k connected clients all asking for millions of keys in the story of each long living connection, the server would end up storing too much information. For this reason Redis uses two key ideas in order to limit the amount of memory used server side, and the CPU cost of handling the data structures implementing the feature:\n- We must create an *invalidation table* which stores rows of clients that have cached a given key.\n    - Such invalidation table can contain a maximum number of entries, if a new key is inserted, the server may evict an older entry by pretending that such key was modified. Once this happens, it sends an invalidation message to the clients.\n\nclients do not need, by default, to tell the server what keys they are caching. Every key that is mentioned in the context of a read only command is tracked by the server, because it could be cached.\n\nThis has the obvious advantage of not requiring the client to tell the server what it is caching. Moreover in many clients implementations, this is what you want, because a good solution could be to just cache everything that is not already cached, using a first-in first-out approach: we may want to cache a fixed number of objects, every new data we retrieve, we could cache it, discarding the oldest cached object. More advanced implementations may instead drop the least used object or alike.\n\n## UE Resources\n- [Redis Tutorials by Flavio](https://flaviocopes.com/tags/redis/)\n- [Realtime delivery Architecture at Twitter](https://www.infoq.com/presentations/Real-Time-Delivery-Twitter/)\n\n## Resources\n- [RedisJSON](https://redis.io/docs/stack/json/) - JSON support for Redis, effectively adding the JSON type, and letting us store, update, and retrieve JSON values\n","n":0.033}}},{"i":400,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n### Keys\nTry to stick with a schema. For instance `object-type:id` is a good idea, as in \"user:1000\". Dots or dashes are often used for multi-word fields, as in `comment:1234:reply.to` or `comment:1234:reply-to`.\n\n### Hash\n`field` and `value` form a pair. Both are strings\n- If this were Javascript, \n    - the object name would be the Redis `key`, \n    - the object key would be the Redis `field`, \n    - the object value would be the Redis `value`\n\n```\nuser:1000: {\n    \"username\": \"antirez\",\n    \"birthyear\": 1970\n}\n```\n\nMethods:\n- `HGET <key> <field>` / `HSET <key> <field> <value>` - get/set a field+value\n\n### Lists\nRedis lists are implemented via Linked Lists.\n- LinkedLists were used (as opposed to arrays) because for a database system it is crucial to be able to add elements to a very long list in a very fast way.\n- When fast access to the middle of a large collection of elements is important, Sorted Sets should be used\n\nCommon use-cases for Lists\n- Remember the latest updates posted by users into a social network.\n    - ex. Twitter takes the latest tweets posted by users into Redis lists.\n    - imagine your home page shows the latest photos published in a photo sharing social network and you want to speedup access.\n        - Every time a user posts a new photo, we add its ID into a list with `LPUSH`.\n        - When users visit the home page, we use `LRANGE 0 9` in order to get the latest 10 posted items.\n- Communication between processes, using a consumer-producer pattern where the producer pushes items into a list, and a consumer (usually a worker) consumes those items and executed actions. Redis has special list commands to make this use case both more reliable and efficient.\n\nMethods:\n- `LPUSH` / `RPUSH` - add element to the left/right\n- `LRANGE <listname> <fromindex> <toindex>` - extract ranges of elements from lists\n\n### Sets\nA set is an unordered collections of unique strings (ie, a [[mathematical set|math.set-theory]])\n\nSets are good for expressing relations between objects. For instance we can easily use sets in order to implement tags.\n- A simple way to model this problem is to have a set for every object we want to tag. The set contains the IDs of the tags associated with the object.\n\nMethods:\n`SADD` - add element to set\n\nMost set operations, including adding, removing, and checking whether an item is a set member, are `O(1)` (highly efficient).\n- except for `SMEMBERS` command, which is `O(n)` and returns the entire set in a single response\n\n```sh\n# Store a value in the set of favorite book IDs for user 123\n> SADD user:123:favorites 742\n(integer) 1\n\n# Check whether user 123 likes book 742\n> SISMEMBER user:123:favorites 742\n(integer) 1\n```\n\n### Sorted Sets\nsimilar to a mix between a Set and a Hash\n- Like sets, sorted sets are composed of unique, non-repeating string elements, so in some sense a sorted set is a set as well.\n- every element in a sorted set is associated with a floating point value, called the score (this is why the type is also similar to a hash, since every element is mapped to a value).\n\nMethods:\n`ZADD` - add element to set\n`ZREVRANGE` - Returns the specified range of elements in the sorted set stored at key\n\n# E Resources\n[Redis Documentation](https://redis.io/topics/data-types-intro)\n","n":0.044}}},{"i":401,"$":{"0":{"v":"Transactions","n":1},"1":{"v":"\nRedis transactions provide us with 2 guarantees:\n1. All the commands in a transaction are serialized and executed sequentially. \n    - This means that a request sent by another client will never be served in the middle of the execution of the transaction.\n2. The `EXEC` command triggers the execution of all the commands in the transaction, \n    - so if a client loses the connection to the server in the context of a transaction before calling the `EXEC` command, none of the operations are performed\n\nA Redis Transaction is entered using the `MULTI` command\n- At this point the user can issue multiple commands. Instead of executing these commands, Redis will queue them. They will all be executed once `EXEC` is called.\n- Calling `DISCARD` will flush the transaction queue and will exit the transaction.","n":0.087}}},{"i":402,"$":{"0":{"v":"Cluster","n":1},"1":{"v":"\nRedis Clusters enable us to scale [[horizontally|deploy.scaling#horizontal-scaling-aka-scaling-out,1:#*]]\n\nWhen we run Redis in a cluster, data is automatically sharded across multiple Redis nodes.\n\nAs an added benefit, running as a cluster also gives a degree of availability during partitions, giving it the ability to continue in the event that there is a fault in one or more of the nodes.\n\nRedis Cluster uses a master-replica model where every hash slot has from 1 (the master itself) to N replicas (N-1 additional replica nodes).\n\nRedis Cluster does not guarantee strong consistency, due to the fact that it uses asynchronous replication.\n- In practical terms this means that under certain conditions it is possible that Redis Cluster will lose writes that were acknowledged by the system to the client.\n- ex.\n  1. Your client writes to the master B.\n  2. The master B replies OK to your client.\n  3. The master B propagates the write to its replicas B1 and B2, but crashes before it is able to replicate to B3, resulting in B3 being behind, yet the client thinking everything has been properly replicated.\n\n### Connections\nEach cluster requires 2 open [[TCP|protocol.TCP]] connections:\n1. a Redis TCP port used to serve clients, e.g., 6379, \n2. a second port known as the cluster bus port\n  -  By default, the cluster bus port is set by adding 10000 to the data port (e.g., 16379);\n\nThe cluster bus port is for node-to-node communication and is used for failure detection, configuration update, failover authorization, and so forth.\n\nBoth ports need to be open to the firewall, otherwise Redis cluster nodes will be unable to communicate.\n\n### Sharding\nEach key (ie. each value to be stored in Redis) is sharded and grouped according to the *hash slot* they belong to.\n- there are 16384 individual hash slots in Redis Cluster.\n- each cluster node is responsible for a subset of the hash slots.\n\nIdentical values within `{}` are guaranteed to be in the same hash slot because they share the same hash tag\n- ex. the keys `user:{123}:profile` and `user:{123}:account` will live in the same hash slot, since they are from the same user.\n- as a result of this, we can operate on these 2 keys in the same multi-key operation.","n":0.053}}},{"i":403,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\nOpen redis-cli shell - `redis-cli -h <host>`\n\n- Get all keys - `keys *`\n  - consider KEYS as a command chiefly for development/debugging purposes. It is not performant. If you're looking for a way to find keys in a subset of your keyspace, consider using SCAN or sets.\n- Get all key's matching prefix pattern - `keys <level1>:<level2>:*` (e.g. `keys books:author:*`)\n- Get key's value - `get <key>`\n- Set new key to hold string value - `set <key> <value>`\n- Sets field in the hash stored at key to value - `set <key> <field> <value>`\n- Get the type of a particular key - `type <key>`\n- Delete key - `del <key>`\n- check how much TTL - `ttl <key>`\n- force expire a key - `expire <key> 1`\n","n":0.091}}},{"i":404,"$":{"0":{"v":"React Native","n":0.707},"1":{"v":"\n## How React Native Works\n- When we `debug remotely`, we are running our JS code in the browser, as opposed to on the device.\n\t- when we do this, a web socket connection is made between the device and the browser\n\t- Since this is a different JS engine than would be used in production, it's possible that errors occur in one but not the other.\n- You can consider React Native as a browser that instead of rendering divs, spans, h1, inputs it renders instead native components. The nice thing is that the native components run in their own native thread (that means you get native performance) and your javascript runs in it's own thread and orchestrates all the native components.\n- In production, when the application starts, it starts running code from your javascript bundle and that will drive which components are to be created on the screen.\n- In development, React Native uses watchman to detect when you've made code changes and then automatically build and push the update your device without you needing to manually refresh it.\n\n### Building the app (run-ios/run-android)\n- When we run `react-native run-ios`, a list of iOS simulators is searched, and the default one is started.\n- Since by default it is run in Debug mode (ie. development mode), a series of `xcrun` commands are run, culminating in the `xcodebuild` commands being run, which results in the app being built on the device.\n- Once the app is successfully built, a request is sent to the metro bundler URL (API at port 8081 mentioned above). Metro will bundle the javascript in response to the app having been built.\n\t- In Release mode (ie. production), the bundle is pre-packaged, which is why we need to change where the ios device looks for the bundle (`App.Delegate.m` file)\n\n## Resources\n- [Starter project (template)](https://github.com/obytes/react-native-template-obytes)\n- [React Native advanced guide](https://github.com/anisurrahman072/React-Native-Advanced-Guide)","n":0.058}}},{"i":405,"$":{"0":{"v":"Threads","n":1},"1":{"v":"\nThere are 2 main threads in a RN app: the main thread  and the javascript thread\n- The main thread runs on native platforms and handles displaying the elements of the UI and processes user gestures.\n- The JS thread executes JS code in a separate JS engine \n\t- Therefore, it deals with the business logic of the app\n\t- Also, it defines the structure and functionality of the UI\n- These 2 threads never communicate directly and never block each other\n- Between these 2 threads is the bridge, which has 3 characteristics:\n\t1. Asynchronous communication between the threads\n\t2. Batched communication between threads\n\t3. Serializable, meaning the threads never operate on the same data— instead exchanging serialized messages\n","n":0.094}}},{"i":406,"$":{"0":{"v":"Storage","n":1},"1":{"v":"\nhttps://github.com/ammarahm-ed/react-native-mmkv-storage","n":1}}},{"i":407,"$":{"0":{"v":"Setup","n":1},"1":{"v":"\n## Android\n1. need an Android SDK (download in SDK Manager of Android Studio)\n2. need a JDK installation\n```\nbrew tap AdoptOpenJDK/openjdk\nbrew cask install adoptopenjdk8\n```\n- check installation success `java -version`\n\n3. set environment variables (verify Android SDK location)\n```sh\nexport ANDROID_HOME=$HOME/Library/Android/sdk\nexport PATH=$PATH:$ANDROID_HOME/emulator\nexport PATH=$PATH:$ANDROID_HOME/tools\nexport PATH=$PATH:$ANDROID_HOME/tools/bin\nexport PATH=$PATH:$ANDROID_HOME/platform-tools\n```\n\n4. Ensure SDK Manager (Android Studio) is pointing to correct path\n5. run `adb` and verify the ANDROID_HOME directory is pointed to.","n":0.13}}},{"i":408,"$":{"0":{"v":"Performance","n":1},"1":{"v":"\nAdding `shouldComponentUpdate` will drastically help with optimization.  Consider this as you are building the app rather than considering it afterwards.\n- Alternatively, if you're concerned about a component re-rendering too many times, try using React.PureComponent\n- [library to check how many times a component re-rendered needlessly](https://github.com/maicki/why-did-you-update)\n    - the library is archived, but apparently works perfectly. alternatively: [why-did-you-render](https://github.com/welldone-software/why-did-you-render)\n- [Spying the queue (focusing on the data passing over the bridge)](https://callstack.com/blog/react-native-how-to-check-what-passes-through-your-bridge/?ref=hackernoon.com)\n","n":0.123}}},{"i":409,"$":{"0":{"v":"Native Module","n":0.707},"1":{"v":"\nnote: Native Modules are set to become deprecated, and will be replaced by Turbo Native Module and Fabric Native Components.\n- see [[react-native.architecture]] for more details.\n\nReact Native Modules allow mobile apps written in React Native to access native platform APIs\n- in other words, it allows JavaScript and platform-native code to communicate over the React Native \"bridge\"\n    - this bridge handles cross-platform serialization via JSON.\n- this bridge is carried out by the NativeModule system exposing instances of Java/Objective-C/C++ classes as Javascript objects. What we get in Javascript is an object that has the methods/properties that existed on the native code classes.\n- ex. If you wanted to use a Couchbase Lite iOS/Android API, you would implement a React Native plugin that exports the Couchbase Lite Android and iOS APIs to Javascript.\n- ex. If you wanted to use the phone's native APIs (e.g. Calendar API), we would first write native code that communicates with those native APIs, then we would make that native code invokable through Javascript via the bridge.\n","n":0.078}}},{"i":410,"$":{"0":{"v":"Metro Bundler","n":0.707},"1":{"v":"\nWhen we run `react-native start` both a node.js server and metro bundler are started.\n- Metro can be thought of as similar to \"webpack for react native\"\n\nMetro is a JavaScript [[bundler|js.bundler]]; It takes in an entry file and various options, and gives you back a single JavaScript file that includes all your code and its dependencies.\n1. Metro builds a dependency graph of all modules required from the entry point (ie, our whole react-native app)\n2. Metro transforms modules into a format that React-Native can understand\n3. Metro serializes the transformed modules to form a bundle\n\t- A bundle is just a bunch of modules combined into a single js file.\n4. The bundlefile gets installed on the device, where its code is then executed.\n\t- Remember that when you are writing code for a React Native application, your code is not \"translated\" to Java/Swift/whatever. The Native Modules will send events to a Javascript thread, and the JS thread will execute your bundled React Native code.\n\nI/O of Metro\n- Input - entryfile along with any options\n- Output - the bundle\n\nMetro bundler is not used for production\n- ex. running `react-native run-ios --configuration Release`\n\n### The Bundle\nThe bundlefile can be found at `localhost:8081/index.bundle?platform=ios&dev=true&minify=false`. \n- This is the API through which the device will access the bundle\n- this is served from memory, therefore does get written into our project directory\n- The bundle may be stored in different formats, such as binary or .bundle file.\n- Metro bundler also translates JSX to standard javascript\n","n":0.065}}},{"i":411,"$":{"0":{"v":"Layout","n":1},"1":{"v":"\nusing packages like `react-native-size-matters` or `react-native-responsive-screen` is essential, because app responsive design does not work the same as web. You need to scale based on pixel density. Do not just style just based on screen width/height. Basically replace all pixel values (the default) with scaled versions of those values using functions from the mentioned packages. Then of course for some things you can also toggle different styles based on screen width/height for some things when necessary.\n","n":0.115}}},{"i":412,"$":{"0":{"v":"React Native Architecture","n":0.577},"1":{"v":"\nReact Native, on a conceptual level, wants to be “agnostic” to its native platform.\n- this is why things like react-native-web and react-native-windows exist.\n\nAll the main activities of React Native are run on one of 3 threads:\n- UI (the application’s main thread) - the Native thread where the native-level perception takes place, and it is the place where our platform (iOS/Android) carries out drawing, styling, and measuring.\n- Native modules - any native modules (camera, location, photos etc.) used by the app are carried out on this thread.\n- JavaScript runtime - the thread where every JavaScript application code will run.\n\n## Bridge (current architecture)\nWhat is known as the \"React Native Bridge\" is made up of 3 core components:\n- Shadow Tree\n- JSON (async)\n- Native Modules\n\nThe bridge is basically responsible for two different behaviours: \n- defining how the UI should look and behave (via the Shadow Tree) \n    - all the UI operations are handled by a series of cross-bridge \"steps\" (React -> Native -> Shadow Tree -> Native UI)\n- managing the native side (via Native Modules)\n\nThese communications happen via asynchronous JSON messages that get batched and sent, back and forth, over one communication channel. \n- As you may expect, this channel can get congested and lead to suboptimal experiences.\n\n## Re-architecture\nIn the current architecture (at the time of writing), the Javascript and Native sides of React Native are not really aware of each other. This means that, to communicate, they rely on asynchronous JSON messages transmitted across The Bridge. These messages are sent to the native code with the expectation (but not guarantee) that they will elicit a response sometime in the future.\n- a consequence of this unawareness of one another is that Native Modules need to be initialized when the app is opened, even when they are not used\n    - The new TurboModules approach allows the JavaScript code to load each module only when it’s really needed, resulting in a much faster startup time for apps that use many native modules.\n\nIn the new architecture, everything will be statically typed (either with Typescript or Flow), which is seen as a source of truth. A tool called *CodeGen* will be used to automate the compatability between Javscript and Native.\n- CodeGen will generate the interfaces needed by Fabric and TurboModules to send messages between JS and Native with confidence\n\n### JSI (Javascript Interface)\nThe new architecture also introduces JSI, which is a layer that sits in front of the Javascript engine (currently JavascriptCore). This gives us several benefits\n- Any other JS engine can be used instead (e.g. [[V8|js.v8]], Chakra)\n- JavaScript can hold reference to C++ Host Objects and invoke methods on them, meaning the Javascript and Native sides will truly be aware of each other. This negates the need to serialize the messages to JSON before passing them across the bridge, removing all congestion on the bridge\n    - remember, C++ has always been one of the ways to share native code across iOS and Android without Javascript, since Android’s native code is written in C\\C++, and Objective-C (used by iOS) is none other than a strict superset of C\n\n### Bridge\nInstead of a single bridge to handle both the native side and the UI, the re-architecture splits these 2 concerns into 2 different components:\n- Fabric, which is the re-architecture of the UI manager\n- TurboModules, which is the “new gen” implementation of the interaction with native side\n\n#### Fabric\nFabric aims to modernize the rendering layer of React Native.\n\nInstead of all UI operations being handled by a series of cross-bridge steps (React -> Native -> Shadow Tree -> Native UI), the UI manager creates the Shadow Tree directly in C++, which reduces the number of jumps between Javscript and Native.\n- the result is a greatly improved responsiveness of the UI.\n\nAlso, by using the JSI, Fabric exposes the UI operations to JavaScript as functions: the new Shadow Tree (which determines what to really show on screen) is shared between the two realms, allowing straight interaction from both ends.\n\n","n":0.039}}},{"i":413,"$":{"0":{"v":"Ramda","n":1},"1":{"v":"\nThere are two main guiding principles of Ramda:\n- Data comes last\n- Everything gets curried\n\nThese two principles lead to a style that functional programmers call *point-free* (a.k.a *tacit programming*)\n- Think of point-free code as “Data? What data? There’s no data here.”\n\nWe can make *point-free* transformations by converting functions that take in data to new functions that accept that data and after having already decided what to do with it:\n```js\nconst forever21 = age => ifElse(gte(__, 21), always(21), inc)(age)\n// becomes:\nconst forever21 = ifElse(gte(__, 21), always(21), inc)\n```\nnow, the function `forever21` can be used without having to worry about data until the last minute.\n- Note: there is no behavioral difference in these two versions. We’re still returning a function that takes an age, but now we’re not explicitly specifying the age parameter.\n\nRamda `map` and `filter` (maybe more?) automatically convert different datatypes into functors\n- This allows us to take a function that was made for an array, and apply it to an object, or even a string\n```js\n// An array\ntransform(['Optimus Prime','Bumblebee','Ironhide','Sunstreaker','Ratchet'])\n//=> [ 'EMIRP SUMITPO', 'EDIHNORI', 'REKAERTSNUS', 'TEHCTAR' ]\n \n// A string\ntransform('Optimus Prime')\n// => [ 'R' ]\n \n// Even an object\ntransform({ leader: 'Optimus Prime', bodyguard: 'Ironhide', medic: 'Ratchet' })\n// => { leader: 'EMIRP SUMITPO', bodyguard: 'EDIHNORI', medic: 'TEHCTAR' \n```\n\n## Math functions\nthese functions take their arguments in what seems like normal order (is the first argument greater than the second?) That makes sense when used in isolation, but can be confusing when combining functions. These functions seem to violate Ramda’s “data-last” principle, so we’ll have to be careful when we use them in pipelines and similar situations. That’s when `R.flip` and the placeholder `R.__` will come in handy.\n\n## Symbol replacements\n### Function methods\n- The following methods are best suited for functions\n    - ex. `either(wasBornInCountry, wasNaturalized)`\n    \n`&&` -> `R.both`\n\n`||` -> `R.either`\n\n`!` -> `R.complement`\n\n### Value methods\n- The following methods are best used with values\n\n`&&` -> `R.and`\n\n`||` -> `R.or`\n\n`!` -> `R.not`\n\n### Inequality\n`gt`, `lt`, `gte`, `lte`\nif we are passing data in, then we most likely want to provide a placeholder for the first arg\nex. using conditionals (`R.ifElse`), `R.pipe`/`R.compose`\n```\nconst forever21 = age => ifElse(gte(__, 21), always(21), inc)(age)\n```\n\n## Debugging Ramda\n[more info](https://blog.carbonfive.com/2017/12/20/easy-pipeline-debugging-with-curried-console-log/)\n\nRamda provides us with a function `R.tap` which we can use to create side effects without interrupting the flow of an existing composition\n- `tap` accepts a function, and returns a function (ex. `console.log`) that may take in the passed in arg\n```\nR.compose(\n    R.tap(x => console.log('REVERSE:',x)),\n    R.map(R.reverse),\n)\n```\nthis allows us figure out what happened after `reverse` was called on the data\n\nControl flow (`if`/`else`) is less necessary in functional programming, but still occasionally useful.\n\n### Data flow\nRemember that Ramda is flexible with how you combine functions, as long as the expected types are received\n```\nconst alwaysDrivingAge = age => ifElse(lt(__, 16), always(16), a => a)(age)\n```\nHere, an age variable goes through the ifElse, and if fails the condition, will proceed to the third arg (else), where age is the first argument of the function\n\n#### Arg order on `pipe`/`compose` when function arg passed in through the data flow\n```\nconst publishedInYear = year => \n    book => \n        book.year === year\nconst titlesForYear = year => \n\tR.pipe(\n\t\t//pIY returns a function that takes a book as its arg\n\t\tR.filter(publishedInYear(year)), \n\t\tR.map(book => book.title)\n\t)\n\nconsole.log(titlesForYear(1934)(books))\n```\n\n#### `R.when`/`R.unless`\nsimilar to `ifElse`, but with only one conclusion. Implicitly, `R.identity` is the second conclusion (ie., it is *else*)\n```\nconst alwaysDrivingAge = age => when(lt(__, 16), always(16))(age)\n```\n\n\n\n## Working with functions that you didn't write\n### `R.apply`\nspread out a single array into individual arguments. Think of it as `spreadArgs()`\nThis is useful for creating a fixed-arity function from a variadic (variable arity) function\n```\nfunction spreadArgs(fn) {\n    return function spreadFn(argsArr){\n        return fn( ...argsArr );\n    };\n}\n```\nThis is related to usingSpread syntax with functions\n```\nconst arr = [1, 2, 3]\nconst fn = (a, b, c)\nfn(...arr) === fn(arr[0], arr[1], arr[2])\n```\n### `R.unapply`\ngather individual args into a single array\n\n### `R.unary`\nCan be used to cut off all arguments past the first. The benefit of this is that if we call a function and pass in too many arguments, the extra ones will just drop off. \nex. map takes a function, and the first argument of map (the iterated value) gets passed to parseInt. Then the second arg (index) gets passed to parseInt, which corresponds to its radix. Since we don't want index to be interpreted this way, we make it a unary function, and the index argument safely falls off\n```\n[\"1\", \"2\", \"3\"].map(R.unary(parseInt)\n```\n\n### `R.flip`\nswap the first and second argument of a function\n\n### `R.__`\nuse a placeholder argument to go in place of a parameter\nWhen we pass data through some sort of chain (`pipe`, `compose`, `ifElse`), that data will be passed through the methods as the argument in the first available spot.\n- ex. if a fn in `pipe` has no args provided, then the data will be the first arg. If it has one arg, then the data will be applied in the second place (and so on). If we use a placeholder `R.__` as the first arg, then the data will be applied to that space, since it is the first available argument.\n- `const forever21 = age => ifElse(gte(__, 21), always(21), inc)(age)`\n\n## Object manipulation\n### `R.assoc`\nupdate (or create) a property of an object\n```\nconst myObj = {\n\tname: 'kyle'\n}\n\nconst newObj = R.assoc('number', '7788713377')\n\nconsole.log(newObj(myObj))\n// { name: 'kyle', number: '77887133' }\n```\n\n### `R.prop`\nread a single property from an object and return the value\n\n### `R.pick`\npick multiple properties of an object and return a new object with just those properties\n- complement of `R.omit`\n\n### `R.has`\nreturn boolean indicating whether or not a property exists on the object\n\n### `R.path`\ndive into nested objects returning the value at the given path\n\n### `R.propOr`/`R.pathOr`\nsearch for the property on an object, allowing you to provide a default value in case the property/path cannot be found on the obj.\n\n### `R.evolve`\ndeclaratively provides transformations to happen on each property of an object:\n- note: cannot add new properties with this\n```\nvar tomato  = {firstName: '  Tomato ', data: {elapsed: 100, remaining: 1400}, id:123};\nvar transformations = {\n  firstName: R.trim,\n  lastName: R.trim, // Will not get invoked.\n  data: {elapsed: R.add(1), remaining: R.add(-1)}\n};\nR.evolve(transformations, tomato); //=> {firstName: 'Tomato', data: {elapsed: 101, remaining: 1399}, id:123}\n```\nalso\n```\n//before\nconst nextAge = compose(inc, prop('age'))\nconst celebrateBirthday = person => assoc('age', nextAge(person), person)\n\n// after\nconst celebrateBirthday = evolve({ age: inc })\n```\n\n## Array manipulation\n`R.nth`\naccess an array at the given index\nequivalent of object's `R.prop`\n\n`R.slice`\nequivalent of object's `R.pick`\n\n`R.contains`\ncheck if array contains given value\nequivalent of object's `R.has`\n\n`R.head`\naccess first element of array\n\n`R.last`\naccess last element of array\n\n`R.append`/`R.prepend`\nFP versions of `.push` and `.unshift`\n\n`R.drop`/`R.dropLast`\nFP versions of `.shift` and `.pop`\n\n`R.insert`\ninsert an element at given index of an array\n\n`R.update`\nreplace an element at the given index of an array\n\n`R.adjust`\nequivalent of object's `R.evolve`, except only works for one element. We would use this function over `R.update` when using a function to update the array element\n```\nconst numbers = [10, 20, 30, 40, 50, 60]\n \nadjust(multiply(10), 2, numbers) // [10, 20, 300, 40, 50, 60]\n```\n\n`R.remove`\nremove element by index\n\n`R.without`\nremove element by value\n\n### `R.reject()`\ncomplement of `R.filter()`. If `filter()` is *filter in*, then `reject()` is *filter out*\n\n### `.reduce()`\n\nwith `initialValue`\n![](/assets/images/2021-03-07-22-18-20.png)\n\nwithout `initialValue`. The first value of the list will act in place of the initialValue and the combining will start with the second value in the list\n![](/assets/images/2021-03-07-22-18-41.png)\n\nIf the array passed to `.reduce()` is empty, then there must be an `initialValue` specified, otherwise there will be an error\n\n### `.map()`\n#### use cases\n- transform a list of functions into their return values:\n```js\nvar one = () => 1;\nvar two = () => 2;\nvar three = () => 3;\n\n[one,two,three].map( fn => fn() );\n// [1,2,3]\n```\n- transform a list of functions by composing each of them with another function, and then execute them:\n```\nvar increment = v => ++v;\nvar decrement = v => --v;\nvar square = v => v * v;\n\nvar double = v => v * 2;\n\n[increment,decrement,square]\n.map( fn => compose( fn, double ) )\n.map( fn => fn( 3 ) );\n// [7,5,36]\n```\n\n### `R.chain` (a.k.a. flatMap)\n\niterate over values of a list, performing the provided function on each element, then concatenating all of the results together\n- what results is if we were to perform a function of each element, then flatten that resulting array\n```js\nvar firstNames = [\n\t{ name: \"Jonathan\", variations: [\"John\", \"Jon\", \"Jonny\"] },\n\t{ name: \"Stephanie\", variations: [\"Steph\", \"Stephy\"] },\n\t{ name: \"Frederick\", variations: [\"Fred\", \"Freddy\"] }\n];\n\nR.chain(entry => [entry.name, ...entry.variations], firstNames)\n// [\"Jonathan\",\"John\",\"Jon\",\"Jonny\",\"Stephanie\",\"Steph\",\"Stephy\",\n//  \"Frederick\",\"Fred\",\"Freddy\"]\n```\n\n### `R.zip`\nAlternate through 2 arrays and take each value that appears at the same index and put them into their own array. The shorter of the 2 lists is considered the list length\n```js\nconst arr1 = [1, 2, 3]\nconst arr2 = ['a', 'b', 'c']\n\nR.zip(arr1, arr2)\n// [[1, a], [2, b], [3, c] ]\n```\n### `R.invoker()` a.k.a. `unboundMethod()`\n\n## Number manipulation\n`R.clamp`\nrestrict a number to be within a certain range\n```js\nR.clamp(1, 10, -5) // => 1\nR.clamp(1, 10, 15) // => 10\nR.clamp(1, 10, 4)  // => 4\n```\n\n### `R.complement()`\n implements the same idea for functions as the ! (not) operator does for values.\n nothig good ever comes out of this\n my daring susan\n why cant we jst love each other aagain???\n\n## Fusion (with `.map()`)\nImagine we have multiple functions that each take 1 argument, and each function's return can be passed directly into the next as input:\n```js\nfunction truncate(word) {\n\tif (word.length > 10) {\n\t\treturn `${word.substring(0, 6)}...`\n\t}\n\treturn word\n}\n\nfunction upper(word) {\n\treturn word.toUpperCase()\n}\n\n\nfunction append1(word) {\n\treturn word + '1'\n}\n```\nNaively, we could do this:\n```js\nconst generateList = arr\n    .map(append1)\n    .map(upper)\n    .map(truncate)\n```\nwhen we pass a function to map, like `.map(fn)`, the element of the list gets passed as the first argument to that function. Here, when we pass `R.pipe`, the element gets passed as first argument to `pipe` (since pipe is ltr)\n```\nconst generateList = arr.map(R.pipe(append1, upper, truncate))\n```\n\n# Lenses\nWith the following more specific ways of creating lenses, `R.lens()` is not often needed\n\n#### `R.lensProp`\nlets us create a lens that focuses on a *non-nested* property of an object\n\n#### `R.lensPath`\nlets us create a lens that focuses on a *nested* property of an object\n\n#### `R.lensIndex`\nlets us create a lens that focuses on an element of an array\n- these are for the times where we know in advance the index that we are interested in, but don't yet have the data (*ex. capitalize first letter*)\n```\nconst toTitle = R.compose(\n\tR.join(''),\n\tR.over(R.lensIndex(0), R.toUpper)\n)\n```\n\n### Three functions for working with lenses\n#### `R.view()`\nread value of the lens\n\n#### `R.set()`\nset the value of the lens\n- note: this does not mutate the original object supplied. In other words, `R.view()` will give us the exact same result before and after #### `R.set()`\n\n#### `R.over()`\napply a transformation function to the lens\n```js\nover(nameLens, toUpper, person)\n// => {\n//   name: 'RANDY',\n//   socialMedia: {\n//     github: 'randycoulman',\n//     twitter: '@randycoulman'\n//   }\n// }\n```\n\n\n","n":0.024}}},{"i":414,"$":{"0":{"v":"CLI","n":1}}},{"i":415,"$":{"0":{"v":"R.pipe","n":1},"1":{"v":"\n`pipe(..)` is identical to `compose(..)` except it processes through the list of functions in left-to-right order:\n`var pipe = reverseargs( compose )`\n\nthe function returned by `pipe` takes the same number of arguments as the first function `.pipe()` is given\n\nwhen we call `operate(3, 4)`, pipe passes the 3 and 4 to the `multiply` function, resulting in 12. it passes that 12 to addone, which returns 13. it then passes that 13 to square, which returns 169, and that becomes the final result of operate.\n```js\nconst operate = pipe(\n  multiply,\n  addOne,\n  square\n)\n```\n\nex. improvement with `R.pipe`\n```js\n// before\nconst titlesForYear = (books, year) => {\n  const selected = filter(publishedInYear(year), books)\n \n  return map(book => book.title, selected)\n}\n\n// after\nconst titlesForYear = year =>\n  pipe(\n    filter(publishedInYear(year)),\n    map(book => book.title)\n  )\n```\n","n":0.091}}},{"i":416,"$":{"0":{"v":"R.compose","n":1},"1":{"v":"\ncomposing two functions and then mapping the resulting function over a functor should be the same as first mapping one function over the functor and then mapping the other one.\n- ie. if you plan to map over the same array multiple times, just compose the functions and then map over the array once with that composed function\n```js\n[\"1\", \"2\", \"3\"].map(r.unary(parseint)\n```\nthe data flow is: \n```\narrayvalue <-- map <-- unary <-- parseint\n```\n(p.s., any time you see data flow presented this way, think `r.compose()`\n\n`parseint` is the input to `unary(..)`. the output of `unary(..)` is the input to `map(..)`. the output of `map(..)` is `arrayvalue`. this is the composition of map(..) and unary(..).\n\nhaving a series of functions whose input is the output provided by the previous (inner) function\n\n![](/assets/images/2021-03-09-09-34-17.png)\n\nencapsulating a series of functions within one function\n![](/assets/images/2021-03-09-09-34-28.png)\n\nfunctions compose from right to left (incl. `r.compose()`)\n\n- expl. this is consistent with how we evaluate functions, from inner to outer (ie. right to left)\nex:\n```js\n[\"1\", \"2\", \"3\"].map(r.unary(parseint)\n```\n\ncomposition is the wrapper around the big machine, whose contents are the parameters of `r.compose()`. in other words, the components of the machine (under the wrapper) are the individual functions that the origvalue goes through (as an argument). the composed machine returns a new function, so it is in essence a function factory.\nrightmost functions are at the top of the machine.\n![](/assets/images/2021-03-09-09-34-44.png)\n","n":0.068}}},{"i":417,"$":{"0":{"v":"Pytorch","n":1},"1":{"v":"\n## Data Primitives\nPyTorch has two primitives to work with data: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`\n\n### Dataset\n`Dataset` represents a map between key (label) and sample (features) pairs of your data.\n- ex. images and their associated labels\n\n### DataLoader\n`DataLoader` wraps an iterable around the `Dataset`\n- this is accomplished by passing `Dataset` as an arg to `DataLoader`, giving us automatic batching, sampling, shuffling and multiprocess data loading.\n\n## Domain-specific Libraries\n- TorchText \n- TorchVision\n- TorchAudio\n- TorchRec - Recommendation Systems\n\n### TorchVision\nEvery TorchVision Dataset includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively.\n\n* * *\n\n## Reproducibility\nGenerating random numbers is an [[essential aspect|ml.deep-learning.nn#how-a-neural-network-learns]] to a deep learning model. However, this presents a problem: if we are generating random numbers, how can we reproduce the same results across different executions of the code and across different machines, given the same data and parameters?\n\nThere are a couple of things we can do to limit the amount of nondeterministic behaviour from Pytorch:\n1. control sources of randomness that can cause multiple executions of your application to behave differently\n2. configure PyTorch to avoid using nondeterministic algorithms for some operations. As a result, multiple calls to those operations, given the same inputs, will produce the same result.\n    - this involves specifying the seed so that the [[PRNG|general.algorithms.RNG]] will produce the same sequence of \"random\" numbers, even run across different machines.\n\n* * *\n\n### Torch Hub / `torchvision.models`\nAllows us to access many pre-built deep learning models (allowing us to leverage [[transfer learning|ml#transfer-learning]])\n\n## Resources\n- [Pytorch Cheatsheet](https://pytorch.org/tutorials/beginner/ptcheat.html)\n\n## Learning Resources\n- https://www.dataquest.io/blog/pytorch-for-beginners/\n- [Learn PyTorch for deep learning in a day. Literally](https://youtu.be/Z_ikDlimN6A?si=FX6o8eF3Xh6fbqnA&t=23078)\n    - [Accompanying notes](https://www.learnpytorch.io/)\n\n","n":0.063}}},{"i":418,"$":{"0":{"v":"Tensors","n":1},"1":{"v":"\nTensors are the primary data structure used for representing and manipulating data in deep learning models.\n- they are the numerical representation of data as it is used as an input to the model, and it is also the numerical representation that is output by the model (ie. representation outputs)\n    - therefore, the inputs and outputs of a neural network are tensors\n- ex. Imagine we have a machine learning model that predicts if an image is of a shirt or not. In this case, the tensors would typically hold the image data in the form of numerical values. The image data is represented as pixel intensities, which can be transformed into tensors.\n    - if we consider what an image is, it's simply an assortment of pixels, where each pixel is some combination of RGB. If we had a pixel art of Mario which was 16x12, then we could create a tensor of shape `(16, 12, 3)`, where the first dimension represents the height, the second represents the width, and the third represents the color channel (ie. each pixel's degree of RGB). With this tensor, we could recreate the image.\n\nTensors are often created internally by Pytorch (e.g., the weights of a neural network), but we as developers sometimes create tensors manually, especially when importing data or during preprocessing.\n\nMany neural networks perform their learning by starting off with tensors full of random numbers, and then adjusting those numbers to better represent the data (by taking the source data, such as an image, and encoding its data and replacing the random numbers within the tensor).\n- this is done to help break the symmetry and prevent the model from getting stuck in a suboptimal solution.\n- it also helps with reinforcement learning algorithms in order to introduce exploration during the learning process. By adding random noise to the actions taken by an agent, it can explore different states and actions, enabling it to discover new and potentially better strategies.\n\n![](/assets/images/2023-07-09-10-19-06.png)\n\nTensors are similar to an `ndarray` (N-dimensional array), due to certain characteristics:\n- it is multidimensional\n    - Tensors can have different dimensions, including 0-dimensional (scalar), 1-dimensional (vector), 2-dimensional (matrix), and higher-dimensional arrays.\n- each element is of the same type\n- each sub-array is of the same (fixed) length\n\nThe main difference between a tensor and an ndarray is that a tensor can run on GPUs and TPUs, in addition to CPUs\n\nThe variable for tensors that are 0 or 1 dimension (ie. scalars or vectors) is lowercase, while the variable for tensors that are more than 2 (ie. matrices and tensors) is UPPERCASE.\n\n## Tensor attributes\nA tensor on PyTorch has attributes:\n\n### Shape\nthe size of the tensor; ie. the size of each layer of nested `[]` \n- ex. a tensor of shape `[2, 3, 4]` would have 2 elements in its outermost brackets, each of which would have 3 elements (ie. 3 arrays), and each element of that array would have 4 elements:\n```py\n[\n    [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 8, 7, 6]\n    ],\n    [\n        [9, 8, 7, 6],\n        [5, 4, 3, 2],\n        [1, 2, 3, 4]\n    ]\n]\n```\n- `TENSOR.shape` (or alias `TENSOR.size()`)\n\n### Data Type\nthe type of data stored in the tensor\n- `TENSOR.dtype`\n- default is `float32`\n- convert type: `TENSOR.type(torch.float32)` OR  `TENSOR.to(torch.float32)`\n\n### Device\nthe device in which the tensor is stored\n- `TENSOR.device`\n- default is `cpu`\n- moving tensor from CPU to GPU: `TENSOR.to('cuda:0')`\n\n### Gradient Requirement\nthe state of whether or not gradients need to be computed for the tensor\n    - `TENSOR.requires_grad`\n- spec: gradient is the same as slope if we are considering a straight line in a graph\n\n* * *\n\nTensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.\n\n```py\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n```\n\n## Tensor Manipulation\n\n### Tensor Operations\nThe neural network will combine these tensor operations in some way in order to find patterns in numbers of a dataset\n\nAll operations (except matrix multiplication) are element-wise, meaning the operator (×, ÷ etc.) is applied to each element in place\n\nOperations include:\n- addition\n- subtraction\n- multiplication\n    - `torch.mul(TENSOR, 10)`\n- division\n- matrix multiplication\n    - `torch.matmul(TENSOR1, TENSOR2)` or `torch.mm()` or `TENSOR1 @ TENSOR2`\n    - matrix multiplication is the most common type of operation found in neural networks\n    - see [[Matrix multiplication|math.algebra.linear#matrix-multiplication]] Dendron node\n\n#### Transpose\nTransposing a matrix is simply swapping the row contents with its column contents.\n\n```py\ntensor_A\n# tensor([[ 7,  8,  9],\n#        [10, 11, 12]])\ntensor_A.T\n# tensor([[ 7, 10],\n#        [ 8, 11],\n#        [ 9, 12]])\n```\n\nWe do this since we cannot multiply two matrices unless # rows of MatrixA = # columns of MatrixB, and # columns of MatrixA = # rows of MatrixB (see [[rules of matrix multiplication|math.algebra.linear#rules]]), we need to transpose one of our matrices if the shapes don't line up properly.\n\n### Tensor Aggregation (tensor methods)\nFinding the `.min()`, `.max()`, `.mean()`, `.sum()` etc. of certain tensor values.\n\n### Reshaping, Stacking, Squeezing and Unsqueezing Tensors\nEach of these methods allows us to manipulate our tensors in order to change their shape/dimension.\n- we often use these to solve issues that might arise due to tensors with mismatched shapes. \n\n- **Reshaping** - reshapes an input tensor to a defined shape\n    - we do this because in order for multiple tensors to work together, we need their shape to correspond to one another   \n    - the new shape of the reshaped tensor must be compatible with the original tensor\n        - ex. if we have a tensor `[1, 2, 3, 4, 5, 6]`, we cannot reshape it into a 2x2 tensor. We could however put it into a 2x3.\n- **View** - return a view of an input tensor of certain shape, but keep the same memory as the original tensor\n    - therefore if we assign a new variable `Z` to the view of an existing tensor and then change `Z`, it will also change the original tensor\n- **Stacking** - combine multiple tensors, either on top of each other (vstack) or side-by-side (hstack)\n- **Squeeze** - removes all `1` dimensions from a tensor\n    - `1` dimension is a dimension (ie. an array) has a single element in it\n    - ex. if we have a tensor of shape `[2, 1, 2, 1, 2]` and squeeze it, it will become `[2, 2, 2]`\n- **Unsqueeze** - add a `1` dimension to a target tensor\n    - we must specify the `dim` argument, which specified which index to add \n- **Permute** - returns a view that rearranges (ie. swaps) the dimensions of a tensor in a specified way\n    - ex. if we have a tensor (`x`) of shape `(2, 3, 5)` and permute it as `(2, 0, 1)`, the new tensor will have shape `(5, 2, 3)`\n\n### Indexing\nSometimes you'll want to select specific data from tensors \n- ex. only the first column or second row.\n\nAside from how indexing normally works (e.g. using `[0][0]` to access first element of the first element), we can also use `:` to specify \"all values in this dimension\"\n\n```py\nx = tensor([[1, 2, 3],\n            [4, 5, 6],\n            [7, 8, 9]])\n\n# Get all values of 0th dimension and the 0 index of 1st dimension\nx[:, 0] # tensor([[1, 2, 3]])\n\n# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\nx[:, :, 1] # tensor([[2, 5, 8]])\n\n# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\nx[:, 1, 1] # tensor([5])\n\n# Get index 0 of 0th and 1st dimension and all values of 2nd dimension (same as x[0][0])\nx[0, 0, :] # tensor([1, 2, 3])\n\n# Get the first 10 values of the 0 dimension\nx[:10]\n\n# Get all values after the 10th index\nx[:10]\n```\n\n## Resources\n[[see tensors Dendron node for mathematical definition|math.algebra.linear.tensors]]","n":0.028}}},{"i":419,"$":{"0":{"v":"Ap","n":1},"1":{"v":"\n- `torch.nn.Module` - The base class for all neural network modules. Our neural network should subclass this module\n- `torch.nn.Parameter` - Stores tensors that can be used with `nn.Module`. These are the parameters that our model should try and learn.\n    - often, a layer from `torch.nn` will set these for us.\n    - If `requires_grad=True`, gradients (used for updating model parameters via gradient descent) are calculated automatically, this is often referred to as \"autograd\".\n- `def forward()` - this defines the computation that will take place on the data passed to the particular `nn.Module`\n    - all `nn.Module` subclasses require us to override this method\n- `torch.optim` - Contains various optimization algorithms (these tell the model parameters stored in `nn.Parameter` how to best change to improve gradient descent and in turn reduce the loss).\n\n## Neural Network instance methods\n- `model.state_dict()` - gives us the state of the model's parameters","n":0.084}}},{"i":420,"$":{"0":{"v":"Python","n":1},"1":{"v":"\n**kwargs** - similar to `...args` in javascript\n\n### Self\n- a reference to the current instance of the class\n- `self` is always the first argument of a class method (including the `init` method)\n    - In the `init` method, `self` refers to the newly created object\n    - In other class methods, it refers to the instance whose method was called.\n\n### `.init`\n- it is a constructor\n- Called when an object is instantiated.\n- It will initialize the attributes of the class\n\n### `super`\n- give the current object access to the methods from its superclass\n- `super` returns a temporary object of the superclass, which allows us to call that superclass's methods\n- Common use case\n    - building classes that extend functionality of previously built classes\n  \n# Celery\n- Async task queueing system that can allow you to queue up jobs at schedules, or have them queue up on a trigger (ex. on some get request).\n\n# Resources\n[Why you need pyenv to manage Python versions](https://realpython.com/intro-to-pyenv/)","n":0.081}}},{"i":421,"$":{"0":{"v":"JS Equivalent","n":0.707},"1":{"v":"\nJavascript -> Python\n- `class Car extends Vehicle` -> `class Car(Vehicle)`\n- `constructor()` -> `__init__(self)`","n":0.277}}},{"i":422,"$":{"0":{"v":"Env","n":1},"1":{"v":"\n### pip\nPackage manager\n\n### pipenv\nCombine Pipfile, pip, and virtualenv into a single toolchain\n- can autoimport `requirements.txt`\n\nGoal is to help users manage environments, dependencies, and imported packages\n\n### virtualenv\nuses the command-line path environment variable to create isolated Python Virtual Environments (similar to `venv`)\n- offers more functionality than `venv`, such as providing convenient features for configuring, maintaining, duplicating, and troubleshooting the virtual environments","n":0.13}}},{"i":423,"$":{"0":{"v":"Anaconda","n":1},"1":{"v":"\nAnaconda is a distribution of Python (as well as R) that is specifically suited for scientific purposes (e.g. data science, machine learning, large-scale data processing, predictive analytics) with the goal of simplifying package management and deployment\n\nAnaconda (and Miniconda) uses the `conda` package manager, allowing us to:\n- manage dependencies\n- create isolated environments (ie standalone directories that contain specific versions of Python and other packages)\n    - `conda create`\n\n### Conda environment\nConda environments allow us to maintain isolated containers of specific versions of Python and 3rd party dependencies.\n\nWhen you run a `conda install` command, it installs the specified package and their dependencies into the currently active conda environment, not just the current directory tree.\n- we can also use `pip install` if using `pip` within a conda environment\n\n## Resources\n- [Anaconda vs Miniconda](https://docs.anaconda.com/free/anaconda/getting-started/distro-or-miniconda/)","n":0.089}}},{"i":424,"$":{"0":{"v":"Protocol","n":1},"1":{"v":"\nMost protocols widely used are synchronous (eg. http)\n","n":0.354}}},{"i":425,"$":{"0":{"v":"HTTP","n":1},"1":{"v":"\nHTTP uses [[protocol.TCP]] as its underlying transport protocol.\n- The HTTP client first initiates a TCP connection with the server, then once the connection is made, the browser and server processes access TCP through their [[socket|network.sockets]] interfaces.\n\n### Credentials policy\n*Only relevant to browsers*\n- Credentials are cookies, authorization headers or TLS client certificates.\n\nCredentials\n- `include` - send a request with credentials included, even if it's cross-origin\n- `same-origin` - only send credentials if the request URL is on same origin as the calling script\n- `omit` - ensure browsers don’t include credentials in the request\n\n- we can use the `Access-Control-Allow-Credentials` response header to tell browsers to expose the response to frontend JavaScript code.\n\t- to do this, the client will need to set the credentials to `include` (`Request.credentials`)\n\n### Breakdown of connection to server\n1. User enters `http://someschool.edu/somedepartment/home.index` in the address bar of a browser\n2. The browser (ie. HTTP client process) initiates a TCP connection to the server `www.someschool.edu` at port `80`\n\t- associated with the TCP connection, there will be a socket at the client and a socket at the server.\n3. The browser sends an HTTP request to the server via its socket.\n\t- The request includes the path name `/somedepartment/home.index`\n4. The server receives the request and retrieves the object (in this case, HTML) from its storage (eg. RAM or disk), encapsulates the object in an HTTP response, then sends that message to the client via its socket.\n5. The HTTP server process tells TCP to gracefully close the TCP connection.\n\t- \"gracefully\" here means that the connection will only close once the client receives the full communication.\n6. The browser receives the response, and the TCP connection is closed.\n7. The response from the server indicates that the encapsulated object is HTML. The client then extracts the file fromm the response message, examines the HTML file, and finds references to 10 JPEG objects.\n8. Steps 1-5 are then repeated for *each* of the referenced JPEG objects.\n\t- These 10 TCP connections are most likely made in parallel, which we would say are 10 serial TCP connections.\n\t\t- This degree of parallelism is determined by the browser, and configurable by the user.\n\t\t\t- by default most browsers open between 5 and 10 parallel TCP connections.\n","n":0.053}}},{"i":426,"$":{"0":{"v":"Webdav","n":1},"1":{"v":"\nHTTP doesn't allow for writing, modifying, moving, etc of a file; it only allows for querying that file. WebDAV enhances HTTP to allow writing.\n\n- an extension of HTTP\n\t- Therefore, gets all of the benefits that HTTP offers, such as encryption, if HTTPS\n\t- Also can use HTTP tools like cURL\n- Since it is an extension of HTTP, it gets access to HTTP verbs. Additionally, it extends these base verbs, giving additional functionality\n\t- ex. COPY, MOVE, MKCOL (make collection, aka directory)\n- a protocol that allows us to create, update, and move documents on a server.\n\t- these are known as *remote web content authoring operations*\n- WebDAV provides a coherent set of methods and headers, and has a system like Express that involves request and response objects\n- ex. perform CRUD operations on information about Web pages, such as their authors, creation dates, etc.\n- The WebDAV protocol enables a webserver to behave like a fileserver\n","n":0.082}}},{"i":427,"$":{"0":{"v":"HTTP Methods","n":0.707},"1":{"v":"\nWhen deciding on which HTTP method to use, always consider it from the client's perspective.\n- ex. if the UI of a webapp has an option to delete content, you may decide that your backend will only soft-delete that content (ie. it will keep the data in the database, but give it a flag such as `isDeleted: true`). This implementation detail is of no concern to the client, and therefore while we may be tempted to implement this functionality as a `PATCH` (since we are modifying data instead of actually deleting it), because the client has no knowledge of that, we should use the `DELETE` HTTP method.\n\n## GET\n- GET requests can be cached\n- GET requests remain in the browser history\n- GET requests can be bookmarked\n- GET requests should never be used when dealing with sensitive data\n- GET requests have length restrictions\n- GET requests should be used only to retrieve data\n\n### Return codes\n- 200 (OK)\n- 400 (Bad Request)\n- 404 (Not found)\n\n## POST\nUse POST when you want to create resources *without* knowing the URI for the new resource\n- however, a POST does not necessarily have to result in a resource that can be identified by a URI.\n\n- POST requests are never cached\n- POST requests do not remain in the browser history\n- POST requests cannot be bookmarked\n- POST requests have no restrictions on data length\n- POST requests are not idempotent\n\n### Status codes\n- 200 (OK) - more than 1 resource created\n- 201 (Created) - only 1 resource created\n- 204 (No Content) - no new resource created (or if a resource was created, but the response doesn't include an entity that describes the result)\n- 400 (Bad Request)\n\n## PUT\nFor PUT requests the client *needs* to know the exact URI of the resource.\n- ex. We cannot send a PUT request to `/projects` and expect a new resource to be created at `/projects/123`\n- this doesn't mean we can't create new resources with PUT; but it does mean the client needs to know how to generate the URI/ID of the new resource.\n    - In cases where this ID is knowable by the client, PUT should actually be preferred over POST, since PUT is idempotent.\n- Therefore, PUT can be thought of as \"create or update\" resource.\n\nPUT should replace the existing resource with the new one. This means we cannot do partial updates.\n- if we want to mimic a partial update, then we'd have to send the whole resource (with the modified field).\n\n### Status codes\n- 201 (Created) - a new resource is created\n- 200 (OK) - an existing resource is modified\n- 204 (No Content) - an existing resource is modified, but the entity isn't included in the response\n\n## PATCH\nPATCH allows us to perform partial modifications to an existing resource.\n\nPATCH requests are neither safe nor idempotent\n\nFor document-oriented databases, [JSON PATCH](https://jsonpatch.com/) is a good way to make changes only to fields that have changed.\n### Status codes\n- 200 (OK)\n- 400 (Bad Request) \n- 404 (Not found)\n- 422 (Unprocessable Entity)\n\n## Delete\n\n### Status codes\n- 204 (No Content)\n- 400 (Bad Request)\n- 404 (Not found)\n\n* * *\n\n## Properties of HTTP methods\n### Safe\nHTTP methods are considered safe if they do not alter the server state, so safe methods can only be used for read-only operations.\n\nSafe HTTP methods are GET, HEAD, OPTIONS and TRACE\n\n### Idempotent\n[[main note|general.terms.idempotent]]\n\nThe idempotent methods are GET, HEAD, OPTIONS, TRACE, PUT and DELETE.\n\nAll safe HTTP methods are idempotent but PUT and DELETE are idempotent but not safe.","n":0.042}}},{"i":428,"$":{"0":{"v":"ETag","n":1},"1":{"v":"\nAn ETag (entity tag) is an HTTP response header that specifies a specific version of a resource.\n\nThe ETag enables servers to inform clients when requested data has not changed. When a client makes a request for a resource, it passes along an ETag (which it got from the previous fetch), and instead of the server just sending back all of the data upon request, it can match the passed ETag with the ETag of the reource and inform the client that the data requested has not changed since the previous fetch.\n\nAny time a resource is changed, a new ETag value must be generated. \n\n### `If-None-Match` Header\nFor GET and HEAD methods, the server will return the requested resource and 200 status, only if it doesn't have an ETag matching the one passed by the client.\n\n### `If-Match` Header\nFor GET and HEAD methods, the server will only return the requested resource (or upload resource for PUT) if the passed ETag matches one of the ETags from the server.","n":0.078}}},{"i":429,"$":{"0":{"v":"CORS","n":1},"1":{"v":"\n## What is it?\nCORS is an HTTP-header based mechanism that allows a server to indicate any origins other than its own from which a browser should permit loading resources.\n- An **origin** is a combination of scheme, domain and port (ie. `https://` + `example.com` + `:443`)\n  - therefore `https://example.com` and `https://api.example.com` are different origins\n- It does this by modifying the response headers returned from the server.\n- ex. if your app uses a headless CMS, then that headless CMS server must be configured to allow requests from your website `www.mypersonalwebsite.com`.\n- note: a scheme can be a *local scheme* (`about://`, `blob://`, `data://`, `file://`) or an *http scheme* (`http://`)\n\nWhen a website can access a resource or execute commands on another domain via HTTP requests, the process is called *cross-origin resource sharing*.\n- ex. you are on a blogging website, and clicking on one of their buttons sends an HTTP request to your bank's website requesting that all money be transferred out.\n- Because of CORS, this of course cannot occur. So if we want to allow certain domains to send requests to be handled by the server, the server needs to have some sort of whitelist of domains.\n\nBy default, if you are trying to access a resource that has a different domain from the one that serves your main website, then CORS will prevent that from happening.\n\nIn a browser console, the following code snippet will always result in a CORS error unless:\n- I'm already on google.com\n- Google's server has enabled cross-origin requests from whatever website (hostname) I'm on \n```js\nfetch(\"https://google.com\", function(error, meta, body){\n    console.log(body.toString());\n});\n```\n\nConsider that CORS restrictions don't come into play for all cross-origin resource sharing, meaning we can request resources without their consent.\n- ex. `<img>`, `<script>`, `<link>`, `<iframe>`, `<video>`, `<audio>`\n\n#### Enabling CORS for a host\n\nThe server is the one that enables the cross-origin requests, not the client\n- [Adding CORS support to server](https://enable-cors.org/server.html)\n\n- If a resource never contains private data, then it's safe to put `Access-Control-Allow-Origin: *`.\n- If a resources sometimes contains private data depending on cookies, it's safe to add `Access-Control-Allow-Origin: *` as long as you also include a `Vary: Cookie` header.\n\n```js\nrouter.use(function (req,res,next) {\n  console.log('/' + req.method);\n  res.header(\"Access-Control-Allow-Origin\", \"https://google.com\");\n  next();\n});\n```\n\nIf you want to make a request from one domain to another, then you need to enable CORS on the server\n\n* * *\n\n### Terminology\nOrigin - the User Agent that initiated the request. In other words, the client.\n\n* * *\n\n## UE Resources\n[MDN guide](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)\n","n":0.05}}},{"i":430,"$":{"0":{"v":"headers","n":1},"1":{"v":"\nSince CORS is administered by the server handling the requests, these are response headers. They allow the server to fine-tune its policy toward handling cross-origin requests.\n\n### `Access-Control-Allow-Origin`\nThis header gives the server the power to determine if the browser should share the response with requesting code from the given origin\n- ex. `*` (wildcard) can be specified for requests without credentials. The value tells browsers to allow any Javascript from any origin to access the resource\n\nBecause setting this value to `*` basically tells the server to handle requests from any client, it's usually not a good idea and specific origins should be set.\n\n### `Access-Control-Allow-Credentials`\nThis header gives the server the power to determine if the browser should expose the response to the front-end Javascript code when the request's [[credentials|protocol.http.requests.prop.credentials]] mode is `include`.\n\n","n":0.088}}},{"i":431,"$":{"0":{"v":"URL","n":1},"1":{"v":"\nA URL contains the following parts:\n1. The protocol (ie. scheme) in use\n    - tells the client what conventions to follow when exchanging messages with the server that has the website.\n2. The hostname of the server\n    - tell the client which server to contact\n3. The location of the file\n4. The arguments to the file e.g. query params\n","n":0.134}}},{"i":432,"$":{"0":{"v":"UDP","n":1},"1":{"v":"\nUDP is a no-frills, lightweight transport protocol, providing minimal services.\n\nUDP is connectionless, so there is no handshaking before the 2 processes start to communicate.\n\nUDP is a fire-and-forget protocol\n\nUDP is a disconnected protocol in which the recipient of a datagram doesn’t send any acknowledgment to the sender\n\nUDP provides low-overhead and faster transmission, but with less reliability and ordered delivery.\n\nUDP provides an unreliable data transfer service, meaning that when a process sends a message into a UDP [[socket|network.sockets]], UDP provides no guarantee that the message will ever reach the receiving process. Furthermore, there is not even a guarantee that the messages will arrive in the order they were sent.\n\nUDP does not perform flow control and does not retransmit lost packets like [[TCP|protocol.TCP]]\n\nUDP is a good choice in situations where delayed data is worthless. \n- ex. in a VoIP phone call, there probably isn’t enough time to retransmit a lost packet before its data is due to be played over the speakers. In this case, there’s no point in retransmitting the packet.","n":0.077}}},{"i":433,"$":{"0":{"v":"TCP","n":1},"1":{"v":"\n## Why use it?\n[[IP|protocol.IP]] is unreliable, in that we can't guarantee that the data that's sent is received without being corrupted. IP cannot even guarantee that the data arrives. \n- TCP is reliable. Interestingly, TCP is built on top of IP. The question is, how is a reliable technology built on top of an unreliable one? The answer is that TCP acts as sort of the manager of a transaction of data from one place to another. If data arrives corrupted, it will repeat the request until data comes in fully-formed. It ensures that missing packets are retransmitted, duplicates are eliminated, and packets are reassembled into the order in which they were sent.\n- Since the end-user never knows about the data-loss that might have occurred somewhere along the way, TCP is effectively an abstraction\n\nTCP considers a packet to be lost if it is not acknowledged within some timeout (which is calculated from observed round-trip times), and lost packets are automatically retransmitted.\n- Although the application does not see the packet loss and retransmission, it does see the resulting delay.\n\nTCP is said to exist between the [[sockets|network.socket]] of two processes\n\nOn the TCP level, the tuple (source ip, source port, destination ip, destination port) must be unique for each simultaneous connection. That means a single client cannot open more than 65535 simultaneous connections to a single server. But a server can (theoretically) serve 65535 simultaneous connections per client.\n- So in practice the server is only limited by how much CPU power, memory etc. it has to serve requests, not by the number of TCP connections to the server.\n\nThe connection between 2 processes via TCP is full-duplex, meaning the two processes can send messages to each other simultaneously.\n- When the application finishes sending a message, it must tear down the connection\n\n- TCP transports application-layer messages such as HTTP requests.\n- Works by establishing a connection between 2 hosts.\n- Data is guaranteed to be delivered, and in the order it was sent.\n- TCP breaks long messages in to shorter segments.\n\n### When not to use TCP\n- TCP is not a suitable protocol for applications that require data to be transmitted in real-time or with strict timing requirements, such as audio or video streaming applications, where even minor delays or packet loss can lead to a degraded user experience. In cases like this a protocol like [[UDP|protocol.UDP]] is preferred.\n\n### TCP flow control\nTCP performs flow control (also known as congestion avoidance or backpressure), in which a node limits its own rate of sending in order to avoid overloading a network link or the receiving node. \n- This means additional queueing at the sender before the data even enters the network.\n\n### Contrasting a TCP network connection with a Circuit-switched network (e.g. telephone connection)\nA phone network is synchronous. \n\nWhen you make a call over the telephone network, it establishes a circuit: a fixed, guaranteed amount of bandwidth is allocated for the call, along the entire route between the two callers. This circuit remains in place until the call ends.\n- an ISDN network runs at a fixed rate of 4,000 frames per second. When a call is established, it is allocated 16 bits of space per frame. Therefore, for the duration of the call, each side is guaranteed to be able to send exactly 16 bits of audio data every 250 microseconds.\n- even as data passes through several routers, it does not suffer from queueing, because the 16 bits of space for the call have already been reserved in the next hop of the network\n- also, because there is no queueing, the maximum end-to-end latency of the network is fixed. We call this a *bounded delay*.\n\nThis is very different from a TCP connection:\n- a circuit is a fixed amount of reserved bandwidth which nobody else can use while the circuit is established.\n- the packets of a TCP connection opportunistically use whatever network bandwidth is available.\n","n":0.039}}},{"i":434,"$":{"0":{"v":"SMTP","n":1},"1":{"v":"\n## The Process\n\nThe MTA receives the email from the MSA, which it receives from the MUA\n- Once the MTA gets the email, relaying comes into play— with the MTAs acting as the relays (SMTP relay)\n\t- In relaying, the email is forwarded to other MTAs if the recipient of the email is not hosted locally on the same system as the sender.\n- Once the relaying process has finished, the email is sent to the Mail Delivery Agent (MDA)— which is the last stop before being delivered to a client's mailbox\n- The whole process uses SMTP, except for the final stage, which is between MDA and and MUA, which uses POP3 or IMAP\n\t- In other words, the process of getting mail from the \"mail server\" onto the email client uses POP3/IMAP.\n\n![](/assets/images/2021-04-07-10-12-35.png)\n- MUA - Mail User Agent (email client)\n- MSA - Mail Submission Agent\n- MDA - Mail Delivery Agent\n- MTA - Mail Transfer Agent\n\n### Mail Transport Agent (MTA)\nA mail server can have many names: mail relay, mail router, Internet mailer. But the most common alias is an mail transport agent (MTA)\n- an MTA is responsible for transferring email among users on the internet\n- MTAs query the MX records and select a mail server to transfer emails\n- Usually, MTAs use a store-and-forward model of mail handling. This means that outgoing mail is put into a queue and waits for the recipient’s server response. An MTA will recurrently try to send emails. If the mail fails to be delivered during the established term, it will be returned to the mail client.\n\n#### MTA impact on email deliverability\nThere are three major factors that email deliverability is based on:\n- sender’s reputation\n- infrastructure & authentication\n- content\n\nThe reputation of the domain and IP address the email is sent from is the most important thing.\n- MTAs can protect and strengthen the reputation of the sender.\n\n##### Brand new IPs\nIf you’re building your reputation from scratch, you should not use your virgin IP address at full load. It has no email sending history and thus needs some warming up. An MTA will let you do this and later slowly increase sending capacity\n\n* * *\n\n## Breakdown of SMTP\n1. Alice sends an email to Bob. \n2. In reality, it goes directly to her mail server, where it is placed in a message queue.\n3. The client-side of SMTP (running on Alice's mail server) sees the message sitting in the queue.\n4. In response, it opes up a TCP connection to an SMTP server running on Bob's machine\n5. After the handshaking process is complete, the SMTP client sends Alice's message into the TCP connection.\n6. The message is received by Bob's mail server. The server then places the message in Bob's mailbox.\n","n":0.048}}},{"i":435,"$":{"0":{"v":"IP","n":1},"1":{"v":"\nIP is often described as an \"unreliable\" protocol because it does not provide any guarantees about the delivery of data packets.\n- packets are sent from a source device to a destination device without any acknowledgement of receipt or verification of the packet's successful delivery, meaning that packets can be lost, delayed, or corrupted in transit, and there is no built-in mechanism to detect or recover from these errors.\n\nEven though IP itself it unreliable, [[protocol.TCP]] is often built on top of it, which is in fact a reliable protocol.","n":0.107}}},{"i":436,"$":{"0":{"v":"FTP","n":1},"1":{"v":"\nThe user interacts with FTP via an FTP user agent.\n- once the hostname is provided by the client, the FTP client process establishes a TCP connection with the FTP server.\n    - after this, the user provides username/password to establish the connection.\n\nOne key difference between HTTP and FTP is that FTP uses two parallel TCP connections to transfer a file: a control connection, and a data connection.\n- *control connection* is for passing control information between the 2 hosts, such as username, password, commands to change remote directory, commands to get/put files etc.\n- *data connection* is for actually sending files back and forth.\n\nUnlike HTTP, FTP is stateful; the server maintains state about the FTP client, most importantly associating the connection with a particular user.\n- also it must maintain state about the user's present working directory of the server.\n- keeping this state significantly limits the total number of sessions that an FTP server can handle.\n\nFTP connections happen on port `21`\n","n":0.08}}},{"i":437,"$":{"0":{"v":"Product Design","n":0.707}}},{"i":438,"$":{"0":{"v":"User Testing","n":0.707},"1":{"v":"\nThink of user testing as high-level workflow tests. While atomic components, such as copy on a button would be tested through A/B testing \n- always let users drive the test. Instead of giving them imperative instructions (*click here, enter your email here, then click here*), give declarative tasks (*create a piece of information for your repository*)\n- Be careful not to use any verbiage that is specific to the app when giving users your app\n    - ex. for Never Forget, don't use the words nugget and bucket, because it gives them insight that organic users won't have the benefit of. Try to use more neutral words, like category.\n- As the user is going through your app, watch their face for signs of surprise or confusion, and question them on it. Make sure you understand why the user was feeling what they felt.\n    - Keep good data on this, because if you start to notice a pattern that occurs at expected areas of the app, then you know you have something that needs to be fixed.\n- Parrot people as they tell you things, because it allows them to expand on the feelings that prompted those words. \n    - \"I felt a little confused here\" —\"Oh, you felt confused here?\"\n- As they click on something, ask them how well it matched their expectations of what would happen when clicked on it.\n    - \"was *this* the screen you were expecting to go to after clicking on *that* button?\"\n- Listen for the *value gain* that the user is suggesting, rather than the specific implementation of that feature.\n    - When people make suggestions, focus on the \"why\" behind those suggestions. Don't focus too much on the implementation of their suggestion. Instead, if you put attention on what goal they are trying to facilitate within their suggestion, then you can extract useful potential information regarding the subject that is receiving the suggestions\n    - ex. Say you have an airbnb like product, and while doing user testing, a user suggests having a back and forward button right on the photo, that takes you to the next property in the list. Maybe you disagree with the design decision of putting the arrows on the main image, but you are missing a valuable piece of feedback here: users may want to navigate more quickly between properties, and may want to do it right from a property page. Take that goal in mind, and devise your own solution for it. The solution could be an overall better navigation system, or just placing the forward/back buttons somewhere else on the page. \n\n## UX Research\n- Everything to do with research can be placed on a grid by how qualitative/quantitative the feedback is, vs the origin of that feedback (what people say vs how they act)\n\n![fe3a55ac2df38afe29080c5838a915ba.png](:/5c1bfd2a9fea435ca57b5ae96c45453b)\n\n- Qualitative methods are largely unstructured, tend to be subjective, are at the softer end of science, and are about establishing insights and theories (which we then test, often using quantitative approaches).\n    - are open-ended (“How might you improve this customer journey?”) \n    - Qualitative methods lead to insights\n- Quantitative methods are largely: structured, tend to be objective, are at the harder — more measurable — end of science, and are about testing theories. They tend to be larger sample sizes and can be run in a more hands-off manner. With quantitative research, user behaviors and attitudes are gathered indirectly.\n    - tend to be yes/no (“Do you use this feature?”).\n    - quantitative methods allow you to test those insights brought by the qualitative methods.\n\n# A/B Testing\n- Think of A/B testing as atomic tests. You want to test which copy on a button is better. Which color is better. Which location of the text is better. What you don't want to test is which workflow is better\n    - If you were to A/B test two different workflows, it would be impossible to know the precise reason that one outperformed the other. At the end of the day, you aren't really learning anything valuable.\n\n\n# Misc\nThere is a distinction between brainstorming and asking for feedback. When asking for feedback you need to clearly present a single idea. When you are brainstorming you are trying to come up with new ideas.\n- Make sure all parties involved know what kind of conversation you are having.\n\n# UE Resources\n- [Jobs to be done (product design)](https://jtbd.info/)\n- [UX research cheat sheet](https://www.nngroup.com/articles/ux-research-cheat-sheet/)\n- [user testing tips](https://www.smashingmagazine.com/2017/11/improve-user-testing/)\n","n":0.037}}},{"i":439,"$":{"0":{"v":"Job to be Done","n":0.5},"1":{"v":"\nA JTBD is a prototype that one aspires to be. This prototype is designed by a company. The company will design a product that creates in you a \"desire\" to get a certain job done. By virtue of the process, the product then becomes the only means of fulfilling that new need and thereby \"getting the job done\".\n- Progress can only happen when we attach and integrate new ideas and new products into our lives. This is the basis for why people gain new jobs to be done. \n- When people start to understand there is a new *job to be done*, they start imagining the product integrated in their life, along with all of the benefits that the product provides.\n- Having a JTBD is a process. A JTBD is not a thing that consumers have, but rather something that they participate in (an analogy is falling in love, which is something we participate in.)\n- A JTBD is distinct from a task (which would be something like \"search a song and add it to your playlist\").\n- A JTBD describes \"a better me\". It answers the question, “How are you better since you started using [product]?”\n![fdf33f277db063fad011a2c2fda8b79c.png](:/8f843f4b9f7743bcb5c3c3a3b9386bf7)\n- People use only one solution at a time for a JTBD.\n- People that are in the process of discovering a solution to a JTBD engage in a mental simulation of that product helping them become better.\n- People don't want the product — they want help making their lives better.\n- JTBD is a useful framework because it helps you keep your mind on what core value you add to the customer's life. This drives innovation in the right direction. \n    - If you were a company that made horse-drawn carriages before cars were made, you should have realized at some point that the job to be done for your customers was to have specific (and imaginable) benefits that arise from being able to travel fast. As long as your product is doing the best job it can in satisfying that customer's job to be done, they will remain a client. As cars come along, customers would have much to gain by switching from your horse-drawn carriage, since the car helps the customer become more efficient in fulfilling his job to be done.\n- Linkedin using JTBD\n![](/assets/images/2021-10-27-20-09-48.png)\n    - All these options aren't describing the product, but are rather describing progress (how can you improve as a result of using the product).\n","n":0.05}}},{"i":440,"$":{"0":{"v":"Typography","n":1},"1":{"v":"\n*\"Typography is the craft of endowing human language with a durable visual form. Typography is clothing for words\"*\n- Type needs to be designed at:\n    - the macro, page-level, considering the overall structure of the page the typographic hierarchy\n    - and at the micro text-level, considering the details\n        - spacing, what words get bolded to draw focus.\n- Consider typographic pairings, a bold sans-serif typeface for headings catches the eye, coupling this with a classic serif for body copy helps aid legibility\n- to pair typefaces, find a ‘superfamily’ and build around that\n    - Consider contrasting typefaces for headings and body copy\n    - generally serif for body, and sans-serif for heading\n\n# Nice fonts\n- bryantLG \n- Kalam, cursive\n\n# UE Resources\n- [selecting typeface for body](https://practice.typekit.com/lesson/selecting-typefaces-for-body-text/)\n- [using shading on text](https://practice.typekit.com/lesson/using-shades/)\n","n":0.09}}},{"i":441,"$":{"0":{"v":"Copy","n":1},"1":{"v":"\n<!-- TODO: broken image -->\n<!-- ![](/assets/images/2021-07-03-21-10-39.png) -->\nIt is interesting what ExpressVPN does here. They tell you what they believe, which gives you added confidence in the company/product, since you can better predict what their activities would look like, based on their claimed beliefs. This has the added benefit of giving people the ability to make future assumptions, since they now know a little more about the company's philosophies.\n\nLook for opportunities to add video game terminology and concepts to your apps\n- Ex. \"Unlock\" is reminiscent of levels of a video game\n","n":0.105}}},{"i":442,"$":{"0":{"v":"Color","n":1},"1":{"v":"\n# UE Resources\nhttps://learnui.design/blog/color-in-ui-design-a-practical-framework.html\n","n":0.577}}},{"i":443,"$":{"0":{"v":"UX","n":1},"1":{"v":"\n# Introducing things the user hasn't seen before\nWhen prompting a user with a decision that is potentially impactful, it is a good idea to provide them with an option that says \"decide later\", then show them where to find that information when they need it\n- this example is from Apple when hooking up an external hardrive, when it asks if you'd like to set up Time Machine to sync with this HD\n\n### Introducing new features\n1. have a tooltip that pops up saying \"new feature coming soon: unlock phone with face\". then there is a button saying \"im interested\". now you have a reason to contact them again with the features. release\n2. when showing a user of a feature that they have not yet explored, it is better to direct them to the normal place where they will enable that feature, rather than just allowing them to turn it on or off right then and there\n\t- ex. in the Quora app, on the front page there was a link that said \"dark mode is here!\". when you click on it, instead of turning dark mode on, it opens the sidebar, and then at the bottom of the sidebar, there is an arrow pointing to the dark mode switch. this has the benefit of teaching users where to find that option so that in the future they already have this knowledge\n\n# UX Process\n- the UX process can be distilled down as follows. It is iterative and cyclical:\nObservation → Idea Generation → Prototyping → Testing\n\nwe need to understand the user's goals and motivations, then design experiences that deliver these high-level goals.\n- ex. With Never Forget, it's better to make the navigation items focused around \"actions\", since that maps to a goal of the user. They want to...\n    1. create a nugget\n    2. browse their nuggets\n    3. explore nuggets\n    4. learn their nuggets\n\n- Ensure every page or screen has a clear call-to-action\n    - Ask yourself, “What is the user trying to achieve?” and design with that in mind\n\n- on making a search box, ask yourself \"is it obvious that the user knows what they are searching for? even on first visit?\n- if searching is something they wouldn't understand on first visit and there is nothing to search anyway, consider hiding the search bar and give some information about what they'd search for instead\n    - ex. once you add some nuggets of knowledge, you can start to search through them here!\n\n## Pillars of design\n### Visual grammar\n- everything in a design can be broken down into points, lines and planes (plane - anything with a surface)\n- Every component should serve its purpose. Take away as much as you can while still serving the goal of the application.\n- When building UI components try and focus on re-using elements and minimizing visual complexity.\n    - With a core set of simplified and elegant components built, focus on combining these to create easily understandable interfaces that ease user interactions.\n### Language & Typography\n#### Language\n- Ask yourself: What’s the message? Then find the right words to communicate that message.\n- Use clear and concise language\n    - The words should be unambiguous (given it's context in the project) and should be as close as possible to describing what the user should expect to happen when engaging with (ex. clicking on) that content.\n- words can help set a tone and voice, as well as establish and reinforce the personality of a brand\n- **Macrocopy** - the words that relate to the heart of the brand. They are closer to the \"why\", and can be considered closely related to the content that marketing would use.\n    - One way of defining macrocopy is to develop a ‘brand dictionary’: a palette of words that defines the brand you’re working on\n    - ex. Consider two different car brands: smart and Mercedes. smart’s brand dictionary might include: agile, expressive, dynamic, fun and friendly. Mercedes’ brand dictionary might include: precision, luxury, stylish, engineered and efficient\n- **Microcopy** - the functional copy. These are the words we use to help guide users through the application.\n    - Microcopy should reflect your brand’s values but is more likely to be consistent from one project to another\n    - ex. Everyone understands what ‘Add to basket’ means, so don’t confuse your users by inventing your own terminology.\n\n### Narrative Design\n- consider rhythm when designing the user flow. Think of it as a narrative. We are taking the user through a journey, and we want to control the intensity of the information that is presented to them.\n    - Consider a James Bond movie. It starts off in full-action, but then it slows down, allows the coals to slow burn in the heat, building up pressure, as it gradually gets back into full-action.\n    - Too much too fast, and the user will be overwhelmed. Too slow, and the user will get bored and will be confused as to the value that is being added to them. Trick is to find the balance.\n\n* * *\n\n### Flexibility-Usability tradeoff\n- As a system gets more functionality and becomes more flexible to the user's desires, there is a natural tradeoff of ease of use of the product (in other words, it becomes more complex to navigate)\n- The ability of an audience to anticipate future uses of a product is a key indicator of how they will value flexibility versus usability in design. When an audience can clearly anticipate its needs, more specialized designs that target those needs will be favored. When an audience cannot clearly define its needs, more flexible designs that enable people to address future contingencies will be favored.\n\n# Concepts\n### Optimistic UI\n- ex. when you send a message in Whatsapp, there is a period of loading between the time you hit \"send\" and the time that message appears in the chat. A non optimistic UI will not show anything in the chat area until the request has completed, while an optimistic one will show the message instantly in the chat, but will show a loader next to it, to indicate the request is still in motion.\n\n# User-Centric design\n### Communicating chronology from a user's perspective\njoplin forums have beautifully user-centric way of showing chronology of comments. Most sites will have a date associated with a comment. However, Joplin augments this by also inlcuding a tag that says \"13 days later\". This effectively gives you the element of chronology that is often ignored when a computer centric, more abstract thing like the date is only given. Showing a date gives no sense of chronology.\n\n### Result-focused (declarative) menu buttons\n[this site](https://vincit.github.io/objection.js/guide/models.html#examples) has done an interesting thing. At the top, there is a link with a star on it, so that people can \"Star\" the repo. However, it simply links to the repo on github. There is an interesting distinction here, in that the maker of the site is defining an action, even though the user cannot actually execute that action on the website. In other words, they need to be shot off to another site in order to take that action, or \"star\" that github repo. This is an example of being more \"declarative\" in your thinking when designing user interfaces. Make things to be more \"action\" focused, rather than what it is you are doing. In the linked website, imagine that the button had just read \"Github\". Fewer people would have clicked \"star\". By giving the user an action to do, they took it.\n- It is generally a better idea to focus buttons on the result, rather than the action. This becomes more true as the idea gets more abstract.\n\n# Allowing user to discover new things in the app\n### Steam Browse set\nSteam has an interesting feature on their app. you get the concept of a browse set, which is a list of 10 new games, shown to you based on your interests. After you've seen all ten, if you want to see more, you can get another set of ten\n\nThis is interesting because it gives a sense of completion to the app. Think of it in terms of WoW daily quests. It gives you a reason to log in to complete your task for the day.\n\nThis is a good idea to get people in NF to add nuggets to their repo. This also blends nicely with the fact that If our pricing model is based around a threshold, then this will encourage users to get to that limit quicker\n\n### Helping users understand why they may not be able to find something\n- User's tend to look for things to do in an application. Because of the way your business logic is set up, the user may not be able to do a particular action. This is the moment where we need to show the user what they *would have* seen, had that business logic been satisfied\n\t- ex. on Freelancer.com, you can't review a freelancer until the payment has been released from the project. Because this is not an entirely obvious thing (in fact other sites do let you do this), freelancer.com should have a highlighted-out section of the freelancer's profile. Otherwise, the user spends a lot of time looking around the site for the ability to rate, and will never find a trace of it. This is a bad UX, and one that could be drastically improved.\n- The idea here is that users want to figure something out. It is easy to focus on only one half of this sentence. Most of the time when you hear \"help users accomplish goals\", we often forget that this desire is being fulfilled if we put up markers along the way that help users navigate their way.\n\n* * *\n\nwhat really counts is not the number of clicks it takes me to get to what I want (although there are limits), but rather how hard each click is—the amount of time that elapses between each click\n- ex. if there are only 2 buttons to click, but I really have to think about what to choose, that is still a bad experience\n\n100ms is the roof for an imperceivable delay.\n- up to 300ms is still considered \"fast\", but the delay can be noticed\n\n# Notable UX Designs\n- [Big Apple Hotdogs](http://www.bigapplehotdogs.com/)\n- [Jira](https://www.atlassian.com/software/jira/mac)\n- iOS \"Shortcuts\" app\n- Mongo Cloud (Atlas) SaaS\n- Blinkist\n- Raspberry Pi\n- Dropbox\n- DigitalOcean\n\t- Specifically the wizards to create droplets, clustered dbs etc.\n\n# UE Resources\n- [wow moment 1](https://www.forentrepreneurs.com/time-to-wow/)\n- [wow moment 2](https://www.appcues.com/blog/finding-your-products-first-wow-moment)\n- [laws of ux](https://lawsofux.com/)\n- [activity centered design](http://bokardo.com/archives/activity-centered-design/)\n- [ux design series (current progress)](https://www.smashingmagazine.com/2018/01/comprehensive-guide-ux-research/)\n- [guide to microcopy](https://www.wheelhousedmg.com/blog/a-brief-guide-to-ux-writing-microcopy-and-content-design/)\n","n":0.024}}},{"i":444,"$":{"0":{"v":"flows","n":1}}},{"i":445,"$":{"0":{"v":"onboarding","n":1},"1":{"v":"\nWealthsimple does a good job of explaining why they need what they are asking from you. People are wary to fill out information, especially when it's personal like address or SIN. Wealthsimple puts us at easy by telling us why they ask, and they let us know that it's ultimtely for our benefit\n![](/assets/images/2022-03-14-22-14-49.png)\n","n":0.137}}},{"i":446,"$":{"0":{"v":"Laws","n":1}}},{"i":447,"$":{"0":{"v":"Services","n":1},"1":{"v":"\n### Hick's Law\n\"The time it takes to make a decision increases with the number and complexity of choices.\"\nApplication of law:\n- Distinguish essential content from secondary content. By enabling users to find a path through fewer choices, you’ll reduce their cognitive burden.\n- ex. When creating navigation instead of providing an endless list of choices, focus on just a few.\n\n### Fitt's Law\n\"The time it takes to acquire a target is a function of the distance to and size of the target.\"\n- ie. The farther away a target is — a button on a screen, for example — the larger it needs to be for a user to be able to reach it easily.\n\n### Miller's Law\n\"The average person can only keep 7±2  items in their working memory. In short: there’s only so much we can hold in our heads in a short space of time.\"\n- Miller’s law is particularly important when we consider how we organize and group information, and is where chunking can come in useful\n- Where possible look for groups of information that can be broken down and chunked, enabling them to be held more easily in users’ working memory.\n\n","n":0.073}}},{"i":448,"$":{"0":{"v":"UI","n":1},"1":{"v":"\n\n- Mimic familiar objects and environments in abstract contexts (e.g., software interfaces) to imply the way in which new systems can be used\n- A satisfactory area alignment can be achieved by positioning an object along the axis of alignment such that an equal amount of area or visual weight hangs on either side — if the object had mass, it would be balanced on the axis.\n- quotes should be aligned based on the text edge and not on the quotation marks\n- When objects are simple and symmetrical, align based on their edges; otherwise, align based on their areas.\n- When designs involve simple and recognizable patterns, consider removing or minimizing the elements in the design that can be supplied by viewers.\n- Angularly-shaped (sharper) objects are more effective at attracting attention and engaging thought; contoured objects are more effective at making a positive emotional and aesthetic impression.\n- Whitespace should be as intentional as content\n\n## Design Principles\n- When designing an interface for users to do things, consider where you expect their eyes to naturally be. Position things in such a way that it takes advantage of where their eyes will be.\n    - Imagine a simple login page. There it a field for username and password, a link below those fields that say \"forgot password?\", and a button in the top right concerner saying \"Log in!\". The natural movement of the eyes for this layout is to look at username, look at password. with their eyes now being at password, they must jump all the way to the top right corner. \"Don't make me think\" states don't make them needlessly thing. A login is simple. The user should be able to do it without even using text anywhere on the page. Don't make them read the text on such an engrained activity as logging in.\n\n## Quotes\n- “Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away”\n\n## UE Resources\n- [7 Rules for creating gorgeous UI design](https://learnui.design/blog/7-rules-for-creating-gorgeous-ui-part-1.html#rule-1-light-comes-from-the-sky)\n\n## UI Inspiration\n- [reddit list](https://www.reddit.com/r/androiddev/comments/4cezsx/where_do_you_usually_go_for_app_ui_design/)\n- [collectui](https://collectui.com/)\n- [UI Design Daily](https://uidesigndaily.com/)\n- DiscordApp\n\n## Resources\n- [Background PNG generator](https://app.haikei.app/)\n    - allows you to make background with squiggly lines, blobs, circles etc.\n\n\nQuestrade:\n- 4390 Raven","n":0.053}}},{"i":449,"$":{"0":{"v":"Components","n":1},"1":{"v":"\n## Component design principles\n- set your touch target (ie. buttons) size to 9 mm square or greater (48×48 pixels on a 135 PPI display at a 1.0x scaling).\n- Keep the number of character per line within 60-80 (30–40 for mobile)\n\n* * *\n\n## Atomic components\n### Navbars\n- when the page the user is on is where an action is accomplished, a navbar is not needed. Presumably, the user came to this part of the app with intention and thus does not need to navigate freely to other parts of the app so easily. In lieu of the missing navbar, there should be a back button available at the top of the screen so the navbar is realistically only 1 navbar click away.\n\n### Icons\n- when picking which icon to represent which idea, think more in terms of what the outcome of the icon helps you accomplish, rather than the method it uses to get there\n    - Google Slides uses the icon of a highlighter to symbolize the act of highlighting text. Makes sense why they would do that, but it's misguided. While you are using a piece of software you and want to accomplish a goal like highlighting text, your eyes naturally start looking for a symbol that shows the result. In other words, they start looking for an icon perhaps with text and a colored background. Instead, they should have been looking for a symbol that is easily mistaken with a pen. This creates friction, and understandably a lot of friction. \n\n### Lists\nanything that can be a bulleted list probably should be.\n","n":0.062}}},{"i":450,"$":{"0":{"v":"Presenting","n":1},"1":{"v":"\nThe purpose of presenting is not to explain the concept. Rather, it is to get the audience excited about whatever it is you are talking about. The fact that there are other people in the audience is key. The energy of the audience helps to sell your concept. As a result of this, a presentation should be full of stories. It's not an expose on what you're presenting.\n- anal: fish on rice and sushi are not the same thing. \"Fish on rice\" is what it is, \"sushi\" is how we market and present it to the world.\n- original idea from Seth Godin\n","n":0.099}}},{"i":451,"$":{"0":{"v":"Postgraphile","n":1},"1":{"v":"\nPostGraphile automatically detects tables, columns, indexes, relationships, views, types, functions, comments and more. It builds a GraphQL server that is highly intelligent about your data, and that automatically updates itself without restarting when you modify your database.\n\nPostGraphile provisions, sets up and tears down a PostgreSQL client automatically for each GraphQL query\n- Setup involves beginning a transaction and setting the relevant session variables, e.g. using your JWT or the pgSettings function\n\n### Smart Comments\n- in postgres, we can make user-friendly remarks on a table called `comments` by using the COMMENT statement\n- Postgraphile leverages this feature to be able to alter functionalities by adding smart comments as comments on a table\n\n## Function\n**Computed Column vs. Custom Query**\n- We can differentiate between computed columns and custom queries by observing that computed columns must accept the table they belong to as a first arg. Of course, to be able to create a computed column we need a table to attach it to.\n- Consider the expansive nature of computed columns. They allow us to augment our existing tables with data that doesn't normally belong there.\n\n### Computed column\n- def - a psuedo column that we can attach to a table that will automatically be reflected in the graphql schema\n- in a function, running `setof nugget` will return a connection\n\n### Custom Queries\n- similar to computed columns, but instead of the function being callable as a node's field (ex. the `buckets` field on `nugget`), the function is callable from the root-level\n- ex. `all_nuggets_with_bucket_id` function\n\n## Auth\n- Postgraphile can generate JWTs easily from inside your PostgreSQL schema.\n\t- to do this, we define a `jwtToken` composite type and we pass it to `jwtPgTypeIdentifier`, and now every time that type is returned from a postgres function, it will be signed with the jwtSecret, and return it as a jwt token as part of the graphql response.\n- when the server receives a jwtToken from the request's authorization headers, like so:\n```\n{\n  \"aud\": \"postgraphile\",\n  \"role\": \"app_user\",\n  \"user_id\": 27\n}\n```\nit will automatically run this code:\n```\nset local role app_user;\nset local jwt.claims.role to 'user_login';\nset local jwt.claims.user_id to '27';\n```\n\n## Exposing HTTP requests to Postgres\n- `pgSettings` lets us set the jwt within postgres' `current_setting` while having access to the request\n\t- this function fires on each request, and everything returned by it will be applied to `current_setting` (with `set_config`)\n\t- ex. we can get the userId from the request and update the value of `user_id` within `current_setting(...)`\n\t- `pgSettings` is a function that can be async\n- instead of passing an object, we can pass `pgSettings` function that will get executed on each request.\n- Everything returned by `pgSettings` is applied to the current session with `set_config($key, $value, true)`\n- You can use `pgSettings` to define variables that your Postgres functions/policies depend on\n\t- When adding variables for your own usage, the keys must contain either one or two periods (`.`)\n\t\t- Variables without periods will be interpreted as internal Postgres settings, such as role, and will be applied by Postgres\n\t\t- All settings are automatically reset when the transaction completes\n\t- Here's an example of switching the user into the Postgres 'visitor' role, and applying the application setting jwt.claims.user_id:\n\t```\n\tpgSettings: async req => ({\n\t    'role': 'visitor',\n\t    'jwt.claims.user_id': req.user ? req.user.id : undefined,\n\t    //...\n\t}),\n\t```\n- role is overridden after pgSettings is applied\n\t- But only if `pgDefaultRole` is set or there's a role property in the claim of the JWT\n\n## Exposing HTTP requests to resolvers\n- `additionalGraphQLContextFromRequest` is an optionally asynchronous function that has access to the req and res objects from your HTTP library (Express)\n\t- The result returned from the function is merged into the GraphQL context object which is passed as the third argument to every GraphQL resolver\n\t- `additionalGraphQLContextFromRequest` let us perform asynchronous actions if we need to, for example looking up the current user in the database with Passport. Once all that is done, we can return an object from this function that will merge with the existing `context` object so the resolvers can access it.\n\n## Graphile Workers\n- allows you to run jobs (e.g. sending emails, performing calculations, generating PDFs, etc) \"in the background\" so that your HTTP response/application code is not held up\n[repo](https://github.com/graphile/worker)\n\n## Extending Graphql Schema (makeExtendSchemaPlugin)\n- when we use `makeExtendSchemaPlugin`, we can define types and resolvers that will get merged into the existing ones generated by Postgraphile\n- the callback returns an object with 2 keys:\n\t1. `typeDefs`\n\t2. `resolvers`\n\t\t- an object whose keys are graphql types, which resolve to an object with `key`-`value` pair of `field`-`resolver function`\n- the `build` argument is supplied to the `makeExtendSchemaPlugin` callback, and it contains lots of information and helpers defined by various plugins\n\t- includes the introspection results, inflection functions, and SQL helper (`build.pgSql`, an instance of `pg-sql2`, a query builder)\n\n* * *\n### Custom mutations/queries\n- By default Postgres assumes all functions will mutate the database. Therefore, if we want the postgres function to show up as a query, we need to mark it `stable`\n- when naming custom functions that get back the user some data, we need to name it as if it were a simple property on an object. We don't want to name is something like `getUsers`. Instead, we want to simply call it `users`. This makes more sense when viewing it from the graphiql perspective and querying via graphql.\n\n* * *\n\n### Pooling\n- if we are using postgraphile, `rootPgPool` Postgraphile doesn't know about it, as we don't pass it to the postgraphile engine. Instead, it is used in the `additionalGraphQLContextFromRequest` callback.\n\n* * *\n\n### Scalar Types\nPostGraphile generates the following scalar types:\n- BigFloat, BigInt, BitString, Boolean, CidrAddress, Date, Datetime, Float, Int, InternetAddress, Interval, JSON, KeyValueHash, MacAddress, MacAddress8, String, Time, UUID\n\n## Resources\n### Quality Repos\n[graphile/starter - lots of really quality code. check out how some of the lower level db config functions work](https://github.com/graphile/starter)\n### Quality Docs\n[lots of overall good info, inc high level setting up the SQL](https://github.com/graphile/postgraphile/blob/v4/examples/forum/TUTORIAL.md)\n","n":0.033}}},{"i":452,"$":{"0":{"v":"Utils","n":1}}},{"i":453,"$":{"0":{"v":"gql","n":1},"1":{"v":"\nThe gql helper is responsible for turning the human-readable GraphQL schema language you write into an abstract syntax tree ([[AST|graphql.ast]]) that the application can understand.\n\nthe `gql` helper differs slightly from the one from `graphql-tag`, namely in how the placeholders work.\n- This `gql` function is designed to work with PostGraphile's inflection system, so you can embed strings directly\n- We can also embed other gql tags directly\n\n```js\nconst nameOfType = \"MyType\"; // Or use the inflection system to generate a type\n\n// This tag interpolates the string `nameOfType` to allow dynamic naming of the\n// type.\nconst Type = gql`\n  type ${nameOfType} {\n    str: String\n    int: Int\n  }\n`;\n\n// This tag interpolates the entire definition in `Type` above.\nconst typeDefs = gql`\n  ${Type}\n\n  extend type Query {\n    fieldName: Type\n  }\n`;\n```\n","n":0.091}}},{"i":454,"$":{"0":{"v":"Embed","n":1},"1":{"v":"\nIf we want to embed a raw js value (function, string, object etc.) into a document, we can use the `embed` function within the `gql` tagged template literal\n- most common use case is to pass the value to a directive\n","n":0.158}}},{"i":455,"$":{"0":{"v":"Schema","n":1},"1":{"v":"\n## `makeExtendSchemaPlugin`\nAllows us to extend the graphql schema that is generated by Postgraphile\n- It does this by allowing us to define additional Graphql Types and Resolvers and merge them into our schema.\n- To accomplish this, the callback we pass to `makeExtendSchemaPlugin` should return the `typeDefs` (schema definition) and the `resolvers` function\n\n### `typeDefs`\nThis allows us to specify how our Graphql schema will be extended.\n- ex. Will we be adding a new subscription on it? A new InputType?\n\n### `build`\nThe `build` argument to the `makeExtendSchemaPlugin` contains information and helpers defined by various plugins in the Postgraphile ecosystem\n- most importantly, it includes:\n\t1. introspection results (`build.pgIntrospectionResultsByKind`)\n\t2. inflection functions (`build.inflection`)\n\t3. an SQL helper (`build.pgSql`, an instance of `pg-sql2`\n\n### `selectGraphQLResultFromTable`\nThis helper populates data that is returned from our resolver\n- It should *not* be used to retrieve data for our resolver to process.\n\t- Instead use `context.pgClient` directly.\n\nThis helper should not be called more than once per resolver (which wouldn't make sense anyway)\n","n":0.081}}},{"i":456,"$":{"0":{"v":"Views","n":1},"1":{"v":"\nWe can use postgres views in such a way that it is exposed on the graphql schema to be consumed, as if it were an actual table.\n\nImagine we have the following:\n```sql\nCREATE TABLE app_public.films (\n  id serial PRIMARY KEY,\n  name text,\n  release_year int,\n  kind text\n);\n\nCREATE VIEW comedies AS\n    SELECT *\n    FROM app_public.films\n    WHERE kind = 'Comedy';\n```\n\nThe view `comedies` is able to be queried directly on the Graphql schema:\n```gql\n{\n  comedies(first: 20) {\n    name\n    releaseYear\n  }\n}\n```\n","n":0.117}}},{"i":457,"$":{"0":{"v":"Directives","n":1}}},{"i":458,"$":{"0":{"v":"@requires","n":1},"1":{"v":"\nWhen extending a schema, it's often because you want to expose data from Node.js that would be too difficult (or impossible) to access from PostgreSQL\n- in other words, we come to a situation where we don't want to rely on Postgres to process and give us data, but instead want to rely on Node to do the job for us.\n\n## USD to CAD example\n- Imagine in our database we store the price of a product in USD. However, we want to be able to query the `priceInCad`, directly as a field on the graphql schema:\n```gql\nquery Product {\n\tproduct {\n\t\tpriceInCad\n\t}\n}\n```\n\nWe can accomplish this by making a custom Postgraphile plugin, which would extend the Graphql schema to include the `priceInCadCents` field on the `Product` type:\n```js\nconst MyForeignExchangePlugin = makeExtendSchemaPlugin(build => {\n  return {\n    typeDefs: gql`\n\t\t\textend type Product {\n\t\t\t\tpriceInCadCents: Int! @requires(columns: [\"price_in_us_cents\"])\n\t\t\t}`,\n    resolvers: {\n      Product: {\n        priceInCadCents: async product => {\n          // Note that the columns are converted to fields, so the case changes\n          // from `price_in_us_cents` to `priceInUsCents`\n          const { priceInUsCents } = product;\n          return await convertUsdToCad(priceInUsCents);\n        },\n      },\n    },\n  };\n});\n```\n\nwe include the `@requires` directive to show that in order to calculate this field, we need the postgres column `price_in_us_cents`\n","n":0.071}}},{"i":459,"$":{"0":{"v":"Pgsubscription","n":1},"1":{"v":"\nProvided by the `@graphile/pg-pubsub` library\n\nThis directive allows us to embed a function that will calculate the PostgreSQL topic to subscribe to based on the arguments and context passed to the GraphQL field\n","n":0.177}}},{"i":460,"$":{"0":{"v":"Pubsub","n":1},"1":{"v":"\nGraphile defines a library `pg-pubsub`\n\nThe main high-level purpose of this library is provide our custom plugins with realtime data, which allows us to add subscription fields to our API.\n- The library uses Postgres' `LISTEN`/`NOTIFY` to provide realtime features\n\nThis library also gives us the `@pgSubscription` directive that we can put on graphql fields\n\n[[Pub-Sub|general.architecture.pub-sub]]\n","n":0.139}}},{"i":461,"$":{"0":{"v":"Philosophy","n":1},"1":{"v":"\n### Default Heuristic\nInstead of searching for the best option, use a technique we call the default heuristic. The premise of this heuristic is that when the cost of acquiring new information is high and the consequence of deviating from a default choice is low, sticking with the default will likely be the optimal choice.\n- It should be any option that gives you very high confidence that it will work.\n- It is something you’ve used before.\n- Something you understand well.\n- Something that has proven itself to be a reliable way for getting things done in the space you’re operating in.\n- It doesn’t necessarily have to be the theoretical best choice.\n- It doesn’t have to be the most efficient. Or the latest and greatest.\n- It simply needs to be a reliable option to get you to your ultimate desirable outcome.\n- It should be very unlikely to fail you; you have to be confident that it’s a very safe bet. In fact, that’s the only requirement.\n","n":0.078}}},{"i":462,"$":{"0":{"v":"Automation","n":1},"1":{"v":"\n![](/assets/images/2021-03-07-22-40-00.png)\n- benefits of automation (not related to time savings):\n\t- prevents you from having to context switch often\n\t- provides you with self-documented code that demonstates how to accomplish a certain task\n\t- removes your need to remember to do a certain task\n","n":0.158}}},{"i":463,"$":{"0":{"v":"Postgres","n":1},"1":{"v":"\nPostgres is a [[DBMS|db.DBMS]].\n- \"SQL tells the database what information you want, and the DBMS determines the best way to provide it.\"\n\nThink of postgres as a [[microservice|general.arch.microservice]] that is stateful.\n- Further, don’t think of it as a storage layer, but rather as a concurrent data access service. The service is capable of handling data processing. When we think of Postgres only as our data layer, we tend to overlook the power that Postgres can actually bring to the table.\n\t- anal: think of The Matrix, where Neo awakes in the pod near the start of the movie. If each pod were a piece of data, the mechanical arms that move and manipulate the pods would be Postgres.\n\t\t- In actuality, it would be both— the whole system. Limiting our thinking to just the data also limits our thinking in terms of what we consider Postgres to be capable of.\n\nIn PG, data consistency is maintained by using a multiversion model ([[db.strategies.concurrency-control.MVCC]]).\n- This means that each SQL statement sees a snapshot of data (a database version) as it was some time ago, regardless of the current state of the underlying data.\n- [source](https://www.postgresql.org/docs/current/mvcc-intro.html)\n\n### Performance\nIt's possible to scale Postgres to storing a billion 1KB rows entirely in memory - This means you could quickly run queries against the full name of everyone on the planet on commodity hardware and with little fine-tuning.\n- Postgres can easily handle 10,000 insertions per second.\n\nIf we have the SQL do the data related heavy lifting, there will often be a net gain in performance. Mostly, it is because round-trip times and latency along with memory and bandwidth resources usage depend directly on the size of the result sets.\n\nIt's rarely a mistake to start with Postgres and then switch out the most performance critical parts of your system when the time comes.\n\n# UE Resources\n- [Exercises for learning SQL with PG](https://pgexercises.com/)\n- [PG simple explanations](https://www.avestura.dev/blog/explaining-the-postgres-meme)\n- [Postgres Do's and Don'ts](https://wiki.postgresql.org/wiki/Don't_Do_This)\n","n":0.056}}},{"i":464,"$":{"0":{"v":"Tools","n":1},"1":{"v":"\n- [SchemaSpy: tool for documenting database](http://schemaspy.org/)\n- [Citus: Convert into distributed database](https://www.citusdata.com/)\n\t- To your application it still looks like a single database, but then under the covers it’s spread across multiple physical machines and Postgres instances.\n","n":0.169}}},{"i":465,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\nPostgreSQL includes many SQL tests to validate its query parser, optimizer and executor. It uses a framework named the *regression tests suite*, based on a very simple idea:\n\n1. Run an SQL file containing your tests (using psql)\n2. Capture its output to a text file that includes the queries and their results\n3. Compare the output with the expected one that is maintained in the repository with the standard diff utility\n4. Report any difference as a failure\n\nYou can have a look at PostgreSQL repository to see how it’s done, as an example we could pick [src/test/regress/sql/aggregates.sql](https://github.com/postgres/postgres/blob/master/src/test/regress/sql/aggregates.sql) and its matching expected result file [src/test/regress/expected/aggregates.out](https://github.com/postgres/postgres/blob/master/src/test/regress/expected/aggregates.out).\n- Implementing regression testing like this is trivial, since the driver is only a thin wrapper around executing standard applications such as *psql* and *diff*\n\nThe idea would be to always have a setup and a teardown step in your SQL test files, wherein the setup step builds a database model and fills it with the test data, and the teardown step removes all that test data.\n\n# Tools\n[RegreSQL: Regression testings for Postgres](https://github.com/dimitri/regresql)\n","n":0.076}}},{"i":466,"$":{"0":{"v":"Pgtap","n":1},"1":{"v":"\npgTAP is a suite of database assertion functions that make it easy to write [[TAP|testing.TAP]]-emitting unit tests \n- These tests can be written in 2 ways:\n    - scripting-style unit testing, typical with TAP test frameworks\n    - xUnit-style test functions\n\nWith pgTAP, we can just compare values directly in the database. There is no need to do any extra work to get the database interface to talk to the database, fetch data, convert it, etc. You just use SQL\n\nExample test:\n```sql\n-- Start a transaction.\nBEGIN;\nSELECT plan( 2 );\n\\set domain_id 1\n\\set src_id 1\n\n-- Insert stuff.\nSELECT ok(\n    insert_stuff( 'www.foo.com', '{1,2,3}', :domain_id, :src_id ),\n    'insert_stuff() should return true'\n);\n\n-- Check for domain stuff records.\nSELECT is(\n    ARRAY(\n        SELECT stuff_id\n          FROM domain_stuff\n         WHERE domain_id = :domain_id\n           AND src_id = :src_id\n         ORDER BY stuff_id\n    ),\n    ARRAY[ 1, 2, 3 ],\n    'The stuff should have been associated with the domain'\n);\n\nSELECT * FROM finish();\nROLLBACK;\n```\n\n## Schema testing\nThere are a wealth of assertion functions to test the schema\n`has_table`, `col_is_pk`\n","n":0.081}}},{"i":467,"$":{"0":{"v":"Terms","n":1},"1":{"v":"\n### Operation\n`select`, `insert`, `update`, `delete`\n","n":0.447}}},{"i":468,"$":{"0":{"v":"Tablespaces","n":1},"1":{"v":"\nTablespaces are where PostgreSQL stores the database objects. Therefore, it is an abstraction between the physical and logical layers (ie. what the data looks like on disk, and what the data looks like in sql-format.)\n- Tablespaces allow you to move your data to different physical locations across drivers easily by using simple commands.\n\t- ex. 2 different objects in one schema might have 2 different underlying tablespaces.\n- By default, PostgreSQL provides you with two tablespaces:\n\t1. The `pg_default` is for storing user data.\n\t2. The `pg_global` is for storing system data.\n- A common use of tablespaces is for performance\n\t- ex. an index that is used often can be tablespaced on a faster SSD. \n\t- ex. a table with cold data that is rarely accessed can be tablespaced on a slower magnetic HDD.\n- At the file layer (of data storage), a single file can only correspond to a single tablespace.\n- Using tablespaces, we can provision different locations for our data, based on the idea of classifying data as hot, warm or cold (determined by frequency that data will be needed)\n\t- This would involve us having a different tablespace in each storage group (ie. a different path to the physical localtion). This means that moving data from hot to warm involves changing the tablespace that data is associated with.\n\nhttps://techchannel.com/SMB/9/2012/storage-groups-hot-warm-cold#:~:text=The%20classifications%20are%20often%20referred,stored%20on%20even%20slower%20storage.\n","n":0.068}}},{"i":469,"$":{"0":{"v":"Table","n":1}}},{"i":470,"$":{"0":{"v":"Joins","n":1},"1":{"v":"\na JOIN is a query that accesses multiple rows of the same or different tables at one time\n```sql\nSELECT * FROM student_grades\n\tJOIN students ON student_grades.student_id = students.id\n```\n- every JOIN must have an ON\n\t- if the column names are identical in both tables, you can use USING\n- we may alias the tablename for one of 2 reasons: firstly, it's convenient, and secondly we might join to the same table several times, requiring us to distinguish between columns from each different time the table was joined in.\n```sql\nselect bks.starttime as start, facs.name as name\n\tfrom\n\t\tcd.facilities facs\n\t\tinner join cd.bookings bks\n\t\t\ton facs.facid = bks.facid\n\twhere\n\t\tfacs.facid in (0,1) and\n\t\tbks.starttime >= '2012-09-21' and\n\t\tbks.starttime < '2012-09-22'\norder by bks.starttime;\n```\n- any time you write `FROM table1, table2` (ie. including 2 tables instead of 1), you are writing a JOIN, even though it doesn't explicitly say `JOIN`\n- all filtering of rows and columns should be done before the JOIN\n\n- Joins are fairly expensive, which might be a reason to denormalize.\n![83fe4afa6ddc9f8d5b3665cfde631660.png](:/cad05cee711947d682fb8e53e85ab34c)\n\n## Inner Join\n- create new tables with common data between the two reference tables.\n- Will not include rows where one of the relevant piece of data are missing\n\t- ex. if joining `students` tables and `cohorts` table, and one student doesn't have a `cohort_id`, then that student won't be included\n- The common columns are typically the primary key columns of the first table and foreign key columns of the second table\n- with inner joins, order doesn't matter\n- the inner join will examine each row in the first table and compare each row's value (specified after `ON`). If these values are equal, the inner join creates a new row that contains columns from both tables\n\t- ex. `... FROM table1 INNER JOIN table2 ON table1_name_col = table2_name_col`\n- ~99% of the time, we want to use an INNER JOIN\n- if we made a venn diagram of table1 and table2, an inner join would give us the intersect.\n\t- ie., performing an inner join will give us the columns(?) in common between the 2 tables\n![820988c3645cc0df3da155abc938228f.png](:/066d1e23ea0a45738633ae20ae3f38ee)\n- you can join a single table with itself. You may do this if you want to derive information from the table (that is not explicitly stored), while using it to determine\n\t- ex. below we are getting \"the members who have bee recommended by another member\". since we are using data within the same table to do the determination, we need to join with itself.\n```sql\nselect\nrecs.firstname as firstname,\nrecs.surname as surname\nfrom cd.members mems\njoin cd.members recs\non recs.memid = mems.recommendedby\norder by surname, firstname\n```\n","n":0.05}}},{"i":471,"$":{"0":{"v":"Table Inheritance","n":0.707},"1":{"v":"\nImagine we had 2 data sets: cities and capitals. A naive approach might be to create 2 tables called `non_capitals` and `capitals`, then join them together in a UNION\n- This approach is bad because problems arise once we need to update multiple rows at once.\n\nInstead, a better solution is to use inheritance:\n```sql\n cities (\n  name       text,\n  population real,\n  elevation  int     -- (in ft)\n);\n\nCREATE TABLE capitals (\n  state      char(2) UNIQUE NOT NULL\n) INHERITS (cities);\n```\n- here, a row of `capitals` will inherit all columns from its parent, and will gain an additional column `state`\n- a table can inherit from more than one other table\n- using the ONLY clause, we can prevent the query from running over tables below the specified table\n\t- ex. in the capital cities example, if we query `SELECT * FROM ONLY cities...`, then we prevent capitals from showing up in the result set.\n","n":0.083}}},{"i":472,"$":{"0":{"v":"System","n":1}}},{"i":473,"$":{"0":{"v":"Information Schema","n":0.707},"1":{"v":"The information_schema is a schema that contains a set of views that contain information about the objects defined in the database.\n\nThe information_schema is part of the SQL standard, unlike the [[system catalog|pg.system.catalog]]\n\n- As a consequence of this, you will not find information on Postgres-specific features in the information schema. An example of this is the fact that we can get all user-defined types on `information_schema.user_defined_types`, but we cannot get enums. Enums aren't part of the SQL spec, and therefore are not findable through the information_schema.\n","n":0.108}}},{"i":474,"$":{"0":{"v":"System Catalog","n":0.707},"1":{"v":"The system catalogs are just regular tables that are updated automatically in response to events.\n- ex. the `pg_database` catalog is updated in response to `CREATE DATABASE MY_DB`\n\nThe system catalogs are the place where a RDBMS stores:\n- schema metadata, such as information about tables and columns\n- and internal bookkeeping information\n\nSystem catalogs are _not_ part of the SQL standard, and are core to Postgres, unlike the [[information schema|pg.system.information-schema]], which is SQL-compliant\n\n","n":0.12}}},{"i":475,"$":{"0":{"v":"Seed","n":1},"1":{"v":"\n### Generate_series\n```sql\nINSERT INTO app_public.entities (id)\nSELECT gen_random_uuid()\n    FROM generate_series(1, 10) x\n;\n\nINSERT INTO app_public.organizations (id, entity_id, owner_id, display_name)\nSELECT gen_random_uuid() AS id, e.id AS entity_id, gen_random_uuid() AS owner_id, random() AS name\n    FROM app_public.entities e\n;\n```\n\n# UE Resources\n[Use generate_series](https://www.citusdata.com/blog/2018/03/14/fun-with-sql-generate-sql/)\n[For fake data generation, have a look at pyramation's faker]\n","n":0.152}}},{"i":476,"$":{"0":{"v":"Scaling","n":1},"1":{"v":"\n### Scaling PG horizontally vs vertically\nThe ability to scale horizontally is impacted by the rate of change in the primary database. The primary can be making many changes concurrently. Those changes are serialized, written to the WAL files, and the WAL information is forwarded to the secondary systems. The secondary systems *must* process this data serially. This induces *lag* into the replicant. It will “fall behind” the primary. When will it catch up? It is unpredictable. We know that it will eventually catch up, assuming that the rate of change on the primary is not constant. If you have a system which can tolerate the inherent lag in [[replication|db.distributed.replication]], then Postgres can scale quite well. However, if your workload can not tolerate this lag, you would think it does not scale well at all.\n\nIf it is about read-heavy workload then you should just add replicas. Add as many replicas as you need to handle the whole workload. You can balance all the queries across the replicas in the round robin fashion.\n\nIf it is about write-heavy workload then you should partition your database across many servers. You can put different tables on different machines or you can shard one table across many machines. In the latter case you can shard a table by a range of the primary key or by a hash of the primary key or even vertically by rows. In each of the cases above you may lose transactionality, so be careful and make sure that all the data changed and queried by a transaction be resided on the same server.\n\nHorizontal scaling in relational databases is hard to achieve because when you have tables (or shards of the same table) across the different cluster nodes, joins usually become very inefficient. Additionally, there is a problem of replication and keeping ACID guarantees while ensuring that all replicas have fresh data\n\n### Streaming Replication\n- streaming replication allows us to continuously apply WAL XLOG records to standby servers, so that they are kept current\n\t- WALs are write-ahead logs and are synonymous with, which are XLOG are transaction logs (X stands for transaction)\n","n":0.053}}},{"i":477,"$":{"0":{"v":"Query","n":1}}},{"i":478,"$":{"0":{"v":"Common Table Expression","n":0.577},"1":{"v":"\nA CTE is basically a way to set a computed result set to a variable, which we can then later use (eg. to join against)\n\na CTE is a temporary result that we can reference with another SELECT.\n- A CTE always returns a result set\n- They are used to simplify queries\n\t- ex. you could use one to eliminate a derived table from the main query body.\n\nA good use-case for a CTE is when we imagine that our result set is grouped by date. Imagine that we want to get a result set showing sales by date. We could certainly query a single table and order by date, but what happens when there are no sales on a particular day? We would end up with gaps in our result set. To solve this, we can make a CTE and join our tables to it.\n```sql\nwith v_date as (\n\tselect current_date\n)\n```\n\n### CTE and UPDATE..FROM\nWe can use a CTE in combination with `UPDATE..FROM` to update every row in the result set of a different query (the CTE)\n- Here, we are updating the `book_orders.purchase_status` column on book_orders that have been `PENDING` for more than 24 hours:\n```sql\nWITH old_books AS (\n\tselect * from app_public.book_orders as book_orders\n\twhere book_orders.purchase_status = 'PENDING'\n\tand created_at < NOW() - interval '24 hours'\n)\nupdate app_public.book_orders set purchase_status = 'EXPIRED'\nfrom old_books\n```\n\n[more](https://www.essentialsql.com/introduction-common-table-expressions-ctes/)\n","n":0.069}}},{"i":479,"$":{"0":{"v":"Pub Sub","n":0.707},"1":{"v":"\nVanilla Postgres allows us to define workers which watch a new events channel, and attempt to claim a new job whenever one is pushed to the channel.\n- Postgres also lets other services watch the status of the events with no added complexity.\n","n":0.154}}},{"i":480,"$":{"0":{"v":"Psql","n":1}}},{"i":481,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\nAll psql commands are just convenience wrappers around underlying SQL statements that are being executed\n- ex. `\\l+` executes an sql query on the `pg_catalog.pg_database` table, joining the `pg_catalog.pg_tablespace` table\n\n## Connecting to psql (from shell)\nEither:\n```\npsql -d mydb -h localhost -p 5432 -U kyle\n```\nor simply:\n```\npsql -d postgresql://kyle@localhost:5432/mydb\n```\nnote: `@` symbol must be entered as `%40` to properly interpret it.\n- ex. if hostname=`kyle@db1464.azure.com`, I connect like `psql -d postgresql://kyle%40db1464.azure.com`\n\n# CLI (psql/pgcli)\n- show help for a command - `\\h <sql command>`\n- connect to db - `\\c mydb`\n- show schemas - `\\dn`\n- display tables in \"public\" schema - `\\dt public.*`\n- display views in \"public\" schema - `\\dv public.*`\n- display table signature `\\d <TABLENAME>`\n- display types `\\dT <SCHEMA>`\n- display enum values ` \\dT+ <TYPE>`\n- display s - `\\du`\n- display schema - `\\dn`\n- display functions - `\\df`\n- display function definition - `\\ef app_public.register_user`\n- display extensions - `\\dx`\n- read in commands from a sql file- `\\i`\n  - ex. `\\i migration.sql`\n  - this is an alternative to shell-level `psql -h localhost -p 5432 -U kyletycholiz -d f1db < migration.sql`\n- set variable in psql - `\\set DATABASE_OWNER nf_dev`\n- set variable's value from command line - `psql --variable \"DATABASE_OWNER=nf_dev\"`\n- list all databases, and show their respective size on disk - `\\l+`\n\n- display info about current connection - `\\conninfo`\n  - current user, database, port etc.\n\n- cycle command history - `<C-r>`\n\n## Named Queries\nCreate a new named query\n```psql\n\\ns <name> select * from ...\n```\n\nCall named query\n```psql\n\\n <name>\n```\n\nDelete a named query\n```psql\n\\nd <name>\n```\n\n### Positional Parameters\nNamed queries support shell-style parameter substitution. Save your named query with parameters as placeholders (e.g. $1, $2, $3, etc.):\n\n```psql\n\\ns user_by_name select * from users where name = '$1'\n```\nWhen you call a named query with parameters, just add the parameters after the query's name. You can put quotes around arguments that include spaces.\n\n```psql\n\\n user_by_name \"Skelly McDermott\"\n```\n","n":0.059}}},{"i":482,"$":{"0":{"v":"Copy","n":1},"1":{"v":"\n## \\copy\n- This is the client-side version of the copy command. It gives us a distinct benefit, which is the source file is required on the client side. In other words, we can execute the copy command on the same machine as where the source file is stored; we don't need to upload the file to the postgres server first.\n- copy is much faster than running `insert into...`\n\n","n":0.121}}},{"i":483,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Copy from CSV into Postgres table\n1. In psql, create a new table that matches the format of the csv headers\n2. In psql, run the COPY command:\n```sql\nCOPY cars\nFROM '/Users/kyletycholiz/Downloads/cars.csv' \nDELIMITER ',' \nCSV HEADER;\n```\nnote: if we don't want to have the table columns match the csv headers, we can specify like so:\n```sql\nCOPY cars(id, name)\n...\n```\n","n":0.137}}},{"i":484,"$":{"0":{"v":"Prod","n":1}}},{"i":485,"$":{"0":{"v":"Logs","n":1},"1":{"v":"\nPostgreSQL implements the `application_name` parameter, which you can set in the connection string and with the SET command within your session. It is then possible to have it reported in the server’s logs, and it’s also part of the system activity view `pg_stat_activity`.\n\nIt is a good idea to be quite granular with this setting, going as low as the module or package level, depending on your programming language of choice. It’s one of those settings that the main application should have full control of, so usually external (and internal) libs are not setting it.\n- doing this, we can find out which part of the code is sending a specific query we can see in the monitoring, in the logs or in the interactive activity views.\n\n","n":0.089}}},{"i":486,"$":{"0":{"v":"Plpgsql","n":1},"1":{"v":"\nPL/pgSQL is an interpreted language, which means that the function body is not evaluated for syntactical soundness as in compiled programming languages like Pascal or C++. Therefore you can write a function which only reveals any errors when it is invoked.\n\nThink of sql commands as having the potential to return rows. In this sense, they are like functions.\n- in javascript we can do something like this:\n```js\nconst returnedValueFromFn = fn()\n```\n\n- in plpgsql we can do something like this:\n```sql\nselect * from users returning * into v_users;\n```\n\nSince these sql commands are like functions, we can imagine that `v_user_id` is an argument to the following query:\n```sql\nselect * from users where id = v_user_id;\n```\n- When executing a SQL command in this way, PL/pgSQL may cache and re-use the execution plan for the command\n\n# E Resources\n[Return multiple fields without making new type](https://stackoverflow.com/questions/4547672/return-multiple-fields-as-a-record-in-postgresql-with-pl-pgsql)\n","n":0.086}}},{"i":487,"$":{"0":{"v":"Types","n":1},"1":{"v":"\nWe can get the type of a column like so:\n```sql\nSELECT pg_typeof(now());\n+--------------------------+\n| pg_typeof                |\n|--------------------------|\n| timestamp with time zone |\n+--------------------------+\n```\n","n":0.236}}},{"i":488,"$":{"0":{"v":"Record","n":1},"1":{"v":"\nA type that holds a single row from a result-set (therefore, not just tables)\n\n`record` type is similar to `row-type`, but `record` doesn't have a predefined structure. The structure only gets established when the `select` or `for` statements assin an actual row to it.\n\nTo access a field in the record, you use the dot notation (.) syntax like this:\n```sql\nrecord_variable.field_name;\n```\n\na record is not a true data type. It is just a placeholder.\n- Also, a record variable can change its structure when you reassign it\n","n":0.11}}},{"i":489,"$":{"0":{"v":"Statements","n":1}}},{"i":490,"$":{"0":{"v":"Raise","n":1},"1":{"v":"\n`RAISE` can be an easy and powerful way to self-document the code.\n\n## Levels\n- DEBUG\n- LOG\n- INFO\n- NOTICE\n- WARNING\n- EXCEPTION (default level)\n - raises an error, aborting current transaction\n\n## Where and when messages get sent\nraising with different levels will generate messages of different priority levels.\n- Where or not a message of a given priority will be sent to the client or not is determined by the `log_min_messages` and `client_min_messages` configuration variables.\n\t- This file also determines if we write to server log\n\n## Body\nAfter specifying the level, we can provide a simple string literal, which can be followed by optional argument expressions to be inserted into the string\n```\nRAISE NOTICE '% %', v_job_id, v_job_name;\n```\n\n### Using\nWe can attach additional info to the error by combining `using` with an option, one of:\n- MESSAGE\n- DETAIL\n- HINT\n- ERRCODE\n\t- Specifies the error code (SQLSTATE) to report, either by condition name or directly as a five-character SQLSTATE code\n- COLUMN\n- CONSTRAINT\n- DATATYPE\n- TABLE\n- SCHEMA\n```\nRAISE EXCEPTION 'Nonexistent ID --> %', user_id\n      USING HINT = 'Please check your user ID';\n```\n","n":0.077}}},{"i":491,"$":{"0":{"v":"Perform","n":1},"1":{"v":"\nallows us to do the work of a SELECT statement, but then immediately discard the result\n- useful for when we are doing side-effects with no useful result.\n\n```sql\nPERFORM create_mv('cs_session_page_requests_mv', my_query);\n```\n\n```sql\nPERFORM graphile_worker.add_job(\n\t'user__audit',\n\tjson_build_object(\n\t\t'type', 'reset_password',\n\t\t'user_id', v_user.id,\n\t\t'current_user_id', app_public.current_user_id()\n\t));\n```\n\n\nrole","n":0.174}}},{"i":492,"$":{"0":{"v":"Assert","n":1},"1":{"v":"\n`ASSERT` is a convenient shorthand for inserting debugging checks, but is not useful for reporting ordinary messages and errors.\n\n- we can give `ASSERT` a value that is always expects to evaluate to true.\n\t- if it does, `ASSERT` does nothing further.\n\t- if it doesn't, an `ASSERT_FAILURE` exception is raised.\n- if we don't pass a message, a default \"assertion failed\" will be used.\n```sql\nASSERT v_user is not null, 'user not found, check the users table';\n```\n","n":0.118}}},{"i":493,"$":{"0":{"v":"Setof","n":1},"1":{"v":"\nWhen our plpgsql function returns a `setof` rows, we must follow a different procedure:\n- the items that are returned by the function are specified by the `RETURN NEXT` or `RETURN QUERY` commands.\n- at the end, a final `RETURN` command with no argument, to indicate the function has finished execution.\n\n`RETURN NEXT` and `RETURN QUERY` do not actually cause execution to return from the function call. Instead, they simply append zero or more rows to the function's result set.\n- As successive RETURN NEXT or RETURN QUERY commands are executed, the result set is appended to further. The final `RETURN` command causes control to exit the function, leaving us with the dataset that has been constructed up until that point\n\t- note: naturally, we can omit the final `RETURN` if we are at the last line anyway\n","n":0.087}}},{"i":494,"$":{"0":{"v":"For Loops","n":0.707},"1":{"v":"\n# For Loops\n```sql\ndo\n$$\ndeclare\n\trec record;\nbegin\n\tfor rec in select title, length\n\t\tfrom films\n\t\twhere length > 50\n\t\torder by length\n\tloop\n\t\traise notice '% (%)', rec.title, rec.length;\n\tend loop;\nend;\n$$\n```\n","n":0.218}}},{"i":495,"$":{"0":{"v":"Performance","n":1},"1":{"v":"\nThe vast majority of slow queries found in the wild are still queries that return way too many rows to the application, straining the network and the server's memory\n\n## Query Plans \nPostgreSQL devises a query plan for each query it receives. Choosing the right plan to match the query structure and the properties of the data is absolutely critical for good performance, so the system includes a complex planner that tries to choose good plans. You can use the EXPLAIN command to see what query plan the planner creates for any query\n\n### EXPLAIN ANALYZE\nTells us things like how is PostgreSQL trying to find the record (using a sequential scan or hash, etc), costs, timing, and performance information.\n\n\"EXPLAIN ANALYZE is the most powerful tool at our disposal for understanding and optimizing SQL queries\"\n- this is a command that accepts a statement such as SELECT ..., UPDATE ..., or DELETE ..., executes the statement, and instead of returning the data provides a query plan detailing what approach the planner took to executing the statement provided.\n\nThis is how you might go about using explain to understand run time characteristics of your queries:\n```sql\nexplain (analyze, verbose, buffers)\n<query here>;\n```\n\n* * *\n\nThe combination of Bitmap Index Scan and Bitmap Heap Scan is much more expensive than reading the rows sequentially from the table (a Seq Scan)\n\nSeq Scan nodes often indicate an opportunity for an index to be added, which is much faster to read\n\n[source](https://thoughtbot.com/blog/reading-an-explain-analyze-query-plan)\n\n# Tools\n[Explain Depesz: explain analyze made readable](https://explain.depesz.com/)\n","n":0.064}}},{"i":496,"$":{"0":{"v":"Operators","n":1},"1":{"v":"\noperators are declared in the system catalog `pg_operator`\n- Every entry in pg_operator includes the name of the procedure that implements the operator and the class OIDs of the input and output types.\n\nTo view all variations of the “||” string concatenation operator, try:\n```sql\nSELECT oprleft, oprright, oprresult, oprcode\nFROM pg_operator WHERE oprname = '||';\n\noprleft|oprright|oprresult|oprcode\n-------+--------+---------+-------\n     25|      25|       25|textcat\n   1042|    1042|     1042|textcat\n   1043|    1043|     1043|textcat\n(3 rows)\n```\n","n":0.128}}},{"i":497,"$":{"0":{"v":"Union","n":1},"1":{"v":"\n\"Take 2+ tables with equal number of columns and combine them into a single result set.\"\n- ie. combines results of 2+ SELECTs into a single result set.\n- \"append\" could have also been an appropriate name\n\nUNION is conceptually opposite of JOIN; it expands the result set in the other direction (y instead of x)\n- JOIN is for taking 2 tables and combining their columns, while UNION is for taking 2 tables and combining their rows(ie. records)\n```sql\nSELECT select_list_1\nFROM table1\nUNION\nSELECT select_list_2\nFROM table2\n```\n- The number and the order of the columns in the select list of both queries must be the same.\n- The UNION operator by default removes all duplicate rows from the combined data set (`UNION ALL` will bring them back)\n- In practice, you often use the UNION operator to combine data from similar tables, which are not perfectly normalized\n- ex. with 2 tables: `popular_movies`, `top_rated_movies`, return the movies found in both tables, removing any duplicates along the way\n<!-- TODO fix imagine -->\n![8d2bb4d874e9ee042506b53f0a5eb8cd.png](:/202885018e6045e4acd744771788c297)\n","n":0.079}}},{"i":498,"$":{"0":{"v":"Intersect","n":1},"1":{"v":"\nReturns only the rows that are available in both result sets.\n- ex. with 2 tables: `popular_movies`, `top_rated_movies`, return only the movies that appear in both tables\n\n<!-- TODO: -->\n![b4e5d3a9f4dd1214eb4273e9d24fa8c7.png](:/da1f2c53c1e349bb944f4f33172bb9b6)\n","n":0.189}}},{"i":499,"$":{"0":{"v":"Except","n":1},"1":{"v":"\nLike UNION and INTERSECT, EXCEPT returns distinct rows from the first (LHS) query that are not in the output of the second (RHS) query.\n- ex. with 2 tables: `popular_movies`, `top_rated_movies`, return the movies that are popular, but not top rated.\n\t- assuming we have `SELECT * FROM popular_movies` first\n<!-- TODO: fix this -->\n![daddae8a308e1bf9405e038b101affbd.png](:/d8dc07ccdd01418c816ddaf6951dfdd7)\n","n":0.139}}},{"i":500,"$":{"0":{"v":"Any","n":1},"1":{"v":"\n`any` compares a scalar value (ie. a base type, like `text` or `number`) to a set of values returned by a subquery.\n\nThe form is:\n```sql\nexpression operator ANY(subquery)\n```\nIn this syntax:\n- The subquery must return exactly one column.\n- The `ANY` operator must be preceded by one of the following comparison operator =, <=, >, <, > and <>\n- The `ANY` operator returns true if any value of the subquery meets the condition, otherwise, it returns false.\n\nNote: `SOME` is a synonym for `ANY`, meaning that you can substitute `SOME` for `ANY` in any SQL statement.\nNote: The `= ANY` is equivalent to IN operator.\n","n":0.101}}},{"i":501,"$":{"0":{"v":"Lang","n":1},"1":{"v":"\n### Single vs Double Quotes\nDouble is for identifiers (tables, columns, schemas, functions); single is for values (mostly strings)\n","n":0.236}}},{"i":502,"$":{"0":{"v":"Views","n":1},"1":{"v":"\nViews are named queries stored in the database. For queries that we perform often, we can effectively give them an alias. The result set of that query is conceptually placed into a table, which is referencable by the name we give it.\n- ex. Imagine we have a join that we perform often. We can put it in a view and get that data like so:\n```sql\nCREATE VIEW myview AS\n    SELECT city, temp_lo, temp_hi, prcp, date, location\n        FROM weather, cities\n        WHERE city = name;\n\nSELECT * FROM myview;\n```\n- Being liberal with use of Views is a good practice, as they allow us to encapsulate details of the structure of our tables behind consistent inferfaces\n\t- This value shows itself as the structure of our tables change, yet we continue to have a consistent inferface.\n- Besides the read-only views, PostgreSQL supports updatable views.\n","n":0.085}}},{"i":503,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n## Data Types\n### Composite Type\n- a type defined by the \"signature\" of the row. In other words, the very combination of columns included in a row make up a **composite type**\n\t- therefore, every column in a given table is an example of a composite type. In fact, every time we create a table, a composite type representing that row's composition is made.\n- PostgreSQL allows composite types to be used in many of the same ways that simple types can be used.\n\t- ex. a column of a table can be declared to be of a composite type.\n\t\t- spec: for instance `first_name` and `last_name`?\n- In Postgres, columns can be of composite types, meaning that we can have a type defined as (email: string, username: string), and have a column in a table called `identity` and have a column with that type.\n- When used in a function, composite types can be thought of as objects, as we would access the value of each subtype with `$1.key`\n\n### Text\n- don't use VARCHAR. just use TEXT, and add length limits\n\n#### Citext\n- Case insensitive text\n- behaves like text, except that it lower-cases everything before comparing it\n\t- Therefore, we may want to store things like email or postal code in `citext` rather than `text`.\n\n### Serial\n- SERIAL is the postgres equivalent of autoincrement in other flavors of SQL\n\t- Consider using uuid's though.\n\n### Range\n- We can specify a range in a single column (ex. timestamps, price etc).\n- The real benefit is that you can then have constraints\n\t- ex. certain time stamps can’t overlap\n- To work with ranges, use the [range operators](https://www.postgresql.org/docs/9.3/functions-range.html)\n[Demo](https://wiki.postgresql.org/images/7/73/Range-types-pgopen-2012.pdf)\n","n":0.062}}},{"i":504,"$":{"0":{"v":"Timestamp","n":1},"1":{"v":"\nFor timestamp with time zone, the internally stored value is always in UTC\n- An input value that has an explicit time zone specified is converted to UTC using the appropriate offset for that time zone.\n\t- If no time zone is stated in the input string, then the system's TimeZone parameter is used.\n\n`+00:00` indicates an `hour:minute` timezone offset\n\n### Timezone\nTimezone can be changed like:\n- `set timezone to 'Europe/Paris';`\n\n### Timestamp vs Timestamptz (with timezone)\nImagine we use `timestamptz`. Now 2 users that are haflway around the world from each other can insert data into the database, and the same exact timestamp will be recorded.\n\nFurthermore, the time that appears in our database will actually change if we change the timezone that is set in the pg client:\n```sql\nset timezone to 'Europe/Paris'\n-- ts looks like this: `2021-04-29 17:51:42.316944+02`\n\nset timezone to 'Pacific/Tahiti'\n-- ts now looks like this: `2021-04-29 05:54:15.419514-10`\n```\n\nTstz can be thought of a way of abstracting timezones. If I am looking at a set of data, I can directly compare all timezones because of this normalization.\n- ex. `2021-04-29 08:51:42.316944-07` and `2021-04-29 08:52:15.419514-07` are the timestamps recorded of 2 users in different timezones, 1 minute apart from each other.\n\nIf you manage an application with users in different time zones and you want to display time in their own local preferred time zone, then you can set timezone in your application code before doing any timestamp related processing, and have PostgreSQL do all the hard work for you.\n\nEven when using timestamps with time zone, PostgreSQL will not store the time zone in use at input time, so there’s no way from our tstz table to know that the entries are at the same time but just from different places.\n\n### Functions\n###### now()\n- returns the timestamp of the current transaction.\n\t- Therefore, the result is equal if we call `now()` in different parts of the transaction.\n\n###### clock_timestamp()\n- returns us the actual timestamp of when the code is being executed.\n\t- Therefore, if we use this in a transaction, the actual time an operation took place *will* be recorded\n\n# References\n[Timezone List](https://www.postgresql.org/docs/8.1/datetime-keywords.html#DATETIME-TIMEZONE-SET-TABLE)\n","n":0.054}}},{"i":505,"$":{"0":{"v":"Range","n":1},"1":{"v":"\n## Inclusive/Exclusive Bounds\n- `[]` is used to represent inclusive values\n- `()` is used to represent exclusive values\n- ex. `SELECT '[3,7)'::int4range` is a range that includes 3, but discludes 7\n\n## Omitting Upper/Lower Bounds\nWe can omit upper bounds with `[7,]`\nWe can omit lower bounder with `[,7]`\n","n":0.151}}},{"i":506,"$":{"0":{"v":"Point","n":1},"1":{"v":"\nPoint is a [geometric type](https://www.postgresql.org/docs/9.4/datatype-geometric.html), which are types that represent two-dimensional spatial objects\n\nPoints are the fundamental two-dimensional building block for geometric types, and are specified in one of 2 ways:\n```\n(x, y)\nx, y\n```\n","n":0.177}}},{"i":507,"$":{"0":{"v":"Numeric","n":1},"1":{"v":"\n`decimal` is an alias for `numeric`\n\nthere are 2 values inherent to a `numeric` value\n1. Precision - total $ of significant digits in a number (ie. size of integer part plus size of decimal part)\n2. Scale - size of decimal part. (therefore integers have scale of 0)\n- these 2 values should be declared explicitly\n\nexample:\n```sql\nnumeric(3,2)\n```\nmeans \"3 digits total, with 2 occurring past the decimal\"\nie. `1.00` and `0.23`, but not `1.235`\n","n":0.121}}},{"i":508,"$":{"0":{"v":"Money","n":1},"1":{"v":"\nDon't use `money`.\n- Instead use `decimal` (aka. `numeric`) with 2 units of forced precision\n\nif you only care about prices to 2 decimal places, then you shouldn't really care about using anything more precise than numeric type with 2 precision decimal places. However, this would coerce incoming values like `44.423` to be rounded to `44.42`, and the amounts could be very far off. Therefore, if we are controlling the prices coming in, and we always know that it will be to 2 decimal places then it shouldn't be a problem.\n- If we are really concerned about this, then we can use microcurrencies, which store the value as a bigint 1 millionth of the numeric value. This allows lots of precisio\n","n":0.092}}},{"i":509,"$":{"0":{"v":"Json","n":1},"1":{"v":"\nJSON is primarily intended to store whole documents that do not need to be manipulated inside the RDBMS\n- Updating a row in Postgres always writes a new version of the whole row. That's the basic principle of Postgres' MVCC model. From a performance perspective, it hardly matters whether you change a single piece of data inside a JSON object or all of it: a new version of the row has to be written.\n- Ideally, JSON documents should each represent an atomic datum that business rules dictate cannot reasonably be further subdivided into smaller datums that could be modified independently.\n\n## Working with JSON\n### Functions\nimagine we pass a serialized json object as an argument to a postgres function like so:\n```\n{\n\tusername,\n\tavatar_url,\n\temail,\n\tfirst_name\n}\n```\nWe can then unpack that arg into a *declared* postgres variable:\n```sql\ndeclare\n\tv_email = profile_object ->> 'email';\n-- imagine this as `email = profile_object.email`\n-- `v_email :=` syntax is identical\n```\n\n#### -> vs ->>\n`->`\nReturns the field as `JSON`\n- because of this, we must use this for chaining to access deeply nested fields:\n```sql\nSELECT info -> 'items' ->> 'product' as product\n```\n\n`->>`\nReturns the field as `text`\n\n#### Contains (`@>`)\nWe can check the JSON to see if it contains certain key-value pairs:\n```sql\nselect media_items\nfrom nuggets\nwhere media_items @> '{ \"type\": \"text\" }'\n```\n\n#### JSON vs JSONB\nIn general, most applications should prefer to store JSON data as jsonb, unless there are quite specialized needs, such as legacy assumptions about ordering of object keys.\n\n##### JSON\n- JSON - stores an exact copy of the input text, which processing functions must reparse on each execution\n\t- Because the json type stores an exact copy of the input text, it will preserve semantically-insignificant white space between tokens, as well as the order of keys within JSON objects\n- JSON values can be manipulated, but must be cast to `text` first.\n\n##### JSONB\n- JSONB - stored in a decomposed binary format that makes it slightly slower to input due to added conversion overhead, but significantly faster to process, since no reparsing is needed\n\t- also supports indexing on GIN and GIST index types.\n\t- does not preserve the order of object keys\n- gives us capability to query into our JSON document for quick lookups\n\n* * *\n\n# E Resources\nhttps://www.postgresqltutorial.com/postgresql-json/\n\n# UE Resources\n[PG plugin to query jsonb data](https://github.com/postgrespro/jsquery)\n","n":0.053}}},{"i":510,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Update statement to change jsonb value\n```sql\nUPDATE test\nSET data = replace(data::TEXT,': \"my-name\"',': \"my-other-name\"')::jsonb\nWHERE id = 1;\n```\n\n#### Use variable/function within JSON insertion\nEasiest way is to just use string concatenation (`||` or `concat()`)\n\nIf value is integer:\n```sql\n('[{\"specCode\": {\"name\": \"Telephone Number\", \"text\": \"TEL_NUM\"}, \"specValue\": {\"code\": null, \"text\":' || tel || '}}]')::json\n```\n\nIf value is not integer (difference in quotes used):\n```sql\n('[{\"specCode\": {\"name\": \"Telephone Number\", \"text\": \"TEL_NUM\"}, \"specValue\": {\"code\": null, \"text\":\"' || tel || '\"}}]')::json\n```\n\n#### Overwriting json value\nHere, we move values from the `list_price` and `paid_price` columns into a combined jsonb column:\n```sql\nupdate app_public.invoice_items as invoice_items\n\tset\n\t\tamount = concat('{\"list_price\":', invoice_items.list_price, ',\"paid_price\":', invoice_items.paid_price, '}')::jsonb\n\twhere id = ii_row.id;\n-- result {\"list_price\": 1465, \"paid_price\": 817}\n```\n","n":0.099}}},{"i":511,"$":{"0":{"v":"Interval","n":1},"1":{"v":"\nAllows us to store and manipulate a period of time.\n```sql\ninterval '2 months ago';\ninterval '3 hours 20 minutes';\ninterval '2 month - 1 day';\n```\n\nInternally, PostgreSQL stores interval values as months, days, and seconds. The months and days values are integers while the seconds can field can have fractions.\n\nThe interval values are very useful when doing date or time arithmetic. For example, if you want to know the time of 3 hours 2 minutes ago at the current time of last year, you can use the following statement:\n```sql\nSELECT\n\tnow(),\n\tnow() - INTERVAL '1 year 3 hours 20 minutes'\n             AS \"3 hours 20 minutes ago of last year\";\n```\n\n### Proper types for selected columns\n```sql\nselect album.title as album,\n\tsum(milliseconds) * interval '1 ms' as duration\n\t...\n```\n","n":0.093}}},{"i":512,"$":{"0":{"v":"Enum","n":1},"1":{"v":"\nEnums in postgres are non-DDL-safe\n","n":0.447}}},{"i":513,"$":{"0":{"v":"Date","n":1},"1":{"v":"\n#### current_date\nspec: when doing \"math\" on dates that don't have a time attached to them, then midnight is used:\n```sql\nselect count(*)\nfrom app_public.users\nwhere created_at > current_date\nand created_at < current_date + interval '1 day'\n```\nIf current_date gets rounded back to midnight, then, the first `where` is saying \"give me all the rows created since midnight\", and the second `where` is saying \"until midnight 24 hours later\"\n\n### Extract\nusing the `extract` function, we can get parts of a date out of a date object\n```sql\nextract('isodow' from date) as dow\n-- 4 (ISO day of week)\nextract('isoyear' from date) as year\n-- 2002 (ISO year)\nextract('week' from date) as week\n-- 52\n```\n\nWe can combine this with `trunc_date` to extract out sub-parts:\n```sql\nselect extract('year' from date_trunc('decade', date)) as decade\n```\n","n":0.094}}},{"i":514,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Create enum\n```sql\ncreate type mood as enum ('SAD', 'OK', 'HAPPY');\n```\n\n### Add value to enum\n","n":0.267}}},{"i":515,"$":{"0":{"v":"Triggers","n":1},"1":{"v":"\nA trigger is a callback that is executed whenever a table (or view) is modified. Triggers can also be set to listen for specific user actions much like a callback.\n- Can also be set up to be executed by using the INSTEAD OF condition.\n\n2 main types of trigger:\n1. row-level trigger\n2. statement-level trigger\n- the difference between these two is in how many times each would be called in response to an event. \n\t-  ex. if you issue an UPDATE statement that affects 20 rows, the row-level trigger will be invoked 20 times, while the statement level trigger will be invoked 1 time.\n\nexamples\n- restrict DML (actions that modify the db) operations to business hours \n- Automatically generate derived column values\n\nWe can think of a trigger like a middleware that sits between the user's request (that interacts with the db) and the sql server. Every time our db is interacted with, listeners are able \"intercept\" the query and act on it\n- Most triggers are only activated by either INSERT or UPDATE statements.\n- We can specify columns on a trigger, which will cause the trigger to only fire if those columns are operated on (ex. we update those columns)\n- If multiple triggers of the same kind are defined for the same event, they will be fired in alphanumerical order.\n\t- which is why it is useful to prepend them with numbers\n\n## Syntax\n- A trigger that is marked `FOR EACH ROW` is called once for every row that the operation modifies.\n- a trigger that is marked `FOR EACH STATEMENT` only executes once for any given operation\n- `WHEN` allows us to determine whether or not the trigger should be fired\n\t- In row-level triggers the `WHEN` condition can examine the old and/or new values of columns of the row\n\t- ex. only execute the function if `OLD.balance` does not equal `NEW.balance`\n\t\t- `WHEN (OLD.balance IS DISTINCT FROM NEW.balance)`\n\t- ex. only execute function if anything has changed\n\t\t- `WHEN (OLD.* IS DISTINCT FROM NEW.*)`\n- `CONSTRAINT` allows us to adjust the timing of when the trigger actually fires.\n\t- The trigger can be fired either:\n\t\t- At the end of the statement which caused the triggering event\n\t\t- At the end of the containing transaction, which is the `COMMIT` (called deferred triggers)\n\t- Each constraint has its own IMMEDIATE or DEFERRED mode.\n\t- only available `AFTER ROW`\n\n# Parts of a Trigger\n- 2 parts to a trigger: the trigger itself, and the function that is executed by the trigger\n\n## Trigger\n\n### Before/After Triggers\nThe trigger can be specified to fire before the operation is attempted on a row, or after the operation has completed (after constraints are checked and the INSERT, UPDATE, or DELETE has completed)\n- Trigger can also be set to fire *instead of* having the operation be performed. This only works on views\n\n#### Before\n- the constraints will not-yet have been checked, allowing us to perform some action before the actual operation has taken place.\n\t- ex. Imagine we want to keep only one of the user's credit cards specified as `is_primary`. Because of the uniqueness constraints applied on the table (allowing only one `is_primary` card per user), we must set `is_primary` to `false` for all of our cards, any time the user attempts to insert a new card where `is_primary` is true. \n\t\t- If we had instead ran the trigger *after* the `insert`, then we might have gotten an error, since we are potentially trying to insert a new card with `is_primary = true`, while one may already exist.\n- If the trigger fires before the event, the trigger can skip the operation for the current row, or change the row being inserted (for INSERT and UPDATE operations only)\n- in `BEFORE`, the `WHEN` condition is evaluated just before the function is executed\n\t- Therefore, using `WHEN` (in the trigger itself) has the same effect as testing the same condition at the beginning of the triggered function with `TG_WHEN`\n\t- The implication of this is that the `NEW` value seen by the function is the current value, and not the value that would exist *following* the operation\n\t\t- Also consider that while this \"current value\" normally means \"prior to the operation being performed\", there is the possibility that previous triggers in the chain have already executed, potentially changing what \"current value\" means to us.\n\t- Another implication is that in `BEFORE`, a trigger's `WHEN` condition cannot examine the columns of the `NEW` row that is to be inserted/updated (because they wouldn't have been set yet)\n\n#### After\n- rows will be impacted with consideration to the constraints\n- If the trigger fires after the event, all changes, including the effects of other triggers, are \"visible\" to the trigger.\n- in `AFTER`, the `WHEN` condition is evaluated after the row update occurs.\n\t- The `WHEN` condition here also determines whether an event is queued to fire a trigger, following the operation.\n\t\t- Therefore, if `WHEN` does not return true, we do not have to queue an event; nor to re-fetch the row at the end of the statement\n\t\t\t- This can result in significant speedups in statements that modify many rows, if the trigger only needs to be fired for a few of the rows.\n\n## Trigger Function\nA trigger function returns a `trigger`\n- The function body must return either `NULL` or a row value having exactly the structure of the table the trigger was fired for.\n\t- ie. the the row being returned must have the same type of the table.\n\nDepending on if we are using row-level triggers or table-level triggers, the function will be called for each row that is affected, or will be called once per table.\n\n### Infinite loops\nif a trigger function updates a row in the same table that the trigger is for and the operation is the same, then it will trigger a recursive infinite loop, since the trigger function will cause the trigger to fire in response to the \n- ex. we have a trigger that is set to fire on update of `users` table, and the trigger function itself updates a row in that table. This update, will cause the trigger to fire again, and so on.\n\n### Return value of trigger functions\n`BEFORE`\n- if we return `null` from the function, we are signalling to the trigger manager signal that we want to skip the rest of the operation for this row, meaning 2 things happen:\n\t1. subsequent triggers will be short-circuited \n\t2. the operation will not occur for this row.\n- if we return a non-`null` value, then the operation proceeds with that value. \n\t- returning a row value that is different from the original value of `NEW` will alter the row to be inserted/updated\n\t\t- Therefore, if we want the trigger to succeed normally without altering the `NEW` row value, then we must return `NEW`\n\t\t\t - This means that we could alter single columns in the row, with `NEW.col = X`, and proceeding to return `NEW`\n- a statement-level trigger fired `BEFORE` always ignores the return value of the trigger function, so it might as well be `null`.\n\n`AFTER`\n- return value of a row-level trigger (or statement-level trigger) fired `AFTER` will always be ignored, so it might as well be `null`\n\n### Trigger Local Variables\nA function called by a trigger receives data about its calling environment (called `TriggerData`)\n- prefixed by `TG_`\n\n`NEW` \n- variable holding the new database row for INSERT/UPDATE operations in row-level triggers\n- type: `record`\n- in statement-level triggers and `DELETE` operations, this value is `null`\n\t- this shows why we can return `null` from the trigger function on `DELETE` operations. There is nothing to return to the table!\n\t- must still return a non-`null` value from a DELETE trigger function, or we will short-circuit the trigger action. Normally, `OLD` is returned, since `NEW` is null\n\n`OLD`\n- variable holding the old database row for UPDATE/DELETE operations in row-level triggers\n- type: `record`\n- in statement-level triggers and `INSERT` operations, this value is `null`\n\t\n`TG_WHEN`\n- evaluates to BEFORE, AFTER, or INSTEAD OF, depending on the trigger's definition.\n- type: `text`\n\n`TG_OP`\n- evaluates to INSERT, UPDATE, DELETE, or TRUNCATE telling for which operation the trigger was fired.\n- type: `text`\n\n`TG_ARGV`\n- the arguments passed in to the trigger's function call\n- type: `text[]`\n\nAlso\n`TG_TABLE_NAME`, `TG_TABLE_SCHEMA`, `TG_NARGS` (# of args)\n\n","n":0.027}}},{"i":516,"$":{"0":{"v":"Transactions","n":1},"1":{"v":"\n[[see also|db.strategies.transaction]]\n\nIf a function call has failed within a transaction block and we try to commit, we will instead rollback. Observe:\n```sql\nBEGIN;\nSELECT 1/0; -- ERROR: division by zero\nCOMMIT; -- error detected, ROLLBACK executed\n```\n\nPostgres transactions are isolated to individual clients, meaning that the same client must be the one to execute all code in a transaction block.\n- In `node-postgres`, we cannot use `pool.query` because of this, (spec: because a pool may be seen as multiple different clients to the postgres server) \n\nIn Postgres\n- To execute a transaction in Postgres, use BEGIN / COMMIT / ROLLBACK\n- in Postgres, DDL commands are transactional, except when the commands are \"high-caliber\", such as creating and deleting DATABASE, TABLESPACE, CLUSTER\n- PostgreSQL supports multi-level transactions on save points level\n\t- If an error occurs inside a transaction, PostgreSQL rolls back the whole transaction but demands a command to complete the current transaction (COMMIT, ROLLBACK, ABORT)\n- Postgres treats every SQL statement as being executed within a transaction implicitly. If we do not issue a BEGIN command, then each statement will be surrounded with BEGIN..COMMIT\n","n":0.076}}},{"i":517,"$":{"0":{"v":"Transaction Variables","n":0.707},"1":{"v":"\nTransaction variables are variables that are cleared when the transaction exits\n```\ncurrent_setting('my_app.user_id', TRUE)\n```\n","n":0.289}}},{"i":518,"$":{"0":{"v":"Schemas","n":1},"1":{"v":"\nA schema is a logical namespace that can contain database objects such as tables, views, indexes, data types, functions, stored procedures and operators\n- by default, we use the `public` schema, if none is specified\n- a schema holds all objects, except for roles and tablespaces.\nA schema is essentially a namespace: it contains named objects (tables, data types, functions, and operators) whose names can duplicate those of other objects existing in other schemas. Named objects are accessed either by \"qualifying\" their names with the schema name as a prefix, or by setting a search path that includes the desired schema(s). A CREATE command specifying an unqualified object name creates the object in the current schema (the one at the front of the search path, which can be determined with the function current_schema).\n- A database can contain one or multiple schemas and each schema belongs to only one database.\n\t- ex. you may have sales schema that has staff table and the public schema which also has the staff table. When you refer to the staff table you must qualify it as either `public.staff` or `sales.staff`.\n- you can use one schema for the tables that will be exposed to GraphQL, another for the tables that should be completely private (e.g. where you store the bcrypted user passwords or other secrets never to be exposed)\n\n## Good practice for Schema building:\n**app_public** - tables and functions to be exposed to GraphQL (or any other system) - it's your public interface. This is the main part of your database.\n**app_hidden** - same privileges as `app_public`, but it's not intended to be exposed publicly. It's like \"implementation details\" of your app_public schema. You may not need it often.\n**app_private** - SUPER SECRET STUFF 🕵️ No-one should be able to read this without a SECURITY DEFINER function letting them selectively do things. This is where you store passwords (bcrypted), access tokens (hopefully encrypted), etc. It should be impossible (thanks to RBAC (GRANT/REVOKE)) for web users to access this.\n- security definer effectively changes the role for the function being executed\n\n#### User tables\n- we should have 2 different tables about users. One should be on the private schema (`user_account`), where password and email will be kept. The other should be on the public schema, where everything else about the user is kept. The two tables have a 1:1 relationship, where the `private.user_account` table has a reference to the `public.user` table.\n\t- [good example](https://github.com/dijam/graphile-jwt-example/blob/master/db.sql)\n\n* * *\n\nAccording to the SQL standard, the owner of a schema always owns all objects within it. PostgreSQL allows schemas to contain objects owned by users other than the schema owner. This can happen only if the schema owner grants the CREATE privilege on their schema to someone else, or a superuser chooses to create objects in it.\n","n":0.047}}},{"i":519,"$":{"0":{"v":"Roles","n":1},"1":{"v":"\nA role is an entity that can own database objects and have database privileges\n- can be considered a \"user\", a \"group\", or both depending on how it is used\n\t- in other words, a user role can be part of a group role\n- must have CREATEROLE privilege or be a database superuser to create roles.\n\nWhen a user logs in, we want them to make their queries using a specific role\n- any time we are running `CREATE USER` or `CREATE GROUP`, we are running `CREATE ROLE` under the hood.\n\t- minor difference: `CREATE USER` also logs the user in, so a `role` having the LOGIN attribute can be thought of as a user \n\t\t- the ability to log in just means the role can be input along with a password as part of the connection string: `postgres://admin@localhost/mydb`\n\t- In the past, there were users and groups. Now, there are just roles. Roles have the ability to log in, have the ability to inherit from other roles, \n\t\t- basically, we moved from the Unix paradigm of users and groups, to the OOP paradigm of having inheritance.\n\t\nAttributes\n- attributes define privileges and info for a role \n\t- ex. login, perform superuser, database creation, role creation, password, etc\n\t\t- by default, roles don't get the ability to log in\n- the attributes are listed after the WITH clause, though that word is optional \n```sql\nCREATE ROLE kyletycholiz WITH\nLOGIN\n...\n```\n\nDefault roles\n- the `postgres` user is automatically created when we install Postgres. It is a superuser that we log into postgres with\n\t- all server processes work on behalf of this user\n\t- all database files belong to this user\n- roles that start with with pg_ are system roles.\n\n- roles are defined at the database cluster level, and so are valid in all databases in the cluster.\ncluster → database → schema → table\n\n## Grant\n- When an object is created, it is assigned an owner. \n\t- The owner is normally the role that executed the creation statement.\n\t- The right to modify or destroy an object is always the privilege of the owner only\n- When we use grant to grant access to a role, the now-empowered role can do 2 things: they can do everything that the source role could do, or they can actually control and become the role that it gained its powers from (this would mean that if an `admin` role became `user_login` role, it would no longer be able to perform any action that have admin-only privileges) \n\nGRANT has 2 variants:\n1. grant privileges on a database object\n\t- inc. table, column, view, sequence, database, foreign-data wrapper, foreign server, function, procedural language, schema, or tablespace\n2. grant membership in a role\n- below, we define a role `user_admin`, then give the role `postgres` the ability to do anything that `user_admin` can do.\n\t- from another viewpoint, `postgres` is gaining the power to do everything that `user_admin` can do.\n```sql\nCREATE ROLE user_admin;\nGRANT user_admin to postgres;\n```\n- GRANTing on a database doesn't grant rights to the schema within. Similarly, GRANTing on a schema doesnt grant rights to tables within.\n\t- ex. if we run `grant usage on schema app_public to user_guest`, we grant the role `user_guest` the right to know that the schema exists, but it doesn't give it the right to interact with the tables within.\n\t\t- if we want to grant read rights to a table, then we need to run `grant select on table <TABLENAME> to user_guest`\n\t- If you have rights to SELECT from a table, but not the right to see it in the schema that contains it, then you can't access the table\n\t\t- Therefore, the user must first be granted rights to the schema, otherwise the rights on the table are useless. \n\t\t- Likewise, if the user had been granted rights to just the schema, but not the tables within, they would still be locked out. \n\t\t\t- It's like a directory tree. If you create a directory somedir with file somefile within it then set it so that only your own user can access the directory or the file (mode rwx------ on the dir, mode rw------- on the file) then nobody else can list the directory to see that the file exists. If you were to grant world-read rights on the file (mode rw-r--r--) but not change the directory permissions it'd make no difference. Nobody could see the file in order to read it, because they don't have the rights to list the directory. If you instead set rwx-r-xr-x on the directory, setting it so people can list and traverse the directory but not changing the file permissions, people could list the file but could not read it because they'd have no access to the file. You need to set both permissions for people to actually be able to view the file. \n- the public schema has a default GRANT of all rights to the role public, which every user/group is a member of. So everyone already has usage on that schema.\n- opposite of GRANT is REVOKE\n*GRANT USAGE* - grants access to objects contained in the specified schema \n\n## Roles & Auth\n- all auth happens through postgres roles and permissions. Postgres is in charge of authenticating requests. \n- Postgres permissions work as a whitelist and not a blacklist (except for functions). \n\t- There are very few defaults, and if we want to blow them away entirely, use `alter default privileges`\n- when an authenticated user makes a request, the role will be changed for that user, having the effect of restricting queries (can be seen in `current_user` variable)\n\t- `current_user` is the username of the current execution context \n- Because we have the concept of default roles, we set a default (such as `user_guest` that every user will get by default. When the user demonstrates a verified JWT token, that role gets changed to one with more authorization rights (such as `user_login`).\n- JWTs can be generated in postgres with `pgjwt` extension\n","n":0.032}}},{"i":520,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n##### Change role\n```sql\nset role postgres;\n```\n","n":0.447}}},{"i":521,"$":{"0":{"v":"Listen/Notify","n":1},"1":{"v":"\n## Overview\nPostgres has a system of asynchronous messages and notifications, implemented by `listen` and `notify` keywords.\n- This means that as soon as a connection is established with PostgreSQL, the server can send messages to the client even when the client is idle.\n\t- This method of communication is also carried out with the `COPY` command.\n\nCommonly, the channel name is the same as the name of some table in the database, and the notify event essentially means, \"I changed this table, take a look at it to see what's new\"\n\n## Notify\n`notify` provides a simple interprocess communication (IPC) mechanism for a collection of processes accessing the same PostgreSQL database.\n\nWhen `notify` is used to signal the occurrence of changes to a particular table, a useful programming technique is to put the `notify` in a rule that is triggered by table updates.\n- In this way, notification happens automatically when the table is changed, and the application programmer cannot accidentally forget to do it.\n\nIn a transaction, `notify` events are not delivered until the surrounding transaction has been committed.\n\n## Listen\nWhen we execute `listen`, we are registering the current postgres session as a listener on the specified notification channel.\n- when the specified channel is `notified` (either by the current session, or another one connected to the same db), all sessions subscribed to that channel will receive the message, and will in turn pass it on to the client whom it is connecting.\n\nThe payload passed to the client includes 3 things:\n1. Notification channel name\n2. Session server's PID\n3. Payload string (`''` if unspecified)\n\nA session's listen registrations are automatically cleared when the session ends.\n\nThe method a client application must use to detect notification events depends on which PostgreSQL API it uses (ie. libpq, libpgtcl)\n\n## `pg_notify(channel_name text, payload text)`\nWe can also send a notification using the `pg_notify` function\n- The function is much easier to use than the `notify` command if you need to work with non-constant channel names and payloads.\n\n## Example\n```psql\npsql## listen my_notification_channel;\nLISTEN\n\npsql## notify my_notification_channel, 'foo';\nNOTIFY\nAsynchronous notification \"my_notification_channel\" with payload \"foo\"  ⏎\nreceived from server process with PID 40430.\n```\n\n## UE Resources\n[Dmitiri Fontaine on Listen-Notify](https://tapoueh.org/blog/2018/07/postgresql-listen-notify/)\n","n":0.054}}},{"i":522,"$":{"0":{"v":"Indexes","n":1},"1":{"v":"\nPostgres has multiple types of [[index|db.strategies.index]]: [[B-tree|general.lang.data-structs.tree.B]], Hash, GiST, SP-GiST and GIN\n- B-tree (balanced tree) is default, and is appropriate for most use-cases\n\nUsing the `CREATE INDEX` command, we are creating a secondary index. Primary indexes are created by default via the primary key.\n- secondary joins are crucial for performing JOINs effectively.\n\nWhether to create a single-column index or a multicolumn index, take into consideration the column(s) that you may use very frequently in a query's WHERE clause as filter conditions.\n- ie. add an index on the columns that we typically use WHERE on\n\nIndexing is often thought of as a data modeling activity. When using PostgreSQL, some indexes are necessary to ensure data consistency (the [[C|db.acid.consistency]] in ACID). Constraints such as UNIQUE, PRIMARY KEY or EXCLUDE USING are only possible to implement in PostgreSQL *with* a backing index. When an index is used as an implementation detail to ensure data consistency, then the indexing strategy is indeed a data modeling activity.\n- In all other cases, the indexing strategy is meant to enable methods for faster access methods to data. Those methods are only going to be exercised in the context of running a SQL query\n\nIn the case of UNIQUE, PRIMARY KEY and EXCLUDE USING, the reason why PostgreSQL needs an index is because it allows the system to implement visibility tricks with its MVCC implementation \n- this fact is what prevents these 2 (concurrently ran) transactions (which should conflict on UNIQUE constraint) from being accepted:\n```sql\nt1> insert into test(id) values(1);\nt2> insert into test(id) values(1);\n```\n\n### Indexing comes at a cost\nDetermining *what not* to index is probably more important than determining *what* to index\n- An index duplicates data in a specialized format made to optimise a certain type of searches. When we `COMMIT;`, every change that is made to the main tables of your schema must have made it to the indexes too.\n\t- As a consequence, each index adds write costs to your DML queries: INSERT, UPDATE and DELETE now have to maintain the indexes too, and in a transactional way.\n\nAvoid indexes on...\n- small tables.\n- Tables that have frequent, large batch update or insert operations.\n- columns that contain a high number of NULL values.\n- Columns that are frequently manipulated should not be indexed.\n\n* * *\n\n[[Index Docs|db.strategies.index]]\n\n# UE Resources\n- [Indexes under the hood, using B-trees](https://rcoh.me/posts/postgres-indexes-under-the-hood/)\n- [provides a quick-n-dirty test at the bottom of the article, to see impact of indexes on millions of rows](https://www.cybertec-postgresql.com/en/postgresql-now-vs-nowtimestamp-vs-clock_timestamp/)\n","n":0.05}}},{"i":523,"$":{"0":{"v":"Strategy","n":1},"1":{"v":"\nYou can begin your indexing needs analysis by listing every query that averages out to more than 10 milliseconds, or some other sensible threshold for your application. The only way to understand where time is spent in a query is by using the EXPLAIN command and reviewing the query plan\n","n":0.141}}},{"i":524,"$":{"0":{"v":"Partial Index","n":0.707},"1":{"v":"\nA partial index is an index built over a subset of a table, and therefore only contains entries for that subset.\n- The subset is determined by a conditional expression given by us (called the predicate).\n\n## Purpose\n- To avoid indexing common values\n    - Since a query searching for a common value (ie. one that occurs for more than a few percent of all table rows) will not use the index anyway, there is no point in keeping those rows in the index at all. Doing this reduces the size of the index, increasing the speed of queries that do ue the index.\n","n":0.1}}},{"i":525,"$":{"0":{"v":"Grant","n":1},"1":{"v":"\nThe GRANT command has two basic variants: one that grants privileges on a database objects, and one that grants membership in a role.\n\nWhen you create a new database, any role is allowed to create objects in the public schema. To remove this possibility, you may issue immediately after the database creation:\n\n```sql\nREVOKE ALL ON schema public FROM public;\n```\nafter the above command, only a superuser may create new objects inside the public schema, which is not practical. Assuming a non-superuser foo_user should be granted this privilege, this should be done with:\n\n```sql\nGRANT ALL ON schema public TO foo_user;\n```\n","n":0.103}}},{"i":526,"$":{"0":{"v":"Func","n":1},"1":{"v":"\n### `$$` Dollar quoting\nhttps://stackoverflow.com/questions/12144284/what-are-used-for-in-pl-pgsql/12172353#12172353\n","n":0.5}}},{"i":527,"$":{"0":{"v":"Window Function","n":0.707},"1":{"v":"\nA window function performs a calculation across a set of table rows. Each row is somehow related to the current row.\n- Window Functions differ from aggregate functions in that they do not cause rows to become grouped into a single output row. Instead, the rows retain their separate identities.\n\t- Behind the scenes, the window function is able to access more than just the current row of the query result.\n- ex. Imagine we want a result set of all employees, and have their individual salaries compared against the average of their individual departments:\n```sql\nSELECT \n\tdepname, \n\tempno, \n\tsalary, \n\tavg(salary) \nOVER (PARTITION BY depname) \nFROM empsalary;\n```\nwould result in:\n```\n  depname  | empno | salary |          avg          \n-----------+-------+--------+-----------------------\n develop   |    11 |   5200 | 5020.0000000000000000\n develop   |     7 |   4200 | 5020.0000000000000000\n develop   |     9 |   4500 | 5020.0000000000000000\n develop   |     8 |   6000 | 5020.0000000000000000\n develop   |    10 |   5200 | 5020.0000000000000000\n personnel |     5 |   3500 | 3700.0000000000000000\n personnel |     2 |   3900 | 3700.0000000000000000\n sales     |     3 |   4800 | 4866.6666666666666667\n sales     |     1 |   5000 | 4866.6666666666666667\n sales     |     4 |   4800 | 4866.6666666666666667\n(10 rows)\n```\n\n### Over\nThe OVER clause determines exactly how the rows of the query are split up for processing by the window function\n- the OVER clause is what causes it to be treated as a window function, and therefore computed across the window frame.\n- A window function call always contains an OVER clause directly following the window function's name and argument(s). \n","n":0.064}}},{"i":528,"$":{"0":{"v":"Custom Functions","n":0.707},"1":{"v":"\nA function is a reusable block of SQL code that returns a scalar value of a set of rows.\n- Functions are transactional by nature. If there is an error somewhere in the function, then the function will be rolled back.\n\t- If a function is called within a transaction block, and the executing code does not reach the concluding `COMMIT`, then all code executed within the function will roll back as well.\n\t- Any `BEGIN...EXCEPT` blocks within the function operate like savepoints like the `SAVEPOINT` and `ROLLBACK TO <SAVEPOINT>` SQL statements.\n- SQL functions execute an arbitrary list of SQL statements, returning the result of the last query in the list.\n\t- In the simple (non-set) case, the first row of the last query's result will be returned\n\t- Alternatively, an SQL function can be declared to return a set, which allows *all* rows of the last query's result to be returned\n- allows us to write functions that can interact on the database. For instance, we can create a function that combines `first_name` and `last_name` to give us `fullName`\n- Since PostgreSQL permits function overloading, the function name alone does not uniquely identify the function to be called;\n\t- the parser must select the rigt function based on the data types of the supplied arguments.\n- Functions and operators in PostgreSQL support polymorphism and almost every part of the system can be extended.\n- By default, functions can be executable by public\n- anonymous IIFEs can be invoked anywhere in the SQL by escaping the function identifiers:\n```sql\nDO \\$\\$\nBEGIN\n  EXECUTE 'GRANT appname TO ' || user;\n  EXECUTE 'GRANT appname_authenticator TO ' || user;\nEND;\n\\$\\$ LANGUAGE PLPGSQL;\n```\n\n## Using functions in queries\n- say we have a function that returns a composite type:\n```sql\nCREATE FUNCTION new_emp() RETURNS emp AS $$\n    SELECT ROW('None', 1000, 25, 'yoyo')::emp;\n$$ LANGUAGE SQL;\n```\n- here we are returning a single column that is a composite type of the signature: string, int, int, string. We also coerce it to the composite type related to the whole `emp` table. This means that the `emp` table has all 4 of those rows, and the act of us coercing means the output of the function can be used anywhere that an explicit `emp` type is required.\n- we could make a SELECT statement to get back a one-column table by using `SELECT new_emp()`\n- since the composite type is also a sort of virtual table, we can use the function as a \"table function\": `SELECT * FROM new_emp();`, which will return 4 columns.\n\n## Syntax\n- use `$$` to open and close the function:\n- stable means that this function does not mutate the database\n\nTerms\n- `setof` - Sets emulate rows of tables.\n\t- `returns setof` and `returns table(column)` are equivalent\n\t- When an SQL function is declared as returning SETOF sometype, the function's final query is executed to completion, and each row it outputs is returned as an element of the result set.\n\t\t- This feature is normally used when calling the function in the FROM clause, since everything after FROM would get interpreted as if it were a base table. In other words, we can add some columns to a base table, and query it as if those added rows were permanent\n\t\t\t- ex. imagine having a function that determined if your salary was above $100,000. we can use a function to get the boolean result and attach it to the base table, so that we can use `FROM employee, getAboveHundred(emp1)\n```sql\nCREATE FUNCTION add(a int, b int) returns int as $$\n select a + b\n$$ language sql stable;\n```\n\n# PL/PGSQL\n### Declare\n- Declare a variable of specified type\n\n### Function Declarations\n#### Strict\n- this means that if the function receives null input, then the function won't be called and the output will automatically be null as well.\n\t- ex. imagine we have a `register_user` function, that takes name, email and password as inputs. If the function does not receive `name`, we want it to fail.\n\n#### Security Definer/Invoker\n- Security Definer - \"the privileges of the function use the privileges (security clearance) of the definer\"\n- Security invoker - \"use privileges (security clearance) of the user who invoked the function\n- this means that the function is created with the privileges of the pg role that created it.\n- This is a way to heighten the rights of the function, and can be thought of similar to sudo, instead of rights being elevated to superuser, rights are only elevated to the creator of the function itself\n\t- ie. if we run our migrations with user `postgres`, then using `security definer` will give our function the rights of `postgres`. This is helpful if we want to insert into a table that is part of a very strict schema, such as `app_private`\n- we can use `security definer` to by pass RLS\n\n### UE Resource\n[insert returning into variable](https://www.xspdf.com/resolution/50004699.html)\n","n":0.036}}},{"i":529,"$":{"0":{"v":"Signature","n":1},"1":{"v":"\n## Return value\n- functions can return base types (string, int) and composite types (collection of columns), or sets of base types and composite types\n- we can return any type from a function. Since the whole row of a table is by definition a composite type, we can specify `returns nugget`. This will specify that the function must return all columns in the `nugget` table.\n\t- If we don't want to return all columns, we can always use the `ROW` construct to specify which columns we want to include in the row signature\n\t- The select list order in the query must be exactly the same as that in which the columns appear in the table associated with the composite type.\n\t- You must typecast the expressions to match the definition of the composite type\n- If the function is defined to return a base table, the table function produces a one-column table (with the column named after the function. If the function is defined to return a composite type, the table function produces a column for each attribute of the composite type.\n\t- using `setof` will return multiple columns\n- the type that follows `returns` has to match up with whatever is SELECTed during the query.\n\t- ex. if we declare `returns bucket`, then we'd better be using `SELECT * FROM bucket`\n\t\t- since in this example we are returning a base table, a one-column table is the result.\n\n### Returning a Set\n- we can also use `RETURNS TABLE(columns)` syntax to return a set\n\t- equivalent to using one or more `OUT` parameters plus marking the function as returning SETOF record (or SETOF a single output parameter's type, as appropriate)\n\t- It is not allowed to use explicit OUT or INOUT parameters with the RETURNS TABLE notation — you must put all the output columns in the TABLE list.\n\n### Output Params\n- an alternate way to define a function's signature (inputs and outputs)\n- The value of output parameters is that they provide a convenient way of defining functions that return several columns, which is why functions can have multiple outputs\n```\nCREATE FUNCTION sum_and_product (IN x int, IN y int, OUT sum int, OUT product int)\n```\n- What has essentially happened here is that we have created an anonymous composite type for the result of the function\n\t- if we wanted to be more explicit, we could have declared a composite type `sum_prod`, made up of the `sum` column and the `product` column, and declared that the function `returns sum_prod`.\n\n## Set returning function\n- *set returning functions* are functions that possibly return more than one row\n\t- currently, `series generating functions` are the only type of `set returning functions`\n- `generate_series(<start>, <stop>, <step>)`\n\t- because this function returns a result set, we can use the function after FROM\n- ex. imagine running:\n```\ngenerate_series(date :'start',\n\t\t\t    date :'start' + interval '1 month'\n\t\t\t\t\t\t\t  - interval '1 day',\n\t\t\t\tinterval '1 day'\n) as calendar(entry)\n```\nwhich would return a set of dates, starting from `start` (a variable we defined earlier), and increasing by intervals of 1 day.\n\t- this would be useful if we have a set of data by year, and have some years where there is no data. Instead of skipping those rows, we might want to display zeros instead. When we join this result set with our data, joining on the `date` column will result in `null` for the missing years. Using `coalesce`, we can default all nulls to zero.\n\n## Params\n### Named params\n2 ways:\n\n```sql\nSELECT * FROM app_private.really_create_user(\n\tusername := $1\n)\n```\n\nor:\n```sql\nselect app_private.really_acquire_book(\n\tpayment_intent_id => $1\n)\n```\n","n":0.042}}},{"i":530,"$":{"0":{"v":"Builtin","n":1}}},{"i":531,"$":{"0":{"v":"Rank","n":1},"1":{"v":"\n`RANK()` lets us assign a rank to every row according to a partition we define with PARTITION BY\n- ex. imagine we had a result set of every person in a company along with their salaries. We want to rank the top earning employees by position (eg. top 3 salespeople; top 3 programmers etc). To get this result set, we can RANK, partitioning over the position_title\n```sql\nselect \n    salary\n    rank() over (partition by position_title order by salary) from employees\n```\n\nThe `RANK()` function can be useful for creating top-N and bottom-N reports\n","n":0.107}}},{"i":532,"$":{"0":{"v":"Lag","n":1},"1":{"v":"\n`lag` allows us to access access rows that appeared before the current row that we are accessing. We give `lag` an offset and we get back the row that appeared `x` spots before the current one.\n- Naturally, this makes `lag` useful in being able to compare the values of the current and previous rows.\n\nThe following gives us the previous row.\n```sql\nlag(invoice, 1)\n```\n","n":0.128}}},{"i":533,"$":{"0":{"v":"Coalesce","n":1},"1":{"v":"\n`coalesce` accepts unlimited params, and always returns the first non-null value\n- In this way, you can think of the final param as the default value.\n\nEmbracing `null` allows us to easily inline defaults\n- You can chain a bunch of inputs and it returns the first non-null value\n- ex. imagine having a result set that shows us how much revenue was earned each year since 2010. We should define a column `coalesce(income, 0) as income` in our select statement, which will return us a 0 if there is no income for the relevant year. This is a nicer alternative than to just omitting rows without data.\n- ex. Lets say you have an input parameter with a default, you can the just wrap the usages of that parameters like this: coalesce (_parameter, default_value) = whatever it tests against\n- ex. Imagine we have a postgres function that accepts 2 args: a `start_date` and an `end_date`, and returns the amount of time that an employee worked at a company. If they currently work there still, the client passes `null` for the value of the `end_date`. In our function, we would have a smart failover that defaults the null value to `now()`.\n- ex. imagine we have a column `discount` that defaults to `null`. We want to use this `discount` column when determining subtotal, so we need to use coalesce to provide us with a default value of 0:\n```sql\nSELECT\n\tproduct,\n\t(price - COALESCE(discount,0)) AS net_price\n```\n","n":0.065}}},{"i":534,"$":{"0":{"v":"Aggregate Functions","n":0.707},"1":{"v":"\ncalled aggregate functions because they are functions that act on multiple rows and give us a single output\n- ex. sum, avg, max, min\n\nUsed to extract information about whole groups of rows, and allow us to easily ask questions like:\n1. What's the most expensive facility to maintain on a monthly basis?\n```sql\nselect max(monthlymaintenance)\nfrom cd.facilities;\n```\n2. Who has recommended the most new members?\n3. How much time has each member spent at our facilities?\n4. what is the most popular type of shoe?\n- Aggregate functions only work in SELECT or HAVING clauses\nTherefore, an aggregate function will take a group of data, perform some function on it, and have a single output that it will print in our result set. An aggregate function can perform transformation on the data, or it can also perform a calculation on it. In either case, the aggregate function is making a consideration of all rows of data, and deciding what to do with it that would result in a single piece of data (int, array etc) that will hold the results of that calculation.\n- This is why when we perform aggregate functions, we need to use ORDER BY, because we need to tell the query what we want to apply the aggregate function to.\n\t- Since the aggregate function results in an aggregate set of data *based on something*, we need to know what that \"based on\" actually is. Is the data getting combined *based on* age? Is it getting combined *based on* nugget (ie. combining buckets from multiple rows in an effort to make a json array)?\n- When you hear \"aggregate function\", you should think of taking data from multiple rows and combining (likely integers) or transforming (likely objects/arrays) it in some way\n\n[some useful examples](hashrocket.com/blog/posts/faster-json-generation-with-postgresql)\n\n# Pitfalls\n- because of the order of execution, aggregate functions cannot be used in the WHERE clause (see queries/WHERE)\n\t- This restriction exists because the WHERE clause determines which rows will be included in the aggregate calculation; so obviously it has to be evaluated before aggregate functions are computed\n","n":0.055}}},{"i":535,"$":{"0":{"v":"Json","n":1},"1":{"v":"\n### `row_to_json`\nturns the table into json\n- each row becomes a json object, and the cumulative of these rows makes up the json array\n- If the query `select * from nugget` gives us the data, then the whole below query gives us that same data as json\n```\nselect row_to_json(nugget)\nfrom (select * from \"nugget\") as nugget\n```\n### `array_agg`\nIt aggregates its argument into a PostgreSQL array\n- accepts a set of values and returns an array. Each arg becomes an element in the array\n\t- we can determine the order of the array with ORDER BY\n- ex. Imagine we had 2 many:many tables (`actors` and `movies`) connected through a junction table `movies_actors`.\n\t- We are interested in viewing which actors were in which movie.\n\t- Assuming that the `movies` table is the table we are joining from, if we were to query the movies, we'd get duplicate rows with the same movie and actor names (since each row is a combination of the two, effectively having the same amount of rows as the junction table).\n![d26ee15403ed820f7c410d859e7c5e56.png](:/20ede76c8a314c528f1b4716c372db0d)\n\t- What we want instead is to show a different movie in each row, and consolidate the actors appearing in that movie into an array\n![939e260f1cc0aab3f7f7e6477164e5ac.png](:/9d7a1c247b064dc285cd0597a477d28a)\n\n### `json_agg`\n- similar to `array_agg`, except that it puts the elements into a json array\n- we could use `json_agg(json_build_object())` if we wanted to build an array of objects\n\nBefore\n```sql\nSelect\n    title,\n    first_name || ' ' || last_name as name\nfrom film\njoin film_actor using (film_id)\njoin actor using (actor_id)\ngroup by title, first_name, last_name\norder by title;\n```\n\nAfter\n```sql\nSELECT\n     title,\n     ARRAY_AGG (\n\t     first_name || ' ' || last_name\n\t     ORDER BY first_name\n     ) actors\n FROM\n     film\n INNER JOIN film_actor USING (film_id)\n INNER JOIN actor USING (actor_id)\n GROUP BY\n     title\n ORDER BY\n     title;\n```\n\n### `json_build_object`\nAllows us to provide key-value pairs, thereby returning us an object\n- if we want to provide an array as the value of a key (ex. `buckets` property of a `nuggets` object), we can provide a SELECT statement in place of the value\n```sql\nSELECT\n    json_build_object(\n        'id',id,\n        'name',name,\n        'comments',(\n            SELECT json_agg(json_build_object(\n                'id', comments.id,\n                'mood', comments.mood,\n                'subject', comments.subject,\n                'content', comments.content,\n                'created_at', comments.created_at\n            ))\n            FROM user_comment_map JOIN comments ON comment_id=comments.id\n            WHERE user_id=users.id\n        )\n    )\n```\nthis works because\n\n### `json_strip_nulls`\n- lets us exclude fields where the value is null\n```sql\nSELECT json_strip_nulls(json_build_object('name', p.name, 'birthday', p.birthday))\nFROM person p;\n```\n\n### `json_populate_record`\n- allows us to give postgres a string of json, which it will then convert into SQL row-format.\n- the first arg we pass to\n\n### `jsonb_pretty`\nprint json pretty in output\n\n## jsonb manipulation methods\n\n### `jsonb_set` & `jsonb_insert`\nWith these methods, it is not possible to add values at the root of the json object.\n\nsignature:\n```\njsonb_set/jsonb_insert(target         jsonb,\n          path           text[],\n          new_value      jsonb,\n          create_missing boolean default true)\n```\n#### `jsonb_insert`\nAllows us to insert into JSON array (while preserving all of the original values)\n","n":0.048}}},{"i":536,"$":{"0":{"v":"Filter","n":1},"1":{"v":"\nAdds a filter to an aggregate function\n- ex. if we are making a json array with `json_agg`, we can filter on anything that would appear in that json\n\t- ex. `filter (where \"bucket\".id is not null)`\n","n":0.169}}},{"i":537,"$":{"0":{"v":"Count","n":1},"1":{"v":"\n### count(*)\nCounts the number of rows returned by a `select *` statement\n\n### count(colname)\nCounts the number of rows, but does not consider `null` values of the specified column\n\n### count(DISTINCT colname)\nCounts the number of rows, but does not consider `null` values of the specified column, and does not consier duplicate values\n- ex. if we are selecting from a `users` table and there is a column `role`, we can `select count(distinct role)...` and we will get a small list of role possibilities\n\nWe often use the COUNT() function with the GROUP BY clause to return the number of items for each group. For example, we can use the COUNT() with the GROUP BY clause to return the number of films in each film category.\n","n":0.091}}},{"i":538,"$":{"0":{"v":"Fdw","n":1},"1":{"v":"\n### Foreign Data Wrappers (FDW)\n[FDW](https://thoughtbot.com/blog/postgres-foreign-data-wrapper)\n","n":0.447}}},{"i":539,"$":{"0":{"v":"RLS (Row Level Security)","n":0.5},"1":{"v":"\nThe main hypothesis is that we should be able to prevent access to specific rows of data based on a policy. That means our application logic only has to worry about `SELECT * FROM my_table` and RLS will handle the `WHERE user_id = my_user_id` part automagically.\n- To put it another way: our queries should only contain the clauses requested by our interfaces and not the filters and conditions demanded by access control in a multi-tenant data store.\n- The current PG role that is accessing the table must have been `grant`ed permission to use it. Otherwise, RLS errors will arise when we try to alter something in the table as that role, because we won't be able to access the table from the outset.\n\na function marked as `SECURITY DEFINER` will bypass RLS\n\n- While GRANT is the privilege system of Postgres, Tables can also have Row Security policies that restrict (on a per-user basis) which rows can be interacted with (INSERT, UPDATE, DELETE, SELECT)\n\t- Policies are created using the CREATE POLICY command\n\t- spec: `grant` means \"I am giving *this* user the privilege to use *this* table\". `create policy` means \"I am specifying the requirements of any particular row that must be satisfied before it is accessed by user\"\n- The basis of row level security is to create policies that define how a user interacts with rows within a given table.\n- By default, tables do not have any policies, and RLS must be opted-in for each table.\n\t- When RLS is enabled, all rows are by default not visible to any roles (superusers still have access)\n- If the value in parentheses after USING evaluates to true, then the user gets permission\n- ex. imagine we have a chat app, and we want to ensure a user can only see messages sent by him, and messages intended for him. Also, we want to ensure that users cannot modify the `message_from` column to make it seem that the message is coming from someone else:\n```sql\nCREATE TABLE chat (\n  message_uuid    UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n  message_time    TIMESTAMP NOT NULL DEFAULT now(),\n  message_from    NAME      NOT NULL DEFAULT current_user,\n  message_to      NAME      NOT NULL,\n  message_subject VARCHAR(64) NOT NULL,\n  message_body    TEXT\n);\n\nCREATE POLICY chat_policy ON chat\n  USING ((message_to = current_user)\n  OR (message_from = current_user))\n  WITH CHECK (message_from = current_user)\n```\n- RLS can be implemented using jwt claims to verify that the user is who they say they are, if we do not want to use `current_user`:\n\t- the second arg true means \"return null if the setting is missing\"\n```sql\nCREATE POLICY chat_policy ON chat\n  USING ((message_to = current_setting('request.jwt.claim.email', true))\n  OR (message_from = current_setting('request.jwt.claim.email', true)))\n  WITH CHECK (message_from = current_setting('request.jwt.claim.email', true))\n```\n- When request contains a valid JWT with a role claim (`jwt.claims.role`), we should switch to the role with that name for the duration of the HTTP request\n\n### RLS Policy using external tables\n- What if we want to enable RLS where `user_id = current_user_id()`, but the current table does not keep a `user_id` column?\n- If we are adding an RLS policy to T1, but the policy depends on a `JOIN`able table T2, then T2 must have `grant`ed privileges to the PG role accessing the table.\n\n```sql\ncreate policy t2_policy_update on t2 for update using (\n  exists (\n    select 1\n      from t1\n      inner join t0\n        on t1.t0id = t0.id\n      where t0.u = session_user\n        and t1id = t1.id\n  )\n)\n```\nSubqueries in RLS policies respect the RLS policies of the tables they reference\n\n[source](https://stackoverflow.com/questions/41354818/postgresql-row-level-security-involving-a-view-or-a-select-with-join)\n\n## Per-command Policies\n### UPDATE\n- Since `UPDATE` involves pulling an existing record and replacing it with a new modified record, `UPDATE` policies accept both a `USING` expression and a `WITH CHECK` expression\n  - `USING` determines which records the `UPDATE` command will see to operate against\n  - `WITH CHECK` defines which modified rows are allowed to be stored back into the table.\n    - If the updated value fails the `WITH CHECK` expression, there will be an error.\n    - If only a `USING` clause is specified, then it will be used for both `USING` and `WITH CHECK` cases (ie. `WITH CHECK` is implemented for us implicitly)\n- Typically an `UPDATE` command needs to read data from columns in the relation being updated (e.g. in a `WHERE` clause, or `RETURNING` clause, or right side of a `SET` clause).\n  - In cases such as these, `SELECT` rights are required on the relation being updated, in addition to the `UPDATE` right.\n\n* * *\n\n## Anatomy\n### WITH CHECK vs USING\n- USING is applied before any operation occurs to the table’s rows\n\t- ex. in the case of updating a nugget, one could not update a row that does not have the appropriate user_id in the first place\n\t- must use USING with DELETE commands because a delete changes no rows, and only removes current ones.\n\t- USING implicitly runs a WITH CHECK with the same clause that USING received, meaning that the verification operation runs both before and after the data is inserted.\n- WITH CHECK is run after an operation is applied, so if it fails, the operation will be rejected\n\t- ex. in the case of an insert, Postgres sets all of the columns as specified and then compares against WITH CHECK on the new row\n\t- must use WITH CHECK with INSERT commands because there are no rows to compare against before insertion\n\n### Permissive or Restrictive\nRLS policies can be either permissive or restrictive\n- permissive (default) - in consideration of all RLS policies, only 1 must pass\n- restrictive - in consideration of all RLS policies, all must\n```\ncreate policy select_all on table_name as permissive using (true)\n```\n\n* * *\n\n### Infinite Recursion\nImagine we are making a RLS policy for `select` on a given table. If we then try and `select` that same table within the `using()` function, we will get an infinite recursion as a result.\n\n* * *\n\n### Check if RLS enabled\n```sql\nselect relname, relrowsecurity, relforcerowsecurity\nfrom pg_class\nwhere oid = 'your_table_name_with_schema'::regclass;\n```\nor\n```sql\nselect * from pg_tables where tablename = 'your_table_name_without_schema'\n```\n\n# UE Resources\n[Good information about RLS](https://info.crunchydata.com/blog/a-postgresql-row-level-security-primer-creating-large-policies)\n[RLS using columns from other tables](https://medium.com/@ethanresnick/there-are-a-few-faster-ways-that-i-know-of-to-handle-the-third-case-with-rls-9d22eaa890e5)\n[](https://blog.crunchydata.com/blog/a-postgresql-row-level-security-primer-creating-large-policies)\n","n":0.032}}},{"i":540,"$":{"0":{"v":"Internals","n":1},"1":{"v":"\n# UE Resources\n- [Internals of Postgres](http://www.interdb.jp/pg/)\n","n":0.408}}},{"i":541,"$":{"0":{"v":"Objects","n":1},"1":{"v":"\n### Object-nature of Postgres\nPostgreSQL is an object-relational database management system (ORDBMS)\n- a postgres database itself is an object. It contains other objects, such as tables, views, functions, and indexes. since a database is just an object, multiple postgres databases can be stored inside a single postgres server.\n- By making everything an object, Postgres is highly extensible, allowing us to build:\n\t- data types\n\t- functions\n\t- aggregate functions\n\t- operators\n\t- index methods\n- A special feature of PostgreSQL is table inheritance, meaning that a table (child table) can inherit from another table (parent table) so when you query data from the child table, the data from the parent table also shows up.\n","n":0.097}}},{"i":542,"$":{"0":{"v":"Extensions","n":1},"1":{"v":"\nWhen installing new PG extensions, we often need to restart the DB. Extensions need to be registered in `shared_preload_libraries`\n\n## PGXS\nWe can compile extensions using a tool called `pgxs`\n- PGXS is a build infrastructure for extensions\n- it allows us to build extensions against an already installed server.\n\n## Misc\n- When trying to install an extension, if we get the error message `cannot find \"postgres.h\"`, this means that to build the extension from source, the postgres header files are needed. There is a package called `postgres-server-dev-<VERSION>`. This will install all header files for us necessary for server development.\n\n## Commands\n- show installed extensions - `\\dx` OR `SELECT * FROM pg_extension`\n- show available extensions - `SELECT * FROM pg_available_extensions`\n\n# Notable Extensions\n### pg_stat_statements\nAllows us to achieve a general system-wide analysis to help us with things like indexing strategy.\n\nIt’s possible to have a list of the most common queries in terms of number of times the query is executed, and the cumulative time it took to execute the query.\n","n":0.079}}},{"i":543,"$":{"0":{"v":"Pgloader","n":1},"1":{"v":"\nPgloader can operate in 2 ways:\n1. Load data from files (ex. CSV)\n2. Migrate the whole db to Postgres (ex. Mysql to Postgres)\n\nEasiest way is to use the docker container:\n```\ndocker run --rm --name pgloader dimitri/pgloader:latest pgloader SOURCE TARGET\n```\n\n# Resources\n[Docs](https://pgloader.readthedocs.io/en/latest/)\n","n":0.162}}},{"i":544,"$":{"0":{"v":"Connecting to Postgres","n":0.577},"1":{"v":"\nWhen a new user connects to the database, postgres will fork a new process.\n- After this point the client and the new process communicate directly, without intervention from the original process.\n- This shows the dependency on processes rather than threads, giving us more stability at the cost of higher connection start-up costs.\n- each connection into Postgres is going to consume about 10 MB of memory from your database\n- These start-up costs can be overcome with pooling\n\nIn postgres, each client connecting to the database has its own process \n- connection can be verified by running `select true as \"Connection test\";`\n\n*Connection String* - the database url used to connect to the database (`postgres:///...`)\n- if we specify the connection string as `postgres:///mydb`, then we are implicitly passing our computer's username and no password, which would be equivalent to `postgres:///kyletycholiz@localhost:5432/mydb` \n\t- spec: are these default variables determined by env variables like PGHOST?\n","n":0.082}}},{"i":545,"$":{"0":{"v":"Connection Pool","n":0.707},"1":{"v":"\n### Why?\n- Connecting a new client to the PostgreSQL server requires a handshake which can take 20-30 milliseconds. During this time passwords are negotiated, SSL may be established, and configuration information is shared with the client & server. Incurring this cost every time we want to execute a query would substantially slow down our application.\n\n- The PostgreSQL server can only handle a limited number of clients at a time. Depending on the available memory of your PostgreSQL server you may even crash the server if you connect an unbounded number of clients. \n\n- PostgreSQL can only process one query at a time on a single connected client in a first-in first-out manner. If your multi-tenant web application is using only a single connected client all queries among all simultaneous requests will be pipelined and executed serially, one after the other. No good!\n\nThe client [[pool|general.patterns.creational.pool]] allows you to have a reusable pool of clients you can check out, use, and return. You generally want a limited number of these in your application and usually just 1. Creating an unbounded number of pools defeats the purpose of pooling at all.\n\n### Implementing\nYou must always return the client to the pool if you successfully check it out, regardless of whether or not there was an error with the queries you ran on the client. If you don't check in the client your application will leak them and eventually your pool will be empty forever and all future requests to check out a client from the pool will wait forever.\n\n### Connection Pooling\n- a pool sits between the postgres frontend (ex. postgraphile, postgres-node) and the postgres server. The pooler speaks the postgres language, so understands all incoming queries.\n\t- Therefore, the server sees the requests as coming from the pg pool, and the postgres frontend sees the pool as handling the requests. \n\t\t- Therefore,\n\t\t\t- without a pool, if we had 20 db connections and 8 were idle, all 20 would be eating up postgres resources (memory)\n\t\t\t- with a pool, those 8 idle connections remain in the pool and don't connect to the database, freeing up resources for us. From the database perspective, there are only 12 connections.\n\t- the frontend/backend paradigm here is no different from in web developent. In postgres, we have users who interact with the database. The client generates a query, and that query gets sent off to the postgres backend, where it interprets the request, performs the actions, then returns the data to the frontend. \n- when a client is connected to the pool, we say it is a \"virtual connection\", and if the pooled client is connected to the database, it is a \"physical connection\" \n- 2 main options: `pgpool-II` and `pgBouncer`\n\t- pgBouncer does one job and does it well. pgpool-II is more of a swiss army knife, with load balancing included\n\t\t- `pgpool-II` seems to be more out-dated\n\t- [comparison](https://scalegrid.io/blog/postgresql-connection-pooling-part-4-pgbouncer-vs-pgpool/)\n\n#### pgBouncer\n- When PgBouncer receives a client connection, it first performs authentication on behalf of the PostgreSQL server.\n\t- when a password is provided, one of 2 things happens:\n\t\t1. PgBouncer first checks the userslist.txt file – this file specifies a set of (username, md5 encrypted passwords) tuples. If the username exists in this file, the password is matched against the given value. No connection to PostgreSQL server is made. \n\t\t2. If passthrough authentication is setup, and the user is not found in the userslist.txt file, PgBouncer searches for an auth_query. It connects to PostgreSQL as a predefined user (whose password must be present in the userslist.txt file) and executes the auth-query to find the user’s password and matches it to the provided value.\n- When the client executes an SQL statement:\n\t1. PgBouncer checks for a cached connection\n\t2. if found, it returns the connection to the client. Otherwise, it creates a new connection\n![a891779b4493983dd662f11960423a72.png](:/bde888d39a1c41fa99e619df6db6f56b)\n","n":0.04}}},{"i":546,"$":{"0":{"v":"Clause","n":1},"1":{"v":"\n## SQL Clause Execution Order\n`FROM ➡ WHERE ➡ GROUP BY ➡ HAVING ➡ SELECT ➡ DISTINCT ➡ ORDER BY ➡ LIMIT`\n\nin other words...\n- The system first executes the FROM clause i.e. it creates the data set on which the WHERE and SELECT clauses are to be run. This step includes any joins specified in the FROM clause.\n- Then, it eliminates rows from this data set by using the filters specified in the WHERE clause.\n- It then groups the data set by the columns specified in the GROUP BY clause, and finally runs the SELECT clause. This is why you can never use the aliases specified in the SELECT clause to filter in the WHERE clause - because the aliases haven’t been declared yet.\n","n":0.09}}},{"i":547,"$":{"0":{"v":"Where","n":1},"1":{"v":"\nWHERE determines which rows will be included in the aggregate calculation\n- this show that the WHERE clause must be evaluated before aggregate functions are computed.\n```sql\n-- WRONG\nSELECT city FROM weather WHERE temp_lo = max(temp_lo);\n\n-- CORRECT\nSELECT city FROM weather\n    WHERE temp_lo = (SELECT max(temp_lo) FROM weather);\n```\n![](/assets/images/2021-03-09-16-30-49.png)\n- In contrast with HAVING, WHERE selects input rows before groups and aggregates are computed\n\t- thus, it controls which rows go into the aggregate computation\n\n- We usually try to keep the where clauses as simple as possible for PostgreSQL in order to be able to use our indexes to solve the data filtering expressions of our queries.\n- the OR operator is more complex to optimize for, in particular with respect to indexes.\n","n":0.093}}},{"i":548,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Delete from one table while using values from another as the condition\nHere, we delete from the `users` table where the `user.id` can be found in the result set. We are doubling up the result set with `UNION`, in an effort to find users that are test users\n\n```sql\ndelete from app_public.users\nwhere id in\n\t(\n\t\tselect user_id from app_public.user_emails where email like 'testuser%@example.com'\n\tunion\n\t\tselect user_id from app_public.user_authentications where identifier = '123456%'\n\t)\n```\n","n":0.123}}},{"i":549,"$":{"0":{"v":"Select","n":1},"1":{"v":"\n### SELECT INTO\n- SELECT INTO creates a new table and fills it with data computed by a query.\n- The data is not returned to the client, as it is with a normal SELECT.\n- The new table's columns have the names and data types associated with the output columns of the SELECT.\n\n#### with `record` type\n```sql\ndo\n$$\ndeclare\n\trec record;\nbegin\n\t-- select the film\n\tselect film_id, title, length\n\tinto rec\n\tfrom film\n\twhere film_id = 200;\n\n\traise notice '% % %', rec.film_id, rec.title, rec.length;\n\nend;\n$$\nlanguage plpgsql;\n```\n","n":0.116}}},{"i":550,"$":{"0":{"v":"Returning","n":1},"1":{"v":"\ntakes in same arguments as the SELECT statement.\n- however, cannot execute aggregate functions like SELECT statements can.\n","n":0.243}}},{"i":551,"$":{"0":{"v":"On Conflict","n":0.707},"1":{"v":"\nThe newly added ON CONFLICT clause allows to specify an alternative to\nraising a unique or exclusion constraint violation error when inserting.\nON CONFLICT refers to constraints that can either be specified using a\ninference clause (by specifying the columns of a unique constraint) or\nby naming a unique or exclusion constraint.  DO NOTHING avoids the\nconstraint violation, without touching the pre-existing row.\n","n":0.131}}},{"i":552,"$":{"0":{"v":"Like","n":1},"1":{"v":"\nuse if you want to match a string against your query\n- ex. return all emails that have *@gmail* in them\n`...WHERE email LIKE '%gmail.com'`\n","n":0.209}}},{"i":553,"$":{"0":{"v":"Insert","n":1},"1":{"v":"\nThere are generally three methods in PostgreSQL with which you can fill a table with data:\n1. Use the `INSERT INTO` command with a grouped set of data to insert new values.\n2. Use the `INSERT INTO` command in conjunction with a `SELECT` statement to insert existing values from another table.\n3. Use the `COPY` (or `\\copy`) command to insert values from a system file.\n\n`insert...on conflict` can be used to implement upsert\n\n### RETURNING\n- allows us to get back the columns that were successfully inserted into the table\n\t- ex. INSERT INTO\n- Using this method, we can get back default values (like id or timestamps)\n- This method is also useful to insert rows into a variable, which we can then simply return from the whole function\n```sql\ndeclare\n  v_person forum_example.person;\nbegin\n\tinsert into forum_example.person (first_name, last_name) values\n\t\t(first_name, last_name)\n\t\treturning * into v_person;\n```","n":0.087}}},{"i":554,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### Inserting a composite type (a whole row at once)\n```sql\ninsert into stats.daily_registration_historical (registration_date, count_registrations)\nselect * from app_private.daily_registration;\n```\n\n##### Insert all default values\n```sql\ninsert into users default values\n```\n\n##### Insert multiple rows\n```sql\ninsert into countries (code) values ('CA'), ('US'), ('FR')\n```\n\n##### Bulk insert\nImagine you need to split a table in two, and move half of the rows from the first to the second:\n```sql\ninsert into second_table (col1, col2) select a, b from first_table;\n```\n","n":0.123}}},{"i":555,"$":{"0":{"v":"Having","n":1},"1":{"v":"\nThe WHERE clause allows you to filter rows based on a specified condition. However, the HAVING clause allows you to filter groups of rows according to a specified condition.\n- In other words, the WHERE clause is applied to rows while the HAVING clause is applied to groups of rows\n- The WHERE clause applies the condition to individual rows before the rows are summarized into groups by the GROUP BY clause. However, the HAVING clause applies the condition to the groups after the rows are grouped into groups.\nex. from a People table, we can SELECT income, and \"filter\" in the rows that have income > 1000000: `... HAVING income > 1000000;`\nIn contrast with WHERE, HAVING selects group rows after groups and aggregates are computed. Thus, the WHERE clause must not contain aggregate functions; it makes no sense to try to use an aggregate to determine which rows will be inputs to the aggregates.\n- The HAVING clause almost always contains aggregate functions.\n","n":0.079}}},{"i":556,"$":{"0":{"v":"Group by","n":0.707},"1":{"v":"\nThe group by clause introduces Aggregates (aka Map/Reduce) in Postgres, which enables us to map our rows into different groups and then reduce the result set into a single value.\n\nGROUP BY allows us to divide rows returned from SELECT into groups. It is only applicable to aggregate functions\n- GROUP BY groups rows sharing a property so that an aggregate function can be applied to each group\n\t- for each group, we can apply an aggregate function, ex `SUM()`\n\nSimilar to JOINs in the sense that it is about \"bringing related data to the same place\", which is done by grouping records by some key.\n\nGROUP BY allows implementing much the same thing than `map`/`reduce` do in Javascript\n- ie. map your data into different groups, and in each group reduce the data set to a single value.\n\nGROUP BY produces a new table reference with only as many columns as are listed after the GROUP BY clause\n- Note that other columns may still be available as arguments of aggregate functions\n- ex. `GROUP BY \"nugget\".id, \"nugget\".title` will reduce the number of available columns in all subsequent logical clauses (inc. SELECT) to 2.\n\t- This is the syntactical reason why you can only reference columns from the GROUP BY clause in the SELECT clause.\n\nIf we simply GROUP BY an id and don't apply any aggregate function, then the functionality will be the same as DISTINCT (remove duplicate rows)\n\t- this is why GROUP BY only really becomes valuable once we add an aggregate function\n\n#### Multiple columns in GROUP BY\n`GROUP BY X` means put all those with the same value for X in the one group\n`GROUP BY X, Y` means put all those with the same values for both X and Y in the one group.\n\n## Examples\n### Average salary by position\n- ex. imagine we have an `employee` table and we are interested in average salaries. Naturally, we have different employee positions, so if we ran the aggregate function AVG, we would get back an average of every salary:\n```sql\nSELECT avg(salary)\nFROM \"employee\"\n```\nsince the average of all salaries is not that interesting, we want to get the average of salaries GROUPed BY position\n```sql\nSELECT position, avg(salary)\nFROM \"employee\"\nGROUP BY position\n```\n- note: if we didn't have the GROUP BY clause here, we would get an error\n\t- here, GROUP BY basically says \"let's see the aggregate data *per* position\"\n- Sometimes we may see syntax such as `GROUP BY 2`. Here, 2 refers to the second column, and would be equivalent to writing out the actual column name.\n","n":0.049}}},{"i":557,"$":{"0":{"v":"Delete","n":1}}},{"i":558,"$":{"0":{"v":"Delete","n":1},"1":{"v":"\n#### Delete using join\n`DELETE JOIN` isn't supported, but we can imitate the functionality:\n```sql\nDELETE FROM table_name1\nUSING table_expression -- the table(s) we are \"joining\"\nWHERE condition\nRETURNING returning_columns;\n```\n","n":0.204}}},{"i":559,"$":{"0":{"v":"Cascade","n":1},"1":{"v":"\nwhenever rows in the parent (referenced) table are deleted/updated, the respective rows in the child (referencing) table with matching FK will be deleted/updated as well\n- put another way, not only do we delete an object (ex. a schema), but we delete everything contained within that object (ex. tables)\n\n- ex. if we mark the `nugget_id` of `nugget_bucket` table as CASCADE, when a row in the `nugget` table is deleted, all rows in `nugget_bucket` table that reference that nugget will also be deleted\n","n":0.111}}},{"i":560,"$":{"0":{"v":"As","n":1},"1":{"v":"\nalias a column name as you are selecting it\n- ex. `SELECT title AS bucket_title...`\n\nalias a tablename\n- ex. `SELECT ___ from users as u`\n\n- the alias only exists during execution of the query\n- if you use column aliases in the SELECT clause, you cannot use them in the WHERE clause.\n","n":0.143}}},{"i":561,"$":{"0":{"v":"Alter","n":1}}},{"i":562,"$":{"0":{"v":"Alter","n":1},"1":{"v":"\n### Alter column to have unique constraint\n`ALTER TABLE foo ADD UNIQUE (column_name);`\n\n### add/drop column constraint\n```sql\nalter table invoice_items alter column invoice_id set not null;\nalter table invoice_items alter column invoice_id drop not null;\n```\n\n### Add non-nullable column to a table\nThis can be tricky, since running\n```sql\nalter table invoice_items add column payment_id uuid not null ...\n```\n\nwill result in an error, since we are trying to modify existing rows in a table to have a `null` value for the new column\n\nInstead, we have a bit of a workaround, setting some junk default value to get around the constraint.\n```sql\nalter table mytable add column mycolumn character varying(50) not null default 'foo';\n-- ... some work (set real values as you want)...\nalter table mytable alter column mycolumn drop default;\n```\n\nIn the case where we are adding a non-nullable FK, we can take a bit of a different approach:\n```sql\nalter table invoice_items add column if not exists payment_id uuid references payments;\n-- ... some work (set real values as you want)...\nalter table invoice_items alter column payment_id set not null;\n```\n\n#### Alternative approach: check constraint\n[[Differences between a column NOT NULL constraint and a CHECK CONSTRAINT not null|dendron://code/sql.constraint#differences-between-a-column-not-null-constraint-and-a-check-constraint-not-null]]\n\nThe trick here is that you can issue a `NOT VALID` option when adding a check constraint. This will tell PostgreSQL that you are aware that the constraint may not be valid on existing data and that it does not have to check it. Subsequent inserts or updates, however, will be enforced.\n```sql\nalter table invoice_items add constraint payment_id_not_null check (payment_id is not null) not valid;\n```\nAfter you have done your operations, then we can `validate` the sql:\n```sql\nalter table invoice_items validate constraint payment_id_not_null;\n```\nThis will scan the table and ensure that the constraints are all passing\n","n":0.061}}},{"i":563,"$":{"0":{"v":"Admin","n":1},"1":{"v":"\n# Overview\n- When you install an instance of Postgres (ex. in Node), there will be a corresponding Postgres server, since postgres uses a client/server model.\n\t- if we use different ports, we can have multiple Postgres servers on a single physical server\n\n## Processes\n### Postgres Server\n- the server program is called `postgres`\n- purposes:\n\t- manages database files\n\t- accepts connections from clients\n\t- performs operations on behalf of clients\n\n### Postgres Client\n- the client is the application that wants to perform database operations\n- clients are diverse in nature, and could be a CLI, GUI, or a library within a web server (like `pg` in node.js)\n\n- Postgres' value is about concurrency and isolation\n\t- The idea: what happens when 2+ people are trying to do the same thing concurrently? how is that handled?\n\t\t- This value is provided by the concept of ACID\n\t\t- The fact that we have transactions in postgres means that we are ACID-compliant.\n","n":0.082}}},{"i":564,"$":{"0":{"v":"Users","n":1},"1":{"v":"\neach database cluster has a set of database users, which are distinct from the users that the OS of the server manages.\n- database users are global across a cluster installation.\n\nusers own database objects, such as tables. These owners can assign priveleges on those objects to other users.\n\nfreshly initialized postgres systems will always contain a predefined user with ID 1, which has the same name as the OS user that initialized the db cluster. However, it is often a best practice to name this user `postgres` instead.\n- this is a superuser\n- To create more users, we need to connect as this user first.\n","n":0.099}}},{"i":565,"$":{"0":{"v":"Settings","n":1},"1":{"v":"\n### System Administration\nthere are 2 functions (getter/setter) that allow us to query and alter run-time configuration parameters:\n1. `current_setting` - returns the current `value` of the setting associated with the provided `key`\n2. `set_config` - pass the setting name and the new value.\n- these functions are executed with a SELECT statement. The result is displayed as a table.\n","n":0.134}}},{"i":566,"$":{"0":{"v":"Search Path","n":0.707},"1":{"v":"\nImplicitly, every object in postgres gets operated on as a path. running `...into nuggets`, postgres runs under the hood `...into neverforget.public.nuggets`.\n- We can run `SHOW search_path;` to get the path\n\t- the first element of the output `$user` indicates that a schema with the same name as current user (ex. a schema called `kyletycholiz`) should be searched first\n\t- the first schema listed (default `public`) is the default location for creating new objects \n\t\t- when objects are referenced without qualifying a schema (ex. `...into nuggets`), the search_path is traversed, starting with public, and onto the next one (by default only 1, so we need to specify more) \n- to modify the `search_path`, we can run `SET search_path TO myschema,public;`, which will inform postgres that `myschema` should be searched first when there is no specified schema.\n","n":0.087}}},{"i":567,"$":{"0":{"v":"Dir","n":1},"1":{"v":"\n# Database File\n- known by the environment variable PGDATA, this is where all the data for a database cluster is stored.\n\t- likely `/etc/postgresql/<VERSION>/main/`\n\n### base/\n- each directory within `base/` is used by a database node in our cluster. Each directory is named after the database's OID\n\t- the files inside are the actual data of the relations (ie. tables, indexes, sequences)\n- database examples: `postgres`, `template0`, `template1`, `neverforget`, which can be seen from psql with `\\l`\n\t- the `postgres` database is not required to exist. It is simply a default that is connected to if no database is specified in the connection string.\n\t- `CREATE DATABASE` works by copying the `template1` database (`template0` can be thought of as a more primitive and stripped down version of `template1`\n\n### pg_hba.conf\n- a file that controls the authentication of clients to connect to the postgres database\n\t- HBA stands for host-based authentication.\n- after making changes to this file, to put them into effect run `SELECT pg_reload_conf();` or `pg_ctrl reload` with superuser.\n- In `psql`, run `SHOW hba_file;`\n","n":0.078}}},{"i":568,"$":{"0":{"v":"Authentication","n":1},"1":{"v":"\nto connect to a PG databse, there are a few different ways to have users authenticate themselves\n- the default authentication method can be found in the `pg_hba.conf` file.\n\t- therefore, if we want to change the default method from peer authentication to md5 (password), we change it here (remember to restart the service)\n\n## Peer Authentication\n- by default, `psql` tries to connect to the postgres database over UNIX sockets. The default authentication method is *peer authentication*, which requires the current UNIX user to have the same username as `psql`\n\t- spec: Therefore, to connect with peer authentication, we need to be logged in on UNIX as the same username as the postgres username we are trying to connect with\n\t- ex. if on UNIX we are logged in as user `kyletycholiz`, then simply executing `psql` without arguments will try and log us in as the postgres user `kyletycholiz`. If this user doesn't exist in postgres, then we will get a peer authentication error.\n- works by obtaining the client's OS username from the kernel, and using it as the allowed database username.\n- Only supported for local connections.\n","n":0.074}}},{"i":569,"$":{"0":{"v":"Acid","n":1},"1":{"v":"\n### How Postgres handles Transactions and Concurrency\nPostgres handles these 2 problems with MVCC (multi-version concurrency control)\n- When you update or delete a row, Postgres doesn't actually remove the row. When you do an UPDATE or DELETE, the row isn't actually physically deleted. For a DELETE, the database simply marks the row as unavailable for future transactions, and for UPDATE, under the hood it's a combined INSERT then DELETE, where the previous version of the row is marked unavailable. These new versions of rows are generally referred to as the \"live\" rows, and the older versions are referred to as \"dead\" rows.\n\n## UE Resources\n[Concurrency control](https://www.postgresql.org/docs/current/mvcc-intro.html)\n","n":0.098}}},{"i":570,"$":{"0":{"v":"Pg Xl","n":0.707},"1":{"v":"\nPostgresXL is a horizontally scalable SQL database cluster\n- It is equipped to handle write-intensive workloads that depend on atomic transactions which are potentially distributed\n\t- OLTP\n- PostgresXL allows you to either partition tables across multiple nodes, or replicate them.\n\t- Partitioning (or distributing) tables allows for write scalability across multiple nodes as well as massively parallel processing (MPP) for Big Data type of workloads.\n\t\t- MPP is using many computers to perform a set of coordinated computations in parallel\n\n## Components of Postgres-XL\n### Global Transaction Monitor\n- ensures cluster-wide transaction consistency\n\t- responsible for issuing transaction ids and snapshots as part of its Multi-version Concurrency Control.\n\n### Coordinator\n- manages the user sessions and interacts with GTM and the data nodes\n\t- parses and plans queries, and sends down a serialized global plan to each of the components involved in a statement.\n\n### Data node\n- where the actual data is stored.\n\t- The distribution of the data can be configured by the DBA.\n- warm standbys of the data nodes can be configured to be failover-ready.\n","n":0.078}}},{"i":571,"$":{"0":{"v":"Paradigm","n":1}}},{"i":572,"$":{"0":{"v":"Reactive","n":1},"1":{"v":"\na declarative programming paradigm concerned with [[data streams|general.arch.streaming]] and the propagation of change.\n\nReactive programming is well-suited for implementing interactive user interfaces and real-time system animation.\n- It's really no coincidence that modern front-end frameworks like React, Svelte, Vue etc. all implement this reactive type of paradigm.\n  - ex. in React, if a piece of state changes, the React engine internally understands the dependency tree of that value. Because of this, it knows when to re-render components, because its dependencies have changed.\n- Microsoft Excel is another program that implements the reactive paradigm. Consider that a calculation in a single cell has dependencies. When the value of those dependencies change, it must propogate to the cell to do a recalculation.\n\nAny component of a system that exists beyond a network call (e.g. a backend database) cannot effectively be a part of this reactive loop, since we can't realistically keep queries updated in real-time.\n- instead database interactions are modeled as side-effects which must interact with the reactive system.\n\n### Example\nIn most programming languages, `a = b + c` will result a value being assigned to `a`. Later on if `b` or `c` changes, it has no impact on the value of `a`, since that value has already been declared.\n\nIn reactive programming, this is not the case. Any future changes to `b` or `c` will cause `a` to \"react\" and update. Therefore, `a` has dependencies of `b` and `c`, and this is registered somewhere. As a result, the language runtime knows based on this dependency graph when recalculations should be made.","n":0.063}}},{"i":573,"$":{"0":{"v":"Procedural","n":1},"1":{"v":"\nIn procedural programming, a program is divided into smaller parts called methods, which are the basic building blocks.\n- methods are how we achieve code reusability\n\n","n":0.2}}},{"i":574,"$":{"0":{"v":"Oop","n":1},"1":{"v":"\n## What is OOP?\nThink of pure OOP as \"objects talking to other objects\"\n\n## Why use OOP?\nOOP chiefly offers flexibility and extensibility\n- Once code is tested and delivered, it rarely has to be changed. There is little fear of new classes breaking old ones.\n  - contrast this with [[procedural programming|paradigm.procedural]], which might involve adding new conditional checks for new features, which would require tests to be written, and carry a high degree of risk that original parts of the program will break.\n\nOOP is for the next programmer, not for yourself or for performance. Secondly, when you need to start dynamically customising behaviour (changing class methods on the fly), it becomes more like writing little programs in their own right that interact. OOP even epitomises the unix philosophy (albeit without forcing interactions to be via text on the command line)\n\nWhen OOP is used well it really clarifies the code, and is a solid way to break down complex algorithms into simpler pieces. A good set of classes is like a well run kitchen where everyone knows their own job, the communication is simple, and no one is stepping on anyone's toes.\n- A poor choice of classes ranges from confusing to a real mess depending on how liberally inheritance was used.\n\n## Deciding when it should be a class\nThe fact that something sounds like an object does not automatically mean that it should be an object in your program. Reflexively writing classes for every concept in your application tends to leave you with a collection of interconnected objects that each have their own internal, changing state. Such programs are often hard to understand and thus easy to break.\n- This is in reference to a program that simulates a village with roads between houses, and a robot that delivers parcels to each house. The program has to track the state of the parcel delivery status, as well as the robot's location in the town. Consider that if our instinct was to have classes for the robot, one for a parcel, maybe one for places, we would have to hold state in objects of different classes. The robot's current location would probably be held in the `Robot` class, while the delivery status of a parcel would probably be held in the `Parcel` class. Furthermore, a location would probably hold the state of all the parcels that are currently delivered to it. Having this state spread out everywhere makes the program hard to understand (and know which of these objects will need to have their state updated).\n  - [source](https://eloquentjavascript.net/07_robot.html#p_XP2aQths2D)\n\nWhen considering how granular we should make our classes, it makes sense to think high-level. Ask the question, \"how can I represent the state of this program in a central and simple way, where I can update the state in a single place in the event of a business-level action (such as moving location, dropping off a letter etc)\". If we had a postman program, which mails a stack of letters to different householders, a natural inclination might be for us to create classes for postman, village etc. Instead, we should consider what the minimal scope of state is. In this example, we only really need to keep track of how many letters there are, and where the current location is. Realistically, all we need is a `VillageState` class.\n```js\nclass VillageState {\n  constructor(place, parcels) {\n    this.place = place;\n    this.parcels = parcels;\n  }\n\n  move(destination) {\n    if (!roadGraph[this.place].includes(destination)) {\n      return this;\n    } else {\n      let parcels = this.parcels\n        .map(p => {\n          if (p.place != this.place) return p;\n          return {place: destination, address: p.address};\n        })\n        .filter(p => p.place != p.address);\n      return new VillageState(destination, parcels);\n    }\n  }\n}\n```\n\nLet's apply the above lesson to TicTacToe. If we were to blindly make every item into it's own object, we would have classes for `Game`, `Board`, `Player`, `Square`. Resisting that temptation, think about how we might go about centralizing the state and the methods that change them. For instance, we don't really need a `Square` class. The duty of a `Square` class presumably is to keep track of the square's current state (empty, X, 0). But this could just as easily be managed by the same class that represents the `Board`. If we used a `Square` class, then we would have to reach into it every time we want to make a calculation about the state of the game.\n- spec: when deciding on what classes to make, think in terms of what state you need to store, and at what level would be appropriate?\n  - ex. in TicTacToe, it might make sense to model a `Player` class, if you are going to use it to determine each player's currently occupied squares (ex. `[0, 4, 7]`). Alternatively, you may decide that that data can be safely stored in the `Game` object as 2 properties, `player1Choices` and `player2Choices`.\n\nIn OOP, dependencies are things that you need to provide to the constructor of the class to create an instance of it.\n\n* * *\n\n## Example: Implementing TicTacToe\nCreate a player, game, and board class. The player class will choose moves, the board class will hold the state of the moves, and the game class will run the game and declare the winner.\n\nIf you want to make this project more challenging then make sure the game class can accept either human or computer players (and the code is exactly the same from the game class's perspective). This will allow you to practice class inheritance and also you can learn to build a simple ai. For the ai you can start with a simple computer that picks randomly. Once you get that done you can basically build a tree structure which builds all the possible future outcomes and make sure the computer selects the most favorable one.\n\n# E Resources \n[TicTacToe Repo in OOP: a great breakdown to understand OOP, without clutter of anything not related to TicTacToe](https://github.com/Anna-Myzukina/Tic-Tac-Toe-OOP)\n","n":0.032}}},{"i":575,"$":{"0":{"v":"Substitutability","n":1},"1":{"v":"\nSubstitutability is an OOP principle stating that we should be able to replace an object (e.g. a class) with a sub-object (e.g. a class that extends the first class) and not break the program.\n\nFormally known as the *Liskov substitution principle* (the *L* in SOLID)","n":0.151}}},{"i":576,"$":{"0":{"v":"Polymorphism","n":1},"1":{"v":"\n# UE Resources\n- [Good explanation](https://stackoverflow.com/questions/154577/polymorphism-vs-overriding-vs-overloading)\n","n":0.447}}},{"i":577,"$":{"0":{"v":"Overriding","n":1},"1":{"v":"\nWhen we have a method in the child class and a method in the parent class, and they both have the same name. In this case, the method defined in the child would override the parent's version.\n\nOverriding implements Runtime [[Polymorphism|paradigm.oop.polymorphism]]\n","n":0.158}}},{"i":578,"$":{"0":{"v":"Overloading","n":1},"1":{"v":"\nWhen two or more methods in the same class have the same name but different parameters, it’s called Overloading.\n- This is distinct from `Overriding` which is when we have a method in the child class and a method in the parent class, and they both have the same name. In this case, the method defined in the child would override the parent's version.\n\nOverloading implements Compile time polymorphism\n","n":0.122}}},{"i":579,"$":{"0":{"v":"Keywords","n":1},"1":{"v":"\n### Access modifiers\n- public\n- private\n- internal\n- `protected` \n    - only inherited classes can access this method\n\n### Abstract classes\nAbstract classes cannot be instantiated, but they can be subclassed.\nWhen an abstract class is subclassed, the subclass usually provides implementations for all of the abstract methods in its parent class. However, if it does not, then the subclass must also be declared abstract.\n\nAbstract classes are for:\n- code sharing among related classes\n- we expect the classes that extend from the abstract classes to have a huge overlap of fields/methods.\n\n### Static method\na static method is part of class, but we do not need to create a instance to call it\n- because static methods do not belong to an object, they therefore cannot be used to access any variables that belong to an object.\n- contrasted with *instance methods*, which must belong to an instance of a class.","n":0.084}}},{"i":580,"$":{"0":{"v":"Inheritance","n":1},"1":{"v":"\nIf [[composition|paradigm.oop.composition]] is about *using* another class/interface, inheritance is about *being* one.\n\nInheritance is most useful for \n- grouping related sets of concepts\n- identifying families of classes\n- organizing the names and concepts that describe the domain\n\n## How to use inheritance\nInheritance is best suited for *differential programming*, whereby we are making something that could be considered a variation of something else. That is, the new thing has some tweaks and enhancements of the original thing.\n- this is appropriate, since when we instantiate the new class, we still want to preserve the interface of the original class.\n\nInheritance should only be used when:\n- Both classes are in the same logical [[domain|general.principles.DDD]]\n- The subclass is a proper subtype of the superclass\n- The superclass’s implementation is necessary or appropriate for the subclass\n- The enhancements made by the subclass are primarily additive.\n\n\n## How to misuse inheritance\nInheritance is prone to misuse.\n\n### Inheritance gives us the properties / methods of its ancestors\nConsider the class\n```js\nclass Stack extends ArrayList {\n  public void push(value: Object) { … }\n  public pop(): Object { … }\n}\n```\n\nThe interface of an instance of this class is bloated. It's reasonable to expect an interface of the class `Stack` to have only `push` and `pop`, but it also includes `get`, `set`, `add`, `remove`, `clear`, among others.\n\nIn carrying out this pattern, we committed 3 mistakes:\n- By inheriting, we implicitly acknowledged that \"a Stack is an ArrayList\". However, this is not true, since `Stack` is [[not a proper subtype|paradigm.oop.substitutability]] of `ArrayList`, because a `Stack` is supposed to enforce *last-in-first-out*. Yes, the `push` / `pop` interface supports that, but the fields exposed by `ArrayList` violate that.\n- By inheriting, we are violating [[paradigm.oop.encapsulation]], since we use `ArrayList` to hold the stack’s object collection. This is an implementation choice that should be hidden from consumers.\n- By inheriting, we are creating a cross-domain relationship. There are 2 different concepts at play here: a randomly-accessible collection (ArrayList), and a [[queue|general.lang.data-structs.queue]] (Stack)\n\n### Mixing Domain classes and Implementation classes\nImagine we wanted to make a variable that represented a subset of customers. Our instinct might be to inherit from `ArrayList` like so:\n```js\nclass CustomerList extends ArrayList {}\n```\n\nThe problem here is that `CustomerList` is a domain class, while `ArrayList` is an implementation class. Anything from the implementation layer should be invisible at the domain layer. Instead, domain classes should *use* implementation classes, not *inherit* from them.\n- *domain* - what our software does (some relation to business logic)\n- *implementation* - how our software works\n\nInstead, we should take an approach where we:\n1. create implementation classes (ie. our mechanical, non-domain structures) by inheriting from utility classes\n2. use these mechanical structures in our domain classes via [[composition|paradigm.oop.composition]], rather than inheritance. In other words, we should not have our domain classes inherit from implementation classes.\n\n## E Resources\n- [Composition vs Inheritance](https://www.thoughtworks.com/en-ca/insights/blog/composition-vs-inheritance-how-choose)","n":0.047}}},{"i":581,"$":{"0":{"v":"Encapsulation","n":1},"1":{"v":"\nEncapsulation is essentially about protecting the private data of an object such that they can only be accessed or mutated via the interface (ie. public API) that is exposed by that object.\n- in the following example, we can only manipulate `count` by calling `increment`; we cannot directly modify it.\n```js\nclass Counter {\n  private count = 0\n\n  public increment() {\n    count++\n  }\n}\n```\n\nGetter and Setter methods should exist for all instance variables that we want to change.\n- By forcing other code to go through setter methods, the setter method can validate the parameter and decide if it’s do-able\n  - ex. \"are negative numbers for this variable allowed?\"\n- Setters and Getters also unify the interface so that consumer code isn't broken\n  - ex. if a consumer accessed an instance variable directly, and then the class was modified so it became `private` with an accompanied Getter (because, for example we wanted to add some logic checks), it would break everyone's code.\n\nEncapsulation refers to the bundling of data with the methods that operate on that data, all the while restricting of direct access to some of an object's components\n\nEncapsulation is used to hide the values or state of a structured data object inside a class, preventing direct access to them by clients in a way that could expose hidden implementation details or violate state invariance maintained by the methods.\n\nTraditionally, JavaScript developers used `_` to prefix the properties or methods that they intended to be private.\n","n":0.065}}},{"i":582,"$":{"0":{"v":"Composition","n":1},"1":{"v":"\nIf [[inheritance|paradigm.oop.inheritance]] is about *being* another class/interface, composition is about *using* one.\n\nwhen a Field’s type is a class, the field will hold a reference to another object, thus creating an association relationship between them. Let’s intuitively define composition as when the class uses another object to provide some or all of its functionality.\n","n":0.137}}},{"i":583,"$":{"0":{"v":"Imperative","n":1},"1":{"v":"\nA language is imperative when they make explicit references to the state of the execution environment\n- think about how calling .open() on a modal relates to this, versus setting a variable isOpen that is listened to by the modal. .open() is explicitly referencing the state of the environment \n","n":0.143}}},{"i":584,"$":{"0":{"v":"Functional Programming","n":0.707},"1":{"v":"\n# Overview\n\"function\" is a reference to how functions work in math.\n- ex. f(x) = 2x² + 5 can be plotted on a graph to make a parabola. In this sense, it can be thought of as a map. An x value of 5 maps to a return value of 55.\n\nThough FP rarely deals with graphs like this, think of it as input values mapping to output\n![](/assets/images/2021-03-09-09-35-55.png)\n\nfunctional languages are closer to math than the more popular imperative languages.\n\nFunctions as [[first-class citizens|general.lang.feat.functions.first-class]] is a key component of Functional Programming, since using higher-order components to achieve composition is a standard practice (remember, `map`, `filter` and `reduce` are all HOCs)\n\nBased on idea of Referential Transparency\n\n### Referential transparency\nThe notion that a function could be replaced by its return value and it wouldn't impact the functionality of the program.\nIf satisfied, this is a clear sign that a function is pure.\nas you're reading a program, once you've mentally computed what a pure function call's output is, you no longer need to think about what that exact function call is doing when you see it in code, especially if it appears multiple times.\nThat result becomes kinda like a mental const declaration, which as you're reading you can transparently swap in and not spend any more mental energy working out.\n\n\n## Abstracting (Generalizing) Functions\nSimilar to how partial application and currying (see Chapter 3) allow a progression from generalized to specialized functions, we can abstract by pulling out the generality between two or more tasks. The general part is defined once, so as to avoid repetition. To perform each task's specialization, the general part is parameterized.\n\nconsider:\n```js\nfunction saveComment(txt) {\n    if (txt != \"\") {\n        comments[comments.length] = txt;\n    }\n}\n\nfunction trackEvent(evt) {\n    if (evt.name !== undefined) {\n        events[evt.name] = evt;\n    }\n}\n```\nthe repetition (generality) between the 2 functions is: storing a value in a data source. the uniqueness (specialty) of them is that one sticks the value on the end of an array, while the other sets the value as a property on an object.\n\nabstracting:\n```js\nfunction storeData(store,location,value) {\n    store[location] = value;\n}\n\nfunction saveComment(txt) {\n    if (txt != \"\") {\n        storeData( comments, comments.length, txt );\n    }\n}\n\nfunction trackEvent(evt) {\n    if (evt.name !== undefined) {\n        storeData( events, evt.name, evt );\n    }\n}\n```\n","n":0.053}}},{"i":585,"$":{"0":{"v":"Transducer","n":1},"1":{"v":"\nA transducer is a function which takes in a reducer, and returns another reducer\n- A transducer takes an object or array, iterating through each value, transforming each element with a composition of transformer functions\n\nThe transduce function is really just a reduce function with an additional argument upfront:\n```js\n// With reduce\nR.reduce(R.flip(R.append), [], autobots)\n\n// With transduce\nR.transduce(transform, R.flip(R.append), [], autobots)\n```\nthe above reduce function iterates the elements of our `autobots` array and appends them to the accumulator\n\nWhen we use `transduce()`, we are passing each item from our list into our transformation function before passing it to our reducing function. So basically, transduce is just a way for us to transform items while reducing them. We are, in fact, *transducing*\n\ntransducers are a generic and composable way to operate on a collection of values, producing a new value or new collection of new values\n\nIn this example, we can see the power of `transduce`. The first way, we need to iterate over `autobots` a total of 3 times. However, when we use `transduce` as in the second way, we only need to iterate over it once.\n- The traditional way, is that we perform `map` on each element of the array, then `map` on each element again, then `filter` on each element. With transduce, we perform `map` on the first element, then `map` again on the first element, then `filter` on the first element. The resulting value is then mapped to the output array, then the second element is performed on.\n```js\nlet autobots = ['Optimus Prime','Bumblebee','Ironhide','Sunstreaker','Ratchet']\n \n// Filter for autobots that contain 'r', uppercase, then reverse\nlet transform = R.compose(\n  R.filter(x => /r/i.test(x)),\n  R.map(R.toUpper),\n  R.map(R.reverse)\n)\n \n// BEFORE\ntransform(autobots)\n// => [ 'EMIRP SUMITPO', 'EDIHNORI', 'REKAERTSNUS', 'TEHCTAR' ]\n\n// AFTER\nR.transduce(transform, R.flip(R.append), [], autobots)\n// => [ 'EMIRP SUMITPO', 'EDIHNORI', 'REKAERTSNUS', 'TEHCTAR' ]\n```\n\nThe word ‘transducer’ itself can be split into two parts that reflect this definition: \n- `transform` — to produce some value from another (ex. `reduce`/`map`).\n- `reducer` — to combine the values of a data structure to produce a new one.\n- this is a really cool concept, because it allows us to abstract away any implementation detais that we\n\n`.map` and `.filter` can be implemented with `.reduce`. Therefore, we see that `.reduce` is a more general function, and `.map` and `.filter` are more specific implementations of `.reduce`. Transduce allows us to treat a reduce-like construct as a *higher order function*, meaning we can pass in the *reducing function* (which is the function found in a reducer)\n- This means we could create `.map`/`.filter` with a transducer\n[more info](https://www.jeremydaly.com/transducers-supercharge-functional-javascript/)","n":0.049}}},{"i":586,"$":{"0":{"v":"Recursion","n":1},"1":{"v":"\nIn general, recursive code helps us avoid complex nested loops and is most useful for tasks that can be defined in terms of similar subtasks.\n\nEach recursive function consists of 2 parts:\n1. Base Case: The base case is where further calls to the same function stop, i.e, it does not make any subsequent recursive calls.\n2. Recursive Case: The recursive case is where the function calls itself again and again until it reaches the base case.\n    - Each child function call returns result to its parent function.\n\nBoth recursion and iteration depend on a condition which determines whether to stop execution or continue it.\n\n## Methods to understand flow of recursive function\n### Visualizing through a stack\nThe concept of a stack is critical in recursion. When you start visualizing your function calls through a stack and how it returns when reaching a base case - the concept of recursive calls and their output becomes easier to comprehend.\n\nRecursing:\n![](/assets/images/2021-10-09-15-16-13.png)\nReturning values to lower stacks:\n![](/assets/images/2021-10-09-15-16-42.png)\n\n### Making a recursive tree\nConsider that each function call must return before subsequent code can be run. In the first recursive step (ie. \"iteration 1\"), the recursing function does not actually return until the base-case is complete. In fact, it is the *final* call of that function to actually return.\n![](/assets/images/2021-10-09-21-07-35.png)\n![](/assets/images/2021-10-09-21-11-00.png)\n* * *\n\n### Parallels with mathematics\nMentally, what's happening is similar to when a mathematician uses a Σ summation in a larger equation. We're saying, \"the max-even of the rest of the list is calculated by `maxEven(...restNums)`, so we'll just assume that part and move on.\"\n```js\nfunction maxEven(num1, ...restNums) {\n\tvar maxRest = restNums.length > 0 ? maxEven(...restNums) : undefined;\n\n\treturn (num1 % 2 != 0 || num1 < maxRest) ? maxRest : num1;\n}\n\nconsole.log(maxEven(1, 6, 3))\n```\n\nOnce the base case is met, the final return value makes its way back up all layers of the call stack to give us that value\n```js\nfunction foo(x) {\n    if (x < 5) return x;\n    return foo( x / 2 );\n}\n```\n\n![](/assets/images/2021-03-09-09-41-09.png)\nRecursion is declarative for algorithms in the same sense that Σ is declarative for mathematics.\n\nusing `reverseString` as an example:\n![[paradigm.functional.recursion.cook#reverse-a-string,1:#*]]\n\nspec: consider what we are doing here, conceptually. We are taking the last character, putting it at the front, and calling the function again with the last character popped off. Each time we recurse, we build up the new string. That is, each \"iteration\" adds one character to the new string. That whole idea can be summed up with just this part of the function: `string[string.length - 1] + ...` (ie. the first part of the return value). Assuming you understand the logic taking place each \"iteration\" (ie. get the last character of a truncated (by one character) string), then you don't really need to focus on the recursive-function-call part of the return value. This logic taking place is reflected in the value that is passed to the recursive function. Once you've internalized this, then you can inherently understand the values you are working with. Given `string[string.length - 1] + ...`, we know that we are just adding the last character to the front.\n\n![[general.lang.feat.functions#memory-allocation-of-recursive-functions]]","n":0.045}}},{"i":587,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Factorial function\nImplementing a factorial is a good starting-point for coming to grips with recursion. It is a simple implementation, and the logic lends itself well to a recursive solution.\n\n```js\nfunction factorial(num) {\n    if (num === 1) return 1\n    return num * factorial(num - 1)\n}\n```\n\nGiven `factorial(5)`, each \"iteration\" looks like this:\n1. return 5 * factorial(4)\n2. return 4 * factorial(3)\n3. return 3 * factorial(2)\n4. return 2 * factorial(1)\n5. return 1\n\nConsider that when we see `factorial(num - 1)`, we really don't know what this value is going to be for each iteration until the very end. This is part of what makes it mentally challenging to parse. If we work backwards and plug in the return value of each step into the return value of the previous step's call to `factorial()`, then it's more clear how the value from the final \"iteration\" bubbles up.\n\n\n### Reverse a string\nWe start from the end of the string. We take the last character of the string string[string.length - 1] and call the function reverseString() again, but with the last character removed. When this child function returns, this string[string.length - 1] will be appended at the beginning of the returned string.\n```js\nfunction reverseString(string) {\n  // Base case\n  if (string === \"\") {\n    return string;\n  }\n\n  // Recursive case\n  else {\n    // take the last character and add it to the reverse of the previous characters\n    return string[string.length - 1] + reverseString(string.substr(0, string.length - 1));\n  }\n}\n```\n\n### Simulate reduce with recursion\neach call of the function separates the first argument from the rest, takes something from that first argument, then calling the function again with the remaining arguments (all but first)\n```js\nfunction sum(num1,...nums) {\n    if (nums.length == 0) return num1;\n    return num1 + sum( ...nums );\n}\n```\n```js\nfunction maxEven(num1,...restNums) {\n    var maxRest = restNums.length > 0 ?\n            maxEven( ...restNums ) :\n            undefined;\n\n    return (num1 % 2 != 0 || num1 < maxRest) ?\n        maxRest :\n        num1;\n}\n```\n","n":0.057}}},{"i":588,"$":{"0":{"v":"Functional Purity","n":0.707},"1":{"v":"\n### Purifying functions\n- Use immutable data structures (ex. Immutable.js)\n- Sometimes you can just shift the side effects out of a function to the part of the program where the call of that function happens. The side effect wasn't eliminated, but it was made more obvious by showing up at the call-site.\n\n```js\nfunction addMaxNum(arr) {\n    var maxNum = Math.max( ...arr );\n    arr.push( maxNum + 1 );\n}\n\nvar nums = [4,2,7,3];\n\naddMaxNum( nums );\n\nnums;       // [4,2,7,3,8]\n```\nto\n\n```js\nfunction addMaxNum(arr) {\n    var maxNum = Math.max( ...arr );\n    arr.push( maxNum + 1 );\n}\n\nvar nums = [4,2,7,3];\n\naddMaxNum( nums );\n\nnums;       // [4,2,7,3,8]\n```\n","n":0.105}}},{"i":589,"$":{"0":{"v":"Lens","n":1},"1":{"v":"\nThink of a lens as something that focuses (zooms in) on a specific part of a larger data structure\n- In this sense, they are similar to stores in [[redux]]\n\nAnother way is to see lenses as little drones that copy a part of an object for us, and delve into it to change the properties, all without mutating the original \n\nGiven a lens there are essentially three things you might want to do\n- view the subpart\n- modify the whole by changing the subpart\n- combine this lens with another lens to look even deeper\n\nLenses can be handy if we have a somewhat complex data structure that we want to abstract away from calling code. Rather than exposing the structure or providing a getter, setter, and transformer for every accessible property, we can instead expose lenses. Client code can then work with our data structure using view, set, and over without being coupled to the exact shape of the structure.\n\nIt is a special `type` that combines a *getter* and a *setter* function into a single unit\n\nThe first fn is the getter, while the second is the setter\n","n":0.074}}},{"i":590,"$":{"0":{"v":"Currying","n":1},"1":{"v":"\nCurrying is the process of transforming a function of N arity (num of args) into N functions of 1 arity.\n- ex. a function with 4 arguments -> 4 functions with 1 argument each\n\n### Partial application\nApplying a certain number of args toward completion of the function (when the last argument is called)\n\nApplication === calling a function and applying it's return value\n\nJavaScript engine does its job in two phases: memory creation (declaring variables/functions and hoisting them), and execution (initializing variables and actually running through code)\n\n## Why use these techniques?\nThe first and most obvious reason is that both currying and partial application allow you to separate in time/space (throughout your codebase) when and where separate arguments are specified, whereas traditional function calls require all the arguments to be present at the same time. If you have a place in your code where you'll know some of the arguments and another place where the other arguments are determined, currying or partial application are very useful.\n\nAnother layer to this answer, specifically for currying, is that composition of functions is much easier when there's only one argument. So a function that ultimately needs three arguments, if curried, becomes a function that needs just one, three times over. That kind of unary function will be a lot easier to work with when we start composing them. But the most important layer is specialization of generalized functions, and how such abstraction improves readability of code.\n\nR.partial says \"you give me a function and any number of arguments you want, then I'll just keep letting you add in as many arguments as you want either indefinitely or until all parameters of the function have been satisfied with arguments\"\n![](:/7548f55bd87c4c279f38a15ebac02ed5)\n\n### Partial application of composed functions\nallows us to compose multiple functions together, while returning a function that will accept more. This has the benefit of allowing us to compose more and more specific functions.\nexpl. `unique` and `words` will get partially applied to `compose`\n\n```js\nconst filterWords = partialRight( compose, unique, words )\nconst biggerWords = filterWords(skipShortWords)\n```\n\n![Untitled Diagram.jpg](:/593b0123095a48c98e17db790864665f)\n\n### Currying composed functions\nsince compose has right-to-left ordering, we normally `R.curry(R.reverseArgs(R.compose), ..)`\n\n*Using partials with `.pipe()`*\n`var filterWords = partial( pipe, words, unique )`\n\n#### Functor\nsomething that can be mapped over\nex. array, object\n- a functor that holds values of type A, when mapped over with a function that takes a value of type A and returns a value of type B, the result must be a functor that holds values of type B\n\n#### Applicative\na subtype of functors for which additional functions are defined.\n[more info](https://medium.com/@JosephJnk/an-introduction-to-applicative-functors-aea966799b1d)\n","n":0.049}}},{"i":591,"$":{"0":{"v":"Opentracing","n":1},"1":{"v":"\n## Overview\nOpentracing is a vendor-agnostic API to achieve distributed tracing in a microservice architecture.\n\nDistributed tracing tracks a single request through all of its journey, from its source to its destination. This means if we have a frontend that makes a request to a server, and that server then fires off a stored procedure from a database, then returns to the server, and triggers some other events to happen, this will all appear on the trace.\n- normal traces will only track a request through a single application domain.\n- Therefore, we can say that distributed tracing is the stitching of multiple requests across multiple systems. The stitching is often done by one or more `correlationId`s, and the tracing is often a set of recorded, structured log events across all the systems, stored in a central place.\n\nIn OpenTracing, a trace is a directed acyclic graph of Spans with References that may look like this:\n```\n[Span A]  ←←←(the root span)\n            |\n     +------+------+\n     |             |\n [Span B]      [Span C] ←←←(Span C is a `ChildOf` Span A)\n     |             |\n [Span D]      +---+-------+\n               |           |\n           [Span E]    [Span F] >>> [Span G] >>> [Span H]\n                                       ↑\n                                       ↑\n                                       ↑\n                         (Span G `FollowsFrom` Span F)\n```\n\nThis allows us to model how our application calls out to other applications, internal functions, asynchronous jobs, etc. All of these can be modeled as Spans\n\n### Span\n![[opentracing.span]]\n\n## Tips for Implementation\n- Use dependency injection where possible. This will make things easily testable and configurable.\n- Follow the idioms of your language and frameworks as much as possible. This will let your team members easily onboard into OpenTracing and tracing in general.\n- Many frameworks provide extensibility points around units of work. For example, Spring Boot has pre- and post-request handlers for web requests. Leverage these as much as possible to save you effort when instrumenting tracing.\n- If you don’t have a framework or can’t use its extensibility points, keep most of the tracing instrumentation as isolated from the business logic as possible. You can use patterns like Decorator and Chain of Responsibility for this, or even aspect-oriented programming. This will make your code’s intent clearer and easier to read. Exceptions to this include when you need to add tags or log an event; these are commonly specific to the business logic your code is executing.\n\n## E Resources\n- [Sentry primer on tracing](https://docs.sentry.io/product/sentry-basics/tracing/distributed-tracing/)","n":0.051}}},{"i":592,"$":{"0":{"v":"Span","n":1},"1":{"v":"\nThe “span” is the primary building block of a distributed trace, representing an individual unit of work done in a distributed system.\n- Each component of the distributed system contributes a span- a named, timed operation representing a piece of the workflow.\n\nA Span represents a separate software system communicating over messaging or HTTP.\n\nSpans can (and generally do) contain “References” to other spans, which allows multiple Spans to be assembled into one complete Trace - a visualization of the life of a request as it moves through a distributed system.\n\nEach span encapsulates the following state according to the OpenTracing specification:\n- An operation name\n- A start timestamp and finish timestamp\n- A set of key:value span Tags\n- A set of key:value span Logs\n- A SpanContext\n\nSpans can connect to each other via two types of relationship: ChildOf and FollowsFrom. ChildOf Spans are spans like in our previous example, where our ordering website sent child requests to both our payment system and inventory system. FollowsFrom Spans are just a chain of sequential Spans. So, a FollowsFrom Span is just saying, “I started after this other Span.”\n\n## Parts of a Span\n### Tag\n![[opentracing.span.tag]]\n\n### Log\n![[opentracing.span.log]]\n\n### SpanContext\nCarries data across process boundaries.\n\nIt has two major components:\n1. An implementation-dependent state to refer to the distinct span within a trace\n    - i.e., the implementing Tracer’s definition of spanID and traceID\n2. Any **Baggage Items**\n    - These are key:value pairs that cross process-boundaries.\n    - These may be useful to have some data available for access throughout the trace.\n\n#### Example Span:\n```\n    t=0            operation name: db_query               t=x\n\n     +-----------------------------------------------------+\n     | · · · · · · · · · ·    Span     · · · · · · · · · · |\n     +-----------------------------------------------------+\n\nTags:\n- db.instance:\"customers\"\n- db.statement:\"SELECT * FROM mytable WHERE foo='bar'\"\n- peer.address:\"mysql://127.0.0.1:3306/customers\"\n\nLogs:\n- message:\"Can't connect to mysql server on '127.0.0.1'(10061)\"\n\nSpanContext:\n- trace_id:\"abc123\"\n- span_id:\"xyz789\"\n- Baggage Items:\n  - special_id:\"vsid1738\"\n```\n","n":0.058}}},{"i":593,"$":{"0":{"v":"Tag","n":1},"1":{"v":"\nkey:value pairs that enable user-defined annotation of spans in order to query, filter, and comprehend trace data.\n\nExamples may include tag keys like db.instance to identify a database host, http.status_code to represent the HTTP response code, or error which can be set to True if the operation represented by the Span fails.\n\n#### Example\n- db.instance:\"customers\"\n- db.statement:\"SELECT * FROM mytable WHERE foo='bar'\"\n- peer.address:\"mysql://127.0.0.1:3306/customers\"\n","n":0.129}}},{"i":594,"$":{"0":{"v":"Log","n":1},"1":{"v":"\nkey:value pairs that are useful for capturing span-specific logging messages and other debugging or informational output from the application itself.\n\nLogs may be useful for documenting a specific moment or event within the span (in contrast to tags which should apply to the span as a whole).\n","n":0.147}}},{"i":595,"$":{"0":{"v":"Open API","n":0.707},"1":{"v":"\n## What is it?\nOpenAPI is a specification for describing REST API formats. \n- Tools like Swagger provide tools for implementing that specification. These tools can be used at different stages of the API lifecycle.\n  - note: Swagger is the prior name of OpenAPI.\n\nOpenAPI 3.0 uses an extended subset of [[json-schema]] to describe data formats.\n- this means that some keywords are supported and some are not. See the [breakdown](https://swagger.io/docs/specification/data-models/keywords/)\n\nAn OpenAPI file (json or yml) allows you to describe your entire API, including:\n- Available endpoints (`/users`) and operations on each endpoint (`GET /users`, `POST /users`)\n- Operation parameters Input and output for each operation\n- Authentication methods\n- Contact information, license, terms of use and other information.\n\n## Why use it?\nThe value of OpenAPI is that it allows APIs to describe their own structure. This allows us to:\n- Design-first users: use Swagger Codegen to generate a server stub for your API. The only thing left is to implement the server logic – and your API is ready to go live!\n- Use Swagger Codegen to generate client libraries for your API in over 40 languages.\n- Use Swagger UI to generate interactive API documentation that lets your users try out the API calls directly in the browser.\n- Use the spec to connect API-related tools to your API. For example, import the spec to SoapUI to create automated tests for your API.\n\n## Example\n```yml\nopenapi: 3.0.0\ninfo:\n  title: Sample API\n  description: Optional multiline or single-line description in [CommonMark](http://commonmark.org/help/) or HTML.\n  version: 0.1.9\nservers:\n  - url: http://api.example.com/v1\n    description: Optional server description, e.g. Main (production) server\n  - url: http://staging-api.example.com\n    description: Optional server description, e.g. Internal staging server for testing\npaths:\n  /users:\n    get:\n      summary: Returns a list of users.\n      description: Optional extended description in CommonMark or HTML.\n      responses:\n        '200':    # status code\n          description: A JSON array of user names\n          content:\n            application/json:\n              schema: \n                type: array\n                items: \n                  type: string\n```\n\n## Implementation \n### `$ref`\n- [main docs](https://swagger.io/docs/specification/using-ref/)\n`$ref` can be used to reference models from other models.\n\nFor example, imagine we had a model for `invoice_items.json`, and we wanted to reference an `invoice.json` model found in the same `schema/` directory.\n```json\n{\n  // ...\n  \"properties\": {\n    \"id\": { \"type\": \"string\" },\n    \"invoice\": { \"$ref\": \"invoice.json\" }\n    }\n  },\n  // ...\n}\n```\n\n`$ref` can also be used to reference a specific value within the same JSON document (JSON pointer notation).\n- ex. `#/components/schemas/user`\n\n\n* * *\n\n# Tools\n### Ajv\nWe can use `ajv` to take in our json schema, and generate validation functions to make sure the data coming in adheres to the schema.\n\n### Redoc\nRedoc takes the OpenAPI definitions (yml file) and generates HTML that can be viewed in the browser.\n- [docs](https://github.com/Redocly/redoc)\n","n":0.049}}},{"i":596,"$":{"0":{"v":"Objection","n":1},"1":{"v":"\nObjection is a relational query builder. \n- get all the benefits of an SQL query builder but also a powerful set of tools for working with relations.\n\n## Parts\n- define models declaratively, and map the relationship between models (1:many etc)\n- QueryBuilder - Every method that allows you to fetch or modify items in the database returns an instance of the QueryBuilder\n\t- therefore most important component in Objection\n\nAll instance methods start with the character $ to prevent them from colliding with the database column names.\n\n### Model\nconsider that in an app, there are different layouts of data:\n1. it could be the layout that exists in the database itself\n2. it could be the layout that exists when the user gets that data back from the database (ie, on the client)\ntherefore, any time we read or write to a database, we are converting data\n\nthere are 4 methods on a model that are used to transform data. They exist by default, but can be overridden. \nthese \"converter methods\" will be called each time data is tranformed: \n1. when we are writing data, we are converting it to the database layout, therefore `$formatDatabaseJson`\n2. when we are reading data, we are converting it to our internal data layout, therefore `$parseDatabaseJson`\n3. when we give data for a query, (for example `query().insert(req.body)`) or create a model explicitly using `model.fromjson(obj)` the `$parsejson` method is invoked\n\t- When you call `model.toJSON()` or `model.$toJson()` the `$formatJson` is called.\n- Note: Most libraries like express and koa automatically call the toJSON method when you pass the model instance to methods like response.json(model). You rarely need to call toJSON() or $toJson() explicitly.\n- All properties that start with $ are also removed from database and external layouts.\n","n":0.06}}},{"i":597,"$":{"0":{"v":"Nosql","n":1},"1":{"v":"\nRelational databases assume that the relationships are of similar importance, document databases assume that relationships form a hierarchical structure and relationships between documents are less important\n\nNoSQL-type DBs get their power from the developer spending a lot more time and care in thinking about exactly how to access data. Many NoSQL databases loosen the constraints on what you can store in a given record, but in return they are a great deal more fussy about how you access records. If you want to skip careful design of how you access records, you want the relational DB.\n\nIf your data cannot be represented on literally a sheet of paper, NoSQL is the wrong data store for you. And I don't mean sheets of paper with references that say \"now turn to page 64 for the diagram\", no, I mean a sheet of paper per document. That is what a normalized record looks like in a document store.\n\nHorizontal scaling is a distinct benefit of NoSQL, which is why companies like Netflix and Spotify use document databases.\n- RDBMSs more lend themselves to vertical scaling, which can get costly.\n\nNoSQL databases fit better into the whole paradigm of distributed computing, and NoSQL databases make the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution but requires data to be easily spread across multiple servers to scale up. Using commodity (affordable, smaller) hardware on-site or in the cloud saves you the hassle of additional software, and NoSQL databases like Cassandra are designed to be scaled across multiple data centers out of the box, without a lot of headaches.\n\nInstead of reshaping data when a query is processed (as an RDBMS system does), a NoSQL database organizes data so that its shape in the database corresponds with what will be queried. \n- This is a key factor in increasing speed and scalability.\n\nGenerally, NoSQL databases sacrifice ACID compliance for scalability and processing speed\n\nGoing from SQL to NoSQL is easier than from NoSQL to SQL\n\nWhen all the other components of our application are fast and seamless, NoSQL databases prevent data from being the bottleneck.\n- Big data is contributing to a large success for NoSQL databases, mainly because it handles data differently than the traditional relational databases.\n\n# Types of NoSQL Databases\n## Document-Based Databases\nDocument-based databases store the data in JSON objects. Each document has key-value pairs like structures:\n\nThe document-based databases are easy for developers as the document directly maps to the objects as JSON is a very common data format used by web developers. They are very flexible and allow us to modify the structure at any time.\n\nEx. Mongo, Couch, Couchbase, DocumentDB(?)\n\n## Key-Value Database\nHere, keys and values can be anything like strings, integers, or even complex objects. They are highly partitionable and are the best in horizontal scaling. They can be really useful in session oriented applications where we try to capture the behavior of the customer in a particular session.\n\nkey-value stores, in general, always maintain a certain number of replicas to offer reliability.\n\nEx. DynamoDB, Redis, Cassandra\n\n## Wide Column-Based Database\nThis database stores the data in records similar to any relational database but it has the ability to store very large numbers of dynamic columns. It groups the columns logically into column families.\n- For example, in a relational database, you have multiple tables but in a wide-column based database, instead of having multiple tables, we have multiple column families.\nCassandra or key-value stores, in general, always maintain a certain number of replicas to offer reliability.\n\nEx. Cassandra\n\n# Implementations\n\n## Cassandra\nA key-value store approach to NoSQL\n\nCassandra's approach to data availability is as follows: Instead of having one master node, it utilizes multiple masters inside a cluster. With multiple masters present, there is no fear of any downtime. The redundant model ensures high availability at all times.\n\nCassandra is designed to manipulate huge data arrays across multiple nodes. \n\nIn contrast to the relational database organizing data records in rows, Cassandra’s data model is based on columns to provide faster data retrieval. The data is stored in the form of hash.\n\ndesigned to be scaled across multiple data centers out of the box, without a lot of headaches.\n\n## DynamoDB (Amazon)\n![[aws.svc.dynamo]]\n\n## Elasticsearch\n![[elastic-search]]\n\n## CouchDB\nLike MongoDB, Couch is a document-oriented NoSQL databases, but Mongo and Couch diverge significantly in their implementations. \n- CouchDB uses the semi-structured JSON format for storing data. Queries to a CouchDB database are made via a RESTful HTTP API, using HTTP or JavaScript. \n- MongoDB uses BSON, a JSON variant that stores data in a binary format. MongoDB uses its own query language that is distinct from SQL, although they have some similarities. \n\nLike Mongo, Couch is schemaless.\n\nCouchDB and MongoDB differ in their approach to the [[CAP theorem|deploy.distributed.CAP-theorem]] \n- CouchDB favors availability and partition tolerance\n    - CouchDB uses eventual consistency. Clients can write to a single database node, and this information is guaranteed to eventually propagate to the rest of the database. \n- MongoDB prefers consistency and partition tolerance.\n    - MongoDB uses strict consistency. The database uses a replica set to provide redundancy but at the cost of availability. \n\nAs of this writing, Google projects the cost of deploying CouchDB on GCP at $34.72 per month. This estimate is based on a 30 day, 24 hours per day usage in the Central US region, a VM instance with 2 vCPUs and 8 GB of memory, and 10GB of a standard persistent disk.\n\n## Couchbase\nEvery Couchbase node consists of a data service, index service, query service, and cluster manager component. Starting with the 4.0 release, the three services can be distributed to run on separate nodes of the cluster if needed.\n\nIn the parlance of CAP Theorem, Couchbase is typically run as a CP system (consistency & partition tolerant)\n\nProvides a SQL-like query language called `N1QL` for manipulating JSON data stored in Couchbase.\n\n## PouchDB\nPouchDB was created to help web developers build applications that work as well offline as they do online.\nIt enables applications to store data locally while offline, then synchronize it with CouchDB and compatible servers when the application is back online, keeping the user's data in sync no matter where they next login.\nInspired by Couch\n","n":0.032}}},{"i":598,"$":{"0":{"v":"Ngrok","n":1},"1":{"v":"\nNgrok will create a secure tunnel on the local machine from a given port (ex. `8000`) to a url hosted on the internet at their domain (ex. `e2210e647fe4.ngrok.io`)\n- when a request somewhere on the internet hits an endpoint of that remote url, ngrok will forward that request on through the tunnel to the local machine.\n\t- ex. Stripe sends a webhook post request to `e2210e647fe4.ngrok.io:8000/webhooks/stripe`. Ngrok sees this, and passes it along to `localhost:8000/webhooks/stripe`, where the `/webhooks/stripe` endpoint defined in the application server can then handle the request\n","n":0.107}}},{"i":599,"$":{"0":{"v":"Nginx","n":1},"1":{"v":"\n## Overview\n- Nginx has 1 master process, and multiple worker processes.\n\t- The master's job is to read configuration files, and to manage the worker processes.\n\t- The worker processes handle the requests.\n- Nginx uses an event-based model to distribute requests among workers\n- The # of workers is specified in the config file, and may either be fixed, or adjustable based on how many cores the CPU has. \n\n### Config file\n- The nginx config file `nginx.conf` is stored either in `/usr/local/nginx/conf`, `/etc/nginx`, or `/usr/local/etc/nginx`.\n- nginx consists of modules which are controlled by directives specified in the configuration file\n\t- Directives can either be simple directives or block directives\n\t\t- simple ends with `;`, block uses `{}`\n- If a directive can have other directives inside, it is called a Context \n\t- ex. `events`, `http`, `server`, `location`\n- If a directive is not placed within a Context, then it is considered to be in the Main Context. \n\t- The `events` and `http` directives reside in the Main Context, `server` in `http`, and `location` in `server`.\n\n## Blocks\n- Nginx divides the configurations meant to serve different content into Blocks, which live in a hierarchical structure.\n- Each time a client request is made to the server, Nginx begins a process of determining which hierarchical block should be used to handle the request.\n\n### Server Block\n- Defines a virtual server used to handle requests of a defined type\n\t- each Server Block functions as a separate virtual web server instance\n- Based on the domain name, port and IP address requested, we can configure multiple server blocks to handle each combination.\n- The `server_name` and `listen` directives are used to determine which server block should be used to fulfill a request. They are used to bind to tcp sockets.\n\t- With `listen`, we can use a lone IP, a lone port, or a combo of the two. If we only specify one, then defaults are used\n\t\t- default port: 80\n\t\t- default IP: 0.0.0.0\n\t- the `server_name` directive is only evaluated when nginx needs to distinguish between server blocks that match to the same level of specificity in the `listen` directive. Put another way, it is a \"specificity tie-breaker\" \n\t\t- in other words, if `example.com` is hosted on `192.168.1.10:80`, a request will always be served by a server block that specifies `listen 192.168.1.10`, even if there is another server block that specifies `server_name example.com`\n\t- Finally, if further specificity is needed, then the Host header from the request (which contains the URI that the client was trying to reach) is used. \n\t\t- When using wildcard matching, the longest match beginning with a wildcard is used\n\t\t\t- ex. if the request has a Host header of `www.example.com`, and we have 3 server blocks with `server_name` of `*.example.com`, `www.example.*` and `*.com`, `*.example.com` would win out.\n- With server blocks, we can run more than one website on a single host\n- in Apache, called *VirtualHost* \n\n### Location Block\n- Lives within a Server Block (or nested in other location blocks).\n- Determine how Nginx should handle the part of the request that comes after IP:port (ie. the URI).\n- Similar to how Nginx has a specificity-based process for determining which server block will process the request, Nginx has an algorithm to determine which location block within the server should be used for handling requests.\n- Location blocks take the following form:\n```\nlocation <optional_modifier> <location_match> {\n}\n```\n- The `location_match` defines what Nginx should check the request URI against.\n- The `optional_modifier` affects the way Nginx will attempt to match the location block.\n\t- ex. check for prefix match (default), check for exact match (`=`), check for case-sensitive Regex (`~`)\n- The URI specified after `location` will be added to the path specified in the *root directive*\n\t- ex. if we specify `root /var/www/` and the location block specifies `/images/`, then the path to the requested file on the local FS will be `/var/www/images`\n- ex. Imagine we had a server block:\n```\nserver {\n    location / {\n        root /data/www;\n    }\n\n    location /images/ {\n        root /data;\n    }\n}\n```\nin response to a request with URI starting with `/images/`, the server will send files from the `/data/images` directory. \n\n## Tasks of Nginx\n### Serving Static Content\n- Nginx can be configured to serve static content, such as HTML and images.\n- this involves setting up of a server block inside the http block with two location blocks.\n\t- multiple server blocks are able to be added, each specifying a different port.\n```\nhttp {\n\tserver {\n\t\t\n\t}\n}\n```\n\n### Reverse Proxy Server\n- When Nginx proxies a request, it sends the request to a specified proxied server, fetches the response, and sends it back to the client\n\t- it is possible to proxy requests to another HTTP server (eg. another Nginx server) or to a non-HTTP server (eg. express.js)\n\t\t- We use a specified protocol like FastCGI to do this\n- We can establish a proxy server by using the `proxy_pass` directive within a *location block*.\n\t- The value of `proxy_pass` is the address of the proxy server:\n```\nlocation /some/path/ {\n    proxy_pass http://www.example.com/link/;\n}\n```\n- In this config, all requests processed to `/some/path/` to be sent to the proxy server at `http://www.example.com/link/`.\n\t- ex. the request with the URI of `/some/path/page.html` will be proxied to `http://www.example.com/link/page.html`\n- to pass a request to a non-HTTP server, the appropriate `*_pass` directive should be used\n\t- ex. `fastcgi_pass`\n\n## Debugging\n- upon changing nginx.conf, we need to reload the nginx server with `nginx -s reload`.\n- logs are stored at either `/usr/local/nginx/logs` or `/var/log/nginx`\n\n### Gotchas\n- A *root directive* should occur outside of the location block. We can then use another *root directive* within a *location block* if we want to override it.\n\t- Conversely, if you were to add a root to every location block then a location block that isn’t matched will have no root. Therefore, it is important that a root directive occur prior to your location blocks, which can then override this directive if they need to.\n- [source](https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/)\n","n":0.032}}},{"i":600,"$":{"0":{"v":"Conf","n":1},"1":{"v":"\nnginx.conf building blocks \n  - worker process    : should be equal to number cores of the server (or auto)\n  - worker connection : 1024 (per thread. nginx doesn't block) \n\n  - rate limiting     : prevent brute force attacks.\n  - proxy buffers     : (when used as proxy server)limits how much data to store as cache\n                         gzip /brotil or compression\n  - upload file size  : it should match php max upload size and nginx client max body size.\n  - timeouts          : php to nginx communication time.\n  - log rotation      : error log useful to know the errors and monitor resources\n  - fastcgi cache     : very important to boost the performance for static sties.\n  - SSL Configuration : there are default setting available with nginx itself \n                        (also see ssl performance tuning).\n\nExample nginx.conf: \n```conf\nuser www-data;                                   \nload_module modules/my_favourite_module.so;      \npid /run/nginx.pid;\n    | Alternative global config for \n    | [4 cores, 8 threads, 32GB RAM] \n    | handling  50000request/sec\n    |\nworker_processes auto;                           | worker_processes 8;\n    | worker_priority -15;\ninclude /etc/nginx/modules-enabled/*.conf;       | \nworker_rlimit_nofile 100000;                     | worker_rlimit_nofile 400000;                                  \n    | timer_resolution 10000ms;\n    |\nevents {                                         | events {\n    worker_connections 1024;                       |     worker_connections 20000;                       \n    multi_accept on;                               |     use epoll;\n}                                                |     multi_accept on;\n| }\n\nhttp {               ←  global config            \n    index index.php index.html index.htm;          \n    # Basic Settings                               \n\n    sendfile on;                                   \n    tcp_nopush on;\n    tcp_nodelay on;\n    sendfile_max_chunk 512;\n    keepalive_timeout 300;\n    keepalive_requests 100000;\n    types_hash_max_size 2048;\n    server_tokens off;\n\n    server_names_hash_bucket_size 128;\n    # server_name_in_redirect off;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    ##\n    # SSL Settings\n    ##\n\n    #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE\n    #ssl_prefer_server_ciphers on;\n    #rate limit zone\n\n    limit_req_zone $binary_remote_addr zone=one:10m rate=3r/m;\n    #buffers\n\n    client_body_buffer_size 128k;\n    client_max_body_size 10m;\n    client_header_buffer_size 32k;\n    large_client_header_buffers 16 256k;\n    output_buffers 1 32k;\n    postpone_output 1460;\n    #Porxy buffers\n    proxy_buffer_size 256k;\n    proxy_buffers 8 128k;\n    proxy_busy_buffers_size 256k;\n    proxy_max_temp_file_size 2048m;\n    proxy_temp_file_write_size 2048m;\n\n    ## fast cgi PHP\n    fastcgi_buffers 8 16k;\n    fastcgi_buffer_size 32k;\n    fastcgi_connect_timeout 300;\n    fastcgi_send_timeout 300;\n    fastcgi_read_timeout 300;\n    #static caching css/js/img\n\n    open_file_cache max=10000 inactive=5m;\n    open_file_cache_valid 2m;\n    open_file_cache_min_uses 1;\n    open_file_cache_errors on;\n    #timeouts\n\n    client_header_timeout 3m;\n    client_body_timeout 3m;\n    send_timeout 3m;\n\n    # Logging Settings\n\n    log_format main_ext ‘$remote_addr – $remote_user [$time_local] “$request” ‘\n    ‘$status $body_bytes_sent “$http_referer” ‘\n    ‘”$http_user_agent” “$http_x_forwarded_for” ‘\n    ‘”$host” sn=”$server_name” ‘\n    ‘rt=$request_time ‘\n    ‘ua=”$upstream_addr” us=”$upstream_status” ‘\n    ‘ut=”$upstream_response_time” ul=”$upstream_response_length” ‘\n    ‘cs=$upstream_cache_status’ ;\n\n    #access_log /var/log/nginx/access.log main_ext;\n    error_log /var/log/nginx/error.log warn;   Read more on nginx error log&common errors\n\n    ##\n    # Gzip Settings #brotil\n    ##\n\n    gzip on;\n    gzip_disable “msie6”;\n\n    gzip_vary on;\n    gzip_proxied any;\n    gzip_comp_level 6;\n    gzip_buffers 16 8k;\n    gzip_http_version 1.1;\n    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript application/x-font-ttf font/opentype image/svg+xml image/x-icon;\n    ##\n    # Virtual Host Configs\n    ##\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;   \n}\n\nserver {             ← Domain level \n    listen 0.0.0.0:443 rcvbuf=64000 sndbuf=120000 backlog=20000 ssl http2;\n    server_name example.com www.example.com;\n    keepalive_timeout         60;\n    ssl                       on;\n    ssl_protocols             TLSv1.2 TLSv1.1 TLSv1;\n    ssl_ciphers               'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS:!RC4';\n    ssl_prefer_server_ciphers on;\n    ssl_session_cache         shared:TLSSL:30m;\n    ssl_session_timeout       10m;\n    ssl_buffer_size           32k;\n    ssl_certificate           /etc/letsencrypt/live/example.com/fullchain.pem;\n    ssl_certificate_key       /etc/letsencrypt/live/example.com/privkey.pem;\n    ssl_dhparam           /etc/ssl/certs/dhparam.pem;\n    more_set_headers          \"X-Secure-Connection: true\";\n    add_header                Strict-Transport-Security max-age=315360000;\n    root       /var/www;\n\n    location {         ← Directory level \n        root /var/www;\n        index index.php index.html;\n    }\n\n    location ~ .php$ {  \n    fastcgi_keep_conn on;\n    fastcgi_pass   unix:/run/php5.6-fpm.sock;\n    fastcgi_index  index.php;\n    fastcgi_param  SCRIPT_FILENAME /var/www$fastcgi_script_name;\n    include fastcgi_params;\n    fastcgi_intercept_errors off;\n    fastcgi_buffer_size 32k;\n    fastcgi_buffers 32 32k;\n    fastcgi_connect_timeout 5;\n    }\n\n    location ~* ^.+.(jpg|jpeg|gif|png|svg|ico|css|less|xml|html?|swf|js|ttf)$ { \n        root /var/www;\n        expires 10y;\n    }\n\n}\n```\n\n- /etc/nginx/conf.d/*: user defined config files\n\nSee also:\nhttps://github.com/trimstray/nginx-admins-handbook\nhttps://github.com/tldr-devops/nginx-common-configuration\n","n":0.044}}},{"i":601,"$":{"0":{"v":"Nextjs","n":1},"1":{"v":"\ncreate pre-rendered react websites, offered from SSR\n\njsx is rendered already into html on the server, and is sent to the client to be displayed\n- vanilla-react will do everything at runtime.\n\nhelps with SEO\n\nnext takes care of routing for us\n- we define new pages in the `pages/` directory, and next picks up on them automatically, giving us the url out of the box.\n\t- next defines a component `<Link />` we can use to handle routes\n\nnext is smart and will not re-render the same content, if it has rendered it already\n\nIf we had a function in a Next.js component that referred to the `window` object, we would get errors. This is because that code is being run on the server, which of course has no concept of the browser's global variables. However, if we were to use `window` in the `useEffect` call, we would indeed get access to the `window` object. This shows that `useEffect` is still done client side, even though functions are understood server-side\n","n":0.078}}},{"i":602,"$":{"0":{"v":"Styled Components in Nextjs","n":0.5},"1":{"v":"\n## SSR Styled components in Next\nStyled-components supports concurrent SSR (server side rendering) with stylesheet rehydration. The basic idea is that when your app renders on the server, you can create a ServerStyleSheet and add a provider to your React tree which accepts styles via a context API. This doesn’t interfere with global styles, such as keyframes or createGlobalStyle and allows you to use styled-components with React DOM’s various SSR APIs.\n","n":0.12}}},{"i":603,"$":{"0":{"v":"Rehydration","n":1},"1":{"v":"\nspec: In traditional server-side rendered applications (think Express-Pug, Ruby on Rails), each value in the HTML is generated on the server, and sent to the client. That means if the page changed, it's because the server sent some new HTML to the client. With Next, the initial render is done via the server at compile time, but the same React code used to generate that HTML is also included on the client (ie. the bundle.js is included in Nextjs, just like it is in vanilla React)\n- This compiled client-side javascript code (originally React code) gets run on the client, building up a picture of what the world should look like. Then it is compared to the HTML in the document. This process is **Rehydration**.\n\n### Render vs Rehydration\nIn a render, vanilla React detects when there are changes to state or props. When there are, React updates the DOM.\nIn a rehydration, React assumes the DOM won't change; It's just trying to adopt the existing DOM.\n\n# E Resources\n[Quality resource on rehydration. Uses the backdrop of a dynamic Navbar to explain](https://www.joshwcomeau.com/react/the-perils-of-rehydration/)\n","n":0.075}}},{"i":604,"$":{"0":{"v":"Prerendering","n":1},"1":{"v":"\n`getInitialProps` is the old way of handling SSR in Next.js. Now we use `getStaticProps` or `getServerSideProps`, depending on if we want SSG or SSR\n\nThere are 2 types of pre-rendering that Next offers:\n- static generations - The `about` page is identical no matter who views that page. - Server-side rendering - HTML is generated by the server on each request.\n- Client-side rendering - as vanilla React does, we can still opt to use CSR if advantageous.\n\nIn the browser devtools, we can find all of the props that are being sent from the server by filtering for `_next_data`. We can see the props object being passed in a `<script />` tag.\n- you can also filter the network requests by the same string to see the Nextjs requests that are made to get the props.\n\n### Static Site Generation (SSG)\nThe idea is \"Why does the html need to be generated on the client, when we can leverage the server to do it beforehand? (ie. at build time)\"\n- this is the default, thus recommended option\n\nIf your page shows frequently updated data, and the page content changes on every request, you *must* use SSR.\n\nStatically generated pages can be cached by CDN\n\nExamples:\n- Marketing pages\n- Blog posts and portfolios\n- E-commerce product listings\n- Help and documentation\n\n#### Incremental Static Regeneration (ISR)\nBy default, `getStaticProps` is run at build time.\n\nHowever, sometimes we want to update static pages *after* we've built our site\n\nWith ISR, we can use static-generation on a per-page basis, without needing to rebuild the entire site\n\nTo do this, we add the `revalidate` prop to `getStaticProps()`\n\nIf we set `revalidate` to `10` (seconds), a few things will happen:\n- Any requests to the page after the initial request and before 10 seconds are also cached and instantaneous.\n- After the 10-second window, the next request will still show the cached (stale) page\n- Next.js triggers a regeneration of the page in the background.\n- Once the page generates successfully, Next.js will invalidate the cache and show the updated page. If the background regeneration fails, the old page would still be unaltered.\n\n### Server-Side Rendering (SSR)\nHere, Next.js pre-renders a page on each request. It will be slower because the page cannot be cached by a CDN, but the pre-rendered page will always be up-to-date. \n\nTo use Server-side Rendering for a page, you need to export an async function called `getServerSideProps`. This function will be called by the server on every request.\n\n#### Usage\n```ts\nfunction Page({ data }) {\n  // Render data...\n}\n\n// This gets called on every request\nexport async function getServerSideProps() {\n  // Fetch data from external API\n  const res = await fetch(`<URL>`)\n  const data = await res.json()\n\n  // Pass data to the page via props\n  return { props: { data } }\n}\n\nexport default Page\n```\n\nWhen you run `yarn build`, it generates 1 HTML document for every route on your site. Every side page, every blog post, every store item — an HTML file is created for each of them, ready to be served up immediately.\n\n`getServerSideProps` is run on every request\n","n":0.045}}},{"i":605,"$":{"0":{"v":"Page","n":1},"1":{"v":"\n### NextPage\nWe can type them using the `NextPage` type.\n```js\nconst Page: NextPage<Props> = (props: Props) => (\n  <main>Your user agent: {userAgent}</main>\n)\n```\n\n## API\n### `getInitialProps`\nEnables SSR, allowing us to do initial data population.\n- ie. sending the page with the data already populated from the server.\n- carries SEO-related benefits\n\nThis method can be used to asynchronously fetch some data, which then gets passed through `props`\n- The first time this method runs, it will be on the server, each time the user navigates to a different route (using `next/link` or `next/router` components), `getInitialProps` will be run on the client.\n\n`getInitialProps` is being migrated away from in favor of `getStaticProps` or `getServerSideProps`.\n","n":0.098}}},{"i":606,"$":{"0":{"v":"Methods","n":1},"1":{"v":"\n### `getInitialProps`\nThis function is called and returns props to the react component before rendering the templates in `/pages`. This is a perfect place for fetching the data you want for a page.\n\n`getInitialProps` works only in files in the pages folder and are used for routing, i.e it will not be called for react components that are included in these pages\n\nAdding a custom getInitialProps in your `_app.js` will disable Automatic Static Optimization in pages without Static Generation.\n\n### `getServerSideProps`\nWhen we specify this function, the Nextjs server will pre-render the html **on each request** before returning it to the client\n- Inside `getServerSideProps`, you can fetch data, perform server-side operations, or do anything you need to prepare the page's data.\n\n`getServerSideProps` returns JSON which will be used to render the page.\n\nUse `getServerSideProps` only if you need to render a page whose data must be fetched at request time.\n- If you do not need to render the data during the request, then you should consider fetching data on the client side or `getStaticProps`.\n\n1. When a user makes a request to a page with `getServerSideProps`, the Next.js server executes this function on the server.\n2. The data you fetch inside the function is then used to pre-render the page's HTML on the server, including the initial data.\n3. The fully rendered HTML page is sent as a response to the client's browser.\n    - When the client's browser receives the HTML, it can display the page immediately without needing to wait for client-side JavaScript to load and render the content.","n":0.063}}},{"i":607,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Prevent render of component until data arrives\nFor example, a header that must show either a login button or a profile dropdown depending on if the user is logged in or not. Until we know, we should render nothing:\n\nbecause the HTML file is built at compile-time in Next, every single user gets an identical copy of that HTML, regardless of whether they're logged in or not. Only once the js bundle is parsed and executed can we render the proper header state.\n```js\nconst Header = () => {\n    if (typeof window === 'undefined') {\n        return null;\n    }\n\n    const user = getUser();\n    if (user) {\n        return (\n        <AuthenticatedNav\n            user={user}\n        />\n        );\n    }\n    return (\n        <nav>\n            <a href=\"/login\">Login</a>\n        </nav>\n    );\n};\n```\n","n":0.092}}},{"i":608,"$":{"0":{"v":"Network","n":1},"1":{"v":"\n# Physical Networks vs. Overlay Networks\n## Physical Network\n- physically, there are connections between nodes in a network. The network essentially is the sum of its physical components that connect computers to one another.\n\t- physical components include: cables, wires, routers, repeaters\n\t- multiple physical networks can make up one big virtual network.\n\t\t- ex. a university spread across many buildings will have many different phsyical networks, though all computers will be able to see and communicate with one another because of the overlayed virtual network that exists on top of the physical one.\n\n## Overlay Network\n- the overlay network is created using logic at the software level to determine where the connections between the nodes are.\n- a network may have multiple virtual layers built on top of the backbone physical one. \n- while in a physical network, each node is connected to another through physical means, such as a cable. In an overlay network, nodes are connected to one another using network addresses. \n- overlay networks *encapsulate* the data before sending it over the network, and unwrapped upon reaching the destination.\n- an overlay network needs to employ a protocol that determines the rules that the hosts of a network must all abide by (IP, VPN, P2P)\n\n# UE Resources\n[Networking tutorials from Flavio Copes](https://flaviocopes.com/tags/network/)\n","n":0.069}}},{"i":609,"$":{"0":{"v":"VPN (Virtual Private Network)","n":0.5},"1":{"v":"\n## What is it?\nA VPN connects your PC, smartphone, or tablet to another computer (called a server) somewhere on the internet, and allows you to browse the internet using that computer’s internet connection\n- So, instead of connecting to the internet via your ISP, you connect to it via your VPN\n- so is it basically a network with a dedicated server that executes the web searches, and delivers that content to the client\n- the VPN is the thing on the server than enables the network to exist\n\nA VPN is similar to a proxy server, but where a proxy server can only redirect web requests, a VPN connection is capable of routing and anonymizing all of your network traffic.\n- also, a VPN works on the operating system level, meaning that it redirects all your traffic, whether it’s coming from your browser or a background app.\n\nex. If a company were to have 2 branches, they would not be able to communicate with one another with only their private IP addresses. They could choose to connect through the internet, but that would not be preferable, since traffic should be limited to those in the company. Instead, the company can use a VPN to bridge the two private networks. \n- alternatively, an IP tunnel can be used.\n- with either method, the result is that we are encapsulating the packets within a protocol layer during trasmission across the public network.\n\n## Why use one?\nA VPN allows you to privately access online activities no matter where you are by encrypting your connection to the Internet\n\n## Split VPN\nYou define some subnets (say 192.168.0.0/24) that should use the VPN, and the rest won't.\n- ex. if we have a Plex server in our local network and we only want to expose access via VPN, we can opt to only route traffic over the VPN for the one subnet with the Plex server in it. Otherwise, users attempting to access the Plex server would have to toggle the VPN on/off each time they wanted to use it.\n\n* * *\n\n## Protocols\n- ex. WireGuard, OpenVPN\n\nWireGuard is the newest protocol which is 20-60% faster than OpenVPN, with state-of-the-art cryptography","n":0.053}}},{"i":610,"$":{"0":{"v":"Tools","n":1}}},{"i":611,"$":{"0":{"v":"Traceroute","n":1},"1":{"v":"\nSee a list of all nodes (routers and end node) that your packets traveled through on their way to an origin server. Each step is a *hop*\n\t- Also tells us how long each jump took (the response time b/w nodes) (RTT; round trip time)\n- RTT tells us how long it took to get to that node (from the previous one) and return to your computer\n\t- There are three RTT columns because the traceroute sends three separate signal packets so that we may be able to spot inconsistencies in the route.\n- The final column has the router IP\n- `*` in the traceroute means that packets were lost\n- consistency of RTTs between columns is what we are looking for when analyzing a traceroute.  \n- when we use traceroute, the packet gets sent to the first router, which sends a packet back to the source. Then the packets continue on to the next router, which again sends a packet back to the source. This pattern continues until we reach the destination node.\n\t- ex. if there are 5 routers between a source node and destination node, then the source will send 6 packets into the network, which each packet addressed to the destination node.\n\t\t- The difference of course with traceroute over a regular packet, is that when an individual router along the chain receives the packet, it does not forward it along. Instead, it sends it back to the source. In this way, the source can determine the route that the packets took to reach the destination.\n\nIssues:\n- if the RTT from one hop to another greatly increases, and continues to increase until the destination, this indicates a problem with the node that first took a long time to respond. \n\t- This often accompanies packet loss (`*`)\n- if the RTT spikes on only one node, then subsequent hops have lower RTT, that doesn't indicate an issue. It just means the slow router gave your packets a lower priority.\n- if the RTT jumps then remains consisistent at that levels, this does not indicate an issue\n- By default, traceroute uses high UDP ports for tracing hosts. Sometimes firewalls block these UDP ports. \n\t- use `-P` flag to use different protocols: (`-P ICMP`, `-P TCP`, `-P UDP`)\n","n":0.052}}},{"i":612,"$":{"0":{"v":"Socket","n":1},"1":{"v":"\nA socket is the interface between the application layer and the transport layer, and can be thought of as the API between the application and the network\n\nAs application developers, we can think of the socket as the dividing point between what we have control over and what we *don't* have control over.\n- realistically, the developer does have control over 2 things on the transport layer\n    1. The choice of transport protocol\n    2. transport-layer parameter, allowing control over things like maximum buffer size, maximum segment sizes etc.\n\nIt is not accurate to say that programs communicate with each other (eg. that an Express server program communicates with a Postgres server program). Realistically, the programs are running as processes on their respective hosts, and it is these processes that communicate with each other, via sockets.\n- anal: If the processes were houses, then the sockets would be the doors that enable access to the processes\n\nthe socket's uniqueness is determined by five factors:\n- the local IP address\n- the local port number\n- the remote IP address\n- the remote port number\n- the transfer protocol (TCP/UDP)\n\n* * *\n\n### How can more than 65,535 clients connect to a server?\nWhile it's true that a machine can only open 65,535 ports, this does *NOT* mean that only 65,535 clients can connect to a server at a time. A server listens only on one port and can have large numbers of open sockets from clients connecting to that one port.\n- On the TCP level the tuple (source ip, source port, destination ip, destination port) must be unique for each simultaneous connection. These 4 factors determine the socket's uniqueness.\n    - That means a single client cannot open more than 65535 simultaneous connections to a single server. But a server can (theoretically) serve 65535 simultaneous connections per client.\n\nIn practice the server is only limited by how much CPU power, memory etc. it has to serve requests, not by the number of TCP connections to the server.\n","n":0.056}}},{"i":613,"$":{"0":{"v":"Servers","n":1},"1":{"v":"\nAll servers need static IPs\n","n":0.447}}},{"i":614,"$":{"0":{"v":"Edge Server","n":0.707},"1":{"v":"\nAn edge server is a server that acts as a gateway so that one network can access another\n\t- In other words, the edge server enables the two networks to communicate.\n- Therefore, the ability for clients to make a network connection to other clients is bottlenecked by the number of edge servers between the two clients;\n- Generally speaking, the farther the connection must travel, the greater the number of networks that must be traversed.\n![](/assets/images/2021-03-11-15-51-29.png)\n- \"edge\" refers to the philosophy of geographically placing the data close to the server (or proxy server) that requests it \n- Edge servers are contrasted with Origin Servers, which is your actual web server (ex. Express).\n- spec: caches want to live as close as possible to the client, while edge servers tend to live further out, closer to the internet (but still closer to the client than the origin server)\n","n":0.084}}},{"i":615,"$":{"0":{"v":"Server Clusters","n":0.707},"1":{"v":"\nA server cluster is a group of computers that function so closely together that you may consider them to be a single computer\n- a cluster will designate each task to a different node so that the responsibility is split\n- Each node would run its own instance of the OS\n\n### Stretched cluster\nA stretched cluster is a deployment model in which two or more host servers are part of the same logical cluster but are located in separate geographical locations.\n","n":0.113}}},{"i":616,"$":{"0":{"v":"Osi Model","n":0.707},"1":{"v":"\nThe OSI model is a framework for understanding how communications work in a computing system. It is an abstract representation, since no attention is paid to the implementation details of each layer. Instead, each layer simply describes its function and purpose. Put another way, it defines what input it expects, and what output it gives. \n- Since each layer interfaces directly with the layers above and below it, the consistence of the input and output offered by each layer is the only thing that matters (ex. as long as L1 receives 0's and 1's and delivers frames, all other details are inconsequential)\n\n### Protocol Data Unit (PDU)\neach layer of the OSI model has the concept of a Protocol Data Unit, which is the format that the data exists in within the current Layer. In other words, it is what an atomic unit of data is called at each layer.\n- All PDUs are composed of a header and payload\n\n## The Protocol Layers\n### L7 Application\t\n- consists of network applications and their application-layer protocols\n- primary user interface with communication system.\n- PDU - messages\n- HTTP, FTP, DNS, SMTP, POP3, SSH, IRC, TLS/SSL, NFS (network FS)\n\n### L6 Presentation \n- Supports the functionality of the application layer by providing services such as formatting and translation of data.\n- provide data encryption and data compression.\n- Data representation (compression, decompression) and encryption\n- ex. SSL, SSH, IMAP, FTP, TLS, MPEG, JPEG\n\n### L5 Session\n- Maintains the transmission path by synchronizing packets and controlling access to the medium by the Application layer.\n- controls the connections between computers\n- provides for delimiting and synchronizing of data exchange, including the means to build \n- ex. API, sockets, HTTP sessions\n\n### L4 Transport\n- Ensures the quality of transmission and determines the best route for transmission of data using the Network layer below.\n- concerned with providing reliable communication over an unsecured network\n- PDU - segments (TCP), datagrams (UDP)\n- goal: deliver the data to the right software application\n- ex. TCP, UDP\n\t\n### L3 Network \n- Finds a route for transmission of data (packets) between 2 routers/hosts, and establishes and maintains the connection between two connected nodes.\n- goal: pass data chunks over multiple connected networks\n- PDU - packet\n- ex. IPv4, IPv6, ICMP, ARP, NAT\n\n### L2 Data Link \n- Creates, transmits, and receives packets. Controls the Physical layer.\n- Concerned with sharing multiple access channels\n- They are used to deliver frames on a LAN\n- PDU - frames\n- goal: organize the 1s and 0s into chunks of data, and get them to the right place on the wire.\n- ex. wifi, ethernet, bluetooth, VLAN, port forwarding procol\n\n### L1 Physical \n- Converts data into bits for transmission and converts received bits into usable data for the layers above it.\n- PDU - bits\n- goal is to send 0s and 1s across a wire\n- ex. fiber optic, copper wire, coaxial cable, wireless, modem, repeaters, ethernet (physical portion), USB\n\n### The Internet Protocol\n- The IP stack consists of L1, L2, L3, L4, L7\n- When an HTTP request is sent, the protocol is established by piggybacking on the TCP connection that had already been made. This TCP connection is enabled by following the internet protocol. This is the point at which the internet protocol determines which routes datagrams\n- The fact that there are 2 layers that are openly missing from the internet stack poses an interesting question: why are they not there? The reason is that the internet leaves these layers up to the application developer. The application developer can use any implementation of L5 and L6 that they choose in order to achieve their goals.\n\n#### Routers\n- As data is sent upstream from a networked-device, it starts at L7 and makes its way down to L1. At L1, it is connected to the link-layer switch and goes up to L2, before going back down to L1. Then it reaches the router, which goes up to L3, then back down to L1, to be repeated depending on the number of subsequent routers. At the final router, the the router's L1 communicates with the L1 of the destination host, as it makes its way back up to L7.\n\n### Airplane trip analogy\n- As we look at the process of planning and taking an airplane, it becomes apparent that there are different layers to the entire process. In fact, each layer appears two times in the whole process— in reverse order:\n1. buy ticket\n2. check baggage\n3. load at gate\n4. takeoff on runway\n5. airplane routing (travel)\n6. land of runway\n7. unload at gate\n8. pickup baggage\n9. complain about ticket.\n\neach layer implements some functionality, and we can see that the opposite action was performed in reverse order.\n- We can also see that each layer provides service to the layer below it.\n\t- ex. the act of checking baggage only makes sense to a ticketed person. \n\t- ex. the idea of unloading at a gate only makes sense to a person on a landed plane.\n- we notice that we can replace any layer in the model, as long as the functionality remains the same. In this way, layers provide consistent interfaces to each other, and don't care about each other's implementation— only the outcome (ie. output) it provides.\n\n### Encapsulation\nas data travels each layer from L7 down to L1, additional information is added. As we pass the information encapsulated in the HTTP request down to L4, the transport layer takes the information of L7 and adds its own information to it. This information that was added is then used by the transport layer of the next node in the chain (likely the destination host). This process of encapsulation continues on down layer by layer until L1.\n- This idea of encapsulation thus demonstrates what is fundamentally different between each PDU: a message (L7) is an encapsulated datagram (L4). In other words, the datagram encapsulates the message. A datagram is fundamentally a message, plus some other information (provided by the layer). Therefore, at each layer, the PDU has 2 types of fields: header fields, and payload fields (the payload is just a packet from the layer above).\n\n### Miscelaneous\n- the very fact that there are layers means that we can treat it as a modular chain, and swap out one L3 implementation for another (such as wifi for ethernet)\n\t- therefore L2 doesn't care if we are using IP or IPX on L3, just like L3 doesn't care if L2 uses wifi or ethernet\n- The price we pay by layering is that we have to map between 32 bit IP addresses (L3) and 48 bit MAC addresses (L2).\n\t- The ARP protocol exists to solve this very problem\n- The [[OS]] may be a participant in any or all of the layers\n\t- ex. at L1, signal processing can be offloaded to a host CPU and that requires a driver which interfaces with the operating system\n","n":0.03}}},{"i":617,"$":{"0":{"v":"LAN (Local Area Network)","n":0.5},"1":{"v":"\nIPs ending in zeros are not actual hosts, but indicate the start of a block (since real IP addresses end in a number between 1 and 254)\n- 255 reserved for masking\n\n### Subnet\n- a subnet is a logical division of an IP network\n\t- an IP network is any network that uses IP address to communicate to other devices.\n\t\t- ex. LAN, the internet, enterprise network.\n\t- you might do this for logical reasons (ex. firewalling) for physical reasons (ex. smaller broadcast domains)\n- to be useful, a router is connected to two or more IP subnets\n- IP subnets exist to allow routers to choose appropriate destinations for packets.\n- Traffic is exchanged between subnetworks through routers\n- subnetting is the process of breaking down a single IP address block into smaller subnetworks called subnets\n\t- the reason we need to subnet is to efficiently distribute IP addresses to reduce wastage\n- the IP address is made up of 2 parts: the subnet number and the host identifier.\n\t- the number of bit-groups that are taken up by the subnet number depends on how the subnet mask is defined.\n\t\t- ex. if the subnet mask is `255.255.255.0`, then the subnet number consists of the first 3 groups. If it is `255.255.0.0`, then it consists of the first 2 groups\n- subnetting is analogous to the concept of *zoning* in city planning\n\n#### Subnet Mask\n- a subnet mask allows a computer/router to determine the portion of the IP address which refers to the network, and the portion that refers to the host machine\n\t- the network portion of an address is represented by 1s (255) in the subnet mask, and the host portion is represented by 0s\n\t- a subnet mask can also tell us the number of hosts within a network\n\t- anal. just like our home address consists of a street name (network ID) and a number (network host), the mask's job is to determine where in the IP address one begins and the other ends.\n- each octet of a subnet mask can either be 255 or 0. When the octet on the mask is 255, that means that the when trying to connect to another node, it is going to go through the router to try and resolve that IP address\n\t- ex. if the host has IP=`168.25.4.6` and mask=`255.255.255.0`, that means that it will only try and hit nodes within the LAN if the IP address starts with `168.25.4`. If the host tried to connect to `168.25.8.2`, it would default to going through the gateway (router)\n\t\t- If the mask=`255.255.0.0`, that means the host will try to hit the node locally only if the IP address starts with `168.25`\n\t\t- if at any point the 255 is \"triggered\" (ie. the external node doesn't satisfy the mask's requirement for enabling local searching), then the external node is said to be \"outside the mask\"\n- When we apply the subnet mask to the IP address, we get the *routing prefix*\n- ex. we have a network with IP=`135.68.2.0`. In that network are 2 host computers with IP=`135.68.2.1` and IP=`135.68.2.2`. The network portion of each host's IP is `135.68.2`\n\t-  a 0 at the end of an IP address indicates that it is a network address\n- anal. house addresses are composed of a streetname and a number\n- with a mask of 255.255.255.0, since only one of the octets of bits refers to the hosts in a network, there can only be 256 nodes within that network\n\t- if the mask were 255.255.0.0, there could be 65,536\n\t- in reality we have to subtract 2 from that total, since there are 2 reserved IP addresses: the Network ID and the Broadcast IP address.\n- a subnet mask is needed to tell us how many computers a node within a subnet has to go through before it gets to another node on that same network (ex. the computers at a LAN party).\n\t- When trying to send a message across the network, the subnet mask will tell us if we can access that node via the current network, or if we can only connect to it through the router (ie. it is on the internet)\n- Another way of looking at the subnet mask is that it tells us which octets of an IP address are devoted to telling us which network the nodes are located in.\n\t- when a mask is 255.255.255.0, then anything is on the same network as the host if the first 3 octets of the IP address are identical.\n\n*Default Subnet Masks*\n- There are 3 classes:\nClass A - 255.0.0.0\nClass B - 255.255.0.0\nClass C - 255.255.255.0\n\nnote: the concept of default subnet masks is becoming less relevant with the adoption of [[CIDR|network.internet.ip#cidr-classless-inter-domain-routing]]\n\n### Modem\n- portmanteau of \"modulator-demodulator\"\n- purpose is to convert data from a digital format to a format that is conducive to transmission over a physical layer\n- Modems can be used with almost any means of transmitting analog signals, from light-emitting diodes to radio\n- Any communication technology sending digital data wirelessly involves a modem.\n\t- ex. satellite, WiFi, WiMax, mobile phones, GPS, Bluetooth and NFC.\n\n### Demilitarized Zone (DMZ)\n- a subnetwork that sits between the network and the router. the DMZ exists so that we can control exposure to certain parts of a network. Anything in it is exposed to untrusted networks (like the internet). The idea is that the DMZ is all that can be accessed externally, so the purpose is added security\n\n### Dynamic Host Configuration Protocol (DHCP)\nDHCP is a server hosted within a network that dynamically assigns IP addresses\n- it is therefore the service that manages the IP address pool in a network\n\n**DHCP Reservation** - set aside an IP address and map it to a specific MAC address. Whenever a device with the specified MAC address enters the network, it is assigned with the specified IP address.\n\n#### Static IP\n- on the device receiving IP assignment (ie. not the router), we can bypass the dynamic assigning of IP addresses by creating a entry in `/etc/dhcpcd.conf` (Linux)\n- Generally accepted best practice is to assign a static address on the device that needs it - ex. a NAS - rather than rely on a DHCP server to give you the address you are expecting.\n\t- This shows that there are 2 ways to achieve a predictable IP address for devices on a LAN\n\n##### Linux approach\nrun `hostname -I`\nappend `ip=YOURIP` to end of `/boot/cmdline.txt`\n\n### Localhost vs LAN\n- localhost (an alias for `127.0.0.1`) is an IP address that is used to test the computer's networking protocols without actually using the LAN that the computer is attached to.\n\t- Called a loopback address, and it is analogous to hooking up an outbound cable out of one end of a machine and into the other (as opposed to that cable being hooked up to a router).\n- On the other hand, the private IP (`192.168.X.X`) is created by your network (the router), and allows us to communicate with other devices in the same network.\n- anal. Imagine there were 2 postal services: 1 for your street (local), and one for the whole world (global). When you write a letter, you give it to your local postman, and he connects it to the global postal network. If you are sending your letter to the localhost network, then it is like writing a letter and handing it to yourself. If you are sending your letter to the `192.168` address, then it is like putting it in a mailbox, where your local postman proceeds to deliver it to you.\n\t- In this analogy it's important to note that the letter never reaches the global post network (ie the internet).\n\n## Routing Schemes\n### Broadcast\ntransfer a message to all recipients simultaneously. broadcasting refers to transmitting a packet that will be received by every device on the network\n- can exist aso low as L2\n\t- meaning we can broadcast on ethernet\n- broadcasting is not implemented on IPv6, since it is considered wasteful to broadcast a message to all nodes, when perhaps only a few need to know about it.\n\t- Instead, IPv6 uses multicast\n\n### Multicast\ngroup communication where data transmission is addressed to a group of destination computers simultaneously.\n- can be 1:many or many:many distribution\n- may exist on L7 (application) or L3 (network assisted).\n\t- if done on L3, the sender of the data can send it in a single transmission.\n- multicasting limits the pool of receivers to those that join a specific multicast receiver group.\n\n* * *\nthe combination of IP and port is an interesting thing. We have the IP address of a node (L3), and a TCP port that corresponds to a particular service running on that machine. This demonstrates how both are addresses for two different layers: L3 and L4. In other words, when the data is heading for the IP address, it is a packet. When it is heading for the port, it is a segment.\n\n\n* * *\n## UE Resources\n[explanation of VLAN](https://serverfault.com/questions/188350/how-do-vlans-work)\n","n":0.026}}},{"i":618,"$":{"0":{"v":"Router","n":1},"1":{"v":"\nRouters are unique in that they have 2 IP addresses: a public WAN-facing one, and a private LAN-facing one.\n- Routers perform the \"traffic directing\" functions on the Internet, forwarding packets from one network to another\n\t- in other words, data packets are forwarded through the networks of the internet from router to router until they reach their destination computer (with routing algorithms determining the choice of route.)\n\t- A router is like a railroad junction: from one incoming track, there are multiple possible destinations, so which one does it choose? You have to configure that (configurable through [[port forwarding|network.lan.router.port-forwarding]])\n- Each router has a prior knowledge only of networks attached to it directly\n- routers gain knowledge of the topology of the network when the routing protocol shares the information of who the router's neighbors are. Each neighbor then shares this information with *their* neighbors, and so on until the whole network is revealed. \n- routing protocols are layer management protocols for L3, regardless of their transport mechanism\n\t- in other words, data may very well travel over L2 or L4\n- **DSL router** - a residential-grade router designed to create LANs and connect them to a WAN (which is provided by the ISP)\n\t- aka residential gateway\n- a residential router uses a modem to connect the LAN to the WAN\n\n### How routers determine which route to take\n- Each packet contains an address in its header, which has a hierarchical structure (like a postal code)\n\t- Each router has a forwarding table which is used to compare against a part of the address to determine the next step in the packet's journey\n\t- Specifically, the routing table is like a hashmap that makes portions of the destination address to outbound links.\n- each time a packet arrives at a router, the router consults its routing table.\n\t- This routing table contains the network ID and the host ID\n\n### Routing Table\nWhenever a node needs to send data to another node on a network, it must first know where to send it. If a direct connection can't be made, then the data has to be sent via other nodes along a route to the destination node.\n- Imagine a node somewhere along this chain receives a packet of data. It has no idea where it came from or where it's going. A routing table solves this problem, as it gives each node in the chain the address for the destination node. \n\t- Effectively, the router says \"I don't know how to deal with 192.168.0.34, but I know that 192.168.0.254 (a router) knows, so if I get a packet destined for that address, I'll just pass it along to that router, since he knows how to deal with it.\" \n- A routing table is a database that keeps track of paths and uses these to determine which way to forward traffic.\n- A routing table is a data file in RAM that is used to store route information about directly connected and remote networks.\n\n* * *\n\n### Core routers\n- core routers are the supercomputers of the internet\n- designed to operate on the internet backbone, as opposed to on the edge of a network (edge router, ex. home network).\n- the core router's purpose is to forward ip packets along. \n- edge routers connect to core routers\n\n* * *\n\n### Seeing all nodes on a local area network:\nWe can see all nodes on a local area network with:\n- `sudo nmap -sn 192.168.1.0/24`\n\t- the 24 is CIDR notation, signifying that we will scan from 192.168.1.0 to 192.168.1.255\n\t\t- the inclusion of 24 means we are scanning an *address block* ^RUsJkF0C\n","n":0.041}}},{"i":619,"$":{"0":{"v":"Port Triggering","n":0.707},"1":{"v":"\nGenerally, port triggering is used when the user needs to use port forwarding to reach multiple local computers. \n\t- port triggering is also used when applications need to open incoming ports that are different from the outgoing port.\n- Port triggering is used by network administrators to map a port or ports to one local computer. \n\t- Port triggering is considered to be dynamic because ports are opened when they are needed and closed when they aren’t in use.\n- When using port triggering, the router is used to monitor traffic within the network. The user specifies a trigger port that sends outbound data. The router then logs the IP address of computers that have sent traffic to that port. The router then opens an incoming port or ports before forwarding the traffic to that location.\n","n":0.086}}},{"i":620,"$":{"0":{"v":"Port Knocking","n":0.707},"1":{"v":"\nPort Knocking is a technique to externally open a port on a firewall.\n- This is done by externally generating a connection attempt on a set of prespecified closed ports. Once the correct sequence of connection attempts is received, the firewall rules are dynamically relaxed to allow the external host to connect over the specified port. \n\t- this is similar to a secret handshake\n- to implement port knocking, we implement a daemon that watches the firewall log for connection attempts. If the attempted sequence is correct, then the daemon sends instructions to modify the firewall rules for the external host.\n\t- daemon examples: *knockd*\n- anal. Imagine we had a 9 pane glass window in a house, and we have a friend on the inside, while we remain on the outside. In private, my friend and I agree that if I tap each pane in a sequence, then he will open up the 3rd pane. For instance, let's imagine the code is: 1, 5, 9, 2, 2, 3. If I don't tap the panels in order, then my friend will simply ignore me. If I am correct however, he will open the 3rd panel\n\t- here, each panel represents a port, I represent an external host, and my friend represents the router/firewall of the network I am trying to access.\n","n":0.068}}},{"i":621,"$":{"0":{"v":"Port Forwarding","n":0.707},"1":{"v":"\nThink of port forwarding as a secret tunnel that bores straight through your router’s firewall, allowing outside traffic to connect directly with a device inside your local network.\n\nPort forwarding (or port mapping) allows remote computers to connect to a specific computer or service on a private network. This allows you to run a web server, game server or a service of your choosing from behind a router.\n- In a typical network, the router has the public IP address and computers/servers obtain a private IP address from the router that is not addressable from outside the network. When you forward a specific port on your router, you are telling your router where to direct traffic for that port.\n\t- use the [canyouseeme utility](https://canyouseeme.org/) to verify the success of that process.\n- Port Forwarding is the act of directly forwarding data packets from one interface (or physical link) to another. This is not really proxying; instead, think of railroad tracks: without port forwarding the track ends at a station, but with port forwarding the track seamlessly continues to the next station.\n- If I were to try and ssh into my home computer from halfway around the world by using my home's public IP address, my home router would receive the request, and not know what to do with it, since the private IP address would then be needed to complete the send.\n- We can specify to a router \"hey, when you receive requests from the internet with ssh (port 22), I need you to pass them along to 192.168.1.74\" \n\t- We give the router a forwarding IP address (the internal IP that the packets are destined for) and a port. The router external IP and port number together serve as a unique identifier, and we specify what happens when the router receives the combination  \n\t\t- \"upon receiving a request on port 8000, forward that request on to 196.168.1.74\"\n- we need to specify an **external port**, which is the port on the router that is open and facing the internet.\n\t- some numbers will be in use already, such as services from email and web server.\n\t- pick port above 5000\n- Port forwarding settings are found in the router (using the modem's IP) \n\n### External port range\n- normally identical to the internal port range. For security purposes, we may change this.\n- We may not like the fact that people on the outside can see what ports are being used. \n\t- ex. 5432 is the normal port for postgres. Recognizability like this introduces security issues.\n- To get around this issue, we can employ external port ranges. We can specify 11111 as the external port, and 5432 as the internal port, and now whenever a request comes in with port 11111, it will get forwarded to the specified internal IP and the internal port. \n\n### Internal port\n- port of the service running on the internal IP host. \n\t- ex, we are running an express server on port 8000\n- we may want to expose port 22 (SSH), port 21 (FTP) \n\n## Safety\n- Port forwarding is generally considered safe as long as your network has a strong firewall. \n\t- Port forwarding on Xbox and PlayStation is safe while port forwarding on PC or for camera surveillance comes with a little more risk. \n\nTips:\n- Only use safe, well-known ports\n- Limit the number of ports you use\n- Use port forwarding sparingly\n- Adjust your firewall rules so only specific IP addresses can get access to the port\n- Regularly update your router firmware and device operating systems\n- Use a VPN on the devices you haven’t used with port forwarding to improve their security\n- Monitor your network for suspicious activity\n\n## Resources\n- [Caddy Reverse Proxy: Setting up for home network](https://caddy.community/t/using-caddy-as-a-reverse-proxy-in-a-home-network/9427)\n\t- [[caddy]]\n","n":0.04}}},{"i":622,"$":{"0":{"v":"NAT (Network Area Translation)","n":0.5},"1":{"v":"\nA Network Area Translation (NAT) allows us to map a router's public internet-facing IP address to 1+ local IP addresses on a LAN\n- the NAT lives on the router.\n\nthe NAT was invented to solve a problem presented by the IPv4 protocol – a shortage of IP addresses\n- when you are at a home network and access a webpage, the request goes from your local machine to the router, where the router translates your machine's IP address to the router's address. Therefore, the server that you connected to only sees your router's IP address as the origin of the request.\n\nThe NAT is the precise reason why if you search something on Google, the results don't show up on your dad's computer.\n\nA side effect of NAT is that machines on the internet cannot initiate communications to local machines; they can only respond to communications initiated by them\n\nNAT Table\n- allows devices on a private network to access a public network\n- each row in the table maps one private address to one public address.\n- When the router receives an outbound request from a host in the LAN, it changes the request headers to have the public IP of the router, and it creates an entry in the NAT table.\n\t- When an external request comes in from the internet to the private network, the router needs to know where to forward those packets, so it looks in the NAT Table to find out which host to send it to.\n- each pairing of our private host IP and external public IP is called a *connection*\n\n## Hole Punching\n- hole punching is a technique to establish a direct connection between two parties, where one or both are behind firewalls using NAT\n- To punch a hole, each client connects to an unrestricted third-party server that temporarily stores private and public IP address and port information for each client. The server then relays each client's information to the other, and using that information, each client tries to establish direct connection;\n\t- once there is a successful connection using valid port numbers, each router accepts and forwards the incoming packets on to the host node on the LAN\n- Hole punching is agnostic to which layer the hole punching is performed at. Therefore, there are different types of hole punching occurring at different layers:\n\t- ICMP hole punching\n\t- UDP hole punching\n\t- TCP hole punching\n- Technologies that use hole punching include:\n\t- VoIP, online games, P2P networking, Skype\n\nWhy is this needed?\n- networked devices with privately/publicly available IP addresses can connect easily. However, when one or both of the hosts are behind different firewalls, we need to implement hole punching to make the connection\n","n":0.048}}},{"i":623,"$":{"0":{"v":"ISP (Internet Service Provider)","n":0.5},"1":{"v":"\n## Networking at ISP Level\n### Peering\nPeering involves two IPSs coming together to exchange traffic with each other freely, and for mutual benefit.\n- Normally, ISPs of competing size have peer-agreements in place, ensuring that all traffic is forwarded freely, since each ISP will make money from their customers anyway\n\n### Transit\nContrasted with Peering, this is when a bigger ISP charges a smaller ISP for the right to use its network in order to have access to the larger internet\n\n* * *\n\nAn ISP is not necessarily a telecommunications company. In fact, it can be a university, or a private company providing internet access directly to its employees.\n\n## Tiers\n![](/assets/images/2021-03-11-17-31-26.png)\n### Tier-1 ISP\n- known as Internet Backbone networks.\n\t- ex. Sprint, Verizon, AT&T etc.\n- link speeds are often higher (sometimes as fast as 2.5-10 Gbps)\n- They are directly connected to all other tier-1 ISPs\n- They are connected to a large number of tier-2 ISPs\n- They are international in coverage.\n\n### Tier-2 ISP\n- usually have regional or national coverage\n- connects to only a few of the tier-1 ISPs.\n- routes traffic through one of the tier-1 ISPs to gain access to global internet.\n\t- pay tier-1 ISPs for this right.\n","n":0.073}}},{"i":624,"$":{"0":{"v":"Internet","n":1},"1":{"v":"\n## What is the Internet?\nThe internet is a network of networks. \n- the first \"network\" in this phrase are the ISPs\n\nThe Internet is a collection of separate and distinct networks\n\t- The relationship between these networks is defined by one of the following:\n\t\t1. Transit (or pay) – The network operator pays money (or settlement) to another network for Internet access (or transit).\n\t\t2. Peer (or swap) – Two networks exchange traffic between their users freely, and for mutual benefit.\n\t- Therefore, in order for a network to reach any specific other network on the Internet, it must either:\n\t\t1. Sell transit (or Internet access) service to that network (making them a 'customer'),\n\t\t2. Peer directly with that network, or with a network which sells transit service to that network\n- the internet is based on the principle that any Internet user can reach any other Internet user as though they were on the same network (*global reachability*)\n\t- Therefore, any Internet connected network must by definition either pay another network for transit, or peer with every other network which also does not purchase transit.\n- Think of the internet as a huge tree with lots of leaves and branches. Your router is the stem of a single leaf (your home). It connects that leaf to the rest of the tree. It doesn't need to know where everything else is, it just needs to know how to transfer data between the twig it is on and the leaf it is connected to. The twig knows what it's connected to, and so on. Some parts of the tree connect directly to many other parts. For example, the trunk is connected to every branch. But, it still has to pass information through those branches and rely upon them to get it to the right twig and leaf.\n\nThe Internet is literally a network of networks, and it’s bound together by BGP. BGP allows one network (say Facebook) to advertise its presence to other networks that form the Internet. As we write Facebook is not advertising its presence, ISPs and other networks can’t find Facebook’s network and so it is unavailable.\n\nThe individual networks each have an ASN: an Autonomous System Number. An Autonomous System (AS) is an individual network with a unified internal routing policy. An AS can originate prefixes (say that they control a group of IP addresses), as well as transit prefixes (say they know how to reach specific groups of IP addresses).\n\nCloudflare's ASN is AS13335. Every ASN needs to announce its prefix routes to the Internet using BGP; otherwise, no one will know how to connect and where to find us.\n\n\n\n### How data moves through a network of links and switches\nThere are two approaches to moving data through a network of links and switches: **circuit switching** and **packet switching**\n\n#### Circuit Switching\n- with *circuit-switched networks*, the resources needed along a path are reserved for the duration of the communication session. this is less efficient, since the circuit is still reserved even when no data is transmitted (ex. on phone call, the line is reserved even when no one is talking)\n\n#### Packet Switching\n- with *packet-switched networks*, the resources are not reserved, and the session's messages use the resources on demand, and therefore may have to wait for access to a communication link\n\t- here, resources would be buffers and the link transmission rate\n- **Packet Switching** is the process of grouping data to be placed into a packet and sent over a network. \n- a **packet switches** comes in two types: routers and link-layer switches.\n\t\n#### Examples\n- anal: We have 2 restaurants: one that takes reservations and one that doesn't. The first has more initial setup, since we have to call to make the reservation— though there is less work to do once we arrive (circuit-switched). Meanwhile, the second takes less time to setup since we don't have to phone ahead, but we have to wait longer once we actually arrive at the restaurant (packet-switched).\n\t- In the non-reservable restaurant, the time spent waiting to get a table is analogous to a *queueing delay* that affects the packets. *Packet loss* would be analogous to arriving at the restaurant and encountering a big line and subsequently being asked to leave.\n- ex: telephone networks are circuit-switched networks, since of you want to send data over a line, you must first establish a connection between sender and receiver. also, once a connection is made, a constant transmission rate is reserved for the duration of the connection\n- ex: the internet on the other hand is a packet-switched network, since when data is sent over a network no bandwidth is reserved. if one of the links is congested because other packets need to be transmitted at the same time, and our packet will have to wait in a buffer at the sending side of the transmission link\n","n":0.035}}},{"i":625,"$":{"0":{"v":"Web","n":1},"1":{"v":"\nA web page consists of objects (HTML file, JPEG images etc), which is addressable with a single URL.\n","n":0.236}}},{"i":626,"$":{"0":{"v":"Network Switch","n":0.707},"1":{"v":"\nWhile routers server as an interface for devices at L3, a network switch uses MAC addresses to forward packets at L2 to a device.\n- ex. Switches for Ethernet are the most common form of network switch\n- switches are built into residential routers\n\nthe internet shares network bandwidth dynamically. Senders push and jostle with each other to get their packets over the wire as quickly as possible, and the network switches decide which packet to send (i.e., the bandwidth allocation) from one moment to the next.","n":0.109}}},{"i":627,"$":{"0":{"v":"SMB","n":1},"1":{"v":"\nSMB is a communication protocol for providing shared access to files, printers, and serial ports between nodes on a network\nSBM is a proprietary protocol used by the Microsoft Windows network file system.\n","n":0.177}}},{"i":628,"$":{"0":{"v":"Repeater","n":1},"1":{"v":"\nA repeater is a networked device that broadcasts the same data out of each of its ports and lets each device decide what data they need\n- since repeaters amplify a signal, they need a source of electric power\n\nRepeaters operate on [[L1|network.osi-model#l1-physical,1:#*]] of the OSI model, since they do not try to interpret the data being transmitted\n- anal: in Lord of the Rings, Gondor has a series of beacons that can be lit to communicate to the whole kingdom that war has been declared. Each repeater (beacon) receives a signal and retransmits it (by lighting up their own beacon)\n","n":0.101}}},{"i":629,"$":{"0":{"v":"P2P","n":1},"1":{"v":"\nP2P networks works don't have a server, which would provide centralized coordination in a traditional client-server model Instead, it is a distributed application architecture that partitions tasks or workloads between peers\n- Each peer has equal privileges and each has equal power.\n- each peer designates a porion of their resources as available for consumption by other nodes in the network. \n\t- ex. can share resources like processing power, disk storage or network bandwidth\n- Peers are both suppliers and consumers of resources, in contrast to the traditional client-server model in which the consumption and supply of resources is divided. Put another way, each node acts as both a client and a server.\n- P2P networks implement an overlay network, whose nodes are a subset of the nodes found in the physical network.\n- data is still exchanged over TCP/IP, but P2P nodes can communicate directly at L7 by using the logical overlay links (each logical overlay link corresponds to a physical link of the underlying network.)\n- ex. WiFi Direct, Skype\n- Because there is no server, other strategies must be taken to accomplish tasks that would otherwise be handled by the server, such as login\n\t- To carry out login duties, a P2P network has what's called a Login Server\n\n## Place in history\n- P2P networks became popular with Napster, because the more traditional way to use the internet is more heavily regulated and corporately-owned. This means that Napster could skirt a lot of the authorities on the matter. \n","n":0.064}}},{"i":630,"$":{"0":{"v":"IP Address","n":0.707},"1":{"v":"\nIP (the Internet Protocol) is unreliable: it may drop, delay, duplicate, or reorder packets.\n\n## IP Address\nIP addresses consist of 32 bits, which is why the address is broken down into 4 parts of 8 bits each.\n- 255 represents 8 bits.\n\nIP addresses are hierarchical, just like [[domain names|dns.domain]].\n- subnetting is the same concept as creating multiple different subdomains. The only difference is that while domains get more specific from left to right, IP addresses get more specific from right to left.\n\n### IP Address Spaces\n- there are 2 main IP Address Spaces: public and private.\n\t- public are routable on the internet, meaning every device on the internet needs its own pubic IP\n- the public address space is further divided into 5 classes:\nClass A\t0.0.0.0 – 126.255.255.255\nClass B\t128.0.0.0 – 191.255.255.255\nClass C\t192.0.0.0 – 223.255.255.255\nClass D\t224.0.0.0 – 239.255.255.255\nClass E\t240.0.0.0 – 255.255.255.255\n\t- A,B,C - devices directly connected to internet\n\t\t- ex. L3 switches, routers, firewalls, servers\n\t- D - multicast traffic\n\t- E - experimental\n- The private address space is divided into 3 classes:\nClass A—10.0.0.0/8 network block\t10.0.0.0 – 010.255.255.255\nClass B—172.16.0.0/12 network block\t172.16.0.0 – 172.31.255.255\nClass C—192.168.0.0/16 network block\t192.168.0.0 – 192.168.255.255\n\n### Address block\n![[network.lan.router#seeing-all-nodes-on-a-local-area-network,1]]\n\n### CIDR (Classless Inter-Domain Routing)\nCIDR is a method for allocating IP addresses and for IP routing.\n- it also allows for a flexible and simplified way to identify IP addresses and route network traffic.\n\nIt is similar to how telephones use:\n- An area code to specify a geographical region\n- A number to identify a specific device\n\nCIDR notation consists of\n- an IP address\n- a forward slash (`/`) \n- a number that ranges from 0 to 32\n\t- represent a range of IP addresses\n\t\t- Every time the number decreases by one (starting at 32), it means the number of IP addresses in that range are doubled.\n\nIP addresses are described as consisting of two groups of bits in the address\n1. the *network prefix*, which are the most significant bits are the network prefix, and which identify a whole network or subnet\n2. the *host identifier, which specifies a particular interface of a host on that network.","n":0.055}}},{"i":631,"$":{"0":{"v":"Network Bridge","n":0.707},"1":{"v":"\nA network bridge is a networking device that creates a single network from multiple networks.\n\t- This process is called *network bridging*\n- Bridging differs from routing, since routing allows multiple networks to communicate independently and yet remain separate.\n\t- On the other hand, bridging connects two separate networks as if they were a single network\n- Occurs at L2\n","n":0.134}}},{"i":632,"$":{"0":{"v":"Broadcast","n":1},"1":{"v":"\n## Atomic Broadcast\nAn atomic broadcast is a broadcast sent out to all nodes of a distributed system.\n\nThe purpose of an atomic broadcast is to address the challenges of distributed consensus, [[fault tolerance|deploy.distributed.fault-tolerance]], and maintaining data consistency in a distributed system. The goal is to achieve consensus among distributed nodes and maintain strong consistency across the system.\n- It does this by ensuring that a set of messages or events are delivered to all participants (nodes) in a distributed system in the same order and in an atomic manner, allowing for coordinated and consistent actions across the entire system\n\nThere are 2 properties of an atomic broadcast:\n1. All nodes in the system receive the messages in the exact same order. This guarantees that the events are processed in the same sequence, ensuring consistency across the distributed system.\n2. The broadcast operation is atomic, meaning either all nodes receive all messages in the agreed-upon order, or none of them do. There is no partial or inconsistent delivery.\n\nWhen implemented correctly, an atomic broadcast will have the following properties:\n- Validity: if a correct participant broadcasts a message, then all correct participants will eventually receive it.\n- Uniform Agreement: if one correct participant receives a message, then all correct participants will eventually receive that message.\n- Uniform Integrity: a message is received by each participant at most once, and only if it was previously broadcast.\n- Uniform Total Order: the messages are totally ordered in the mathematical sense; that is, if any correct participant receives message 1 first and message 2 second, then every other correct participant must receive message 1 before message 2.\n\n[[zookeeper]] uses an atomic broadcast as its basic building block\n\n### Use cases\n- Fault Tolerance and Replication: Atomic broadcast helps maintain fault tolerance in the event of node failures. Even if some nodes crash and recover, the atomic broadcast ensures that they receive the missed messages in the same order as other nodes, preventing inconsistencies.\n- Data Replication: In distributed databases or storage systems, atomic broadcast is used to replicate data changes across multiple replicas to maintain strong consistency.\n    - ex. in a distributed database, atomic broadcasts may occur every time there is a write operation to maintain data consistency across all replicas.\n    - ex. in a Slack application, the atomic broadcast should achieve the following:\n        - validity: all users will eventually receive the message published by a user in a Slack channel\n        - integrity: a published chat message is received by each user not more than once\n        - total order: all users receive the chat messages in the identical order\n- Distributed Consensus: Paxos (a consensus protocol) uses Atomic Broadcasts to ensure that all nodes agree on a single shared state or decision. \n    - here, the atomic broadcast ensures that all nodes have the same view of the system's state.\n- Replicated State Machines: In replicated state machine systems, where multiple nodes maintain identical copies of a state machine, atomic broadcast is used to ensure that all state machines receive the same sequence of commands in the same order.\n\n### Example: Slack\nWhen a user sends a message in a Slack channel, how can we make guarantees about the integrity of the received messages? How can we be sure that each client received all messages and in the proper order?\n\n1. User A sends a message in the Slack channel.\n2. The Slack backend server receives the message from User A and needs to ensure that this message is delivered to all other members of the channel.\n3. To achieve atomic broadcast, Slack can utilize a consensus algorithm, such as Paxos or Raft, to ensure that the message is consistently delivered and processed in the same order by all replicas of the channel across multiple servers.\n4. The consensus algorithm allows the servers hosting the channel to agree on the order of the messages to be delivered to the users. It ensures that all servers have the same set of messages in the same order, avoiding any discrepancies.\n5. Once consensus is reached, the message is replicated and delivered to all other members of the channel. This ensures that all users see the same message in the same sequence, maintaining consistency across the system.\n6. Slack can implement a mechanism to acknowledge message delivery to the sender and other members of the channel. This way, all users are aware that the message has been successfully broadcasted and received.","n":0.037}}},{"i":633,"$":{"0":{"v":"Nestjs","n":1},"1":{"v":"\nNestjs is a server-side application framework\n- highly opinionated\n- supports multiple data paradigms, such as ClientServer, [[pub/sub|general.patterns.messaging.pubsub]] etc.\n- achieves SOLID principles by the use of [[modules|nestjs.modules]].\n- module dependency is achieved with [[constructor dependency injection|general.patterns.dependency-injection#nestjs-constructor-dependency-injection]]. \n  - Modules import [[providers|nestjs.providers]] to perform some purpose (such as retrieve data from a database, send a message etc.), and those providers are made available within the module. To pass\n- Nestjs provides an all-tools-supplied approach. \n  - ex. It has modules out of the box for connections to most types of databases/datastreams (e.g. RDBs, Redis, Kafka, RabbitMQ) and \n\n## Platform\nNestjs is built on top of an HTTP Node framework\n- default options are [[Express|express]] and Fastify\n\nDepending on which underlying platform we use, we will get access to its API (`NestExpressApplication` or `NestFastifyApplication`)\n- the resulting `app` object will have access to the methods of whichever underlying platform we choose.\n```ts\nconst app = await NestFactory.create<NestExpressApplication>(AppModule);\n```\n\nin Nest, almost everything is shared across incoming requests. \n- ex. We have a connection pool to the database, singleton services with global state, etc.\n\nBy default, modules are eagerly loaded (which means that as soon as the application loads, so do all the modules, whether or not they are immediately necessary).\n- this is fine for the most part, but if we are running in a [[serverless|deploy.serverless]] environment, this would be problematic.\n  - in this case we want to lazy load the modules.\n\n![](/assets/images/2022-05-16-13-27-38.png)\n\n### Lifecycle\nA Nest application, as well as every application element, has a lifecycle managed by Nest. \n- Nest provides lifecycle hooks that give visibility into key lifecycle events, and the ability to run registered code in your `module`, `injectable` or `controller` when they occur.\n\nLifecycle events can be broken into 3 phases: *initializing*, *running* and *terminating*\n\n### Dependency Injection and Decorators\nNest is built around the pattern of [[dependency injection|general.patterns.dependency-injection]], which is achieved through [[decorators|js.lang.decorator]].\n- This is mostly done through the constructor (therefore, it's constructor-based dependency injection)\n\nThe general pattern is that we create a [[provider|nestjs.providers]] by decorating a class with `@Injectable`, then we pass that class into another c\n- the `@Injectable` decorator enables the class to be registered and managed by the [[IoC|general.patterns.IoC]] container (`@Modules`)\n\nWhen we inject a class into another class, Nest will instantiate the class and pass it to the controller's constructor.\n\nBecause of Typescript in Nestjs, it is very easy to manage dependencies, since they are resolved by type.\n\n### Context\nGuards, filters and interceptors are meant to be generic. That is, they don't care if they are being used in an HTTP context or WebSockets context.\n- Nestjs provides utility classes that provide information about the current execution context.\n  - this information can be used to build generic [[guards|nestjs.guard]], filters, and [[interceptors|nestjs.interceptor]] that can work across a broad set of [[controllers|nestjs.controllers]], methods, and execution contexts.\n- Two of these utility classes are `ArgumentsHost` and `ExecutionContext`.\n\n#### `ArgumentsHost`\n`ArgumentsHost` is simply an abstraction over a handler's arguments.\n- ex. for HTTP server applications, the `host` object encapsulates Express's `[request, response, next]` array\n- ex. for Graphql server applications, the `host` object contains the `[root, args, context, info]` array.\n\nThis class provides methods for retrieving the arguments being passed to a handler. With it, we choose the appropriate context, and then we retrieve the arguments.\n\n##### Methods\n- `getType()` tells us which context is being used (`http`, `rpc` etc.)\n- `getArgs()` gets us back an array of arguments being passed to the handler\n  - this is generic, since we don't know what arguments we are getting back (since the context has not been specified). For this reason, using `switchTo...` methods might be preferable, since they return more specific types (such as `HttpArgumentsHost` or `RpcArgumentsHost`)\n- `switchToHttp()`\n```ts\nconst ctx = host.switchToHttp();\nconst request = ctx.getRequest<Request>();\n```\n\n#### `ExecutionContext`\n`ExecutionContext` extends `ArgumentsHost`, and gives us additional details about the current execution process.\n\nNest provides an instance of `ExecutionContext` in places you may need it, such as in the `canActivate()` method of a guard and the `intercept()` method of an interceptor. \n\n##### Methods\n- `getClass()` Returns the type of the controller class which the current handler belongs to.\n- `getHandler()` Returns a reference to the handler (method) that will be invoked next in the request pipeline.\n  - ex. in an HTTP context, if the currently processed request is a POST request, bound to the `create()` method on the `CatsController`, `getHandler()` returns a reference to the `create()` method and `getClass()` returns the `CatsController` type (not instance).\n\nThe ability to access these references gives us the opportunity to access the metadata set through the `@SetMetadata()` decorator from within guards or interceptors.\n\n### Metadata and Reflection\n- [docs](https://docs.nestjs.com/fundamentals/execution-context#reflection-and-metadata)\n\nWe can attach custom metadata to route handlers with the `@SetMetadata()` decorator, which can then be accessed from within the class via dependency injection (with the `Reflector` helper class) to make certain decisions.\n- ex. we can create some metadata about the roles that a user has.\n\n\n* * *\n\n## Core Components\n### Modules\n![[nestjs.modules]]\n\n### Controllers\n![[nestjs.controllers]]\n\n### Providers\n![[nestjs.providers]]\n\n### Middleware\n![[nestjs.middleware]]\n\n## Misc\n- In Nest, a microservice is defined as an application that uses a different transport layer than HTTP.","n":0.035}}},{"i":634,"$":{"0":{"v":"Testing in Nestjs","n":0.577},"1":{"v":"\nThe `Test` class is useful for providing an application execution context that essentially mocks the full Nest runtime, but gives you hooks that make it easy to manage class instances, including mocking and overriding.\n\n### `createTestingModule`\n`Test.createTestingModule()` takes a module metadata object as its argument. It returns a `TestingModule` instance which in turn provides a few methods. \n- to this method we pass an object that looks the same as when we are defining a `@Module` in a `.module.ts` file.\n  - we are really just trying to simulate the module in a minimalist way, where we include only what is needed for the particular test file.\n- the resulting `TestingModule` is an isolated NestJS runtime. As a result, it gets all the NestJS behaviors like dependency injection.\n  - for unit tests, `compile()` is the most important one, which bootstraps a module with its dependencies (similar to the way an application is bootstrapped in the conventional `main.ts` file using `NestFactory.create()`), and returns a module that is ready for testing.\n\nThe resultant TestingModule is limited to what you define when using the Test class (e.g. which `providers`/`controllers` you include)\n- ex. if you pass `TweetsService` to `providers`, then you will get access to all of the methods inside `TweetsService`.\n  - you will want to make an instance of an injectable/controller:\n  ```ts\n  service = module.get<TweetsService>(TweetsService);\n  ```\n\n### Mocking\nImagine we have the following TestingModule:\n```ts\nconst module: TestingModule = await Test.createTestingModule({\n  controllers: [UsersController],\n  providers: [UsersService]\n}).compile()\n```\n\nIf `UsersService` has dependencies that need to be injected in order for it to work, we will get the `Nest can't resolve dependencies` error. If we don't care about the dependencies of the `UsersService`, we can simply mock it:\n```ts\nlet controller: UsersController\nconst mockUsersService = {}\nbeforeEach(() => {\n  const module: TestingModule = await Test.createTestingModule({\n    controllers: [UsersController],\n    providers: [UsersService]\n  })\n    .overrideProvider(UsersService)\n    .useValue(mockUsersService)\n    .compile()\n\n  service = module.get<TweetsService>(TweetsService);\n})\n```","n":0.058}}},{"i":635,"$":{"0":{"v":"Runtim","n":1},"1":{"v":"\n## Dependencies\nSay we are making a simple app with an HTTP route (to provide post data). Our app consists of a controller (HTTP endpoint), a provider (logic to access the data) and main module.\n1. In our controller, we declare the PostsService as a dependency in the constructor\n  - PostsService is available to be injected in the first place because it has the `@Injectable` decorator.\n2. In PostsModule, the class `PostsService` is associated with a token also called `PostsService` (`providers` key)\n  - the token is used by the Nestjs runtime to request an instance of a class (most often by the same name. This is configurable, and is what we are doing when we provide the long-form object with keys `provide` and `useFactory`/`useValue`, `useClass`)\n```ts\n@Module({\n  controllers: [PostsController],\n  providers: [PostsService],\n})\n```","n":0.089}}},{"i":636,"$":{"0":{"v":"Providers","n":1},"1":{"v":"\n## What is it?\nA provider is a class that can be injected into the constructor of other classes (dependency injection)\n- We mark the class as a `provider` by using the `@Injectable()` decorator, which lets us pass the class into a controller (spec: not limited to controllers), where it is then instantiated by the [[IoC container|general.patterns.IoC#ioc-container,1:#*]] (ie. the NestJS runtime)\n\nThe main idea of a provider is that it can be injected as dependency; this means objects can create various relationships with each other, and the function of \"wiring up\" instances of objects can largely be delegated to the Nest runtime system.\n\nA provider \"provides\" some value to the dependent that it is being used in. \n- a service is a provider that, when injected into a class, allows the class to use the methods defined on the provider itself.\n  - ex. we could have a `NuggetsService` provider, whose role is to fetch nuggets. Now, from other providers we can inject the `NuggetService`, allowing us to fetch nuggets from that other provider.\n\nBy default, a Provider belonging to a Module will be available within the module.\n- in other words, it does not have to be imported in the `module` if we are accessing the provider from within it.\n\nProviders normally have a lifetime (\"scope\") synchronized with the application lifecycle. \n- When the application is bootstrapped, every dependency must be resolved, and therefore every provider has to be instantiated. \n- Similarly, when the application shuts down, each provider will be destroyed. \n  - there are ways to make your provider lifetime request-scoped as well.\n\nProviders are plain JavaScript classes that are declared as providers in a module.\n\nResponsible for things like:\n- data storage and retrieval (Service)\n- repositories, factories, helpers\n\nNest has a built-in [[IoC container|general.patterns.IoC#ioc-container,1:#*]] that resolves relationships between providers.\n\nThe `@Injectable()` decorator attaches metadata, which declares that the class can be managed by the Nest IoC container\n\n## How does it work?\nthe syntax `providers: [CatsService]` is short-hand for the more complete syntax:\n\n```ts\nproviders: [\n  {\n    provide: CatsService,\n    useClass: CatsService,\n  },\n];\n```\n- with this syntax, we can clearly see that the `CatsService` token is being associated with the class `CatsService`\n  - the shorthand exists since the most common use-case is that a token is used to request an instance of a class by the same name.\n- the value for `provide` (ie. the token) can also be a string or [[js.lang.type.symbol]]. In this case, it can only be injected with the `@Inject` decorator, where the token is then referenced: `@Inject('CONNECTION') connection: Connection`\n\n\n### Registration process\nRegistration happens in the `module` file.\n\nRegistration is about associating a token (name/id) with a class.\n\n```ts\n// app.module.ts\n\n// this is shorthand\nproviders: [CatsService],\n\n// for this\nproviders: [\n  {\n    provide: CatsService, // token\n    useClass: CatsService,\n  },\n];\n```\nSeeing the explicit construction helps to see how the registration process (into the Nestjs IoC container) is really just a mapping between the token and the class.\n- This is done for convenience to simplify the most common use-case, where the token is used to request an instance of a class by the same name.\n\n### Injecting non-service based Providers\nThough the most common use is to inject services, we can really inject any kind of value, allowing us to do things like put an external library into the Nest IoC container, or replace a real implementation with a mock object (useful for testing).\n- ex. here, we are associating a string-valued token (`'CONNECTION'`) with a pre-existing connection object we've imported from an external file:\n```ts\nproviders: [\n  {\n    provide: 'CONNECTION',\n    useValue: connection,\n  },\n],\n```\n\nWhich can be used in a provider:\n```ts\n@Injectable()\nexport class CatsRepository {\n  constructor(@Inject('CONNECTION') connection: Connection) {}\n}\n```\n\n### Asynchronous providers (`useFactory`)\nAt times, the application start should be delayed until one or more asynchronous tasks are completed. \n- ex. you may not want to start accepting requests until the connection with the database has been established. \n\nThis can be accomplished by registering a provider as an `async` function along with the `useFactory` syntax.\n```ts\nproviders: [\n  {\n    provide: 'ASYNC_CONNECTION',\n    useFactory: async () => {\n      const connection = await createConnection(options);\n      return connection;\n    },\n  }\n]\n```\n\nThis factory function returns a provider, which can then be injected into a class like any other provider with `@Inject('ASYNC_CONNECTION')`\n\nHere, Nest will await resolution of the `useFactory` promise before instantiating any class that depends on (injects) such a provider.\n\n* * *\n\n## Custom Providers\nCustom providers let us do things like:\n- instantiate a provider by ourselves (instead of delegating that responsibility to the Nestjs runtime)\n- re-use an existing class in a second dependency\n- override a class with a mock version for testing\n\n\n### Class Provider (`useClass`)\n`useClass` is the default way to specify which provider the token will resolve to.\n- in other words, the shortform will resolve to this syntax.\n\nWe can dynamically determine the class that the token will resolve to by using a ternary operator in the `useClass` key.\n- ex. imagine you have a default `ConfigService` (therefore, high level of generality/abstraction). Depending on which environment we are in, we want to provide the Nestjs IoC container with a different implementation of the class.\n\n### Value Provider (`useValue`)\nSpecifying a provider like this is useful for injecting a constant value.\n- ex. putting an external libary into the Nestjs container.\n- ex. replace a real implementation with a mock object\n\nThe value to `useValue` will be a simple object that has the same interface as the service it is replacing.\n\n### Factory Provider","n":0.034}}},{"i":637,"$":{"0":{"v":"Pipe","n":1},"1":{"v":"\nA pipe is a [[provider|nestjs.providers]] that operates on the arguments passed to a [[controller|nestjs.controllers]] route handler.\n- just before the handler is invoked, Nest will interpose a pipe and it will receive the arguments destined for the handler, allowing it to operate on them.\n- when the pipe has finished its operation, it returns control to the handler, which takes in those (potentially) transformed arguments.\n\nPipes have two typical use cases:\n1. transformation: transform input data to the desired form (e.g., from string to integer)\n2. validation: evaluate input data and if valid, simply pass it through unchanged; otherwise, throw an exception when the data is incorrect\n\nNestjs comes with built-in pipes, for use-cases such as:\n- parsing a UUID\n- parsing an array\n\n### Usage\nA pipe is created by making a class that extends the `PipeTransform<T, R>` interface and is annotated with the `@Injectable()` decorator.\n- `T` - type of the input value\n- `R` - return type of the `transform()` method\n\nEvery pipe must implement the `transform()` method to fulfill the `PipeTransform` interface contract.\n- `transform(value, metadata)`\n\nTo use a pipe, we need to bind an instance of the pipe class to the appropriate context.\n- ex. we might want to bind a `ParseIntPipe` with a particular route handler to make sure it runs before the handler method is called.\n\n```ts\n@Get(':id')\nasync findOne(@Param('id', ParseIntPipe) id: number) {\n  return this.catsService.findOne(id);\n}\n```\n\nThis code ensures that only one of two things will happen:\n1. either the parameter we receive in the `findOne()` method is a `number` (as expected in our call to `this.catsService.findOne()`), \n2. or an exception is thrown before the route handler is called. \n\nAs with other providers in Nestjs, we pass the class rather than an instantiation of the class, leaving the responsibility of instantiation up to the framework. This is to enable [[dependency injection|general.patterns.dependency-injection]].\n- alternatively, we can pass an in-place instance (`new ParseIntPipe(options)`) if we wish to pass options to customize the built-in pipe's behaviour.\n\nNestjs supports both sync and async pipes.\n- therefore the `transform()` method may use `async/await`","n":0.056}}},{"i":638,"$":{"0":{"v":"Modules","n":1},"1":{"v":"\nModules are the way to organize your components in Nest.\n- Decorating a class with `@Module` provides metadata that Nest makes use of to organize the application structure.\n\nEach application has at least one module, a root module. \n- this is the starting point Nest uses to build the application graph, which is the internal data structure used to resolve module and provider relationships and dependencies.\n- this is the module that we register in our `main.ts` with `NestFactory.create(AppModule)`\n\nFor most applications, the resulting architecture will employ multiple modules, each encapsulating a closely related set of capabilities.\n- good practice is to have each module correspond to a [[domain|general.principles.DDD]]\n\nThe module encapsulates providers by default\n- This means that in order to inject providers into a different module, they must be exported from the imported module.\n\nmodules are singletons by default\n- thus you can share the same instance of any provider between multiple modules effortlessly.\n- Every module is automatically a shared module: Once created it can be reused by any module\n  - to do this, simply add the service to the `exports` array of the shared module. Now, any module that imports the shared module will have access to the service listed in that `exports` array.\n\n![](/assets/images/2023-01-23-12-12-32.png)\n\n```ts\n@Module({\n  // correspond to the HttpService class that gets injected into the provider (service).\n  imports: [HttpModule],\n  providers: [AutomationService],\n  exports: [AutomationService],\n})\nexport class AutomationModule {\n  // Modules can inject providers (e.g. for configuration purposes):\n  constructor(private automationService: AutomationService) {}\n}\n```\n\nModules themselves cannot be injected due to circular dependency.\n\n## Parts of a module\nThe `@Module()` decorator takes a single object whose properties describe the module:\n- `providers` \n- `controllers`\n- `imports`\n- `exports`\n\n### `providers`\nThe `providers` property is an array of providers that will be instantiated by the Nest injector and that may be shared at least across this module. \n- When we put a provider here, we are registering it with the [[IoC container|general.patterns.IoC#ioc-container,1:#*]] (ie. the NestJS runtime) so that it can be instantiated at the site where it is injected (this is dependency injection).\n\n### `controllers`\nthe set of controllers defined in this module which have to be instantiated\n\n### `imports`\nthe list of imported modules that export the providers which are required in this module\n- put another way, if there is a provider from another module and we want to use it in our module, then we must add the provider to that module's `exports` list, and then include that module in our `imports` list.\n\nthis array enables sharing of providers across modules\n\nDon't add the same provider to multiple modules. Instead, export the provider, and import the module.\n\n### `exports`\nthe subset of providers that are provided by this module and should be available in other modules which import this module. You can use either the provider itself or just its token (provide value)\n\nyou may consider the exported providers as the module's API\n\nModules can export their internal providers\n- In addition, they can re-export modules that they import.\n\n## Dynamic Modules\nDynamic modules enable us to easily create customizable modules that can register and configure [[providers|nestjs.providers]] dynamically.\n\nDynamic modules are created in the module class:\n```ts\nimport { createDatabaseProviders } from './database.providers';\nimport { Connection } from './connection.provider';\n\n@Module({\n  providers: [Connection],\n})\nexport class DatabaseModule {\n  // forRoot may be async or sync\n  static forRoot(entities = [], options?): DynamicModule {\n    const providers = createDatabaseProviders(options, entities);\n    return {\n      module: DatabaseModule,\n      providers: providers,\n      exports: providers,\n    };\n  }\n}\n```\n\n- [docs](https://docs.nestjs.com/fundamentals/dynamic-modules)\n\n## Global Modules\nGlobal modules are useful for when you want to provide a set of providers which should be available everywhere out-of-the-box (e.g., helpers, database connections, etc.)\n\nWe can make a set of providers global with the `@Global` decorator:\n```ts\n@Global()\n@Module({\n  // Module configuration\n```\n\n## CLI\n- generate a module named \"cats\" - `nest g module cats`","n":0.041}}},{"i":639,"$":{"0":{"v":"Middleware","n":1},"1":{"v":"\nNestjs middleware is equivalent to [[Express middleware|express.middleware]]\n\nCustom middleware is implemented either in either a function, or in a class with an `@Injectable()` decorator.\n- The class should implement the `NestMiddleware` interface, while the function does not have any special requirements.\n\nMiddleware fully supports dependency injection.\n\nWe set up middleware using the `configure()` method of the module class. \n- Modules that include middleware have to implement the `NestModule` interface.\n\n```ts\n@Module({\n  imports: [CatsModule],\n})\nexport class AppModule implements NestModule {\n  configure(consumer: MiddlewareConsumer) {\n    consumer\n      .apply(LoggerMiddleware)\n      .forRoutes('cats');\n  }\n}\n```","n":0.113}}},{"i":640,"$":{"0":{"v":"Microservices","n":1},"1":{"v":"\nIn Nestjs, a microservice is an application that uses a transport layer that is not HTTP (such as TCP, which is the default)\n- the specific transport layer implementation that is used is called a *transporter*.\n- most transporters support both request-response and event-based message styles.\n\nA Nestjs application can exchange messages or publish events to a Nest microservice using the `ClientProxy` class\n- this class notably provides the `send()` and `emit()` methods, allowing us to communicate with remote microservices.\n  - `send()` returns a cold [[general.patterns.behavioural.observable]] to which we can subscribe.\n    - *cold* meaning we have to explicitly subscribe to it before the message will be sent\n- this class is lazy; it only initiates a connection just prior to when the first microservice call is made.\n- this class can be instantiated by either:\n  - importing `ClientsModule` and registering it into one of your [[nestjs.modules]] as part of the `imports` array. This enables us to (dependency) inject an instance of `ClientProxy`:\n  ```js\n  constructor(\n    @Inject('NAME_OF_SERVICE_AS_REGISTERED_IN_MODULE') private client: ClientProxy,\n  ) {}\n  ```\n  - using the `@Client()` decorator, which is the non-preferred way\n\n## Handlers\nOnly to exist within `@Controller()` decorated classes, since they are the entry points for your application\n\n### `@MessagePattern()`\nDecorator to create a message handler based on the request-response paradigm\n- ex. Here, the `accumulate()` message handler listens for messages that fulfill the `{ cmd: 'sum' }` message pattern. The handler takes in a single argument `data`, which is passed from the client.\n```ts\n@MessagePattern({ cmd: 'sum' })\naccumulate(data: number[]): number {\n  return (data || []).reduce((a, b) => a + b);\n}\n```\n\nMessage handlers support both sync and `async`\n\nA message handler is able to return an [[observable|general.patterns.behavioural.observable]].\n- in this case, the result values will be emitted until the stream is completed.\n\nThere is a bit of overhead involved in this paradigm, since the Nestjs runtime creates 2 channels to handle the request and response.\n\n### `@EventPattern()`\nDecorator to create an event handler based on the event-based paradigm (ie. when you want to publish events without waiting for a response).\n\nIdeal use-case is if you would like to simply notify another service that a certain condition has occurred in this part of the system.\n\nYou can register multiple event handlers for a single event pattern (ie. the decorator arg) and all of them will be automatically triggered in parallel.\n\nIf using Kafka, you may want to access the message headers. In order to accomplish that, you can use built-in decorators as follows\n- note: types are obtained from `@nestjs/microservices` library\n\n```ts\n@EventPattern(env.KAFKA_TOPIC)\nasync stateChangeHandler(@Payload() event: Record<string, unknown>, @Ctx() context: KafkaContext) {\n  const label = 'AppController#stateChangeRivian';\n  await this.allTopics(event, context.getTopic(), label);\n}\n```\n- note: You can also pass in a property key to the `@Payload()` decorator to extract a specific property from the incoming payload object. \n  - ex. `@Payload('id')`","n":0.048}}},{"i":641,"$":{"0":{"v":"Interceptor","n":1},"1":{"v":"\nAn interceptor is a [[Provider|nestjs.providers]] that makes it possible to\n- bind extra logic before / after route handlers are called\n  - this is possible because the provided `intercept()` method wraps both the request and response streams.\n    - we are able to build custom logic that runs after due to the [[observable|rxjs.objects.observable]] pattern.\n- transform the result returned from a function\n- transform the exception thrown from a function\n- extend the basic function behavior\n- completely override a function depending on specific conditions (e.g., for caching purposes)\n\nThese capabilities of interceptors are inspired by Aspect Oriented Programming (AOP)\n- AOP itself is about adding behavior to existing code without modifying the code itself, allowing us to implement logic like *\"log all function calls when the function's name begins with 'set'\"*\n  - effectively, what we are doing here is injecting some custom logic immediately prior to some handler being called. Thus, we are adding functionlity, but the original code remains unaltered.\n\nan interceptor class is annotated with the `@Injectable()` decorator and implements the `NestInterceptor` interface.\n\n```ts\n@Injectable()\nexport class LoggingInterceptor implements NestInterceptor {\n  // Because the interceptor class implements the `NestInterceptor` class, our interceptor has access to the `intercept()` method\n  intercept(context: ExecutionContext, next: CallHandler): Observable<any> {\n    console.log('Before...');\n\n    const now = Date.now();\n    return next\n      .handle()\n      .pipe(\n        // tap() is from RxJS and is used to perform side-effects for notifications from the source observable\n        tap(() => console.log(`After... ${Date.now() - now}ms`)),\n      );\n  }\n}\n```\n\n### `intercept()`\nThe `intercept` function returns an [[observable|rxjs.objects.observable]]\n\nArgs:\n- The `ExecutionContext` provides methods that allow us to query for information about the current execution process.\n- `CallHandler` provides access to the response stream\n","n":0.062}}},{"i":642,"$":{"0":{"v":"Guard","n":1},"1":{"v":"\nA Guard is a [[provider|nestjs.providers]] that enables us to implement [[authorization|auth]]\n- in traditional [[express]] apps this is usually handled by middleware.\n  - the main difference is that middleware is dumb, not knowing which handler will get called after it calls `next()`. On the other hand Guards have access to the `ExecutionContext` instance. Therefore, they know exactly what will be executed next.\n\nGuards are invoked before [[pipes|nestjs.pipe]] or [[interceptors|nestjs.interceptor]]\n\nA guard is created by making a class that extends the `CanActivate` interface and is annotated with the `@Injectable()` decorator.\n\nGuards determine (at run-time) whether a given request will be handled by the route handler.\n- ex. they can check conditions such as permissions, roles, ACLs, etc. to make that determination.\n\n","n":0.093}}},{"i":643,"$":{"0":{"v":"Controllers","n":1},"1":{"v":"\nControllers are responsible for handling incoming requests and returning responses to the client.\n- Controllers should only handle HTTP requests; delegating more complex tasks to providers\n\nMost often, providers are injected into controllers, allowing the controller the ability to call methods on the instantiated provider\n- to do this, both the controller and the provider must be registered in the module\n\nThe routing mechanism controls which controller receives which requests.\n\nNormally, each controller has more than one route, and different routes can perform different actions.\n- in a REST api, `/customers` and `/products` would be 2 different controllers\n  - ex. route path prefix implemented as `@Controller('customers')`\n\nThe decorators (e.g. `@Controller`, `@Get`) associate classes with required metadata and enable Nest to create a routing map (which associates which requests belong with which controllers)\n\nRoute handlers can either return a [[Promise|js.lang.promises]] or an [[observable|rxjs.objects.observable]] stream:\n```ts\n// Promise approach\n@Get()\nasync findAll(): Promise<any[]> {\n  return [];\n}\n\n// Observable approach\n@Get()\nfindAll(): Observable<any[]> {\n  return of([]);\n}\n```\n\nIf returning an observable stream, Nest will automatically subscribe to the source underneath and take the last emitted value (once the stream is completed)","n":0.076}}},{"i":644,"$":{"0":{"v":"CLI","n":1}}},{"i":645,"$":{"0":{"v":"NAS (Network-attached Storage)","n":0.577},"1":{"v":"\n# Resources\n[nas solution that contains services like SSH, FTP etc](https://www.openmediavault.org/)\n","n":0.316}}},{"i":646,"$":{"0":{"v":"Mysql","n":1},"1":{"v":"\nMySQL is not case sensitive\n\n### Users\n- list all users - `SELECT User FROM mysql.user;`\n- connect as user \"monica\" - `mysql -u monica -p`\n- change user password - `ALTER USER 'userName'@'localhost' IDENTIFIED BY '<my-password>';`\n\n### Databases\n- change database - `use monica;`\n- see open transactions - ` SELECT * FROM information_schema.innodb_trx;`\n\n### Tables\n- show tables - `show tables;`\n- drop table - `drop table <table-name>`\n- show columns - `show columns from <table-name>`\n\n### Dump\n- Backup: `mysqldump -h <hostname> -u <user> -p <databasename> > dump.sql`\n- Restore: `mysql -u root -h 127.0.0.1 -p my-database-name < dump.sql`","n":0.107}}},{"i":647,"$":{"0":{"v":"Mongoose","n":1},"1":{"v":"\nNote: do not use arrow functions in mongoose, since it prevents binding\n\n# Hooks (a.k.a. *Middleware*)\nHooks are useful for atomizing model logic and avoiding nested blocks of asynchronous code.\nOther use cases:\n- complex validation\n- removing dependent documents\n    - (removing a user removes all his blogposts)\n- asynchronous defaults\n- asynchronous tasks that a certain action triggers\n    - triggering custom events\n    - notifications\n## pre-hooks\nThere are two types of pre-hooks:\n### Serial\nThe MW functions are executed one after another\n- Note: calling `next()` does not immediately stop execution of the function. To do this, we would need to call `return`:\n```\nvar schema = new Schema(..);\nschema.pre('save', function(next) {\n  if (foo()) {\n    console.log('calling next!');\n    // `return next();` will make sure the rest of this function doesn't run\n    /*return*/ next();\n  }\n  // Unless you comment out the `return` above, 'after next' will print\n  console.log('after next');\n});\n```\n### Parallel\nThe hooked method does not get executed until `done` is called by each middleware:\n```\nvar schema = new Schema(..);\n\n// `true` means this is a parallel middleware. You **must** specify `true`\n// as the second parameter if you want to use parallel middleware.\nschema.pre('save', true, function(next, done) {\n  // calling next kicks off the next middleware in parallel\n  next();\n  setTimeout(done, 100);\n});\n```\n## post-hooks\nFunctions that are executed *after* the hooked method and all of its `pre` middleware have completed.\n\nDo not directly receive control flow\n- i.e. no `next` or `done` callbacks are passed to it\n\n`post` hooks are a way to register traditional event listeners for these methods\n- ex. when the `save` pre-hook happens, this function will execute:\n```\nschema.post('save', function(doc) {\n  console.log('%s has been saved', doc._id);\n});\n```\n\n## Types of middleware\n### Document MW\n`this` refers to the document itself\n\nHook methods:\n- `init`\n    - initialize a document without setters\n    - Called internally after a document is returned from mongodb.\n- `validate` (a.k.a. *pre-save*)\n    - Executes registered validation rules for this document.\n    - if a validation rule is violated, save is aborted\n- `save`\n    - Saves this document\n    - calling this (as a pre-hook) will trigger `validate`, though `validate` will execute before `save`\n- `remove`\n    - Removes the document from database  \n### Model MW\n`this` refers to the model (schema)\n### Query MW\n`this` refers to the query\n- `count`\n- `find`\n- `findOne`\n- ...\nMongoose will not execute a query until `then` or `exec` has been called on it. The power of this comes when we want to build complex queries (ex. using `populate`/`aggregate`)\n- Note: `.then()` in Mongoose are not actually `promises`. If you need a fully-fledged promise, use `.exec()`\n```\nUser.find({ username }) // will not execute\n\n// callback\nUser.find({ name: 'John' }, (err, res) => {}) // Will execute\n\n// .then()\nUser.find({name: 'John'}).then(); // Will execute\n\nPromise.all([User.find({name: 'John'}), User.find({name: 'Bob'})]) // Will execute all queries in parallel\n\n// .exec()\nUser.find({name: 'John'}).exec(); // Will execute returning a promise\n```\n### Aggregate MW\nAggregate MW executes whe you call `exec()` on an aggregate object\n`this` refers to the aggregation object (`<Model>.aggregate`)\n\nIf any middleware calls `next()` or `done()` with an argument of type `Error`, the flow is interrupted and the error is passed to the callback\n\n# Populate\nLets us reference documents in other collections\n- similar to JOIN in SQL dbs\n\n*population* is the process of automatically replacing the specified paths in the document with document(s) from other collection(s).\n\nIn models, we give the `ref` option to a field (property within a document) to indicate which model to use during population.\n```\nconst userSchema = new mongoose.Schema({\n    username: String,\n    posts: [{\n        type: mongoose.Schema.Types.ObjectId, //an array of object ids\n        ref: 'Post' //which model to use\n    }]\n})\n\nconst postSchema = new mongoose.Schema({\n    content: String,\n    author: {\n        type: mongoose.Schema.Types.ObjectId,\n        ref: 'User'\n    }\n})\n```\nLater, when we are within our controller, we will be using `.populate()` on the fields that have `type = mongoose.Schema.Types.ObjectId`:\n```\nUser.findOne({ username: 'Kyle' })\n    .populate('posts') //we want populate to work with the posts field in the user collection\n    .exec((err, posts) => { // similar to .then()\n        if (err) return err\n        console.log('populated user:', posts)\n    }\n```\nThis query will return the specific document within the user collection where `username = 'Kyle'`, and it will *populate* the posts field with all posts made by 'Kyle':\n```\n{\n    _id: 123,\n    username: 'Kyle',\n    posts: [\n        \"s8fm39m\",\n        \"c83ncm8\",\n        \"w822m02\"\n    ]\n}\n```\n# Schemas\n## Instance Methods\nHere, instance refers to the document (since a document is an instance of a model)\n\nInstance methods are defined like so:\n```\n// define a schema\nvar animalSchema = new Schema({ name: String, type: String });\n\n// assign a function to the \"methods\" object of our animalSchema\nanimalSchema.methods.findSimilarTypes = function(cb) {\n    return this.model('Animal').find({ type: this.type }, cb);\n};\n```\nNow, all `animal` instances will have a `findSimilarTypes` method available on them:\n```\nvar Animal = mongoose.model('Animal', animalSchema);\nvar dog = new Animal({ type: 'dog' });\n\ndog.findSimilarTypes(function(err, dogs) {\n    console.log(dogs); // woof\n});\n```\nImagine we were making a Medium.com clone. Our Article model would have a field called `claps`. We could define an instance method called `clap()`, which when executed, would increment the field `claps`\n## Static Methods\nWhereas instance methods are defined on the instance (document), static methods are defined on the Model itself.\n```\n//static method\nconst fido = await Animal.findByName('fido');\n\n//instance method\nconst dogs = await fido.findSimilarTypes();\n```\nThe previous two could not be swapped (ex. `Animals.findSimilarTypes()`), since it would not make sense. \n- Since `Animals` is a model and it has no `type`. This naturally would only exist on an instance of the model\n\n## Query Helper\nInstance methods for Mongoose queries:\n```\nanimalSchema.query.byName = function(name) {\n    return this.where({ name: new RegExp(name, 'i') });\n};\nvar Animal = mongoose.model('Animal', animalSchema);\n\nAnimal.find().byName('fido').exec(function(err, animals) {\n    console.log(animals);\n});\n\nAnimal.findOne().byName('fido').exec(function(err, animal) {\n    console.log(animal);\n});\n```\n## Virtual\nLets us define `getters` and `setters` that don't get persisted to the database\n- Imagine we need a variable `fullName`, but on the `User` model, we only store `first` and `last`. The naive way would be to concatenate these 2 variables each time. Instead, lets define a virtual so that we can use this \"pseudo-field\" in our application:\n```\npersonSchema.virtual('fullname').get(function() {\n    return `${this.first} ${this.last}`\n}\n```\n\n### Accessing the parent documents from the child document\n- ex. Tour has *1:many* relationship with reviews\n- We can add a pre-hook MW function onto the tour model\n```\nparentSchema.virtual('reviews', { //the name of the virtual field\n  ref: 'Review', //the child model\n  foreignField: 'tour', //name of the field in the child model that contains the reference to the current (parent) model\n  localField: '_id' //name of the field where the ID is stored on the current (parent) model.\n});\n```\nThen, to actually populate this virtual field, we just have to use the `populate()` method within our `getTour` handler.\n\n## Nested routes\n- Having 2+ resources at the same url\n- \n\n## Improving Read Performance with Indices\n- see bottom of https://medium.com/@SigniorGratiano/modelling-data-and-advanced-mongoose-175cdbc68bb1\n- this also allows us to carry out logic such as \"each user can only review a tour one time\" \n\t- ` reviewSchema.index({ tour: 1, user: 1 }, { unique: true });`\n\n# E Resources\n[Data modeling and Referencing other collections](https://medium.com/@SigniorGratiano/modelling-data-and-advanced-mongoose-175cdbc68bb1)\n","n":0.031}}},{"i":648,"$":{"0":{"v":"Mongo","n":1},"1":{"v":"\n`mongod` is the main daemon process. Think of it as the API of the Mongo Engine. It handles data requests, manages data access, and performs background management operations.\n- A replica set is a group of `mongod` processes that maintain the same data set.\n\t- anal: spinning up multiple pods of the same REST API server. The reason for both is the same: to provide redundancy and high availability.\n\n* * *\n\n## Normalizing data\n- `normalized` - when data is normalized, it means that instead of having documents called `Folders` that has a field called `files` (an array of actual objects— not just the references to the objects), we have documents that are completely separated from one another, and they just make reference to one another.\n\t- a.k.a Referencing and Embedding\n- denormalized:\n```js\nfolder = {\n  name: \"tagA\",\n  files: [\n\t\t{\n\t\t\ttitle: \"Tut #1\",\n\t\t\tauthor: \"bezkoder\"\n\t\t},\n\t\t{\n\t\t\ttitle: \"Tut #2\",\n\t\t\tauthor: \"zkoder\"\n\t\t}\n\t]\n}\n```\n- normalized:\n```js\nfolder = {\n  name: \"tagA\",\n  tutorials: [\"238fnnc3\", \"edn38bsn39\"]\n}\n```\n\n### When to Normalize/Denormalize\nif we were to break \"one to many\" relationships down, we could have:\n\t- *1:few* - ex. 1 movie has a few awards\n\t- *1:many* - ex. 1 movie has potentially 1000s of reviews\n\t- *1:tonnes* - ex. 1 chat app has millions of messages\n- We need to distinguish between these relationships when we need to determine whether to normalize or denormalize\n\t- note: This granularity isn’t necessary in relational databases.\n\n#### Deciding based on relationship type\n- For *1:few*, we always use denormalized (embedded) data\n- For *few:few* we can just embed\n\t- what about *few:many*?\n- For *1:tonnes* and *many:many*, almost always use normalized (referenced) data\n- *1:many* could go either way\n\n#### Deciding based on how data will be interacted with\n- If the data is mostly read and does not change quickly, we should probably embed\n\t- ex. photo gallery related to a movie: once we gather the photos, we probably won’t be updating them too frequently.\n- If the data is updated frequently, we should probably reference\n\t- ex. user generated reviews and ratings of movies. Those vote counts are going to be changing all the time, and we don’t want to query the whole movie document every time a vote is cast. for this reason, it makes sense for reviews to be its own collection with its own documents that reference its parent\n- Imagine Instahop, where each tour guide is also a user. If we were to embed the tourguide directly into the `tour` document, changing any fields in the guide's document would be a pain, since we'd have to update it both in the `tour` document and the `user` document\n\t- Child referencing would be best used here\n\n- in Mongoose, `populate()` is how to denormalize the data as we are sending it to the client\n\n## Referencing other collections\n### Types of referencing\n#### Child referencing (child ignorance/parent holds the child reference)\n- best for *1:Few* relationships\n\n#### Parent referencing (parent ignorance/child holds the parent reference)\n- best for *1:Many*/*1:Tonnes* relationships\n\n#### Two-way referencing\n- best for *Many:Many* relationsips\n\n- If we have a Nuggets collection and a Buckets collection, we have a many-to-many relationship between the two. We could Have a field in the Buckets model called `nuggets`, which would be an array of nugget ids. This would be sufficient for some cases, but what if we wanted to get a list of the buckets that a particular nugget belongs to, while we are looking at a nugget? To make this easier, we should have a field in the Nuggets model called `buckets`. This makes things easier in a way, but means that if we want to put the nugget in another bucket, we need to update both the document in the Buckets collection and the document in the Nuggets collection. In other words, using this schema design means we can no longer change the bucket field from the Nugget collection in a single atomic update.\n\n## Sharded Cluster\n- spec: instead of Mongo deploying a single instance of your database (on a single computer), your database is deployed on different machines (the cluster), and are sharded\n\t- sharded means that not all of the data related to the application will be stored in a single database. Multiple will be used, each one storing different data\n- the `mongos` provides the interface between the client and the sharded cluster\n\t- From the perspective of the application, a mongos instance behaves identically to any other MongoDB instance\n- MongoDB shards data at the collection level, distributing the collection data across the shards in the cluster.\n\t- In other words, the data of a collection is partitioned and stored in different clusters\n- MongoDB uses a *Shard Key* to determine where the data should be split for storage across the different shards.\n\t- Therefore, the shard key consists of a field or multiple fields in the documents.\n\n\n## UE Resources\n- [$ variable](https://docs.mongodb.com/manual/reference/operator/projection/positional/)\n- [design patterns](http://thetechnick.blogspot.com/2016/06/mongodb-design-patterns.html)\n","n":0.036}}},{"i":649,"$":{"0":{"v":"Replica Set","n":0.707},"1":{"v":"\nA replica set is a group of `mongod` processes that maintain the same dataset.\n\nReplica sets provide redundancy and high availability.\n\nA replica set contains several data bearing nodes and optionally one arbiter node. \n\n### Primary and Secondary nodes\nOf the data bearing nodes, one and only one member is deemed the primary node, while the other nodes are deemed secondary nodes.\n- The primary node receives all write operations and records all changes to its data sets in its operation log (ie. `oplog`).\n- The secondaries replicate the primary's oplog and apply the operations to their data sets (asynchronously) so they are brought in sync with the primary node.\n- If the primary is unavailable, an eligible secondary will hold an election to elect itself the new primary. \n\n### Arbiter\nIn some circumstances (such as you have a primary and a secondary but cost constraints prohibit adding another secondary), you may choose to add a \nmongod instance to a replica set as an arbiter. An arbiter participates in elections but does not hold data\n\n### Failover\nWhen a primary does not communicate with the other members of the set for more than the configured `electionTimeoutMillis` period (10 seconds by default), an eligible secondary calls for an election to nominate itself as the new primary. The cluster attempts to complete the election of a new primary and resume normal operations.\n- until the election is complete, the replica set cannot process write operations.\n- The median time before a cluster elects a new primary should not typically exceed 12 seconds","n":0.063}}},{"i":650,"$":{"0":{"v":"Realm","n":1},"1":{"v":"\nThe client-side library for local persistence is called Realm Database, while the associated Firebase-like backend service is called Realm Object Server.\n\n## Partitioning\nMongo needs a way to know which data it should give the user access to. Of course, any one user should not receive all of the data in the entire application, only what is owned by them. When using RealmDB, each partition is composed of Realms.\n- A realm is a collection of Realm objects that all share a *partition key*. Each client subscribes to a different set of realms, which passively synchronize changes as network availability allows.realm\n- each partition value (ex. one userId) maps to a different realm. Therefore, Documents that share the same partition value belong to the same realm\n- Most common realms would be defined by userId or teamId, meaning all the data with the same userId will be partitioned and synced with the RealmDB on the user's phone.\n- You can combine these partitioning strategies to create applications that make some data publicly available, like announcements or an organizational chart, but restrict other data to privileged users.\n\t- To combine strategies, you can use a generic partition key field name like `_partitionKey` with a partition value structured like a query string. For example: `_partitionKey: \"user_id=abcdefg\"`\n\t\t- Structuring your partition values like a query string lets you take advantage of multiple partitioning strategies at once, providing the power of user, team, and public realms all at once\n\nSince all documents that share the same partition value also share the same permissions for each user, you should select a key whose unique values correspond with permissions within your application. Consider which documents each user needs to read, and which documents each user needs to write. What separates one user’s data from another? The concepts of ownership (which users can change which data?) and access (which users can see which data?) decide how you should partition your data.\n","n":0.056}}},{"i":651,"$":{"0":{"v":"Mongo CLI","n":0.707},"1":{"v":"\n- `show dbs` - list all dbs\n- `use <never-forget>` - use the Never Forget db\n- `show collections` - show all collections in the current db\n- `db.users.find()` - get all documents from the collection users\n- `db.insertOne({})`\n","n":0.169}}},{"i":652,"$":{"0":{"v":"Change Streams","n":0.707},"1":{"v":"\nTo use Change Streams, we switch to the collection (`use <collection>`) then open a cursor on a collection with:\n```sh\nlet cursor = db['collection-name'].watch()\n```\n\n- we can also open it on a whole database with `db.watch()`\n\nWe poll for change events using:\n- `cursor.hasNext()` - returns boolean\n- `cursor.next()` - return the next change event (or error)\n- `cursor.tryNext()` - returns the next change event (or null)\n\nChange Event:\n- `_id` - change eventId (not the `_id` of the document that was changeed)\n  - this is used as a `ResumeToken`\n- `operationType` - insert, delete, update, replace etc.\n- `fullDocument` - the latest version of an entire document of insert, delete, update, and replace events.\n  - if there are multiple changes on the same document over a period of time but none of those change events got processed in the intervening period, the `fullDocument` will still show the latest data, as opposed to the data we'd have as a result of any specific operation.\n- `updateDescription` - fields that were updated or removed and the values of those fields after the change.\n\nChange Streams are filterable, which is accomplished by defining an [[aggregation|mongo.aggregation]] pipeline.\n\nChange Streams are resumable, which is accomplished by passing either ResumeTokens or timestamps (`resumeAfter`) when configuring the [[cursor|db.strategies.cursors]].\n- A strategy is to store ResumeTokens in some data layer (e.g. another collection of the same Mongo database) so that the consumer of the stream knows from where to start polling.\n  - ex. if we are using Lambda to consume events, the Lambda will have to know from where to resume polling, since Lambdas are ephemeral.\n\n## Cook\nAll commands can be run from `mongo` shell\n\n```js\nlet cursor = db.collection.myCollection.watch()\ncursor.next()\n```","n":0.061}}},{"i":653,"$":{"0":{"v":"Atlas","n":1},"1":{"v":"\nAtlas is the cloud-based database-as-a-service (DBaaS) of Mongo. Basically, this solution lets Mongo take care of all the deployment details. The MongoDB instance is hosted on their servers, and we just need to use those servers, and pay a service charge for doing so. This is an alternative to self-hosting those MongoDB instances and taking care of all of those details yourself.\n- This is comparable to how Firebase works. You delegate your having to create a backend to Firebase, meaning Google will host your servers, and all you need to do is communicate with them.\n- Atlas will use a cloud-provider, like AWS, Azure, or Linode\n","n":0.098}}},{"i":654,"$":{"0":{"v":"Aggregation","n":1},"1":{"v":"\nAn aggregation allows us to process data records and return computed results.\n- group values from multiple documents and give the ability to perform operations on the grouped data, returning a single result\n\nYou can use aggregation operations to:\n- Group values from multiple documents together.\n- Perform operations on the grouped data to return a single result.\n- Analyze data changes over time.\n\nCan be done in 3 ways:\n[more info](https://docs.mongodb.com/manual/aggregation/#aggregation-map-reduce)\n- Aggregation pipeline\n- Map-reduce function\n- single purpose aggregation methods\n\n## Aggregation Pipeline\nAggregation pipelines are the preferred way to perform aggregations in Mongo.\n\nAn aggregation pipeline consists of one or more stages that process documents\n- Each stage performs an operation on the input documents. \n  - ex. a stage can filter documents, group documents, and calculate values.\n- The documents that are output from a stage are passed to the next stage.\n- An aggregation pipeline can return results for groups of documents. \n  - ex. return the total, average, maximum, and minimum values.\n\n### `$facet`\nA facet processes multiple aggregation pipelines within a single stage (ie. the `$facet` stage) on the same set of input documents.\n- input documents are passed to the `$facet` stage only once.\n- `$facet` enables various aggregations on the same set of input documents, without needing to retrieve the input documents multiple times.\n\nSimilar to the `group by` function of a SQL database\n\nex. in Amazon when you make a search, the sidebar will show relevant things to group by, such as brand, price range etc. If their database was built in Mongo, it would be accomplished with facets.\n\nThe `$facet` stage has the following form:\n```js\n{ $facet:\n   {\n      <outputField1>: [ <stage1>, <stage2>, ... ],\n      <outputField2>: [ <stage1>, <stage2>, ... ],\n      ...\n   }\n}\n```\n\n- [docs](https://www.mongodb.com/docs/manual/reference/operator/aggregation/facet/)","n":0.061}}},{"i":655,"$":{"0":{"v":"Mobile","n":1},"1":{"v":"\n[Basecamp Mobile Hybrid Architecture](https://medium.com/signal-v-noise/basecamp-3-for-ios-hybrid-architecture-afc071589c25)\n","n":0.5}}},{"i":656,"$":{"0":{"v":"Machine Learning","n":0.707},"1":{"v":"\nMachine learning is the ability for computers to learn something without explicitly being programmed to know that thing.\n- it is done by turning data into numbers (which are then stored in [[tensors|math.algebra.linear.tensors]]), and then passing it into a neural network (which tries to find patterns in those numbers)\n    - in fact, you can use machine learning for pretty much anything as long as you can convert the inputs into numbers and write the machine learning algorithm that will allow the machine to find the patterns.\n- The whole premise of machine learning is to learn a representation of the input and how it maps to the output\n    - ex. imagine we had an input $X$ and an ouput $Y$. We are curious to see how these 2 variables relate to each other. Using machine learning, we could reveal that a simple regression formula ($Y = a + bX$) lies beneath the relationship. In this case, machine learning is about discovering the nature of that relationship with only the input and the output.\n\nWhereas traditional programming is focused on defining inputs and rules in order to arrive at an output, machine learning is focused on defining inputs and an output, and figuring out the rules necessary to arrive at that output\n- ex. we are building a program to cook a roast chicken. The traditional programming paradigm would have us defining the ingredients (ie. inputs) and defining the cooking steps (ie. rules), which yields the cooked meal (ie. output). In machine learning, we define the ingredients, define the output, and the machine learning algorithm will figure out how to make the meal itself.\n\n![[hardware.GPU#use-in-machine-learning]]\n\n# Machine Learning Paradigms\nThere are three basic machine learning paradigms:\n- Supervised Learning\n- Unsupervised Learning\n- Reinforcement Learning\n\n![](/assets/images/2023-07-08-21-35-07.png)\n\n## Supervised Learning\nSupervised learning is using labeled datasets to train algoritms to classify data or predict outcomes. It does this by iteratively making predictions on the data and adjusting for the correct answer.\n- \"labeled\" means that the rows in the dataset are tagged or classified in some interesting way that tells us something interesting about that data\n    - ex. \"is this a picture of a t-shirt?\", \"does the picture of the plant have mites on the leaves?\"\n\nThe goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output.\n- it is the job of [[backpropagation|ml.deep-learning.nn.functions#backpropagation]] to train a neural network to learn the appropriate internal representation of how an input relates to an output.\n\nIn supervised learning, the goal is to predict outcomes for new data. You know up front the type of results to expect.\n\nSupervised learning models tend to be more accurate, but also require much more human intervention\n- ex. a supervised learning model can predict how long your commute will be based on the time of day, weather conditions and so on. But first, you’ll have to train it to know that rainy weather extends the driving time.\n\n### Classification Model\nThis method involves us recognizing and grouping ideas/objects into predefined categories\n- ex. customer retention - we can make a model that will help us identify customer that are about to churn, allowing us to take action to retain them. We can do this by analyzing their activity\n- ex. classify spam in a separate folder from your inbox.\n- ex. sentiment analysis to determine the sentiment or emotional polarity of a piece of text, such as a review or a social media post. \n\nClassification models are more suitable when the task involves assigning discrete labels\n- ex. Is a given email message spam or not spam?\n- ex. Is this an image of a dog, a cat, or a hamster?\n\nCommon types of classification algorithms are:\n- linear classifiers\n- support vector machines\n- decision trees\n- random forest\n\n### Regression Model\nThis method involves us building an equation using various input values with their specific weights, determined by their overall value of the impact on their outcome. In this way, it helps us understand the relationship between dependent and independent variables\n- ex. airlines use these models to determine how much they should charge for a particular flights, using various input factors such as days before departure, day of week, destination etc.\n- ex. Weather forecasting - well-suited for regression, since they can estimate a numerical value based on the input features. \n\nRegression models are helpful for predicting numerical values based on different data points, such as sales revenue projections for a given business.\n\nRegression models are more suitable when the output is a continuous value\n- ex. What is the value of a house in California?\n- ex. What is the probability that a user will click on this ad?\n\nPopular regression algorithms are: \n- linear regression\n- logistic regression\n- polynomial regression\n\n## Unsupervised Learning\nUsing machine learning algorithms to analyze and cluster unlabelled datasets, allowing us to discover hidden patterns/groupings without the need for human intervention.\n\nWith an unsupervised learning algorithm, the goal is to get insights from large volumes of new data. The machine learning itself determines what is different or interesting from the dataset.\n\nUnsupervised models still require some human intervention for validating output variables. \n- ex. an unsupervised learning model can identify that online shoppers often purchase groups of products at the same time. However, a data analyst would need to validate that it makes sense for a recommendation engine to group baby clothes with an order of diapers, applesauce and sippy cups.\n\nUnsupervised learning models are used for three main tasks: \n- clustering\n- association\n- dimensionality reduction\n\n### Clustering\n- ex. customer segmentation - it is not always clear how individual customers are similar or different from one another. Clustering algorithms can take into account a variety of information on the customer, such as their purchase history, social media activity, geography, demographic etc., with the goal being to segment similar customers into separate buckets so the company be more targetted with their efforts\n\n### Association\n- ex. recommendation engines\n\n### Dimensionality Reduction\nTechniques that reduce the number of input variables in a dataset so we don't let redundant parameters overrepresent the impact on the outcome.\n\n## Reinforcement Learning\nReinforcement Learning is semi-supervised learning where we typically have an agent take actions in an environment. The environment will then reward the agent for correct moves, or punish it for incorrect moves\n- Through many iterations of this, we can teach a system a particular task\n- ex. with self-driving cars\n\n## Transfer Learning\nTake a pattern that one model has learned from a certain dataset, and apply those learnings to a different model to give us a head start.\n- More specifically, it's about leveraging the knowledge learned from one task to aid the learning process in another related task. This can be done within the same model or across models.\n- ex. for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks.\n\n# Machine Learning Approaches\n### Seq2Seq (Sequence to Sequence)\nHere, we put one sequence into the model, and get one out\n- ex. with Google Translate, we put in our sequence of source language text and get our target language out\n- ex. with speech recognition, we put in our sequence of audio waves and get some text out\n\n### Classification/regression\nClassification is about predicting if something is one thing or another (e.g. if an email is spam or not)\n- Computer Vision\n    - ex. recognizing a truck within a security camera picture. \n        - In this case, the regression predicts where the corners of the box should be (predicting a number is what regression does), and the classification part would be the machine recognizing whether or not the particular vehicle was the one that did the hit and run.\n        ![](/assets/images/2023-07-09-08-40-22.png)\n- Natural Language Processing\n\n* * *\n\n## Datasets\nThe idea is that we want to split up our data and assign different portions to different sets.\n- the main reason to split datasets into Training, Validation, and Test sets is to ensure that we don't overfit our model to the data it was trained on.\n\n### Training Set\nThe training set is the data that the model learns from\n\nThis usually encompasses around 60-80% of our data\n\n### Validation Set\nThe validation set is the data that the model gets tuned to\n\nValidation sets are used often, but are not required like training sets and test sets are.\n\nThe validation set helps tune hyperparameters and choose the best version of the model\n\nThis usually encompasses around 10-20% of our data\n\n### Test Set\nThe test set is the data that the model gets evaluated on to test what it has learned (ie. it is the final evaluation)\n\nThe test set should always be kept separate from all other data, since we want our model to learn on training data and then evaluate it on test data to get an indication of how well it generalizes to unseen examples.\n\nThis usually encompasses around 10-20% of our data\n\n## UE Resources\n- [Why are machine learning operations run on GPUs?](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)\n","n":0.026}}},{"i":657,"$":{"0":{"v":"Terminology","n":1},"1":{"v":"\n### Label\nA label is the thing we're predicting—the $y$ variable in simple linear regression. \n- ex. The label could be the future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip, or just about anything.\n\n### Feature\nA feature is an input variable—the x variable in simple linear regression. A simple machine learning project might use a single feature, while a more sophisticated machine learning project could use millions of features, specified as:\n\n$$\nx_{1}, x_{2}...x_{N}\n$$\n\n- ex. In a spam detector, the features could include: words in the email text, sender's address, time of day the email was sent, email contains the phrase \"one weird trick.\"\n\n### Example\nAn example is a particular instance of $x$ (note: $x$ is a vector)\n- each example is often represented by a vector of features.\n\nExamples can be either unlabeled or labeled.\n- A labeled example includes both feature(s) and the label\n- An unlabeled example contains features but not the label.\n\n### Model\nA model defines the relationship between features and label. \n- ex. a spam detection model might associate certain features strongly with \"spam\". \n\nThere are 2 principle phases of a model's life:\n1. *Training*, which means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label.\n2. *Inference*, which means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (`y'`). \n    - ex. during inference, you can predict `medianHouseValue` for new unlabeled examples.\n\n### Parameter\nA model parameter is a configuration variable that is internal to the model and whose value can be estimated from data.\n- that is, a parameter is a value that the model sets itself\n\nIn the context of [[neural networks|ml.deep-learning.nn]], `parameters` usually refer to `weights` and `biases` of the network.\n\n### Hyperparameter\nA hyperparameter is a value that we (as a developer) set.\n- ex. `learning rate`\n\n### Epoch\nEach time a dataset passes through an algorithm, it is said to have completed an epoch\n- therefore, one epoch is one entire passing of training data through the algorithm\n\nAn epoch is a hyperparameter that determines the process of training the machine learning model","n":0.053}}},{"i":658,"$":{"0":{"v":"Deep Learning","n":0.707},"1":{"v":"\n## What is it?\nDeep learning is a subset of machine learning that uses artificial neural networks to mimic the learning process of the human brain\n- [[Neural networks|ml.deep-learning.nn]] make up the backbone of deep learning algorithms\n\nThe \"deep\" in deep learning is referring to the depth of layers in a neural network. A neural network that has at least 1 hidden layer can be considered a deep learning algorithm. This is generally represented using the following diagram:\n![Deep neural network](/assets/images/2023-07-07-21-58-46.png)\n- Classical, or \"non-deep\", machine learning is more dependent on human intervention to learn.\n\nDeep learning is thought of as \"scalable machine learning\", since it automates a lot of the feature extraction process, and eliminates some need for human intervention whcih enables use for large datasets.\n\n\"Deep\" machine learning can leverage labeled datasets (ie. [[supervised learning|ml#supervised-learning]]) to inform its algorithm, but it doesn’t necessarily require a labeled dataset.\n- It can ingest unstructured data in its raw form (e.g. text, images), and it can automatically determine the set of features which distinguish \"pizza\", \"burger\", and \"taco\" from one another.\n\nBy observing patterns in the data, a deep learning model can cluster inputs appropriately.\n- we could group pictures of pizzas, burgers, and tacos into their respective categories based on the similarities or differences identified in the images.\n- however, a deep learning model would require more data points to improve its accuracy, whereas a machine learning model relies on less data given the underlying data structure.\n\nDeep learning is primarily leveraged for more complex use cases, like virtual assistants or fraud detection.\n\nMost deep neural networks are feed-forward, meaning they flow in one direction only from input to output. \n- you can also train your model through *backpropogation*, meaning it moves in opposite direction from output to input.\n    - this allows us to calculate and attribute the error associated with each neuron, allowing us to adjust and fit the algorithm appropriately.\n\n## Why use Deep learning?\nWe use deep learning when we have a long set of rules that the program needs to adhere to\n- ex. a program to cook a roast chicken is easy since the ruleset is small. a program to enable cars to drive autonomously is complicated, because the rules involved are enormous\n- We want to discover insights within a large set of data\n\nDeep learning is a bit of a chainsaw solution to a problem. Sometimes all we need is a pair of scissors to solve in the problem, in which case deep learning is overkill. Traditional methods are lighter, easier to train, and require less training data.\n- ex. XGBoost is still the state of the art for tabular data because those datasets are typically too small for deep learning.\n\n### Why not use it?\n- machine learning doesn't offer us much in the way of explanation for the patterns that it finds\n- machine learning isn't deterministic— the results are based on probability.\n\n## Resources\n- [Hugging Face](https://huggingface.co/learn/nlp-course/chapter1/1)\n- [Practical Deep Learning for Coders](https://course.fast.ai/)\n- [Dive into Deep Learning](https://d2l.ai/)\n- [Neural Networks and Deep Learning online book](http://neuralnetworksanddeeplearning.com/)","n":0.045}}},{"i":659,"$":{"0":{"v":"Neural Network","n":0.707},"1":{"v":"\n## Overview\nNeural networks are preferred when you have unstructured data (ie. data that is not neatly formatted in tables)\n- that's not to say that they can't work well with structured data. It's more the case that traditional machine learning algorithms like decision trees or SVMs often handle structured data well.\n\nAt a basic level, a neural network is comprised of four main components: \n- inputs\n    - ex. your inputs may have a binary value of 0 or 1\n- weights - Larger weights make a single input’s contribution to the output more significant\n- a bias or threshold - the output value of any node must be above the threshold for data to be sent to the next layer of the network\n    - a threshold value of 5 would translate to a bias value of –5.\n- an output (`y-hat`)\n\n### Node (a.k.a Neuron)\nEach node corresponds to a neuron in a biological neural network\n\nThink of each node as its own [[linear regression|statistics.regression]] model.\n- since linear regression can be used to predict future events\n\nThink of a neuron as a function that takes in the outputs of all nodes from the previous layer and returns a number between 0 and 1.\n- this number is called the *activation*, and it is the output of the *activation function*\n- the activations in one layer determine the activations in the subsequent layer.\n- note: While many activation functions produce values between 0 and 1 (like the sigmoid function), not all do. For instance, the ReLU (Rectified Linear Unit) function produces values between 0 and infinity, and the tanh function outputs values between -1 and 1.\n\n### Weight\nEach connection between 2 nodes is assigned a weight, which indicates the strength of that connection, and how much influence the input has on the node.\n\nEach node (the current node) receives input from *every* node of the previous layer. To each of these inputs we apply a weight that affects how much of a contribution that particular node of the previous layer has on the current node\n\nWeight is a fundamental concept to neural networks because inputs naturally have a different magnitude of effort on the outcome\n- ex. in a model that predicts housing prices, the recency of a paint job and the number of bedrooms are both inputs to the model that affect the price, but the latter has a much bigger impact on the ultimate price of the house.\n\nWeights are the primary mechanism by which neural networks learn. During training, the network gets feedback on its predictions in the form of a loss or error. To minimize this error, the model uses optimization techniques (like gradient descent) to adjust the weights. Over time, the model gradually makes more and more accurate predictions.\n\nThe main difference between [[regression|statistics.regression]] and a neural network is the impact of change on a single weight. \n- In regression, you can change a weight without affecting the other inputs in a function. this isn’t the case with neural networks. Since the output of one layer is passed into the next layer of the network, a single change can have a cascading effect on the other neurons in the network.\n\nThe neural network figures out the weights on its own\n\na.k.a representation, patterns, numbers, features\n\n### Bias\nThe bias is a value that is applied to the weighted sum of all neurons from the previous layer, adjusting the importance of that neuron. \n- In other words, the bias gives us some indication of whether or not that neuron tends to be active or inactive.\n- More specifically, bias shifts the activation function along the input axis, essentially determining the threshold at which the neuron begins to activate.\n\n![](/assets/images/2023-08-10-21-26-46.png){max-width: 200px}\n\n### Layers\nNeural networks are composed of layers. Generally they are:\n- input layer\n- hidden layers\n- output layer\n\n![](/assets/images/2023-07-02-20-17-39.png)\n\nEach layer is usually a combination of linear functions (ie. straight lines) and non-linear functions (ie. non-straight lines)\n\n### How a neural network learns (simplified)\nA neural network learns by:\n1. starting with random numbers\n2. perform tensor operations\n3. update random numbers to try and make them better representations of the data\n4. repeat\n\n## Types of Neural Network\n![[ml.deep-learning.nn.types]]\n\n* * *\n\n### Example: number recognition\nImagine we have a model that takes an image (28x28 pixels) of a handwritten number and tells us what number it is. \n- *First layer* - Our neural network will start with 784 neurons ($28x28$), and each value between 0 and 1 will represent its grayscale level (from black to white). These 784 neurons make up the first layer of our neural network.\n- *Last layer* - Our last layer will be composed of 10 neurons, each representing a digit from 0-9. \n- *Hidden layers* - Though hidden layers are a black box, it helps to think of them in terms of how they *might* behave. Generally when solving problems with computers, it's helpful to break problems down into smaller problems, then use the smaller building block solutions to amount to a bigger solution. In this case, it's difficult to recognize the number $9$. However, it's easier to recognize a shape with a loop and a tail (the top and bottom part of the number, respectively). There are however, different numbers that have loops ($6$, $8$, $0$) and different numbers that have tails ($7$, $1$). So we might think, \"What if each node in our second last layer represented a different component shape? Then the activation functions of the *loop node* and the *tail node* would output a number close to 1, which would be enough information to tell us that we have the number $9$\".\n    - of course, then the question becomes \"how do we recognize shapes like loops and tails?\". We can continue to break down these shapes to reveal straight-ish edges. That is, we can think of the upper loop of the number $9$ as being made up of ~5 straight-ish edges. When our neural network receives the handwritten $9$, it can break it down first into edges, then determine which shapes those edges make, and then finally determine which number it is based on which shapes it has found.\n    - [video demonstration](https://youtu.be/aircAruvnKk?t=443)\n\nThe activation of these neurons (often simply a number between 0-1) represents how much the model thinks that the input image is that particular number\n- ex. if our input drawing was of a 9, then the final neuron in the last layer will have the highest activation value\n\nIn this example, the *weights* of each node activation tell us the pixel pattern that that node is picking up on, while the *bias* tells us how high the weighted sum needs to be before we consider the neuron to be meaningfully active\n\n## UE Resources\n- [Neural Networks and Deep Learning Book](http://neuralnetworksanddeeplearning.com/)\n    - has a lot of code examples and gives a good fundamental overview. Recommended by 3Blue1Brown\n\n## E Resources\n- [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk&t=707s)\n    - explains the fundamentals of a neural network, including layers, neurons, biases, weights etc.","n":0.03}}},{"i":660,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n\nData is passed from one layer to the next in what's called a **Feedforward neural network (FNN)** \n- so-called because connections between the nodes do not form a cycle.\n\nOther types of Neural Network:\n- **Convolutional Neural Networks (CNNs)** are commonly used as the model architecture in image classification tasks\n- **Recurrent Neural Networks (RNNs)** are commonly used in natural language processing tasks\n- **Transformer** are commonly used in natural language and speech\n- **Fully Connected Neural Networks (FCNN)**","n":0.116}}},{"i":661,"$":{"0":{"v":"Functions","n":1},"1":{"v":"\n### Activation Function\nThe activation function is used to determine the output of the neuron. It's called an activation function, because the result is either that the neuron lights up, or it doesn't.\n\nEach hidden layer has its own activation function, which passes information from the previous layer into the next one.\n- Once all the outputs from the hidden layers are generated, then they are used as inputs to calculate the final output of the neural network\n\nThe activation function of a current node is applied to the weighted sum of all nodes of the previous layer to that current node, plus the bias. That is:\n\nactivation of a node $= \\sigma(w_{1}a_{1} + w_{2}a_{2} +...+ w_{n}a_{n} + b)$\n- $\\sigma$ - activation function (e.g. Sigmoid, ReLU)\n- $w$ - weight of connection\n- $a$ - activation level of previous layer's node\n- $b$ - bias\n\nThe most common activation function is *ReLU*, but historically *Sigmoid* was also used.\n\nThe Activation Functions can be basically divided into 2 types:\n1. Linear Activation Function\n2. Non-linear Activation Functions\n    - most common\n\n### Loss Function (a.k.a Cost Function/Error Function)\nA loss function tells us how far off the output of a neural network is from the real-world expectation. As a result, the purpose is to tell us how well the neural network is performing and provide a value that can be used to update the model's parameters during the training process.\n\nLoss functions enable the model to learn from the training data by minimizing the discrepancy between predictions and ground truth.\n- By iteratively adjusting the model's parameters to minimize the loss function, the model improves its ability to make accurate predictions.\n\nThe goal during training is to minimize the output of the loss function by adjusting the model's parameters.\n\nTo get the value of the loss function, for each output node, we take the difference between its activation value and the value we expected to have, square it, then add them all up.\n- the closer the result is to ideal, the lower the sum will be.\n\nIn the below example, the results of the model are shown in the yellow-outlined layer. We see that the model incorrectly predicted the number $3$, thereby resulting in a high value from the loss function.\n![](/assets/images/2023-08-11-21-21-27.png)\n\nIt's important to remember that this loss function involves an average over all of the training data, meaning if we minimize the output of the function, it means it's a better performance on all of that sample data.\n\nWhen we talk about a network \"learning\", all we mean is that the network is just minimizing the output of its loss function.\n\nThe loss function takes as input all the weights/biases of the network's neurons, and outputs a single number (the measure of how effective the model was)\n\nThe gradient of the loss function tells us how to alter the weights/biases cause the fastest change to the output of the loss function\n- in other words, it answers the question \"which changes to which weights matter the most?\"\n\nIn Pytorch, the term *criterion* may be used to refer to a loss function.\n\nLoss functions\n- Mean Absolute Error & Mean Square Error - used for regression problems (therefore, predicting a number)\n- Cross Entropy Loss - used with classification problems\n\n#### Mean Absolute Error (MAE)\nThe below image shows the first stage of our model. It gives us:\n- our training data (blue)\n- our ideal testing data (green), ie. what the model should ideally predict\n- our initial (random) prediction\n![](/assets/images/2023-08-19-18-12-05.png)\n\nIf we were using a Mean Absolute Error loss function, what we would essentially do is get the distance between each red dot and its corresponding green dot (say the first is 0.4 difference), then get the mean of that difference amongst all of the dots. This would be our Mean Absolute Error, and the result of our loss function.\n\n- note: In [[pytorch]], the MAE loss function is [nn.L1Loss](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)\n\n### Optimization Algorithm (a.k.a Optimizer)\nThe optimization algorithm is responsible for updating the model's parameters iteratively based on the gradients of the loss function.\n\nThe optimization algorithm aims to find the set of parameter values that minimizes the *loss function*, leading to improved model performance.\n\nThe loss function and optimization algorithm work hand in hand during training. The loss function provides a measure of how well the model is doing, and the optimization algorithm guides the updates of the model's parameters to minimize the loss function.\n\n#### Gradient Descent\nGradient descent is an example of an optimization function.\n- The main objective of gradient descent is to find the parameters (like the weights in a neural network) that minimize the loss function.\n\nGradient descent is the process of repeatedly nudging an input of a function by some multiple of the negative gradient, and it's a way to converge upon some local minimum of a loss function (demonstrated as a valley in the graph below).\n\n![](/assets/images/2023-08-11-21-36-24.png)\n\nAn important note to understand about gradients is that it is reasonably easy to find a local lowest point, but it is difficult to find the global lowest point.\n\n#### Hyperparameters\nsee [[Hyperparameters|ml.terminology#hyperparameter]]\n\n- *Learning Rate* - the higher the learning rate, the more it adjusts each of the model's parameters with each iteration of the model (ie. each time the optimization function gets called)\n\n### Backpropagation\nBackpropagation is the algorithm used in supervised learning for computing the gradient efficiently. This is effectively the heart of how a neural network learns.\n- It involves calculating the gradients of the loss with respect to the network's parameters (weights and biases). \n    - This is achieved by applying the chain rule of calculus. \n- The gradients tell us how much each parameter should be adjusted to minimize the loss.\n\nGiven a [[neural network|ml.deep-learning.nn]] and a loss function, the backpropagation calculates the gradient of the error function with respect to the neural network's weights.\n- it does this by performing a backward pass to adjust the network model's parameters, aiming to minimize the mean squared error (MSE).\n\nThe \"backwards\" part of the name stems from the fact that calculation of the gradient proceeds backwards through the network, with the gradient of the final layer of weights being calculated first and the gradient of the first layer of weights being calculated last.\n\nBackpropagation is short for \"backward propagation of errors\"","n":0.032}}},{"i":662,"$":{"0":{"v":"Memory","n":1},"1":{"v":"\nWhen a software program uses memory there are two regions of memory they use, (apart from the space used to load the bytecode), [[Stack|memory.stack]] and [[Heap|memory.heap]] memory.\n\n* * *\n\n### Memory Hierarchy\nMemory hierarchy is the pyramid formed when we stratify different implementations of memory [[storage|storage]] and categorize them according to the level of [[abstraction|general.terms.abstraction]] they exist at.\n- ex. implementations are anything from circuit-level processor [[resistors|hardware.circuit-board.resistor]] and [[registers|hardware.cpu.register]] to hard drives and USB sticks.\n![](/assets/images/2022-03-10-10-59-56.png)\n\nThe main idea of memory hierarchy is that the storage at one level serves as a cache for storage at the next lower level\n\nBecause of physical laws, larger storage devices are slower than smaller storage devices.\n- ex. a disk drive on a typical system might be 100 times larger than the main memory, but might take 10,000,000x longer to read a word from disk than from memory. Similarly a typical register file (CPU) stores only a few hundred bytes of information, as opposed to millions of bytes in main memory. However, the processor can read data from the register file almost 100x faster than from memory.\n\n### Memory Leak\nin languages without garbage collection, memory leaks occur when we fail to free up memory (deallocate) after a [[pointer|memory.pointer]] no-longer serves its purpose (such as when we never reference that variable again until end of execution)\n\n### Memory Cell\nThe memory cell is the fundamental building block of memory.\nThe memory cell itself is an electronic circuit that stores a single bit of binary information (ie. 0 or 1)\n- This is achieved by sending either a high voltage signal (1) or low signal (0)\n","n":0.062}}},{"i":663,"$":{"0":{"v":"Virtual Memory","n":0.707},"1":{"v":"\nVirtual memory is when the hierarchy of storage managed by the [[os.kernel]]. It is achieved when the kernel is in charge of the [[memory hierarchy|memory#memory-hierarchy,1:#*]]\n\nVirtual memory is typically segregated into user space and kernel space. Access rights are required to access and execute programs.\n- this provides us with memory protection and hardware protection from malicious or errant software behaviour.\n","n":0.13}}},{"i":664,"$":{"0":{"v":"Stack","n":1},"1":{"v":"\nContrast with [[heap|memory.heap]]\n- A place in memory where variables declared by a function are stored\n- Any time we create a variable In a normal way, we are putting it on the stack\n- Every time a function declares a new variable, it is \"pushed\" onto the stack. Then every time a function exits, all of the variables pushed onto the stack by that function are deleted.\n- when a function exits, all of its variables are popped off of the stack. Thus stack variables are local in nature\n- variables are declared, stored and initialized during runtime.\n- Storage is temporary, and when The computing task is complete, the memory location will be erased\n- Data structure is linear\n- Data is physically located together (contiguous blocks)\n- Variable de-allocation is automatic\n- The stack is a LIFO data structure\n","n":0.087}}},{"i":665,"$":{"0":{"v":"Pointer","n":1},"1":{"v":"\nA pointer is a variable that stores both a memory address and the type of data that resides at that memory location.\n- Obtaining the value stored at that location is known as *dereferencing the pointer*\n    - anal: The index within a textbook has page numbers which reference pages in the book. Dereferencing the pointer would be done by flipping to that page and reading the text.\n\t- the type of the pointer tells the compiler what operations can be performed through that pointer\n- a pointer is a very thin abstraction built on top of a language's own addressing capabilities\n- Each unit of memory in a system is assigned a unique address (memory location).\n\t- A pointer is an object that stores this address.\n\t\t- This is similar in concept to how we dont store images in a database, we just store its url location\n\t- Conceptually, the fact that memory is just stored in blocks makes memory itself a very large array.\n\t- arrays work a bit differently in that the variable points to the address (location) of the first character in the array \n- pointers are used partly because they are sometimes the only way to express a computation, and partly because they usually lead to more compact and efficient code than can be obtained in other ways\n\t- They are especially efficient when used in repetitive operations\n- pointers are used for constructing *references*, which in turn are fundamental to constructing nearly all data structures\n- using a pointer to retrieve the value that is stored at the relevant memory location is called **dereferencing the pointer**\n- anal:if storage were a reference book, then the page number in the index would be the pointer, and flipping to that page and reading the text would be dereferencing that pointer.\n- the format of a pointer is dependent on the underlying computer architecture (since pointer's pertain to physical memory locations)\n- Pointers and arrays are closely related\n\t- Any operation that can be achieved by array subscripting can also be done with pointers\n\t- consider that when we declare an array `x` of length 10, we are reserving 10 contiguous memory cells. We can create a pointer that points to the first element with `ptr = &x[0];`. Now, we can copy the contents of that first element to `y` with `y = *ptr;`.\n\t\t- Since elements in an array are contiguous, by defintion `ptr+1` points to the second element in the array, and `*(ptr+1)` refers to the *contents* of `x[1]` \n\t- ex. a char occupies a single byte, a short occupies 2 contiguous bytes, a long occupies 4 contiguous bytes, and so on.\n\t\t- Naturally, this means that contiguous groups of memory cells can be manipulated\n- A pointer is a group of cells (often two or four) that can hold an address\n\n### Example\nImagine we executed the following code:\n```\nint a = 5;\nint *ptr = NULL;\n```\nassuming that `a` is stored at `0x8130` and `ptr` at `0x8134`, our memory would look like:\n| Address | Value |\n|---------|------------|\n| 0x8130  | 0x00000005 |\n| 0x8134  | 0x00000000 |\n\nnow this code is run:\n```\nptr = &a;\n```\nAnd our memory looks like this:\n| Address | Value |\n|---------|------------|\n| 0x8130  | 0x00000005 |\n| 0x8134  | 0x00008130 |\n\nnow we can dereference `ptr`:\n```\n*ptr = 8;\n```\nAnd our computer takes the contents of `ptr` (0x00008130), locates the address, and assigns 8 to that location, yielding this memory:\n| Address | Value |\n|---------|------------|\n| 0x8130  | 0x00000008 |\n| 0x8134  | 0x00008130 |\n","n":0.042}}},{"i":666,"$":{"0":{"v":"Leak","n":1},"1":{"v":"\nA program claims a portion of memory to perform its work. When it no longer needs that memory, it releases it so it can be reused (normally by the same program). When a program \"forgets to\" forfeit that memory, it builds up, resulting in a memory leak.\n\nThe main cause for leaks in garbage collected languages are unwanted references.\n\n## UE Resources\n- [Memory leaks in Javascript](https://auth0.com/blog/four-types-of-leaks-in-your-javascript-code-and-how-to-get-rid-of-them/)","n":0.125}}},{"i":667,"$":{"0":{"v":"Heap","n":1},"1":{"v":"\nContrast with [[stack|memory.stack]]\n- A place to store global variables. \n- The heap is not automatically managed for you\n- Data structure is hierarchical\n- Variables need to be deallocated manually\n- Garbage collection runs on the heap\n- The heap is slower, but it can also store much more data than the stack\n- In C, `malloc` and `calloc` are methods used to interact with the heap. Once memory has been allocated on the heap, we must use `free()` to deallocate that memory. \n\t- Failure to do this results in what’s called memory leaks. \n- Pointers must be used to access memory on the heap\n- allocating memory is done on the heap, not the stack (as with other variables)\n- dynamic memory allocation can only be made through pointers and names (variable names) can't be given\n\t- `malloc()` is for allocating memory blocks from the heap in C\n\nHeaps are used for dynamic memory allocation and unlike [[stacks|memory.stack]], the program needs to look up the data in heap using pointers.\n- Therefore it can store more data than a stack, but lookups take longer.\n\nWhen we talk about \"memory management\" we are mostly talking about managing the Heap memory.\n\nA heap is a [[tree structure|general.lang.data-structs.tree]]\n![](/assets/images/2023-07-24-21-59-36.png)","n":0.072}}},{"i":668,"$":{"0":{"v":"Buffer","n":1},"1":{"v":"\nA Buffer is an in-memory (typically [[RAM|hardware.ram]]) collection of raw bytes\n- Because computers store data in bytes, a simple way to think of a Buffer is to think of it as an array of bytes:\n```js\nconst buffer = [11111111, 11011001, 10010001]\n```\n\nThe point of a Buffer is to simply store anything as an array of bytes. The reason this is important is because every thing in computing communicates in bytes.\n\n- It can be thought of as a temporary place to put things that need to be worked on or processed,\n    - anal: like a to-do pile of work on your desk: Instead of just doing work the moment it’s given to you, you have people put it in a folder on your desk so you can work on everything at a steady pace\n- Buffering is a *technique*, not one specific place in the computer.\n\n### with Streams\na Buffer is what you get from or put to a Stream. The stream then reads or writes the buffer to the input or output target that the Stream is connected to.\n\n### Examples\n\n#### Movies\nThe rate at which you download a file may fluctuate but if the playback of that video did too it would be very awkward to watch. A buffer is a temporary storage place for some of that video so the downloading process can put it somewhere as it comes in and the playback process can play it at a steady rate.\n- For video, what you do is to first retrieve (for example 10 seconds) and then start to play. If the network drops some packet (and the data along with it) you can ask for it to be retransmitted to fix the problem.\n- if the buffer empties without having an end-of-file marker (a special flag designed to tell the playback device that no more data is needed), the program won’t know what data to display/playback next. Thus in the context of video/audio, buffering is when the video/audio buffer is filled with data when it has processed all the data in the buffer without receiving an end-of-file marker.\n- When you see a “buffering” message in the middle of playing a video, that means that the internet got so slow that the player ran out of video to display. A smart player might notice that 10 seconds (or whatever) of buffered video wasn’t enough, given how flaky the internet is at this location is, so maybe it’ll bump up the buffer to 20 or 30 seconds.\n\n#### Keyboard\nWhen you type on your keyboard, the keystroke data is put into a buffer. Then depending on what program, text box, etc. you are using, the processor processes the data and makes the appropriate updates to where it is needed (like showing it on screen, applying any commands or special processes to it like copy and paste functions, storing the information in RAM/ROM, etc.) Typically each process has a separate buffer which your CPU can access, handle reading from, writing to, and processing, as well as share information between other buffers. i.e. your keyboard has one buffer, your mouse another, your display another, your web browser another etc. and your processor handles information sharing between all of them.\n","n":0.044}}},{"i":669,"$":{"0":{"v":"Maven","n":1},"1":{"v":"\nMaven is a build tool like [[gradle]]\n\nHelps:\n- download 3rd party packages\n- give the project structure through a set of conventions\n\nMaven projects have a `pom.xml` file \n\n## `pom.xml`\n\n### Properties\n- `<source>11</source>` means \"project should be built *in* Java 11\n- `<target>11</target>` means \"project should be built *for* Java 11\n\n### Build\nAllows you to specify plugins, which are extensions to Maven which help it build our artifact.\n- ex. `maven-jar-plugin`, which is used to make a jar out of your compiled classes and resources\n\n### Dependencies\nA dependency is just a Jar file which will be added to the classpath while executing the tasks.\n\nA depdendency can include a `<scope>` field, which specifies when a dependency should be installed\n- ex. `<scoope>test</scope>`, meaning the dependency will only be installed during tests.\n- ex. `<scope>compile</scope>`","n":0.09}}},{"i":670,"$":{"0":{"v":"Markdown","n":1},"1":{"v":"\n## Math symbols\n- [Math in markdown cheatsheet](https://rpruim.github.io/s341/S19/from-class/MathinRmd.html#:~:text=Math%20inside%20RMarkdown,10n%3D1n2.)\n\n- `%` - $\\%$\n- `~` - $\\sim$\n- R - $\\mathbb{R}$","n":0.25}}},{"i":671,"$":{"0":{"v":"Mac","n":1}}},{"i":672,"$":{"0":{"v":"Osa","n":1}}},{"i":673,"$":{"0":{"v":"JavaScript","n":1},"1":{"v":"\nnote: Documentation for Javascript as an Osascript language is pretty bad. It should be avoided. If possible, stick with Applescript.\n\n## CLI\n### Run script in terminal\n```sh\nosascript -l JavaScript myScript.scpt\n```\n\n### See available languages\n```sh\nosalang -l\n```\n\n### Start/Exit application\n```js\nconst itunes = Application('iTunes');\nitunes.activate();\n// Play a song.\nitunes.play();\nitunes.quit();\n```\n\n### Get current application that's running the script\n```js\nvar app = Application.currentApplication();\n```\n\n### Get topmost application\n```js\n// Get the name of the current application.\nvar system = Application(\"System Events\");\n\nvar proc = system.processes.whose({ frontmost: {'=': true } }).name();\n```\n\n# Resources\n[docs(ish)](https://developer.apple.com/library/archive/releasenotes/InterapplicationCommunication/RN-JavaScriptForAutomation/Articles/Introduction.html#//apple_ref/doc/uid/TP40014508)\nhttps://www.macstories.net/tutorials/getting-started-with-javascript-for-automation-on-yosemite/\nhttps://developer.apple.com/library/archive/documentation/LanguagesUtilities/Conceptual/MacAutomationScriptingGuide/index.html#//apple_ref/doc/uid/TP40016239-CH56-SW1\n","n":0.117}}},{"i":674,"$":{"0":{"v":"Applescript","n":1},"1":{"v":"\nAppleScript is a scripting language for doing inter-application communication (IAC) using Apple events (ex. open a file, save a file). Most often, these actions are synchronous.\n\nWhereas [[Apple events|mac.apple-events]] are a way to send messages into applications, AppleScript is a particular language designed to send Apple events\n- the AppleScript language is designed on the natural language metaphor\n\nAppleScript can send and receive Apple events to applications, and can act as a connector between different apps.\n\nAppleScript relies on the functionality of applications and processes to handle complex tasks\n\nAppleScript can be compared to a Unix shell in terms of its purpose\n\nWhile not all apps are considered scriptable, any app with a graphical user interface responds to Apple Events at a minimal level. This is because OS X uses Apple Events to instruct all apps to perform core tasks such as launching, quitting, opening a document, and printing\n\nA handler in AppleScript is equivalent to a function/method in Javascript\n\nThe heart of the AppleScript language is the use of terms that act as nouns and verbs that can be combined. For example, rather than a different verb to print a page, document or range of pages (such as printPage, printDocument, printRange), AppleScript uses a single \"print\" verb which can be combined with an object, such as a page, a document or a range of pages.\n```\nprint page 1\nprint document 2\nprint pages 1 thru 5 of document 2\n```\n\n### Example Simple Web Gallery\n1. Open a photo in a photo-editing application (by sending that application an Open File Apple event).\n2. Tell the photo-editing application to reduce the resolution of the image\n3. Tell the photo-editing application to save the changed image in a file in some different folder (by sending that application a Save and/or Close Apple event).\n4. Send the new file path (via another Apple event) to a text editor or web editor application\n5. Tell that editor application to write a link for the photo into an HTML file.\n6. Repeat the above steps for an entire folder of images (hundreds or even thousands of photos).\n7. Upload the HTML file and folder of revised photos to a website, by sending Apple events to a graphical FTP client, by using built-in AppleScript commands, or by sending Apple events to Unix FTP utilities.\n\n# E Resources\n[Good guide that goes over Mac Scripting](https://developer.apple.com/library/archive/documentation/LanguagesUtilities/Conceptual/MacAutomationScriptingGuide/index.html#//apple_ref/doc/uid/TP40016239-CH56-SW1)\n","n":0.052}}},{"i":675,"$":{"0":{"v":"Dictionary","n":1},"1":{"v":"\nMac apps publish a dictionary of addressable objects and operations. Applescript leverages this dictionary to be able to communicate with those programs.\n- These dictionaries can be viewed with the Script Editor program, and then File > Open Dictionary (`cmd+shift+o`)\n- At its core, this scripting dictionary is a `.sdef` file that is stored in the app bundle.\n\nEvery scriptable app implements its own scripting features and exposes its own unique terminology through a scripting dictionary\n\n### Types of Terminology\nSuite - A suite is a grouping of related commands and classes\n- includes terminology supported by most scriptable apps, such as an `open` command, a `quit` command, and an `application` class.\n\nCommand - A command is an instruction that can be sent to an app or object in order to initiate some action.\n- ex. `delete`, `make`, `print`\n\nClass - A class is an object within an app, or an app itself\n- ex. Mail has an application class, a message class, and a signature class, among others\n\nProperty - A property is an attribute of a class. \n- ex. the message class in Mail has many properties, including `date received`, `read status`, and `subject`.\n\n### Concepts\n#### Inheritance\ndifferent classes often implement the same properties. \n- ex. in Finder, the file and folder classes both have creation date, modification date, and name properties\n- Rather than defining these same properties multiple times throughout the scripting dictionary, Finder implements a generic `item` class\n    - any properties of the `item` class also apply to the `file` and `folder` classes\n\n#### Containment\nClasses of a scriptable app reside within a certain containment hierarchy. The application is at the top level, with other classes nested beneath.\n- ex. Finder contains `disks`, `folders`, `files`, and other objects.\n- ex. Mail contains `accounts`, which can contain `mailboxes`, which can contain other `mailboxes` and `messages`.\n\n","n":0.059}}},{"i":676,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Comment\n```osascript\n(* commented out *)\n-- commented out\n```\n\n### Display dialog box (similar to console.log)\n```\ndisplay dialog result as text\n```\n","n":0.243}}},{"i":677,"$":{"0":{"v":"MacOS","n":1},"1":{"v":"\nMacOS is a graphical [[operating system|os]].\n- Darwin is the actual operating system that is used, and it is built with the [[unix]] philosophy.\n- Darwin can be thought of as the set of components upon which MacOS is built\n\n### Launch Agent\n- a launch agent is a simple script used by `launchd` that causes the system to run programs at system startup.\n\t- ex. postgres, Dropbox\n","n":0.126}}},{"i":678,"$":{"0":{"v":"Time Mach","n":0.707},"1":{"v":"\nTime Machine stores hourly backups for 24 hours, daily backups for the past month, and weekly backups for all previous months for as much space as you have on your Time Machine backup drive. \n","n":0.169}}},{"i":679,"$":{"0":{"v":"Launchd","n":1},"1":{"v":"\nThe filename must be the same as the `Label` key value:\n`local.script.backup-everdo.plist`\n```xml\n<key>Label</key>\n<string>local.script.backup-everdo</string>\n```\n\n# Cook\n#### List all\n`launchctl list`\n\n# Resources\n[Quick n' dirty guide](https://www.splinter.com.au/using-launchd-to-run-a-script-every-5-mins-on/)\n[GUI for launchd](https://www.soma-zone.com/LaunchControl/)\n","n":0.218}}},{"i":680,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n## Utility\ncmd shift 3 - take a full screenshot\ncmd shift 4 - take a sectioned screenshot\ncmd shift 5 - take video recording\n\n- for all, add ctrl to save into `~/Downloads/screenshots`\n\n## Navigation\nctrl + cmd + f - toggle fullscreen\n","n":0.162}}},{"i":681,"$":{"0":{"v":"Apple Events","n":0.707},"1":{"v":"\nApple events are a high-level message-based form of Interprocess Communication (IPC), used to communicate between local or remote application processes\n\nAn Apple Event contains:\n1. Attributes describing how the event should be handled\n2. optional parameters to the event handler that receives the event\n\nExample\n- When we drag-n-drop a file onto the TextEdit.app icon in Finder, what happens is that Finder commands TextEdit to open that file by sending it an `odoc` (open doc) event with a list of file identifiers as its parameter\n![](/assets/images/2021-05-07-09-48-28.png)\n\nWith proper bindings, Apple events can be created and sent with programming languages\n- ex. from our client application, we might call `iTunes().play()`, causing a hook/Play event to be sent from the client application to iTunes, instructing it to start playing. Applications may respond to an incoming Apple event by sending a reply event back to the client application\n\n## Apple Event Object Model (AEOM)\nThe AEOM is a view-controller layer that provides a user-friendly representation of the application's internal data, allowing clients to identify and manipulate parts of that structure via Apple events\n- An incoming Apple event representing a particular command (get, set, move, etc.) is unpacked, and any object specifiers in its parameter list are evaluated against the application's AEOM to identify the user-level object(s) upon which the command should act\n- The command is then applied these objects, with the AEOM translating this into operations upon the application's implementation-level objects\n    - These implementation-level objects are mostly user-data objects in the application's Model layer\n\n![](/assets/images/2021-05-07-09-57-56.png)\n\n- The AEOM represents user data as a tree-shaped object graph, whose nodes are connected via 1:1 and/or 1:many relationships.\n- AEOM objects are identified by high-level queries (comparable to XPath or CSS selectors), not low-level chained method calls.\n- While the Apple Event Object Model is sometimes described by third-parties as being similar to DOM, this is inaccurate as AEOM operates at a much higher level of abstraction than DOM.\n\nThe AEOM is a tree-like structure made up of objects. These objects may contain descriptive attributes such as class, name, id, size, or bounds; for example:\n\n```\nfinder.version\nitunes.playerState\ntextedit.frontmost\nfinder.home\ntextedit.documents\nitunes.playlists\n```\n\nUnlike other object models such as DOM, objects within the AEOM are associated with one another by relationships rather than simple physical containment\nRelationships between objects may be one-to-one:\n```\nfinder.home\nitunes.currentTrack\n```\nor one-to-many:\n```\nfinder.folders\nitunes.playlists\n```\n\nto show how how this tree-like structure is not related to containment (as the DOM is), consider that the following object specifiers all identify the same objects (files on the user's desktop):\n- the first specifier describes the files' location by physical containment; the other two use other relationships provided by the application as convenient shortcuts\n\n```\nfinder.disks[\"Macintosh HD\"].folders[\"Users\"].folders[\"kyletycholiz\"].folders[\"Desktop\"].files\n\nfinder.desktop.files\n\nfinder.files\n```\n\nHere is the AEOM for a simple hypothetical text editor:\n![](/assets/images/2021-05-07-10-08-32.png)\n","n":0.048}}},{"i":682,"$":{"0":{"v":"LSP (Language Server Protocol)","n":0.5},"1":{"v":"\nThe problem is that there is a matrix of code editors and languages to support. It doesn't make sense that every editor has extensive support for js, ruby, python, C... Why not just make dedicated servers that will handle all things for a specific language (syntax checking, autocomplete, go-to-file etc).\n- For instance, we have vim and a language server that handles everything for javascript. This same language server can be used for VSCode, Sublime, IntelliJ etc.\n![](/assets/images/2021-03-11-19-53-45.png)\n","n":0.115}}},{"i":683,"$":{"0":{"v":"Linux","n":1},"1":{"v":"\nTechnically, Linux is only a kernel. The [[operating system|os]] we run is Ubuntu, or Mint, or Debian etc.\n- Linux can therefore be thought of as a family of operating systems, all connected by the fact that they all use the Linux kernel to perform backend functions.\n\nThe Linux kernel is versioned like this:\n\n> 4.20.16-200.fc29.x86_64\n- The first 3 bytes are [semver](https://semver.org/).\n- `200` refers to bug fixes and patches in this release.\n- `fc29.x86_64` refers to the distribution, here 64 bit version of Fedora 29.","n":0.111}}},{"i":684,"$":{"0":{"v":"Linux OS","n":0.707},"1":{"v":"\n### Systemd\n- systemd is a collection of programs that provides system components for Linux.\n- Its purpose is to unify system configuration across different Linux distributions.\n- Systemd's primary component is a system and service manager, which is an init (boot) system used to manage user processes.\n\t- systemd also provides replacements for various daemons and utilities of the Linux system, including device management, login management, network connection management.\n- Effectively, systemd is used on most Linux systems, and has replaced distribution-specific init systems. \n- below is the systemd startup log.\n![74a24a20baffe9c81ed4a14a5d4c398a.png](:/3d7b66b1422e473096a6481247f59393)\n\n- `systemctl` is a utility used to control systemd \n\t- ex. we can issue a command to restart the `ssh` server\n","n":0.097}}},{"i":685,"$":{"0":{"v":"Linter","n":1},"1":{"v":"\n### Linters have two categories of rules:\n\n1. Formatting rules: eg: max-len, no-mixed-spaces-and-tabs, keyword-spacing, comma-style…\n- this is something that Prettier would handle\n\n2. Code-quality rules: eg no-unused-vars, no-extra-bind, no-implicit-globals, prefer-promise-reject-errors…\n","n":0.189}}},{"i":686,"$":{"0":{"v":"Lerna","n":1},"1":{"v":"\nLerna allows us to have multiple packages within our project. Complete with each package will be a `package.json` and a corresponding `node_modules` directory\n\nWe can list one of our packages as a dependency of another one by including the package in the `dependencies` section of the dependent's `package.json` \n\nThe root of a project with Lerna will have a `lerna.json`, as well as a `packages/` directory\n- Each sub-package of our project must be within the `packages/` directory \n\n- To run `lerna bootstrap` is to install all dependencies in sub-modules\n- When we run `bootstrap`, lerna will call `yarn` in each module, then create symlinks between the packages that refer to each other in the dependent's `node_modules`\n\t- ex. If we have 3 modules: Addition, Subtraction, and Calc (which performs the add/subtract operations), then Calc's `node_modules` will contain symlinks to the Addition and Subtraction modules. \n- if we use hoisting, then symlinks don't play a part. Instead, if we have `react` in 2 different submodules, then hoisting will allow us to remove `react` from those submodules, install it at the root level, then allow the node recursive resolver to handle resolution for us. In other words, from within the submodule, when we require `react`, it will look for the package in its nearest `node_modules`, not find it, then continue upwards until it does, which will be at the root.\n\t- The reason why hoisting works is due to the resolve algorithm of node require\n\n### Duplication\n- naturally, having multiple sub-packages in a project will result in duplicate package listings. Lerna offers hoisting, which allows us to effectively list the same package in multiple places, but have the packages install at the root level\n\t- therefore, the duplicated package (ex. React) will be in the root directory's `node_modules`, even though it is listed in the sub-package's `package.json` \n- If we have a project with 2 sub-modules: A and B, and both have React as a dependency, we can run `lerna bootstrap --hoist` at the root, which will remove React (and all of its dependencies) from `A/node_modules` and `B/node_modules`, and move them to the root level `node_modules`. Because of the recursive nature of how `node_modules` are resolved, this will cause the sub-module's package.json to look upwards in the directory hierarchy until the react binary is found. \n\t- Lerna will warn us when running this command if we have version mismatches\n\n### Commands\n- `bootstrap` - generate `node_modules` for packages.\n- `clean` - remove all `node_modules` that are not in root directory\n- `create` - add a new sub-package to your project\n- `run` - run the script that is listed in each sub-`package.json`\n\t- Therefore, it will run only npm or yarn commands (with the script that is listed in package.json)\n- `exec` - run a command inside each package\n\t- similar to `run`, but is not restricted to running scripts in `package.json` \n\n### Misc\nYou don't actually need to run lerna bootstrap if you're using yarn workspaces.\n","n":0.046}}},{"i":687,"$":{"0":{"v":"Kibana","n":1},"1":{"v":"\nKibana enables you to interactively explore, visualize, and share insights into your data and manage and monitor the stack.\n\nTo use Kibana, we need to pass an enrollment token that is provided from the ElasticSearch server\n- this enrollment token is output to the log the first time we run the ES server.\n\n## Using with Docker\n[guide](https://www.elastic.co/guide/en/kibana/current/docker.html)\n\n- You may need to mount a [[volume|docker.containers.volumes]] on the kibana container in order to access a `kibana.yml` locally\n    - give it contents:\n```yml\nserver.host: \"0.0.0.0\"\n```\n\n- if Kibana web portal is stuck \"configuring\" forever, just reload the URL and enter username/password","n":0.104}}},{"i":688,"$":{"0":{"v":"Karabiner Elements","n":0.707},"1":{"v":"\n### Complex Modifications (json)\n- found in `~/.config/karabiner/assets/complex_modifications`\n\nFrom\n- this field specifies the key combination that we want to press\n\nTo\n- this field specifies the key combination that we want the system to understand\n\n### Modifiers\nMandatory\n- this key must be pressed\n\nOptional\n- this key can be pressed and the mapping will be registered, though it is not necessary\n- ex. useful if say we have `<Shift-Ctrl-j>` mapped to \"switch workspace\", and we also don't care if we happen to also be holding down `cmd`.\n\n## Tools\n[Complex rules generator](https://genesy.github.io/karabiner-complex-rules-generator/)\n\npackage.json has a pre/post version of every script\n","n":0.107}}},{"i":689,"$":{"0":{"v":"Kafka","n":1},"1":{"v":"\n## What is it?\nKafka is a [[distributed|deploy.distributed]] [[Pub-Sub|general.patterns.messaging.pubsub]] messaging system.\n- capable of handling a high volume of data\n- messages are persisted on disk and replicated within the cluster to prevent data loss.\n\nKafka is built on top of the ZooKeeper synchronization service.\n\nTraditionally, we use only databases as sole data storage, and think of that data in terms of \"things\", along with their state.\n- Kafka encourages us to think of events first, and things second.\n\nKafka stores data in a distributed log instead of a traditional database.\n- a log is an ordered sequence of events, along with state and a description of what happened\n- Kafka is a system for managing these logs (in Kafka, a log is called a [[topic|kafka.topic]])\n\nKafka's key attributes are:\n- immutable data\n- high availability\n- high throughput\n- scalable\n\n- logs are easy to build at scale\n- Implementations of Kafka are declarative\n- Kafka can be quite complex to operate.\n    - AWS offers MSK (Managed Streaming for Kafka), making the management of a Kafka cluster trivial.\n- Kafka provides database-like durability guarantees\n\nApache Kafka is based on the commit log, and it allows users to subscribe to it and publish data to any number of systems or real-time applications. \n\n## Why use it?\nExample applications include managing passenger and driver matching at Uber\n\nData schemas are paramount in Kafka, since malformed events sent through the system can crash downstream subscribers\n- therefore, Kafka’s streams can require both backward and forward [[compatibility|api.compatability]]: old messages must be readable by new consumers; new messages must be readable by old consumers.\n    - this is why Confluent offers a schema registry tool.\n\nThere are five major APIs in Kafka:\n- *Producer API* – Permits an application to publish streams of records.\n- *Consumer API* – Permits an application to subscribe to topics and processes streams of records.\n- *Connector API* – Executes the reusable producer and consumer APIs that can link the topics to the existing applications.\n- *Streams API* – This API converts the input streams to output and produces the result.\n- *Admin API* – used to manage Kafka topics, brokers and other Kafka objects.\n\n### Topic\n![[kafka.topic]]\n\n### Kafka Connect\nImagine we have multiple databases, a legacy service, and a SaaS product, and we want a way to get the data that they produce into Kafka.\n- Kafka Connect helps us get that data into Kafka, and back out again.\n- Kafka Connect is a general term to refer to 100's of pluggable modules that handle the I/O of the whatever service we are connecting to.\n    - ex. There would be a connector to capture row-level changes in a Postgres database.\n\nConnectors are either Source or Sink Connectors, and are responsible for a some of the Task management, but not the actual data movement.\n\n### Kafka Streams\n- A Java API that performs helps us perform grouping, aggregating, filtering, enrichment (ie. table joining)\n    - in other words, it's an expression-based language-embedded approach to relational queries.\n- the API would be used from within the services\n- This is available to us out of the box as a consequence of using Kafka.\n\nKafka Streams is a client library which provides an abstraction to an underlying Kafka cluster, and allows for stream manipulation operations to be performed on the hosting client. The abstraction provided for us is load-balanced by default,\n\n#### KSQL\n- a language that allows us to to use SQL-like syntax to query data from one Topic, and output it into another Topic.\n- Solves the problem statement: imagine we want to perform some analysis on data kept in Kafka, but we don't want to stand up a separate service to consume that data.\n\n### Confluent\n- a distribution of Kafka.\n- open source, but offers a paid managed service (similar to Docker)\n\n### Kafka vs Logstash\n[Kafka is a cluster; Logstash is a single instance](https://stackoverflow.com/questions/40864312/how-logstash-is-different-than-kafka)\n\n### Kafka vs RabbitMQ\nThese are different forms of communicating. When service A calls service B to exchange information, it's similar to a phone conversation. I ask you a question, and you respond. In the meantime we're both occupied with that conversation; you have to have time to talk with me, and so do I.\n\nRabbitMQ and Kafka (and similar tools) also support two more types of communication:\n1. *Topics*: a topic is a one-way one to many conversation. Basically it's like standing on a soapbox and shouting to a megaphone. Whoever is interested can listen; I don't care if that is zero people or a hundred. I send my message and the broker (kafka/rabbit) makes sure whoever is subscribed to the topic all get the message.\n2. *Queues*: a queue is similar to sending letters to a company. I send a letter complaining about their service, and within that company someone opens and reads this letter. I'm not involved anymore after I send  the letter. While there may be a hundred workers opening letters, only one of them is going to be handling mine. Copies don't magically appear.\n\nSo both queues and topics are one-way communication that are fire and forget (asynchronous). This as opposed to TCP (and HTTP on top of that) that are two-way synchronous communication. The broker guarantees that the other side gets the message. Depending on how you configure (for example) Kafka; you can either have a single service get a message (queue example), all of the services get the same message (topic example) or something in between.\n\nWhen working with microservices you want every unit to be as decoupled as possible.\n- Let's say you have a component that handles user information (`A`) and you have some other module that need to do something every time a user updates his info (`B`). The classic way to do it would be to make `A` send some signal (usually an http request) to `B` so it knows it has work to do. But, for that, `A` needs to know a lot about `B`. And what if you now have `C` that needs to be triggered to? Or `D` that needs to process just some of the users? `A` would need to keep track of all of this, meanings they are highly coupled.\n- With a broker, `A` just publishes that there's a change, and all other components can subscribe to this event and react accordingly. This way, components can be added or removed with minimal impact on the overall structure.\n\n## Do you need Kafka?\nFrom Reddit [thread](https://www.reddit.com/r/apachekafka/comments/hyxezo/kafka_when_to_use_and_when_not_to_use/):\n\nKafka is like an event bus for distributed messages. It can't solve the intractable problem of distributed systems, but it does provide a nice framework for handling messages at scale.\n\nI would say if you don't need Kafka, then that's a perfectly good reason not to use it. You might have a monolith, you might have a series of services that all own their data within a database, and they all communicate via REST APIs. If that works, then Kafka won't really serve a purpose.\n\nYou start to need Kafka when you run into the constraints of distributed scale. If you have more events than any single worker can handle, and you've decided you can't increase the available CPU and memory any further, then you start facing distributed messaging problems.\n\nAs soon as two computers located some distance from each other try to determine what is true, and what happened first, you'll run into problems. Your next goal is to try and figure out which constraints you're willing to bend.\n\nMaybe your pipeline doesn't actually care about order, but absolutely cares that you don't drop a single record. Maybe you only care about throughput. Maybe you care about order, but not about availability. Eventually somethings gotta give. Kafka just helps you manage that infrastructure, it doesn't solve the underlying issues.\n\nIf you run an app that collects logs and metrics, order might not matter. Just send everything down the pipe and you'll aggregate later. But you probably wouldn't run your banking transaction on it.\n\n### Using Kafka in a Logging system\n\"There are plenty of open-source tools available for logging. We decided to use Graylog—an excellent tool for logging—and Apache Kafka, a messaging system to collect and digest logs from our containers. The containers send logs to Kafka, and Kafka hands them off to Graylog for indexing. We chose to make the application components send logs to Kafka themselves so that we could stream logs in an easy-to-index format. Alternatively, there are tools that retrieve logs from outside the container and forward them to a logging solution.\"\n\n## UE Resources\n- [The place to start learning Kafka](https://kafka-tutorials.confluent.io/)\n- [Kafka course from Confluence](https://developer.confluent.io/learn-kafka/apache-kafka/topics/)\n- [Getting Started with Kafkajs](https://www.confluent.io/blog/getting-started-with-kafkajs/)\n- [Kafka with Azure Functions](https://github.com/Azure/azure-functions-kafka-extension)\n- [Connect Architecture](https://medium.com/@Instaclustr/apache-kafka-connect-architecture-overview-842097d3eb96)\n- [Fine-tuning Kafka performance](https://developers.redhat.com/articles/2022/05/03/fine-tune-kafka-performance-kafka-optimization-theorem#)\n- [Kafka Whitepaper](https://notes.stephenholiday.com/Kafka.pdf)\n\n## Tools\n- [Kafka dashboard](https://github.com/cloudhut/kowl)","n":0.027}}},{"i":690,"$":{"0":{"v":"Topic","n":1},"1":{"v":"\nA topic is an ordered collection of events (ie. messages, records) stored in a durable way.\n- A topic represents a single type of event\n- Durable - written to disk, and replicated\n\nA topic can be thought of as a feed name.\n\nMessages in a topic must be first serialized so that it can be efficiently transmitted and stored in a Kafka topic.\n- when you communicate with Kafka using a native client, that native client contains serializers that know how to serialize the data (in the case of producers) and deserialize the data (in the case of consumers)\n\nA topic has a Log which is the topic’s storage on disk.\n- a log is broken up into partitions and segments\n\nCan be thought of as a *real-time stream*\n- Topics can be thought of as a database table\n  - ex. if we were doing this in Postgres, we could have tables `geofence_entry_events`, `low_tire_pressure_warning_events`\n\nKafka Services can be used as a sort of glue between microservices of an application. A microservice can consume a message from the Kafka Topic, and produce an output which gets registered to another Topic.\n- Since it can act as glue between many services, we can produce an output from these Topics that can be consumed by a new service to perform real-time analysis of that data\n  - This is contrast to the old-school method of running a batch-process overnight\n\nEvents in a topic are guaranteed to be in order (spec: by id)\n\n### Partitioned nature of Topics\nTopics are partitioned, meaning a topic is spread over a number of \"buckets\" located on different Kafka brokers.\n- When a new event is published to a topic, it is actually appended to one of the topic's partitions\n- Each partition in the Kafka cluster has a leader and a set of replicas among the brokers.\n\nKafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written.\n\nEvents with the same event key (e.g., a customerId or vehicleId) are written to the same partition\n\nPartition strategy: related events should be on the same partition","n":0.054}}},{"i":691,"$":{"0":{"v":"Stream","n":1},"1":{"v":"\nKafka Streams is a stream-processing (Java-based) API that gives us access to all of the computational primitives of stream processing (filtering, grouping, aggregating, joining etc.)\n- Using Streams negates our need to write framework code on top of our consumer API to do these things\n- Using Streams also provides direct support for the potentially large amounts of state that result from doing stream processing operations, negating our need to have to manage that state (and thereby negating our need to have to worry about fault-tolerance)\n    - this is notable, since just about anything interesting we are going to be doing in a consumer is going to be stateful (e.g. grouping events by a field with many unique values, then performing an aggregation over that group every hour).\n\nA Kafka Streams application is a consumer group, meaning it has its own scaling capabilities built in.\n\nKafka Streams is a library rather, than an infrastructure\n- that is, it's something you add to an application\n\nStreams persists the state to internal topics in the Kafka cluster, allowing us to restore state in the event of failure.\n- this helps not only in event of failure, but also when we're adding/removing stream processing nodes to our consumer group.\n\nOver time, the consumer-side of a Kafka system tends to grow in complexity (though not so much producers)\n\nhttps://kafka.apache.org/documentation/streams/","n":0.068}}},{"i":692,"$":{"0":{"v":"Schema Registry","n":0.707},"1":{"v":"\nSchema Registry provides a centralized repository for managing and validating schemas for topic messages\n- It also provides a versioning mechanism so we can update the schema as needed.\n- prior to interacting with the topic, the producer and consumer can each retrieve the latest version of the schema and validate their data against it.\n\nSchema Registry also defines the format of the serialized data and is used to validate the data as it is being deserialized by another system or application\n- That is, it is Schema Registry that knows if you serialized your Kafka messages with Avro, [[json-schema]] or Protobuf.","n":0.101}}},{"i":693,"$":{"0":{"v":"Producer","n":1},"1":{"v":"\nThe Kafka producer is conceptually much simpler than the [[kafka.consumer]], since it doesn't have to worry about [[group coordination|kafka.consumer#consumer-groups]].\n\nOnce the target partition is determined (via the **partitioner**), the producer sends a produce request to the leader of that partition.\n- therefore, all writes to the partition must go through the partition leader.\n\n### Partitioner\nWhen a producer sends a message to a Kafka topic, the partitioner is invoked to determine the target partition based on certain criteria.\n- therefore, the partitioner is responsible for determining which partition a message will be written to.\n\nThe purpose of a partitioner is to provide an even distribution of messages across the available partitions of a topic.\n\nThe out-of-the-box Kafka partitioners guarantee that all messages with the same key will be sent to the same partition.\n- of course, if the key is `null`, there is no longer a guarantee\n\nThe default partitioner (`DefaultPartitioner`) uses a round-robin strategy to distribute messages across partitions\n\nA custom partitioner can be made in order to achieve specific business rules or other requirements\n- ex. it can consider the message key, specific message attributes, or external factors such as load balancing, with the goal being to ensure that related messages are written to the same partition, or to achieve a specific partitioning strategy that aligns with your application's needs.","n":0.069}}},{"i":694,"$":{"0":{"v":"Local","n":1},"1":{"v":"\nThe easiest way to get Kafka running locally is with docker-compose.yml\n\n```yml\nversion: '3.2'\nservices:\n  zookeeper:\n    image: wurstmeister/zookeeper\n    ports:\n      - \"2181:2181\"\n  kafka:\n    image: wurstmeister/kafka:latest\n    ports:\n      - target: 9094\n        published: 9094\n        protocol: tcp\n        mode: host\n    environment:\n      HOSTNAME_COMMAND: \"docker info | grep ^Name: | cut -d' ' -f 2\"\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://_{HOSTNAME_COMMAND}:9094\n      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094\n      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n```","n":0.132}}},{"i":695,"$":{"0":{"v":"Consumer","n":1},"1":{"v":"\nA consumer is a client application that subscribes to (reads and processes) events\n- it works by issuing \"fetch\" requests to the brokers leading the partitions it wants to consume.\n\n### Offset\nThe offset is like a bookmark within a partition of a topic, representing the consumer's current position in a log of messages.\n- Each message in a Kafka topic is assigned a unique offset as it is written to the partition.\n- Each request to the brokers specifies the offset, and consumers are able to rewind it to re-consume data if desired.\n- Offsets are persistent and maintained by the Kafka broker.\n- Offsets are used to provide the \"at-least-once\" delivery guarantee in Kafka.\n- Typically, a consumer will persist the offset so that in case of failure, they can resume processing from where they left off.\n\n### Consumer Groups\nIn a large-scale system, it is unlikely that we would have a single instance of a Kafka Consumer app. If we were using Kubernetes, we would have multiple pods of this application. The question then becomes: \"how do we ensure that there is no overlap between each pod and the [[kafka.topic]] messages it consumes?\"\n\nThe purpose of consumer groups in Kafka is to enable (horizontally) scalable and parallel consumption of messages from topics.\n- Also, consumer groups provides scalability, fault tolerance, and load balancing for your Kafka consumers within the Kafka Consumer app. If a pod fails or a new pod is added, Kafka will automatically handle the partition reassignment to ensure efficient consumption and fault tolerance within the consumer group.\n\nconsumer group coordination is done based on the `group.id` configuration parameter.\n","n":0.062}}},{"i":696,"$":{"0":{"v":"Connect","n":1},"1":{"v":"\nKafka Connect is Kafka's integration API and subsystem, and answers questions like:\n- \"how do we get data from other systems into our Kafka topics?\"\n    - answer: source connectors\n- \"how do we get data from our Kafka topics into our other systems (sink)?\"\n    - answer: sink connectors\n\n![](/assets/images/2023-06-27-08-49-13.png)\n\nKafka Connect is an ecosystem of pluggable connectors\n- a connector is simply a `.JAR` file \n\nThe job of many source/sink connectors is part of the well trodden path.\n- that is, the code that moves data from a topic to an S3 bucket, from a topic to ElasticSearch, from a topic to records in a relational database is unlikely to vary from one business to the next.\n\nConnect abstracts away much of the data integration code, and allows us to write JSON config in its place.\n- ex. the following JSON is how we would stream data from Kafka into ElasticSearch\n    - by doing this, we no longer need to write the code that subscribes to a topic, gets messages, and uses the ElasticSearch API\n    - As long as someone has already written an ElasticSearch connector, we can deploy that connector to our Connect cluster and POST the JSON file to the REST endpoint of the Connect cluster. By doing this, the `.JAR` file that is deployed to the cluster becomes instantiated as a runtime connector.\n```json\n{\n \"connector.class\": \"io.confluent.connect.elasticsearch.ElasticsearchSinkConnector\",\n \"tasks.max\": \"1\",\n \"topics\": \"simple.elasticsearch.data\",\n \"name\": \"simple-elasticsearch-connector\",\n \"connection.url\": \"http://elasticsearch:9200\",\n \"type.name\": \"_doc\"\n}\n```\n\nTo a Kafka cluster, Connect looks like a producer or consumer (or both)\n\nConnect runs on hardware that is independent of the Kafka brokers themselves.\n\nConnect is designed to be scalable and fault-tolerant\n- this means we can have a cluster of Connect workers to share the load of moving data in and out of Kafka topics.\n\n### Worker\nA Connect Worker is a node in the Connect cluster. \n\nThe worker runs 1+ Connectors.\n\n## Resources\n- [Confluent hub: list of connectors](https://www.confluent.io/hub/)","n":0.057}}},{"i":697,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\nIf you see the documentation after running a command, this means your command is wrong. Scroll up to see details about the error message\n\n`--zookeeper` option is outdated and shouldn't be used\n- Since Kafka v0.10, the consumer is leveraging a Kafka connection string, not Zookeeper. This is due to how consumer offsets are stored.\n\n## `kafka-topics`\n### Create topic\n```sh\nkafka-topics --create \\\n  --bootstrap-server localhost:9092 \\\n  --replication-factor 1 \\\n  --partitions 3 \\\n  --topic my_topic\n```\n\n### List topics\n```sh\nkafka-topics --list --bootstrap-server localhost:9092\n```\n\n### Describe a topic\n```sh\nkafka-topics --bootstrap-server localhost:9092 --describe --topic my_topic\n```\n\n### Increase # of partitions of a topic\n```sh\nkafka-topics --bootstrap-server localhost:9092 --alter --topic my_topic --partitions 5\n```\n\n### Delete a topic\n```sh\n# You can specify a comma delimited list of topics to delete more than one topic at a time\nkafka-topics --bootstrap-server localhost:9092 --delete --topic my_topic\n```\n\n## `kafka-console-producer`\n### Produce message\nBy default messages sent to a Kafka topic will result in messages with `null` keys. We must use the properties parse.key and key.separator to send the key alongside messages.\n\n```sh\nkafka-console-producer --bootstrap-server localhost:9092 --topic my_topic\n# any line of text you write after the '>' will be sent to the Kafka topic\n\n# or, pass messages (newline separated) from a file\nkafka-console-producer --bootstrap-server localhost:9092 --topic my_topic < topic-input.txt\n```\n\n## `kafka-console-consumer`\n### Consume only future messages\n```sh\nkafka-console-consumer --bootstrap-server localhost:9092 --topic my_topic\n```\n\n### Consume all historical messages\n```sh\nkafka-console-consumer --bootstrap-server localhost:9092 --topic my_topic --from-beginning\n```","n":0.07}}},{"i":698,"$":{"0":{"v":"Broker","n":1},"1":{"v":"\nThe broker is the main vehicle for the movement of data in Kafka.\n- it handles *all* requests from *all* clients (both producers and consumers)\n- It also manages replication of data across a cluster, as well as within topics and partitions.\n\n![](/assets/images/2023-06-26-21-52-48.png)\n\nA Broker is a Kafka server that runs in a Kafka Cluster. Therefore, Kafka Brokers form a cluster. \n- A Kafka cluster is made up of multiple Kafka Brokers that exist on multiple servers.\n- Kafka Brokers contain topic log partitions. \n\nConnecting to one broker bootstraps a client to the entire Kafka cluster. \n- For failover, you want to start with at least three to five brokers. A Kafka cluster can have, 10, 100, or 1,000 brokers in a cluster if needed.\n\nThe default port for the broker is `9092`\n\n### Bootstrap server\nWe know that a kafka cluster can have 100s or 1000nds of brokers (kafka servers). But how do we tell clients (producers or consumers) to which to connect? Should we specify all 1000nds of kafka brokers in the configuration of clients? no, that would be troublesome and the list will be very lengthy. Instead what we can do is, take two to three brokers and consider them as bootstrap servers where a client initially connects. And then depending on alive or spacing, those brokers will point to a good kafka broker.\n- these can be listed in `bootstrap.servers`\n\n### Replication factor\nReplication factor must be equal to or less than the number of brokers you have\n\nhttps://www.educba.com/kafka-replication-factor/","n":0.064}}},{"i":699,"$":{"0":{"v":"K8s","n":1},"1":{"v":"\n## Overview\n### What is it?\nKubernetes is a distributed system for scheduling programs to run in a cluster.\n- ex. it can schedule...\n    - long-running HTTP services, like [[GraphQL servers|graphql.server]], REST servers etc.\n    - [[daemon sets|k8s.controllers.daemon-set]] that run on each node in the cluster\n    - [[cron jobs|unix.cli.cron]] that run on a schedule.\n\nKubernetes is all about abstracting how, when and where containers are run. \n- As such, we declaratively describe what we want, and don't care how it is carried out\n \nKubernetes is a [[container orchestrator|docker.orchestrators]]. It can also be thought of as a container scheduler.\n- All containers in Kubernetes are scheduled as pods\n    - We don't tell Kubernetes to run a container. Rather, we tell it to create a pod that wraps a container.\n\nKubernetes uses a [[client-server architecture|general.arch.client-server]]\n\n### Why use it?\nKubernetes is designed to be resilient to errors— it has one [[etcd cluster|k8s.node.master.components.etcd]] storing all the state, an [[API server|k8s.node.master.components.api-server]] (which is simply a REST interface to that database), and a collection of stateless [[controllers|k8s.controllers]] that coordinate all cluster management.\n\nIt’s not enough to run containers. We need to be able to scale them, to make them fault tolerant, to provide transparent communication across a cluster\n- Containers are only a low-level piece of the puzzle. The real benefits are obtained with tools that sit on top of containers. Those tools are today known as container schedulers. They are our interface. We do not manage containers, they do.\n\nKubernetes follows the ethos that we should not run our containers directly, and should instead trust Kubernetes to handle container scheduling for us.\n- The chief reasoning for this is that containers by themselves don't provide fault tolerance. They cannot be deployed easily to the optimum spot in a cluster, and are not operator friendly\n\nToday, modern infrastructure is created from immutable images. Any upgrade is performed by building new images and performing rolling updates that will replace VMs one by one. Infrastructure dependencies are never changed at runtime\n- One of the inherent benefits behind immutability is a clear division between infrastructure and deployments. Until not long ago, the two meshed together into an inseparable process. With infrastructure becoming a service, deployment processes can be clearly separated, thus allowing different teams, individuals, and expertise to take control.\n\nKubernetes offers us process isolation (via [[docker]]), and an immutable way to deploy software.\n\n- We can use it to deploy our services, to roll out new releases without downtime, and to scale (or de-scale) those services.\n- We can move a Kubernetes cluster from one hosting vendor to another without changing (almost) any of the deployment and management processes.\n\n“A Kubernetes cluster is a good example of an abstraction over compute resources: there are many hosted and self-managed implementations of it on different platforms, all of which offer a common API and common set of capabilities.”\n- That is, to a large extent, it doesn't matter if you are using AWS, Azure etc. Most of the config of K8s will be identical. Thus, Kubernetes is a good abstraction over compute as a resource.\n\nKubernetes is usually seen as a way to manage your production infrastructure. But it can be configured to manage our test environments on the fly. Imagine using Kubernetes to host a Preview environment. Every pull request can be run in an isolated test environment at the push of a button.\n- \"isolated\" here truly means isolated. the app itself (with the code changes in the PR branch) with its own dedicated instances of SQL Server, Redis, Elasticsearch, and additional services pieces. All spun up from scratch within minutes and running in a handful of containers in a dedicated namespace, just for you and anyone who’s interested in your PR.  \n    - the benefit of this really shines in a [[microservice architecture|general.arch.microservice]]. In a monorepo, you just checkout someone's branch and run the code, but how do you do that if the code depends on many external services?\n\n* * *\n\n## Terminology\n### Kubernetes Resource\nA resource is an endpoint in the [[Kubernetes API|k8s.node.master.components.api-server]] that stores a collection of API objects of a certain kind. \n- ex. the built-in pods resource contains a collection of [[Pod objects|k8s.objects.pod]].\n\n* * *\n\n## Parts of Kubernetes\n### Cluster\nA Kubernetes cluster is a set of node machines for running containerized applications.\n\nThis is composed of:\n- the [[master node|k8s.node.master]]\n- the [[worker node|k8s.node.worker]]\n\nClusters in Kubernetes are identified by their respective Certificate Authority (CA) certificates.\n\n* * *\n\n### Vanilla Kubernetes\nincludes the bare-bones:\n- control-plane (which includes etcd, the api-server, a scheduler, and the controller-manager). \n- worker nodes (with each node running a kubelet, kube-proxy, and a container runtime like Docker). \n\nThis means we'll have to add ourselves:\n- add an ingress-managed load balancer, \n- autoscaling, \n- [[CI/CD|deploy.CI-CD]]\n- and install software to handle logging, monitoring, and alerting.  \n\n### Managed Kubernetes\nKubernetes is not simple to manage installations, provisions, upgrades, SLAs and scaling.\n- This can be handled for us with a managed Kubernetes service.\n\nA managed K8s service allows for easier cluster creation, load balancing, auto scaling, auto upgrades, auto repairs, logging, monitoring, etc.\n\n\n* * *\n\n### Kubernetes vs Docker Swarm mode\nIn Kubernetes, an application can be deployed using a combination of pods, deployments, and services (or micro-services).\n\nWhereas, in Docker Swarm, applications can be deployed as services (or micro-services) in a Swarm cluster. YAML files can be used to specify multi-container. Moreover, Docker Compose can deploy the app.\n\n# E Resources\n- [Why to use Kubernetes from Day 1](https://stackoverflow.blog/2021/07/21/why-you-should-build-on-kubernetes-from-day-one/)\n- [Kubernetes Illustrated Children's Guide](https://chrisshort.net/kubernetes-illustrated-childrens-guide/)\n- [1 year using Kubernetes in production](https://techbeacon.com/devops/one-year-using-kubernetes-production-lessons-learned)\n\n# UE Resources\n- [Kubernetes for Sysadmins](https://www.youtube.com/watch?v=HlAXp0-M6SY)\n- [Stop reverse engineering applications and start monitoring from the inside](https://vimeo.com/173610242)\n- [Kubernetes the hard way](https://github.com/kelseyhightower/kubernetes-the-hard-way) \n    - \"optimized for learning, which means taking the long route to ensure you understand each task required to bootstrap a Kubernetes cluster.\"","n":0.033}}},{"i":700,"$":{"0":{"v":"Addons","n":1}}},{"i":701,"$":{"0":{"v":"Kube DNS","n":0.707},"1":{"v":"\nWhen using a service, requests to one of the exposed ports will be picked up by Kube DNS and forwarded to one of the pods.\n- ex. as a user, I can access the exposed port (ie. **27017**:34172) through any node of the cluster. Kube DNS is the component that picks up those requests and forwards them on.\n","n":0.132}}},{"i":702,"$":{"0":{"v":"Kubernetes YML","n":0.707},"1":{"v":"\nIn most cases, Kubernetes has sensible defaults, but there are things we will generally want to tweak\n\nMultiple resources can be grouped together in a single manifest, separated by a line with three dashes (`---`). \n- It’s a common pattern to have Service and Deployment together.\n\n### Strategy\n#### RollingUpdate (default)\n- creates a new ReplicaSet with zero replicas and, depending on other parameters, increases the replicas of the new one, and decreases those from the old one. This process is done one at a time, and as a new ReplicaSet is added, and old one is removed.\n- The process is finished when the replicas of the new ReplicaSet entirely replace those from the old one.\n\nCan be fine-tuned with the `maxSurge` and `maxUnavailable` fields.\n- `maxSurge` defines the maximum number of Pods that can exceed the desired number (set using replicas). It can be set to an absolute number (e.g., 2) or a percentage (e.g., 35%). The total number of Pods will never exceed the desired number (set using replicas) and the maxSurge combined. The default value is 25%.\n- `maxUnavailable` defines the maximum number of Pods that are not operational. If, for example, the number of replicas is set to 15 and this field is set to 4, the minimum number of Pods that would run at any given moment would be 11. Just as the maxSurge field, this one also defaults to 25%. If this field is not specified, there will always be at least 75% of the desired Pods.\n\n#### Recreate\n- this will kill all the existing Pods before an update\n- this resembles the processes we used in the past when the typical strategy for deploying a new release was first to stop the existing one and then put a new one in its place.\n  - This approach inevitably leads to downtime\n- The only case when this strategy is useful is when applications are not designed for two releases to coexist.\n- ask yourself the following question: Would there be an adverse effect if two different versions of my application are running in parallel? If yes, then Recreate might be a good option\n- ex. Since most databases cannot have multiple instances writing to the same data files, killing the old release before creating a new one is a good strategy when replication is absent. This strategy is more suitable for singe-replica databases\n\n### Selector\nUsed to select which pods should be included in the ReplicaSet. It does not distinguish between the Pods created by a ReplicaSet or some other process. In other words, ReplicaSets and Pods are decoupled.\n\n- If Pods that match the selector exist, ReplicaSet will do nothing. If they don’t, it will create as many Pods to match the value of the replicas field.\n- Not only that ReplicaSet creates the Pods that are missing, but it also monitors the cluster and ensures that the desired number of replicas are (almost) always running. In case there are already more running Pods with the matching selector, some will be terminated to match the number set in replicas.\n","n":0.045}}},{"i":703,"$":{"0":{"v":"Tools","n":1},"1":{"v":"\n- [switch between contexts/namespaces easily](https://github.com/ahmetb/kubectx)\n  - like nvm for K8S\n- [PolarisBest Practices for Kubernetes Workload Configuration](https://github.com/FairwindsOps/polaris)\n- [Kubernetes production best practices checklist](https://learnk8s.io/production-best-practices)\n- [Playground to learn K8s](https://labs.play-with-k8s.com/)","n":0.2}}},{"i":704,"$":{"0":{"v":"Kustomize","n":1},"1":{"v":"\n### What is it?\nKustomize is a tool that allows us to manage K8S objects via a [[YML]] configuration file.\n\nIt allows you to create an entire Kubernetes application out of individual pieces — without touching the YAML configuration files for the individual components.\n\n### What does it do?\nIt allows us to...\n- generate resources from other sources\n- set cross-cutting fields for resources\n- compose and customizing collections of resources\n\nIt does this via layering to preserve the base settings of your applications and components by overlaying declarative yaml artifacts (called *overlays* or *patches*) that selectively override default settings without actually changing the original files.\n- this approach is powerful because it allows us to combine common off-the-shelf applications ([COTS](https://kubectl.docs.kubernetes.io/references/kustomize/glossary/#off-the-shelf-configuration)) with ones that are internally created ([bespoke](https://kubectl.docs.kubernetes.io/references/kustomize/glossary/#bespoke-configuration)).\n\n### Why use it?\nBenefits of Using Kustomize:\n1. Reusability\n    - Kustomize allows you to reuse one base file across all of your environments (development, staging, production) and then overlay unique specifications for each.\n\n2. Fast Generation\n    - Since Kustomize has no templating language, you can use standard YAML to quickly declare your configurations.\n\n3. Easier to Debug\n    - YAML itself is easy to understand and debug when things go wrong. Pair that with the fact that your configurations are isolated in patches, and you’ll be able to triangulate the root cause of performance issues in no time. Simply compare performance to your base configuration and any other variations that are running.\n\nAnother benefit of utilizing patches is that they add dimensionality to your configuration settings, which can be isolated for troubleshooting misconfigurations or layered to create a framework of most-broad to most-specific configuration specifications.\n\nImagine we found a third-party [[chart|k8s.tools.helm#chart]] that we want to use as a basis for our cluster. It is 98% what we want, but requires some slight modifications to fit our use-case, so we fork the Chart, make the modifications, and apply it to our cluster. Some time later a new version of the Chart is released, so we have to fork it again and apply the same modifications.\n- this approach is of course untenable, so we use Kustomize to have our custom patches overlayed on top of the base Chart.\n- In our [[CI/CD|deploy.CI-CD]] pipeline, we can imagine that we are fetching the yaml files (the Chart) from [[helm|k8s.tools.helm]], and then our Kustomize yaml files are being patched over. These patches can be environment-specific values based on the events.\n    - ex. if branch is `master` and we are deploying to production, then Kustomize will apply the yaml files in `k8s/overlays/production`.\n\n### Bases and Overlays \n#### Bases directory\n- A base is a directory with a `kustomization.yaml`, which contains a set of resources and associated customization. \n- A base could be either a local directory or a directory from a remote repo, as long as a `kustomization.yaml` is present inside. \n- A base has no knowledge of an overlay and can be used in multiple overlays. \n- The `kustmization.yaml` file is the most important file in the base folder and it describes what resources you use.\n\n#### Overlay directory\n- The overlays folder houses environment-specific overlays. It has 3 sub-folders (one for each environment).\n    - ex. imagine we want to use different [[Service types|k8s.objects.service.types]] for each environment: ClusterIP for dev, NodePort for staging, and LoadBalance for production.\n- An overlay is a directory with a `kustomization.yaml` that refers to other kustomization directories as its bases. \n- An overlay may have multiple bases and it composes all resources from bases and may also have customization on top of them.\n\n#### Example\nThis is an example of a Kustomize configuration.\n- each environment has different HPA (Horizontal Pod Autoscaling) settings \n\n```\n├── base\n│   ├── deployment.yaml\n│   ├── hpa.yaml\n│   ├── kustomization.yaml\n│   └── service.yaml\n└── overlays\n    ├── dev\n    │   ├── hpa.yaml\n    │   └── kustomization.yaml\n    ├── production\n    │   ├── hpa.yaml\n    │   ├── kustomization.yaml\n    │   ├── rollout-replica.yaml\n    │   └── service-loadbalancer.yaml\n    └── staging\n        ├── hpa.yaml\n        ├── kustomization.yaml\n        └── service-nodeport.yaml\n```\n\n\n# Resources\n- [Hello world example](https://www.mirantis.com/blog/introduction-to-kustomize-part-1-creating-a-kubernetes-app-out-of-multiple-pieces/)\nhttps://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/\n","n":0.04}}},{"i":705,"$":{"0":{"v":"Helm","n":1},"1":{"v":"\nHelm is a package manager for Kubernetes.\n\n### Chart\nHelm uses a packaging format called *charts*, which is a collection of yaml files that describe a related set of Kubernetes resources. \n- A single chart might be used to deploy something simple, like a [[node|js.node]] [[deployment|k8s.objects.deployment]], or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.\n\n## Resources\n- [Helm charts docs](https://helm.sh/docs/topics/charts/)\n- [intro guide](https://www.programmingwithwolfgang.com/helm-getting-started)\n- [DigitalOcean intro guide](https://www.digitalocean.com/community/tutorials/an-introduction-to-helm-the-package-manager-for-kubernetes)","n":0.12}}},{"i":706,"$":{"0":{"v":"Dashboard","n":1},"1":{"v":"\n[Kubernetes dashboard](https://github.com/kubernetes/dashboard) gives us a GUI where we can perform many of the same actions that [[k8s.kubectl]] lets us.\n\nTo run it, use the manifest file:\n`kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml`\n\n- It’s a single manifest file that will create a deployment, service, and the necessary permissions for the dashboard to run. All these resources will be created in the kubernetes-dashboard namespace.","n":0.131}}},{"i":707,"$":{"0":{"v":"Resource Management","n":0.707},"1":{"v":"\n`requests` and `limits` are the mechanisms Kubernetes uses to control resources such as CPU and memory. \n- `requests` are what the container is guaranteed to get. If a container requests a resource, Kubernetes will only schedule it on a node that can give it that resource. \n- `limits`, on the other hand, make sure a container never goes above a specific value.\n  - as a general best practice, never set CPU limit\n  - also as a general best practice, always set memory limit == request\n\nThe most common resources to specify are *memory* and *CPU*, which are collectively referred to as *compute resources*\n- Memory resources are considered non-compressible resources– if a container exceeds its limit, it will get terminated.\n- CPU resources are considered compressible resources– if a container exceeds its limit, it will get throttled. This means the CPU will be artificially restricted, giving your app potentially worse performance.\n\n#### Example config:\n```yml\nresources:\n  limits:\n    memory: \"400Mi\"\n    cpu: \"1\"\n  requests:\n    memory: \"200Mi\"\n    cpu: \"500m\"\n```\n\n### Custom Resource Definition (CRD)\nCRD is somewhat of an alternative to ConfigMap: https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\n\nCRD is an extension mechanism that enables users to define custom resources and their behavior in a Kubernetes cluster. \n\nWith CRDs, we can introduce and manage our own custom resources, which may not be part of the core Kubernetes API.\n\n* * *\n\n### CPU\nCPU resources are defined in millicores or “thousandth of a core.” \n- ex. In the above example, the container process needs 500/1000 (50%) of a core and is allowed to use at most 1000/1000 (100%) of a core.\n\nBy default, the kubelet uses [CFS](https://en.wikipedia.org/wiki/Completely_Fair_Scheduler) quota to enforce pod CPU limits, and uses two configuration options:\n1. `cpu_period_us` - (100ms default) The CPU interval the scheduler uses to reset the used quota for a process.\n2. `cpu_quota_us` - The runtime for which the process can use the CPU in a given period before being reset\n  - If your application is multi-threaded, then `cpu_quota_us` is calculated across all the threads.\n    - ex. in the case of the NodeJS application, which runs 4 threads (libuv) by default for system tasks like networking, the quota per thread will be 25ms (=100/4). So, in a given `cpu_period_us` (100ms), if your application spends more than 25ms processing, it will be throttled for 75ms of that `cpu_period_us` cycle (see below)\n  \n![](/assets/images/2022-08-25-09-18-22.png)\n\nIf a request takes 100ms to complete, in the above scenario, it will take at least 3 times cpu_period_us to complete, which will end up being ~325ms. This happens because, during the throttled period, the application is essentially \"paused\" even though CPU capacity might be available.\n\n## UE Resources\n- https://home.robusta.dev/blog/stop-using-cpu-limits/\n- https://www.netdata.cloud/blog/kubernetes-throttling-doesnt-have-to-suck-let-us-help/","n":0.049}}},{"i":708,"$":{"0":{"v":"Probes","n":1},"1":{"v":"\nProbes allow us to do health checks on containers.\n\nKubernetes is designed to start up containers automatically when the original fails, but what happens if the container didn't fail, but is serving content 3x slower because of a memory leak? We need to be able to check the statuses of containers, and we can do that with Probes.\n\n## Probe Types\n### `livenessProbe`\nCan be used to confirm whether an app running in a container is in a healthy state.\n- If the probe fails, Kubernetes will kill the container and apply restart policy which defaults to Always.\n\n\nThe below probe will make a get request, and will kill the container if it fails.\n```yaml\n# podfile\n...\nlivenessProbe:\n\thttpGet:\n\t\tpath: /this/path/does/not/exist\n\t\tport: 8080\n\tinitialDelaySeconds: 5\n\ttimeoutSeconds: 2 # Defaults to 1\n\tperiodSeconds: 5 # Defaults to 10\n\tfailureThreshold: 1 # Defaults to 3\n```\n\n### `readinessProbe`\nThe readinessProbe should be used as an indication that the service is ready to serve requests.\n- e.g. our app depends on Redis, so the success of the readinessProbe should therefore depend on Redis having been initialized properly.\n\nWhen combined with [[Services|k8s.objects.service]] construct, only containers with the readinessProbe state set to Success will receive requests.\n\nThe lack of the retry mechanism in Kubernetes is mitigated with readinessProbe, which we added to the ReplicaSet.\nThe readinessProbe has the same fields as the livenessProbe\n\nthe readinessProbe is used by the iptables\n","n":0.069}}},{"i":709,"$":{"0":{"v":"Kubernetes Objects","n":0.707},"1":{"v":"\nAn object is a persistent entity in the Kubernetes system. Each object represents a part of the state of our cluster, and all of them together represent the whole desired state.\n\nObjects can describe:\n- What containerized applications are running (and on which [[nodes|k8s.node.worker]])\n- The resources available to those applications\n- The policies around how those applications behave (e.g. restart policies, upgrades, [[fault tolerance|deploy.distributed.fault-tolerance]] etc.)\n\nAn object is a \"record of intent\"— it is our way of declaring the cluster's desired state.\n- once you create the object, the Kubernetes system will constantly work to ensure that object exists.\n\nTo create, read or modify an object, we interact with the [[Kubernetes API|k8s.node.master.components.api-server]].\n- If we are using [[k8s.kubectl]], calls to the API will be made for us.\n\nAlmost every object should take this shape:\n```js\n{\n  spec: {\n    // desired state\n  },\n  status: // current state of the object, managed by the control plane\n}\n```\n\nEven though we can create services by running `kubectl expose`, we should typically follow a documented approach by using YAML files.\n\n## Fields\n### `selector` \n```yml\nselector:\n  matchLabels:\n    app: hellok8s\n```\n\nWhat we are doing in this field is telling Kubernetes that this object is managing all the pods that have a label called `app` with the value `hellok8s`. This is what links our object to pods.\n- Labels are simply key-value pairs that you define for your pods, and that’s what is used to find all the pods that a Deployment needs to look after.\n\n","n":0.066}}},{"i":710,"$":{"0":{"v":"Volume","n":1},"1":{"v":"\nA volume can be backed by a variety of implementations, including files on the host machines, AWS Elastic Block Store (EBS), and Network File System (NFS).\n","n":0.196}}},{"i":711,"$":{"0":{"v":"Kubernetes Service","n":0.707},"1":{"v":"\n## What is it?\nA Service is an abstraction which defines a logical set of [[pods|k8s.objects.pod]], and the policy by which they can be accessed.\n- this \"set\" is determined by a *selector*\n- a Service is what allows our consumer pods to not care about which provider pods they are getting data from. In other words, without Services, we'd have to know the IP addresses to connect to. With Services, we (as the consumer pod) get to just say \"connect me to any pod within that Service\".\n\nServices enable communication between pods\n- Without services, the only communication available by default is between containers within a single pod (via localhost)\n  - this is because pods by nature are ephemeral. While true that a pod could communicate to another pod by knowing its IP address, once that pod is destroyed and recreated it's given a new IP and thus no longer reachable by the previous means.\n\nServices should not be used to enable external access to an application.\n\nWhile [[Pods|k8s.objects.pod]] come and go, Services provide a stable endpoint to access them. \n- therefore, we generally never send requests directly to a pod but always through a service.\n- The link between services and pods happens through labels. Any pod that has the labels defined in the service’s selector can receive requests sent to that service.\n\nServices are based on resources.\n- ex. we create a service based on Pods through a ReplicaSet. Put another way, a service is created by exposing the ReplicaSet. This is why we specify the resource type when we are running `kubectl expose`\n\nA service is an abstract way to expose an application (running on a set of Pods) as a network service.\n- this decouples work definitions from the pods\n- can be thought of as a logical set of pods and a policy by which to access them\n  - the front end shouldn't care which backend it uses. Services allow us to achieve this decoupling\n\nA service sits in front of the [[pods|k8s.objects.pod]] and takes care of receiving requests and delivering them to all the pods that are behind it.\n\nWithout a service object,\n- we'd need to use `port-forward` to redirect traffic from our local machine to a pod. Otherwise, a user that doesn’t have access to our Kubernetes cluster is not able to access this application\n- pods can't communicate with each other, short of hard-coding their IP address (which resets whenever the pod is re-created)\n- requests only set sent to a single pod\n\nA service provides access to pods from inside the cluster ([[Kube Proxy|k8s.node.worker.components.kube-proxy]]) or from outside the cluster ([[Kube DNS|k8s.addons.kube-dns]])\n\nA service tells the rest of the Kubernetes environment (including other pods and replication controllers) what services your application provides. While pods come and go, the service IP addresses and ports remain the same.  Other applications can find your service through Kurbernetes service discovery\n\nSince pods of a single service can exist on different machines, it makes sense for us to be able to interact with the service itself, so that we can orchestrate activities between all containers that are part of a service(?)\n\nWhen a service is created, it inherits all of the labels of the resource type (eg. ReplicaSet) that the service is based on.\n- The service is not directly associated with any controller (eg. ReplicaSet), but rather it is associated with the underlying Pods via the matching labels.\n\nThe problem with services is that each application can be reached through a different port, and we cannot expect users to know the port of each service in our cluster.\n- it is a bad practice to publish fixed ports through Services, because it is likely to result in conflicts or, at the very least, create the additional burden of carefully keeping track of which port belongs to which Service.\n\n## Service Type\n[[k8s.objects.service.types]]\n\n## Service discovery\nGiven that we have a few applications running in our cluster, each backed by a Kubernetes service providing a stable endpoint that we can use to reach them, we still need a way to actually find these services. That is, if `app-a` wants to talk to `app-b` using `service-b`, how does it know where it should send requests to?\n\nServices can be accessed by hard-coding the IP address, but preferably, they can be discovered through two principal modes:\n- DNS ([[k8s.addons.kube-dns]])\n  - ex. If you create a service called `service-a`, Kubernetes will add an entry for this service in its DNS, so any pod will be able to call, for example, `http://service-a:4567`. That will be correctly resolved to the service’s IP\n  - DNS is the easier approach\n- Injected environment variables\n  - since the env variables are never updated, they aren't as reliable as the DNS approach\n\nKubernetes converts Service names into DNS's and adds them to the DNS server.\n- This is a cluster add-on that is already set up by Minikube.\n\n### Services and Env Variables\n\nEvery Pod gets environment variables for each of the active Services\n![[dendron://code/k8s.kubectl.cli#list-env-variables-in-a-pod,1:#*]]\n\nEnv provide a reference we can use to connect to a Service and, therefore to the related Pods.\n\nThrough the service IP (`kubectl describe svc <servicename>`), we can access the service externally. This IP matches the values of the environment variables `GO_DEMO_2_DB_*` and `GO_DEMO_2_DB_SERVICE_HOST`.\n\n### Service discovery breakdown\n\n1. When the api container go-demo-2 tries to connect with the go-demo-2-db Service, it looks at the nameserver configured in /etc/resolv.conf.\n   - kubelet configured the nameserver with the kube-dns Service IP (10.96.0.10) during the Pod scheduling process.\n2. The container queries the DNS server listening to port 53. go-demo-2-db DNS gets resolved to the service IP 10.0.0.19.\n   - This DNS record was added by kube-dns during the service creation process.\n3. The container uses the service IP which forwards requests through the iptables rules.\n   - They were added by kube-proxy during Service and Endpoint creation process.\n4. Since we only have one replica of the go-demo-2-db Pod, iptables forwards requests to just one endpoint.\n   - If we had multiple replicas, iptables would act as a load balancer and forward requests randomly among Endpoints of the Service\n\n![](/assets/images/2021-06-01-08-40-26.png)\n\n* * *\n\n### Motivation\n\nEach Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.\n\nThis leads to a problem: if some set of Pods (call them “backends”) provides functionality to other Pods (call them “frontends”) inside your cluster, how do the frontends find out and keep track of which IP address to connect to, so that the frontend can use the backend part of the workload?\n\n### Under the hood — Adding a service\n\n1. In running `kubectl expose rs -f rs/go-demo-2.yml`, Kubernetes client (kubectl) sent a request to the API server requesting the creation of the Service based on Pods created through the go-demo-2 ReplicaSet.\n2. Endpoint controller is watching the API server for new service events. It detected that there is a new Service object.\n3. Endpoint controller created endpoint objects with the same name as the Service, and it used Service selector to identify endpoints (in this case the IP and the port of go-demo-2 Pods).\n4. kube-proxy is watching for service and endpoint objects. It detected that there is a new Service and a new endpoint object.\n5. kube-proxy added iptables rules which capture traffic to the Service port and redirect it to endpoints. For each endpoint object, it adds iptables rule which selects a Pod.\n6. The kube-dns add-on is watching for Service. It detected that there is a new service.\n7. The kube-dns added db's record to the dns server (skydns).\n   ![](/assets/images/2021-05-31-10-06-52.png)\n   ![](/assets/images/2021-05-31-21-38-25.png)\n\n* * *\n\n### Selector\n\nA service contains selector labels which are used to establish communication with the Pods containing the matching labels. There is no relation between the service and the ReplicaSet; both reference Pods through labels.\n\nThe selector is used by the Service to know which Pods should receive requests. It works in the same way as ReplicaSet selectors. In this case, we defined that the service should forward requests to Pods with labels type set to backend and service set to go-demo. Those two labels are set in the Pods spec of the ReplicaSet.\n\n```yaml\nselector:\n  type: backend\n  service: go-demo-2\n```\n\n### Request forwarding\n\nEach Pod has a unique IP. Incoming requests will be forwarded on to Pods in a round-robin style, that is similar to load balancing.\n\n## Example\n```yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: hellok8s-svc\nspec:\n  type: NodePort\n  selector:\n    app: hellok8s\n  ports:\n  - port: 4567\n    nodePort: 30001\n```\n\nOf note here, we have:\n- `kind: Service`\n- give it a name with `name: kellok8s-svc`\n- `spec` section\n","n":0.027}}},{"i":712,"$":{"0":{"v":"Service Types","n":0.707},"1":{"v":"\n#### ClusterIP (default)\nexposes the port only inside the cluster, making it inaccessible from the outside world.\n- Therefore, use this service when we want to enable communication between pods, while preventing external access.\n- With ClusterIP, all the Pods in the cluster can access the `TargetPort` (the port of the associated Pod that will receive all the requests)\n- used when we only need to give other applications that are running inside our cluster access to our pods.\n  - ex. If we have 3 replicas of application `app-a` and another application `app-b` needs a stable endpoint to access these replicas, we could create `service-a` using the ClusterIP type.\n\n#### NodePort\nExposes ports to all the nodes\n- If the service is of type NodePort, ports will be available both within the cluster as well as from outside by sending requests to any of the nodes.\n- NodePort is an extension of ClusterIP\n  - therefore everything we can do with a ClusterIP, we can also do with a NodePort service. \n- NodePort is the port which we can use to access the Service and, therefore, the Pods from the outside world.\n  - It works by opening a port on all the worker nodes we have in our cluster, and then redirecting requests received on that port to the correct location, even if the pod we are trying to reach is physically running on a different node.\n  - in most cases the port should be randomly generated to avoid clashes\n\n![](/assets/images/2022-02-28-22-03-11.png)\n- here, external clients would be able to access either http://node1-ip:30001 or http://node2-ip:30001\n- When one of the nodes receives a request on this port, it will find our service that will then be able to decide which pod should receive the request (even if the pod is physically running in another node)\n\n#### LoadBalancer\nThe LoadBalancer type is an extension of the NodePort type, but it will try to provision a Load Balancer on the cloud provider we are running.\n- Therefore, it is only useful when combined with cloud provider’s load balancer.\n- ex. if our Kubernetes cluster is running on AWS when we create a LoadBalancer service, it would automatically create an ELB\n- If we have a cloud provider already, this is probably the easiest way to expose an application running in Kubernetes to the outside world.\n\nThe way it works is pretty similar to the other service types, but instead of having to connect to a worker node IP and port, we can send requests to this Load Balancer, and it will route them to our pods the same way.\n\n![](/assets/images/2022-02-28-22-09-50.png)\n\n#### ExternalName\nmaps a service to an external address (e.g., `kubernetes.io`)\n- This service type is a little bit different, as it is not used to provide a stable endpoint to an internal service but to an external one.\n  - ex. we have a database running at `my-db.company.com`. Since we declare this as the ExternalName, our pods can talk to this service without having to know the real address where our database is running. Now if this database changes location, the service is the only place we need to change. Pods are not affected.\n- This service has limited usage.","n":0.044}}},{"i":713,"$":{"0":{"v":"Service Accou","n":0.707},"1":{"v":"\nServiceAccount objects allow clients within the Kubernetes cluster to authenticate to the [[k8s.node.master.components.api-server]]\n- A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object.\n\nWhen Pods contact the API server, they authenticate as a particular ServiceAccount. \n- There is always at least one ServiceAccount in each namespace (If you do not specify a ServiceAccount when you create a Pod, `default` is used).","n":0.12}}},{"i":714,"$":{"0":{"v":"Kubernetes Pod","n":0.707},"1":{"v":"A Pod is a way to represent a running process in a cluster.\n- Pod refers to a pod of whales or pea pod\n\nPods live within [[worker nodes|k8s.node.worker]]\n\nPods are the smallest and simplest unit of replication in K8s \n- Pods are the building blocks of Kubernetes, just as containers are the building block of Docker.\n  - In Docker, we think in terms of processes. In Kubernetes, we think in terms of multiple processes (co-existing to perform one task)\n\nA pod is a collection of containers that share resources\n- Though realistically, we tend to only have a single container in a Pod. We might see more than 1, but it normally isn't more than 2 or 3.\n- In cases where we have more than one container in a pod, it's probably because the containers all support the primary application\n  - ex. Imagine we have a primary [[nginx]] container, and alongside we have a container whose job is to regularly pull a github repo and update the website that the nginx container is serving. These 2 containers working together can be thought of a single atomic unit.\n  ![](/assets/images/2022-02-25-16-34-02.png)\n\nA pod is designed to run multiple cooperative processes that could be seen as a single cohesive piece of work. This is the level of abstraction that we live at in Kubernetes.\n\nAll the containers in a pod run on the same machine.\n- That is, a pod cannot be split across multiple nodes\n\nA pod provides a way to set `.env` variables, mount storage, and feed other information into a container\n\nA pod encapsulates one or more containers deployed together on one host, thereby sharing the same resources (of the host)\n- ex. if we have 5 containers of a mongodb service deployed, and 3 of them were on the same host (ex. same machine), those 3 together would be called a Pod\n\nPods are not long-lived services. Even though Kubernetes is doing its best to ensure that the containers in a Pod are (almost) always up-and-running, the same cannot be said for Pods. In Kubernetes, containers are fault-tolerant, but pods are not.\n\n- If a Pod fails, gets destroyed, or gets evicted from a Node, it will not be rescheduled.\n- Similarly, if a whole node is destroyed, all the Pods on it will cease to exist.\n\nEach pod gets its own IP address, though it is unreliable, since pods are designed to be short-lived; so the creation of a new pod would result in a new IP\n\nWhen the container inside a pod exits, the pod dies too.\n\nWhen a container inside a pod fails, Kubernetes will create a new container based off the same image:\n\n```sh\n$ kubectl exec -it db pkill mongod\n$ kubectl get pods\n```\n\nproduces (note how RESTARTS is 1):\n\n```\nNAME READY STATUS  RESTARTS AGE\ndb   1/1   Running 1        13m\n```\n\nEverything in a pod is tightly coupled.\n\nThe containers in a pod are not necessarily Docker containers, though it is the most common implementation.\n\nNormally pods are not created by hand. Instead, we depend on higher level constructs like Controllers to do that for us.\n\nPods...\n- are mortal. They are born and cannot be resurrected once they die.\n- are not intended to run multiple instances of the same application,\n\nContainers within a pod...\n- share an IP address and port space, and can find each other via localhost.\n- share storage space\n\nIn a pre-container world, being executed on the same physical or virtual machine would mean being executed on the same logical host.\n\n- logical host would contain relatively tightly coupled code\n\n### How many containers in a pod?\n\nEven though a Pod can contain any number of containers, the most common use case is to use the **single-container-in-a-Pod** model\n\n- Imagine we had your express api server image and a postgres image. If we put both of these in a single pod, we would no longer be able to have different numbers of containers. For instance, we could not have 2 api containers and 1 postgres container.\n\nThere are scenarios when having multiple containers in a Pod is a good idea. However, they are very specific and, in most cases, are based on one container that acts as the main service and the rest serving as side-cars.\n\nA frequent use case is multi-container Pods used for:\n\n- Continuous integration (CI)\n- Continious Delivery (CD)\n- Continuous Deployment processes (CDP)\n\n### PodTemplate\n\nWhen we create a pod, a hash of the PodTemplate is taken and it appended to the Pod name. This means that 2 pods created from identical PodTemplates on different machines will produce the same hash.\n\n- This is also how Git SHAs work.\n\nspec: PodTemplate is in the ReplicaSet\n\n## Pod Scheduling\n\n### Major components\n\nThere are 3 major components: API Server, Scheduler, Kubelet\n\n#### API Server\n\nCentral component of the K8s cluster\n\n- runs on the master node\n  - with Minikube, both master and worker nodes are baked into the same VM. Realistically, the K8s cluster should have the two separated on different hosts.\n\nMost of the coordination in Kubernetes consists of a component writing to the API Server resource that another component is watching. The second component will then react to changes almost immediately.\n\n#### Scheduler\n\nThe scheduler is also running on the master node.\n\n- Its job is to watch for unassigned pods and assign them to a node which has available resources (CPU and memory) matching Pod requirements.\n\n#### Kubelet\n\n[[reference|k8s.node.worker.components.kubelet]]\n\n### Process of creating a pod\n\nie. when running `kubectl create -f pod/db.yml`\n\n1. kubectl (the K8s client) sends a request to the API Server, requesting the creation of a pod\n2. Since the scheduler is watching the API server for new events, it detected that there is an unassigned Pod.\n3. The scheduler decided which node to assign the Pod to and sent that information to the API server.\n4. Kubelet is also watching the API server. It detected that the Pod was assigned to the node it is running on.\n5. Kubelet sent a request to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines a single container based on the mongo image.\n6. Finally, Kubelet sent a request to the API server notifying it that the Pod was created successfully.\n\n![Pod Scheduling Sequence](/assets/images/2021-05-28-11-22-21.png)\n\n### Pod CPU resources\nPods have two resource types for CPU in K8S: requests and limits. \n- The request is a guaranteed allocation that we expect the pod to utilize. \n- The limit is the cap that we don't want our pod to EVER exceed, as your thread will receive WAIT responses and can skew time sensitive apps. \n  - Therefore, hitting your limit should be considered a bug in right sizing calculations.\n\nCPU in k8s are measured in millicores (1000m = 1 [core, or AWS EC2 vCPU])\n- https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu\n\nSidebar: Memory is also included in requests, except when you hit the limit, the pod is terminated. This is easly identified because the exist signal will exceed 128. \n- You can find the original signal code simply by subtracting 128. Example: Exit Code 137: Indicates failure as container received SIGKILL. 137-128=9, ie `kill -9 <PID>`\n\n* * *\n\n## Pod Definition File\n\n```yml\n# using v1 of the K8s Pods API\napiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    type: db\n    vendor: MongoLabs\nspec:\n  containers:\n  - name: db\n    image: mongo:3.3\n    command: [\"mongod\"] # the command that should be executed when the container starts\n    args: [\"--rest\", \"--httpinterface\"]\n\n```\n","n":0.029}}},{"i":715,"$":{"0":{"v":"Pod CLI","n":0.707},"1":{"v":"\n`kubectl run db --image mongo`\n- create a pod with a mongo database\n\t- this is similar to how we can `docker run` to create a container.\n\t- running this command will pull the image from Docker Hub\n- this is not how we normally run pods\n","n":0.152}}},{"i":716,"$":{"0":{"v":"PodDisruptionBudget","n":1},"1":{"v":"\nPodDisruptionBudget is a configuration that lets us limit the number of concurrent disruptions that our application experiences.\n- the purpose is to increase availability.\n- in other words it lets us decide \"how many instances can be down at the same time for a short period due to a voluntary disruption?\"\n  - ex. if you have 3 pods and `minAvailable: 2`, then Kubernetes can only kill 1 pod\n\nMost likely, we use PDB because we want to protect an application specified by a Kubernetes controller:\n- Deployment\n- ReplicationController\n- ReplicaSet\n- StatefulSet\n\n### `minAvailable` and `maxUnavailable`\n`minAvailable`\n- a description of the number of pods from that set that must still be available after the eviction, even in the absence of the evicted pod\n  - ex. if you set `minAvailable` to 10, then 10 Pods must always be available, even during a disruption. This means that evictions are allowed as long as they leave behind 10 or more healthy pods selected by the PodDisruptionBudget's `selector`\n\n`maxUnavailable`\n- a description of the number of pods from that set that can be unavailable after the eviction.\n  - ex. With a `maxUnavailable` of 5, evictions are allowed as long as there are at most 5 unhealthy replicas among the total number of desired replicas.","n":0.071}}},{"i":717,"$":{"0":{"v":"Namespace","n":1},"1":{"v":"\nA namespace is a virtual cluster that, as the name suggests, serves to create a distinct area within the physical cluster. It allows Kubernetes to manage multiple clusters (which could be multiple different applications (term loosely used) within the same product) in a single physical cluster\n\nNamespaces provide us a way to group and segment pods, rcs, volumes, and secrets\n- A namespace functions as a grouping mechanism inside of Kubernetes. Services, pods, replication controllers,\nand volumes can easily cooperate within a namespace, and the namespace provides a degree of isolation from\nother parts of the cluster.\n","n":0.104}}},{"i":718,"$":{"0":{"v":"Kubernetes Ingress","n":0.707},"1":{"v":"\n### What is it?\nIngress objects manage external access (e.g. web traffic via http routes) to the applications running inside a Kubernetes cluster. They are essentially a collection of rules that allow inbound connections to reach the services within a K8s cluster.\n- an ingress can be seen as an abstraction layer on top of routing.\n\nYou can think of an Ingress as something that sits in front of several services, and based on some rules that you define, it will decide which [[service|k8s.objects.service]] a given request should be sent to.\n\nIngresses act as traffic cop, routing traffic from outside the cluster into destination points within the cluster.\n- One single external ingress point can accept traffic destined to many different internal services.\n\nDon't confuse Ingress objects with Ingress controllers\n- Ingresses (a.k.a. Ingress objects) are the objects that use the controller to define an http load balancer to facilitate web traffic in and out of the cluster.\n  - therefore an ingress controller is a dependency of an ingress\n- An Ingress controller is responsible for fulfilling the Ingress, usually with a loadbalancer.\n  - e.g. `ingress-nginx`, `kubernetes-ingress`\n\n### Why do we need it?\nit may seem that the use-case of allowing external traffic to the cluster is already solved by [[services|k8s.objects.service]], but services don't make applications truly accessible, because we still need forwarding rules based on paths and domains, SSL termination and a number of other features.\n- in a more traditional setup, we’d probably use an external proxy and a load balancer. Ingress provides an API that allows us to accomplish these things, in addition to a few other features we expect from a dynamic cluster.\n- You can configure an ingress to serve as the sole entry point to your Kubernetes cluster.\n  - doing this would involve changing the service type of your services in the cluster to be of type [[ClusterIP|k8s.objects.service.types#clusterip-default,1:#*]]\n\nWe can combine an ingress with an external tool like [ExternalDNS](https://github.com/kubernetes-sigs/external-dns), which abstracts some of the DNS duties away from us.\n- in a broad sense, ExternalDNS allows us to control DNS records dynamically via K8S resources. It does this while being agnostic to the specific DNS provider (ie. server) that you are using (like [[aws.svc.route53]] or Google Cloud DNS). \n  - Therefore, it is not a DNS server itself.\n  - e.g. ExternalDNS can make Kubernetes resources discoverable via public DNS servers\n\n### How does it work?\nIn order for an Ingress resource to work we need to have an Ingress Controller running in our cluster. \n- This controller is what will decide what happens when you create a new Ingress\n    - ex. [Nginx Ingress Controller](https://www.nginx.com/products/nginx-ingress-controller)\n\nThe Ingress controller must specify a provider. Out of the box, Kubernetes supports Nginx, AWS, GCE, but we can also get more controllers, like Apache.\n\nWith Ingress, we can configure an external load balancer directly from Kubernetes\n\n### Example\n`ingress.yml`\n```yml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hello-ingress\n  annotations:\n    # We are defining this annotation to prevent nginx\n    # from redirecting requests to `https` for now\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\n  rules:\n    - host: nginx.local.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: nginx-svc\n                port:\n                  number: 1234\n          - path: /hello\n            pathType: Prefix\n            backend:\n              service:\n                name: hellok8s-svc\n                port:\n                  number: 2345\n    - host: hello.local.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: hellok8s-svc\n                port:\n                  number: 3456\n```\n\nThis gets applied with the following command\n```sh\nkubectl apply -f ingress.yml\n```\n\nWe now have an ingress listening on `localhost:80`\n- here, we are defining a single rule that says that requests sent to the path \"/\" will be sent to our nginx-svc service, using the port 1234.\n- accessing the above URL, we should see the default nginx page.\n![](/assets/images/2022-02-28-22-33-06.png)\n- naturally we can add multiple paths, otherwise it wouldn't be much different from the [[LoadBalancer service type|k8s.objects.service.types#loadbalancer,1:#*]].\n  - now, depending on the path the ingress receives, it sends the request to different services.\n\n- also, we were able to define subdomains.\n","n":0.04}}},{"i":719,"$":{"0":{"v":"HorizontalPodAutoscaler","n":1},"1":{"v":"\nA HPA controls the scale of a [[k8s.objects.deployment]] and its [[k8s.controllers.replica-set]]\n- The HPA will also instruct the workload resource to scale back down when load decreases.\n\nA HPA automatically updates a workload resource (such as a [[k8s.objects.deployment]] or StatefulSet), with the aim of automatically scaling the workload to match demand.\n\n[[Horizontal scaling|deploy.scaling]] here means that the strategy is to deploy more pods (scale out), rather than adding more computational resources like CPU and memory to existing pods.\n\n\n### How it works\nThe HPA runs within the [[Control Plane (master node)|k8s.node.master]]\n\nHPA is implemented as a control loop that runs intermittently (ie. it's not a continuous process).\n- default is every 15s\n\nOnce each period, the controller manager queries for information about how resources are being utilized, and compares it against the metric specified in the HPA spec definition.\n- the target resource is found by `scaleTargetRef`.\n- it then selects the pods based on the target's resources `.spec.selector` labels and obtains the metrics.\n\nExample:\n```yml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: $APP_NAME\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1beta1\n    kind: Deployment\n    name: $APP_NAME\n  minReplicas: 2\n  maxReplicas: 4\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 80\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: 100\n```\n","n":0.072}}},{"i":720,"$":{"0":{"v":"Deployment","n":1},"1":{"v":"\n## Overview\nA Deployment controls the deployment and running of a pod on your cluster.\n\nDeployment objects manage the creation of pods for us. \n- If we run a [[k8s.objects.pod]] directly and it fails, Kubernetes will not automatically reschedule it.\n\nAside from rescheduling our pods when they die, Deployments can also:\n- scale our applications by increasing or decreasing the number of replicas we have running\n- handle the rollout of new versions of our application, so we can go from v1 to v2 without any downtime. \n- allow us to easily rollback bad releases, as well as preventing bad releases from going through altogether \n\nKubernetes Deployments are all about: how do we swap out the old [[ReplicaSets|k8s.controllers.replica-set]] with the new ones, and not lose any downtime in the process? How do we gently replace them 1 by 1, without having to replace them all at once?\n- ex. Imagine that we want to start using the Docker image `Postgres:12.2` instead of `Postgres:11.0`. The deploy.yml file specifies that we want to have 3 ReplicaSets. The implication of this, is that if we run `kubectl set image -f deploy.yml <containername>=postgres:12.2`, we would expect the Kubernetes engine to drop one `11.0` RS, and add a `12.2`, then drop another `11.0` and add a `12.2`, and so on until the rollout is complete and we only have `12.2`.\n  - note: when we rollback (ie. undo a rollout), this process happens in reverse.\n\nImagine the systems of Kubernetes, Docker, and everything in between. If Docker were at the bottom of a physical chain, we could run containers ourselves with `docker run`. Moving up, we can create pods directly with `kubectl create pod\n\n### The Problem\nWith just [[Pods|k8s.objects.pod]], [[ReplicaSets|k8s.controllers.replica-set]] and [[Services|k8s.objects.service]], we can deploy, scale and enable communication for our application. \n- However, this is all useless if we cannot update those applications with new releases. This is the problem that Deployments solve.\n\nThe desired state of our applications is changing all the time. The most common reasons for new states are new releases. The process is relatively simple. We make a change and commit it to a code repository. We build it, and we test it. Once we’re confident that it works as expected, we deploy it to a cluster.\n- It does not matter whether that deployment is to a development, test, staging, or production environment. We need to deploy a new release to a cluster, even when that is a single-node Kubernetes running on a laptop. No matter how many environments we have, the process should always be the same or, at least, as similar as possible. We need to do all of this with zero-downtime. Deployments allow us to do that.\n\n### What is it?\nA deployment is a higher-level abstraction that controlling and scaling a set of pods\n- Behind the scenes, it uses a ReplicaSet to keep the Pods running, but it offers sophisticated logic for deploying, updating, and scaling a set of Pods within a cluster.\n\nA Deployment provides declarative updates for Pods and ReplicaSets.\n- You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate\n\nThe service in front of the deployment has an IP address, but this address only exists within the Kubernetes cluster. This means the service isn’t available to the Internet at all.\n\n### Zero-Downtime Deployment\nDeployments support rolling updates and rollbacks\n\nSimilar to [[Graphile-Migrate's|graphile-migrate]] opinion about rolling forward only (ie. adding new migrations rather than removing them), we should follow this approach to deployments in Kubernetes (although the opinion is not as strong as Graphile-Migrate's).\n- We definitely do not want to rollback (ie. `kubectl rollout undo`) in the situation where there is a database change, which would result in backend services that depend on a different version of the database.\n\nNew deployments do not destroy ReplicaSets, but rather scale them to 0. Therefore, to undo a rollout all we need to do is scale the bad ReplicaSets to 0 and scale the good ReplicaSets to our desired number.\n- This fact is reflected when we run `kubectl rollout history`. When we rollback, what we will see is that the associated command with the new revision will be the initial command that was used to create it (eg. `kubectl create --filename=deploy/go-demo-2-api.yml`). It might be natural to think that the command would be `kubectl set image`, but this is not the case.\n\n* * *\n\nJust as we are not supposed to create Pods directly but using other controllers like ReplicaSet, we are not supposed to create ReplicaSets either. Kubernetes Deployments will create them for us.\n- Therefore, if we naively observe creating Pods via the 2 procedures, there is no difference:\n  - Deployment -> ReplicaSet -> Pods\n  - ReplicaSet -> Pods\n\nHowever, the advantage becomes evident if we try to change some of its aspects.\n- ex. we might choose to upgrade MongoDB to version 3.4.\n\nThe Deployment Controller is what creates the objects that are listed in the `deploy.yml` file. It watches the API server for new events, and creates the objects (eg. ReplicaSet) in response.\n- Once the ReplicaSet is created, the ReplicaSet Controller (which also watches for API events) notices that a new ReplicaSet was created, and in response, creates the Pods that it specified.\n- From there, the scheduler takes over, which watches for unassigned pods and assigns them to nodes.\n- Following that, Kubelet (which is watching for pods being assigned to nodes) causes the containers to be run (via Docker), then sends a signal to the API server indicating the updated status of the Pods.\n\nBelow, we tell the Deployment that it is to manage all the pods that have a label called `app` with the value `nginx`\n\nExample object spec for a Kubernetes Deployment:\n```yml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2 # tells deployment to run 2 pods matching the template\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n```","n":0.032}}},{"i":721,"$":{"0":{"v":"Configmap","n":1},"1":{"v":"\nA ConfigMap is an object used to store **non-confidential** data in key-value pairs. Pods then consume ConfigMaps as environment variables, command-line arguments or as configuration files in a volume.\n\nConfigMaps get mounted on pods in a deployment\n\nThere are four different ways that you can use a ConfigMap to configure a container inside a Pod:\n- Inside a container command and args\n- Environment variables for a container\n- Add a file in read-only volume, for the application to read\n- Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap","n":0.105}}},{"i":722,"$":{"0":{"v":"Kubernetes Node","n":0.707},"1":{"v":"\nA node is a worker machine in Kubernetes, responsible for running containers and other workloads. Nodes are part of a cluster and communicate with the master to receive instructions on deploying and managing containers.\n\nThere are 2 types of Node in Kubernetes:\n- A single [[k8s.node.master]] (a.k.a Control Plane)\n- One or more [[k8s.node.worker]] (a.k.a. compute machines)\n\nCumulatively these nodes make up a *Kubernetes Cluster*.\n- At a minimum, a cluster contains a control plane and one or more compute machines (nodes).\n\n### Node Pool\nA node pool is a subset of nodes within a cluster that share the same configuration settings. \n\nThese settings include the machine type, disk type and size, and network settings. \n\nNode pools provide a way to group nodes based on specific requirements.","n":0.091}}},{"i":723,"$":{"0":{"v":"Worker Node","n":0.707},"1":{"v":"\nUsually we just call these *nodes*, and refer to the [[master node|k8s.node.master]] as the *control plane*.\n\nThe worker nodes contain the services necessary to run Pods.\n- meaning that it hosts [[pods|k8s.objects.pod]]\n\nMultiple worker nodes could either be distinct machines, or could be VMs on the same machine.\n\nThe worker nodes have the following characteristics:\n- we should never need to interact with it directly\n- should be easily replaceable\n- multiple applicatons can run on the same node\n- one application can have multiple replicas spread across many nodes\n\nA worker node runs the services necessary to support the containers that make up your cluster's workloads.\n- this includes the container runtime, and the K8s node-agent [[Kubelet|k8s.node.worker.components.kubelet]]\n\n### Components running in a node\n- [[k8s.node.worker.components.kubelet]]\n- [[k8s.node.worker.components.kube-proxy]]\n- Container runtime (the software the runs the [[docker.containers]])\n\n### Adding nodes to the API server\nThere are two main ways:\n1. The kubelet on a node self-registers to the control plane\n2. The developer manually adds a Node object","n":0.082}}},{"i":724,"$":{"0":{"v":"Components","n":1}}},{"i":725,"$":{"0":{"v":"Kubelet","n":1},"1":{"v":"\nKubelet is the Kubernetes Node Agent, and it communicates with the control plane and is responsible for starting and running container scheduled in that node.\n\nKubelet runs on each node. Its primary function is to make sure that assigned pods are running on the node.\n- It watches for any new Pod assignments for the node. If a Pod is assigned to the node Kubelet is running on, it will pull the Pod definition and use it to create containers through Docker or any other supported container engine.\n","n":0.108}}},{"i":726,"$":{"0":{"v":"Kube Proxy","n":0.707},"1":{"v":"\nkube-proxy is a network proxy that runs on each node in your cluster\n\nkube-proxy maintains network rules on nodes.\n- These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.\n\nkube-proxy watches and detects when there are new services, and new endpoint objects.\n- it also manages iptable rules which capture traffic to the Service port and redirect it to endpoints. For each endpoint object, it adds an iptables rule which selects a Pod.\n\nkube-proxy interfaces with the k8s API server and iptables\n","n":0.108}}},{"i":727,"$":{"0":{"v":"Master Node","n":0.707},"1":{"v":"\nThe Master node (or, control plane) is considered the brain of the cluster\n- we send desired state in the form of a `yml` file to the master node, and it figures out how to achieve that through the worker nodes\n    - this comparison of desired state to actual state is always happening, and the master node is constantly working to make sure the actual matches the desired.\n\nThe control plane is accessible by the internet via a URL.\n\nThe control plane is the brain of the cluster. It is responsible for maintaining the desired state of the cluster, such as which applications are running and which container images they use.\n\nNodes actually run the applications and workloads, and are spun up during the cluster creation process.\n\nThe cluster enables Kubernetes to schedule and run containers across a group of machines.\n- The heart of this whole idea is that Kubernetes containers aren’t tied to individual machines. Rather, they’re abstracted across the cluster.\n\nA Kubernetes cluster has a desired state, which defines which applications or other workloads should be running, along with which images they use, which resources should be made available for them, and other such configuration details.\n- This desired state is described in a YAML file\n\nThe control plane includes the K8s API server, storage, scheduler and core [[resource controllers|k8s.controllers]]\n- it is therefore responsible for determining what gets run on all of the cluster's nodes.\n","n":0.066}}},{"i":728,"$":{"0":{"v":"Master node Components","n":0.577},"1":{"v":"\nThe master node's components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events (for example, starting up a new pod when a deployment's replicas field is unsatisfied).\n\nMaster node components can be run on any machine in the cluster. However, for simplicity, set up scripts typically start all control plane components on the same machine.\n\n### etcd\netcd is the heart of the Kubernetes cluster.\n- it is a key value store used as Kubernetes' backing store for all cluster data.\n- [[k8s.node.master.components.etcd]]\n\n### kube-apiserver\nexposes the Kubernetes API\n- The API server is the front end for the Kubernetes control plane.\n[[k8s.node.master.components.api-server]]\n\n### kube-scheduler\nA component that watches for newly created Pods with no assigned node, and selects a node for them to run on.\n\n### kube-controller-manager\nruns controller processes.","n":0.089}}},{"i":729,"$":{"0":{"v":"etcd","n":1},"1":{"v":"\nSince Kubernetes is distributed, it needs a distributed database (running across multiple machines at a time) that is easy to store data across the cluster and watch for changes to that data. etcd is the database to handle this.\n\netcd can be thought of as the heart of the Kubernetes cluster. The API server is a stateless REST/authentication endpoint in front of etcd, and then every other component works by talking to etcd through the API server.\n\nThe name \"etcd\" originated from two ideas: \n1. the unix `/etc` folder and \"d\"istributed systems. The `/etc` folder is a place to store configuration data for a single system whereas etcd stores configuration information for large scale distributed systems. Hence, a \"d\"istributed `/etc` is \"etcd\".\n\nEtcd is not coupled to the Kubernetes platform\n\nEtcd solves the same problem as [[zookeeper]]: distributed system coordination and metadata storage.\n- The improvements etcd made over Zookeeper include:\n    - Dynamic cluster membership reconfiguration\n    - Stable read/write under high load\n    - A multi-version concurrency control data model\n    - Reliable key monitoring which never silently drop events\n    - Lease primitives decoupling connections from sessions\n    - APIs for safe distributed shared locks\n\netcd manages a lot of the tricky problems in running a distributed database – like race conditions and networking – and saves Kubernetes from worrying about it.\n\nKubernetes uses etcd as a key-value database store. It stores the configuration of the Kubernetes cluster in etcd.\n- It also stores the actual state of the system and the desired state of the system in etcd.\n- It then uses etcd’s watch functionality to monitor changes to either of these two things. If they diverge, Kubernetes makes changes to reconcile the actual state and the desired state.\n\nEverything in K8s other than etcd is stateless. If etcd isn't running, you can't make any changes to your Kubernetes cluster (though existing services will continue running!).\n- this means that if have the etcd data, we effectively have a snapshot of our entire cluster, meaning that in the event of a failure, we can simply restore it.\n\nIf any of the K8s core components (e.g. API server, controller manager, scheduler etc.) have to restart, all they need to do to continue operating seamlessly is read the relevant state in etcd.\n\nAlso...\n- Anything you might read from a kubectl get xyz command is stored in etcd.\n- Any change you make via kubectl create will cause an entry in etcd to be updated.\n- Any node crashing or process dying causes values in etcd to be changed.\n- The set of processes that make up Kubernetes use etcd to store data and notify each other of changes.\n\nIf your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for those data.","n":0.047}}},{"i":730,"$":{"0":{"v":"API Server","n":0.707},"1":{"v":"\nThe core of Kubernetes' [[control plane|k8s.node.master]] is the API server.\n\nThe API that is exposed allows end users, different parts of your cluster, and external components to communicate with one another.\n\nThrough the API, we can query and manipulate the state of [[API objects|k8s.objects]] in Kubernetes (for example: Pods, Namespaces, [[ConfigMaps|k8s.objects.configmap]], and Events).\n\nMost of the time we use [[kubectl|k8s.kubectl]] which interacts with the API server for us, but we can also interact with it directly.\n\nhttps://kubernetes.io/docs/concepts/overview/kubernetes-api/","n":0.117}}},{"i":731,"$":{"0":{"v":"Minikube","n":1},"1":{"v":"\nMinikube is a virtual machine that runs locally and has the necessary Kubernetes components deployed into it. The VM will get configured with Docker and Kubernetes via a single binary called localkube.\n\nIf we were using Docker Swarm (instead of Kubernetes), we'd be able to `docker swarm init` to create a local Docker Swarm Cluster. Minikube is a service that allows us to have this level of simplicity with Kubernetes\n\nIn Minikube, there's only 1 server that acts as both the master and the node\n\n### localkube\nProvides a single-node cluster running locally on our machine\nThe localkube library provides everything needed to run a Kubernetes cluster locally\n","n":0.099}}},{"i":732,"$":{"0":{"v":"Minikube commands","n":0.707},"1":{"v":"\nMinikube commands are almost exactly the same as those from Docker Machine which, on the other hand, are similar to those from Vagrant.\n\n`minikube start --vm-driver=virtualbox`\n- creates a new VM based on the Minikube image, and deploys the necessary Kubernetes components into it.\n- The VM will get configured with Docker and Kubernetes via a single binary called *localkube*.\n- This image includes several binaries, including Docker engine\n- after creating a new VM, we'll need to connect the docker client to the docker server with `eval $(minikube docker-env)`\n\n`minikube dashboard`\n- open a browser-based UI\n\n`minikube docker-env`\n- output the docker environment variables to console\n- these env variables are relevant for the docker engine running in the image\n\n`minikube stop`\n- stop the cluster, but preserve cluster state and data\n","n":0.091}}},{"i":733,"$":{"0":{"v":"Labels","n":1},"1":{"v":"\nLabels are key/value pairs that are attached to objects, such as pods\n- Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system\n- Labels can be used to organize and to select subsets of objects.\n\nLabels allow for efficient queries and watches and are ideal for use in UIs and CLIs\n\nLabels can be seen with `kubectl describe pod`\n\nLabels enable users to map their own organizational structures onto system objects in a loosely coupled fashion, without requiring clients to store these mappings.\n\n```\n\"metadata\": {\n  \"labels\": {\n    \"key1\" : \"value1\",\n    \"key2\" : \"value2\"\n  }\n}\n```\n\nIt is throught Labels that Pods and ReplicaSets are able to be loosely coupled.\n\nLabels are what a ReplicaSet uses to identify the pods that are in existence that satisfy the RepicaSet's requirement for pod count.\n- ex. we have a ReplicaSet that specifies 4 pods should exist. If we remove the `service` label from a pod, then ReplicaSet will no longer be aware of it, and will create a brand new pod in order to reach its quota again. At the end of the day, we will have one extra pod (5 total) in the cluster. Of course, this pod with the removed `service` label will be running freely and won't be controlled by the ReplicaSet, since the labels no longer match.\n  - Following this exercise, if we then add back the label that we removed, the ReplicaSet will be made aware of it, and will remove one of the other pods to reach its desired state of 4 pods.\n","n":0.061}}},{"i":734,"$":{"0":{"v":"Kubectl","n":1},"1":{"v":"\n`kubectl` is used to manage a cluster and applications running inside it.\n- When we send commands with `kubectl`, we are sending commands to the master node of the cluster.\n- `kubectl` is a Kubernetes client that connects with the Kubernetes API, much like how psql is a client to a postgres server\n\nMost often, we provide information to `kubectl` in the form of a `.yml` file.\n- kubectl converts the information to JSON when making the API request.\n\nKubectl also supports the management of Kubernetes [[objects|k8s.objects]] using a [[kustomization|k8s.tools.kustomize]] file\n\n### Kubeconfig files\n`kubectl` uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\n- the kubeconfig file is used to configure access to clusters\n- note: there is no single kubeconfig file; it is a generic way of referring to configuration files.\n\nBy default, `kubectl` looks for a file named config in the `$HOME/.kube` directory\n\n#### Context\nA context is essentially the combination of a user and a cluster configuration/certificates.\n- ex. referring to using ContextA could mean using User5 with Cluster2, depending on how we configure it in `~/.kube/config`\n\nA context provides us with a way to manage and switch between different clusters and user configurations.\n\nIt consists of the following components:\n- cluster\n- user: an entity that interacts with the Kubernetes API\n- namespace: a way to divide cluster resources between multiple users or projects.\n\nIt’s important to understand that a Kubernetes context only applies to the client side. The Kubernetes API server itself doesn’t recognize context the way it does other objects such as pods, deployments, or namespaces.\n\nKubernetes configuration files (typically located at `~/.kube/config` or a file specified by the `KUBECONFIG` environment variable) store context information along with other configuration details. The `kubectl` command-line tool uses this configuration file to determine which cluster, user, and namespace to interact with.\n\nContexts are useful in scenarios where you work with multiple Kubernetes clusters, such as development, testing, and production environments. They help ensure that your kubectl commands are directed to the intended cluster and namespace with the correct user credentials.","n":0.055}}},{"i":735,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n## Deploy Kustomize to Kubernetes cluster\n1. Build and push docker image to registry\n2. From `k8s/` directory, run `kubectl apply -k .`\n","n":0.218}}},{"i":736,"$":{"0":{"v":"Kubectl CLI","n":0.707},"1":{"v":"\nAll of these commands send requests to the Kubernetes API\n\n## Tips\n- For any command, you can specify which namespace to use with the `-n` flag. \n\t- ex. `kubectl get pod -n <namespace>`\n\t- By default, the `default` namespace will be used.\n- Instead of passing the identifier of the actual Kubernetes object, we can pass the yaml config file with the `-f` flag\n\t- ex. `kubectl describe -f <config>.yml`\n- run shell command in a pod - `kubectl exec -it -- <podname>`\n\t- open a bash shell in pod - `kubectl exec -it <podname> bash`\n\t- list processes - `kubectl exec <podname> -- ps aux`\n\t- list env variables - `kubectl exec <podname> env`\n- pass `-c <containername>` to specify which container to run the process in.\n\n## Read\n#### Config\n`kubectl config current-context`\n- see where kubectl is pointing\n\n`kubectl config view`\n- show (redacted) contents of `~/.kube/config`\n\n#### Describe\n- give information on a resource (aka Kubernetes object)\n`kubectl describe pod <podname>`\n\n#### Get\n`kubectl get pods`\n- get all pods of a cluster\n- pass `-o wide` or `-o yaml` for more info\n- `--show-labels` to see the labels on each pod\n\n`kubectl get namespaces`\n\n`kubectl get all --all-namespace kubectl get all --all-namespacess`\n- get all components that make up the cluster\n\n`kubectl get nodes`\n- get all nodes of a cluster\n\n`kubectl get rs`\n- get all ReplicaSets\n\n`kubectl get ep`\n- get endpoints of a cluster\n\n`kubectl get -f pod/go-demo-2.yml -o jsonpath=\"{.spec.containers[*].name}\"`\n- format output as only including the `name` property, within `spec.containers`\n\n#### Logs\n- get logs from a pod\n`kubectl logs <podname>`\n- `-f` to follow the logs in real-time\n\n#### Context\n`kubectl config get-contexts`\n- get clusters\n\n`kubectl config use-context <CLUSTER-NAME>`\n- switch cluster\n\n## Create/Update\n#### Apply\n`kubectl apply -f rs/rs.yml`\n- apply a configuration to a resource.\n- we can create resources with this command, by `apply`ing to a resource that doesn't exist.\n\t- we can also `create` with `--save-config`\n\n#### Create\n`kubectl create -f pod/db.yml`\n- create a pod based on the db.yml file\n- note: we didn't need to specify that we were creating a pod, since the resource type is already defined in the yml file (`kind` field).\n- `--record` allows us to track each change to our resources such as a Deployments.\n\n## Destroy\n#### Delete\n`kubectl delete pod <podname>`\n- delete a pod\n\n`kubectl delete namespace <namespace>`\n- delete a namespace\n\n* * *\n\n#### Expose\n- expose a resource as a new Kubernetes service\n\t- the resource can be a Deployment, another Service, a ReplicaSet, a ReplicationController, or a Pod\nex\n```sh\nkubectl expose rs go-demo-2 \\\n\t--name=go-demo-2-svc \\\n\t# this port will be exposed on every node of the cluster to the outside world,\n\t# and it will be routed to one of the Pods controlled by the ReplicaSet\n\t--target-port=28017 \\\n\t--type=NodePort # what type of service?\n```\n\n#### Label\n- We can remove a label from a pod:\n\t- `kubectl label pod/pod.yml service-` will remove the service label (the `-` at the end is the syntax for removing labels)\n\n#### Rollout\n- allows us to manage the rollout of a resource\n`kubectl rollout status -w -f deploy/go-demo-2-api.yml`\n\n`kubectl rollout history -f deploy/go-demo-2-api.yml`\n- This lets us see how many revisions of the software there have been, and which command created each revision.\n`kubectl rollout undo -f deploy/go-demo-2-api.yml`\n`kubectl rollout undo -f deploy/go-demo-2-api.yml --to-revision=2`\n\n#### Set\n- set configuration properties for resources\n\t- ex. update docker image of a pod template, update env variables of a pod template etc.\n- note: the output of this command only indicates that the definition (ex. of the image used in the Deployment) was successfully updated. This means that if we used an image that doesn't exist, we would still get a \"success\" output from this command.","n":0.042}}},{"i":737,"$":{"0":{"v":"Errors","n":1},"1":{"v":"\n> deployment \"DEPLOYMENT_NAME\" exceeded its progress deadline\n\n-  this error can show for just about any reason that would prevent the k8s cluster from running and/or obtaining a given image (e.g. being unable to fetch it from the container registry, readiness probe failures).\n","n":0.154}}},{"i":738,"$":{"0":{"v":"Kubernetes Controller","n":0.707},"1":{"v":"\nA controller is a [control loop](https://kubernetes.io/docs/concepts/architecture/controller/) that watches the (shared) state of the cluster through the [[API server|k8s.node.master.components.api-server]], and makes changes so that the actual state matches the desired state.\n\nA controller watches the API server for new events. When it detects that there is a new object (eg. a new ReplicaSet), it acts in accordance with the type of controller it is.\n- ex. In the case of a ReplicaSet, the controller would create pods equal to the number found in the replica-set yaml file\n\nControllers themselves are stateless.\n","n":0.107}}},{"i":739,"$":{"0":{"v":"Replication Controller","n":0.707},"1":{"v":"\nThrough the replication controller, Kubernetes will manage your pods’ lifecycle, including scaling up and down, rolling deployments, and monitoring\n![](/assets/images/2021-05-28-11-33-47.png)\n\nWe use replication so that our cluster doesn't die if we lose a node.\n\nSetting up replication isn't a set-and-forget operation.","n":0.162}}},{"i":740,"$":{"0":{"v":"ReplicaSet","n":1},"1":{"v":"A ReplicaSet ensures that a set of identically configured Pods are running at the desired replica count. If a Pod drops off, the ReplicaSet brings a new one online as a replacement.\n\nReplicaSets are considered a low-level type in Kubernetes\n\n- Higher level abstractions can be used instead, like [[deployments|k8s.objects.deployment]] and [[daemon sets|kubernetes.daemon-set]]\n\nIf we run pods without a controller, than a failing pod will remain dead (and will not be restarted by Kubernetes). Pods run in this manner have no fault-tolerance.\n\n- ReplicaSet serves as a self-healing mechanism\n- Pods associated with a ReplicaSet are guaranteed to run. They provide [[fault tolerance|deploy.distributed.fault-tolerance]] and high availability.\n\nA ReplicaSet operates at the cluster level, and therefore has control over all pods in the cluster\n\nReplicaSets are rarely used independently. You will almost never create a ReplicaSet directly just as you’re not going to create Pods.\n\n- Instead, we tend to create ReplicaSets through [[Deployments|k8s.objects.deployment]]. In other words, we use ReplicaSets to create and control Pods, and Deployments to create ReplicaSets (and a few other things).\n\nReplicaSet `MatchLabels` are used to identify pods and must be the same as the pod labels\n\nSince ReplicaSets and Pods are loosely coupled objects with matching labels, we can remove one without deleting the other.\n\n- We can, however, pass `--cascade=true` if we want to delete pods that are associated with a RS\n\nReplicaSet uses labels to decide whether the desired number of Pods is already running in the cluster\n\n- this fact means that if we are to delete a RS and then re-create it, it will use the same pods that were running in the cluster. This shows how a RS knows what the pods it defines look like, and if there is a pod (or pods) already in existence that satisfy that definition, then it will simply consider them as part of its pod # count.\n\n## Process of Creating a ReplicaSet\n\n1. Kubernetes client (kubectl) sent a request to the API server requesting the creation of a ReplicaSet defined in the rs yaml file.\n2. The controller is watching the API server for new events, and it detected that there is a new ReplicaSet object.\n3. The controller creates two new pod definitions because we have configured replica value as 2 in the rs yaml file.\n4. Since the scheduler is watching the API server for new events, it detected that there are two unassigned Pods.\n5. The scheduler decided which node to assign the Pod and sent that information to the API server.\n6. Kubelet is also watching the API server. It detected that the two Pods were assigned to the node it is running on.\n7. Kubelet sent requests to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines two containers based on the mongo and api image. So in total four containers are created.\n8. Finally, Kubelet sent a request to the API server notifying it that the Pods were created successfully.\n\n![](/assets/images/2021-05-30-18-19-36.png)\n\n","n":0.046}}},{"i":741,"$":{"0":{"v":"Node controller","n":0.707},"1":{"v":"\nThe node controller is responsible for evicting [[pods|k8s.objects.pod]] and moving them to another node if a node becomes unresponsive.","n":0.229}}},{"i":742,"$":{"0":{"v":"Endpoint Controller","n":0.707},"1":{"v":"\nThe endpoint controller watches the API server for new service events. When it detects that there is a new [[service|k8s.objects.service]] object, it creates endpoint objects with the same name as the Service, and it uses Service selector to identify endpoints (eg. the IP and port of the pods)\n\nThe endpoint controller exists in the Kubernetes [[master node|k8s.node.master]], and interfaces with the k8s API server\n","n":0.126}}},{"i":743,"$":{"0":{"v":"DaemonSet","n":1},"1":{"v":"\nA DaemonSet is a type of controller that ensures that a copy of a specific Pod is running on each node in the cluster. \n- As a cluster grows and shrinks, the DaemonSet spreads these specially labeled Pods across all of the nodes.\n\nThe primary purpose of a DaemonSet is to deploy and manage background services, daemons, or other system-level processes that should run on every node in the cluster.\n\nDaemonSets have many uses\n- One common pattern is to use a DaemonSet to install/configure software on each host node in the cluster.\n  - ex. we can use a DaemonSet to install a [[Datadog Agent|datadog#datadog-agent,1]] in our cluster\n\nCommon use cases for DaemonSets include:\n- Monitoring Agents: Deploying monitoring agents on every node to collect metrics and logs.\n- Storage Daemons: Running storage-related daemons (e.g., distributed storage daemons) on each node.\n- Network Daemons: Deploying network-related services (e.g., proxies or VPN agents) on every node.\n\n### Example\nIn this example, the DaemonSet ensures that one Pod with the specified template runs on each node in the cluster, labeled with app: example. The DaemonSet automatically adjusts as nodes are added or removed from the cluster.\n\n```yml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: example-daemonset\nspec:\n  selector:\n    matchLabels:\n      app: example\n  template:\n    metadata:\n      labels:\n        app: example\n    spec:\n      containers:\n      - name: example-container\n        image: example-image:latest\n```","n":0.07}}},{"i":744,"$":{"0":{"v":"Components","n":1}}},{"i":745,"$":{"0":{"v":"CRD (Custom Resource Definition)","n":0.5},"1":{"v":"\nA Custom Resource Definition (CRD) defines a new resource type, and tells Kubernetes about it\n- Once a new resource type is added, new instances of that resource may be created.\n\nHandling CRD changes is up to us as the developer, though a common pattern is to create a custom controller that watches for new CRD instances, and responds accordingly.\n","n":0.131}}},{"i":746,"$":{"0":{"v":"K6","n":1},"1":{"v":"\nK6 is an open-source framework for implementing [[load testing|testing.method.load]]\n\nK6 tests work well when run within a [[CI-CD|deploy.CI-CD]] pipeline or on a schedule.\n\nK6 does not use the [[js.v8]] engine and is not [[nodejs|js.node]] based.\n- therefore we can't import node modules into our K6 script\n\n## K6 Script\n### Making scripts\nK6 scripts can be made in several ways:\n- *K6 test builder* - In the K6 UI, create the test with an endpoint\n- *K6 recorder* - with a browser extension, simply record the actions by clicking through your application's UI.\n- *converter* - generate K6 scripts from HAR files, Postman collections, [[OpenAPI|open-api]] specifications\n- *manual* - write the script (with Javascript) by hand\n  - tip: use VSCode extension\n\n### Components\n- The default export of the script is the entrypoint for the virtual users (VUs), similar to `main()` of other programming languages.\n  - code inside this function runs over and over as long as the load test is running, while the other functions are *init* functions that only run once per VU.\n\n## Gotchas\n- all variables must be prefixed with `K6_` (can be added in the CI-CD tool UI, like Gitlab)\n- `.js` files work best\n\n## Resources\n- [K6 test lifecycle](https://k6.io/docs/using-k6/test-life-cycle/)\n- [K6 API (including built-in modules)](https://k6.io/docs/javascript-api/)","n":0.072}}},{"i":747,"$":{"0":{"v":"Jupyter","n":1},"1":{"v":"\n### Jupyter kernel\nA Jupyter kernel is a programming language-specific engine that executes the code contained in a Jupyter notebook.\n\nThe kernel is what communicates between the Jupyter notebook and the Python interpreter.\n- therefore, each kernel is tied to a specific python interpreter (e.g. the interpreter installed at `/usr/local/bin/python3.11`).\n\nWithin Python itself, you can have different kernels for different conda environments.\n\n### Jupyter Server\nIf we have a PC with high performing GPU then we can run the Jupyter Server from that computer, and connect remotely to it from our work computer.","n":0.107}}},{"i":748,"$":{"0":{"v":"Cmds","n":1},"1":{"v":"\n### Next/Prev cell\n- (vscode) ctrl+m, ctrl+n/p\n- (colab) ctrl+m, n/p","n":0.333}}},{"i":749,"$":{"0":{"v":"JSON","n":1},"1":{"v":"\nJSON is an injection-safe data format, since it does not allow referencing external entities.\n- contrast this with XML, which does allow referencing external entities.\n\nIn order for a Javascript object to be serializable as JSON, it must be sanitized of the following values:\n- `undefined`\n- nested objects\n- non-UTF-8 values\n\n### JSON Pointer\nA JSON Pointer describes a slash-separated path to traverse the keys in the objects in the document. Therefore, `/properties/street_address` means:\n\n1. find the value of the key `properties`\n2. within that object, find the value of the key `street_address`\n\nThe URI `https://example.com/schemas/address#/properties/street_address` identifies the highlighted subschema in the following schema.\n\n```json\n{\n  \"$id\": \"https://example.com/schemas/address\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"street_address\":\n      { \"type\": \"string\" }\n  }\n}\n```","n":0.097}}},{"i":750,"$":{"0":{"v":"JSON Patch","n":0.707},"1":{"v":"\nJSON Patch is a format for describing changes to a JSON file\n- think of it like a patchfile in Git: instead of sending over the entire repo with a single change, we send a diff that describes the changes that we've made.\n\nJSON Patch can be used in combination with [[HTTP Patch|protocol.http.methods#patch,1:#*]] methods to perform partial updates\n\nA JSON Patch document is just a JSON file containing an array of patch operations. \n- The patch operations supported by JSON Patch are `add`, `remove`, `replace`, `move`, `copy` and `test`. \n- The operations are applied in order: if any of them fail then the whole patch operation should abort.\n\nThe original document\n```json\n{\n  \"baz\": \"qux\",\n  \"foo\": \"bar\"\n}\n```\n\nThe patch\n```json\n[\n  { \"op\": \"replace\", \"path\": \"/baz\", \"value\": \"boo\" },\n  { \"op\": \"add\", \"path\": \"/hello\", \"value\": [\"world\"] },\n  { \"op\": \"remove\", \"path\": \"/foo\" }\n]\n```\n\nThe result\n```json\n{\n  \"baz\": \"boo\",\n  \"hello\": [\"world\"]\n}\n```","n":0.085}}},{"i":751,"$":{"0":{"v":"JSON Schema","n":0.707},"1":{"v":"\nJSON Schema allows us to validate our JSON documents (including through automated tests). We can create a `json` file for each domain object (ie. what we would find in each table of an [[SQL|sql]] db), and we specify things like...\n- what properties this object has\n- what types they are\n- which ones are required\n- what restraints they have (e.g. max length, regex match etc.)\n\nThe first key-value pair of the schema should be declaring which dialect of JSON Schema that our schema uses:\n- this is called a *meta schema*, since it is a schema that describes another schema.\n```json\n{\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\"\n}\n```\n\n## Code reuse\nLike any other code, schemas are easier to maintain if they can be broken down into logical units that reference each other as necessary. \n- In order to reference a schema, we need a way to identify a schema. Schema documents are identified by non-relative URIs.\n\nSchemas need an identifier (ie. `$id`) if we want to reference them from other schemas.\n- schemas without an identifier are known as *anonymous schemas*.\n\n### Methods\n#### JSON Pointer:\n![[json#json-pointer,1]]\n\n#### $ref\nThe value of `$ref` is a URI-reference that is resolved against the schema’s Base URI (ie. the value of `$id`).\n\n## Tools\n### `json-schema-to-typescript`\nWe can use [this library](https://github.com/bcherny/json-schema-to-typescript) to generate types from the json schema.\n\n### Ajv\nWe can use `ajv` to take in our json schema, and generate validation functions to make sure the data coming in adheres to the schema.\n\n## Resources\n- [Online JSON Schema Validator](https://www.jsonschemavalidator.net/)","n":0.065}}},{"i":752,"$":{"0":{"v":"Javascript","n":1},"1":{"v":"\n`__proto` is the connection between object instances and the prototypes that they \"inherit\" from\n\n* * *\n\n## Modules\nThe general pattern of a module is a function that defines private variables and functions; creates privileged functions which, through closure, will have access to the private variables and functions; and that returns the privileged functions or stores them in an accessible place.\n\n# UE Resources\n- [State of JS](https://2021.stateofjs.com/en-us/)\n- [Dan Abramov JS Fundamentals course: highly recommended](https://justjavascript.com/)","n":0.119}}},{"i":753,"$":{"0":{"v":"V8 Engine","n":0.707},"1":{"v":"\nSince Javascript is an interpreted language, it needs an engine to interpret and execute code.\n- The V8 engine interprets JavaScript and compiles it down to native machine code.\n\nV8's compiler is lazy— it won't convert a function's code until that function is called.\n\n### Pipeline\nImmediately prior to execution, Javascript gets compiled to native machine code.\n1. Right before execution, an [[general.lang.AST]] is generated from the Javascript with a parser.\n2. Ignition (the interpreter) generates Bytecode from this AST.\n    - bytecode is an abstraction on top of machine code. As a result, it uses the same computational model as the [[hardware.cpu]]\n3. TurboFan (the optimizing compiler) takes the Bytecode and generates optimized machine code.\n    - known as *just-in-time compilation*, and this is where Javascript engines get their speed edge from.\n\n![](/assets/images/2022-04-20-14-57-08.png)\n\n\nThe V8 engine itself it written in C++\n\nSince JavaScript is single-threaded V8 also uses a single process per JavaScript context\n- therefore if you use service workers it will spawn a new V8 process per worker.\n\n### Bytecode\nV8 bytecodes are small building blocks that implement Javascript functionality when composed together.\n- bytecode is about 25%-50% the size the average baseline machine code.\n\nEach bytecode specifies its inputs and outputs as [[register|hardware.cpu.register]] operands.\n\nBytecode works at the level of abstraction where the code is determining how to store and retrieve values stored in memory register.\n\nBytecode uses 2 type of register:\n1. accumulator register\n  - the `return` keyword of a function basically says \"get me the value in the accumulator register\"\n2. normal register (e.g. `r0`, `r1` ...)\n\nWe can see the bytecode generated with\n```\nnode --print-bytecode myFile.js\n```\n\n#### Example syntax\n- Load (`Ld`) small integer (`Smi`) 42 (`[42]`) into the accumulator (`a`) register\n```\nLdaSmi [42]\n```\n\n## Memory structure of V8 Engine\n![](/assets/images/2022-04-19-14-57-02.png)\n\nResident Set - A running program has some allocated memory in the V8 process, called the *resident set*.\n\n### Heap memory\nThis is where objects or dynamic data is stored\n\nOnly the *new space* and *old space* is managed by garbage collection.\n\nThe heap is divided into:\n- New space\n- Old space\n- Large object space\n- Code space\n- Cell space, property cell space, and map space\n\n#### New space\n- New space is made of two parts, *to* and *from*.\n- New space is where new objects live. \n- These objects are short-lived.\n- objects stored here are quickly garbage collected\n- The size of this space can be controlled using the V8 flags `--min_semi_space_size` (initial) and `--max_semi_space_size` (max).\n\n#### Old space\n- objects that survived two rounds of garbage collection are moved to the old space.\n- Old space contains 2 parts:\n  - old pointer space - Contains survived objects that have pointers to other objects.\n  - old data space - Contains objects that just contain data (e.g. strings, numbers, arrays)\n- The size of this space can be controlled using the V8 flags `--initial_old_space_size` (initial) and `--max_old_space_size` (max).\n\n### Stack memory\nthere is one stack per V8 process\n\nThis is where static data including method/function frames, primitive values, and pointers to objects are stored.\n\nThe stack memory limit can be set using the `--stack_size` V8 flag.\n\nThe Stack is automatically managed and is done so by the operating system rather than V8 itself.\n\n## References\nhttps://deepu.tech/memory-management-in-v8/","n":0.045}}},{"i":754,"$":{"0":{"v":"Node.js","n":1},"1":{"v":"\n## How Node works\nNode.js is a JavaScript runtime environment that processes incoming requests in a loop, called the event loop.\nNode runs as a single process. Other languages may handle \"concurrency\" by having multiple threads handle tasks. However, Node achieves this by having async. When an async action happens, the javascript code will not get blocked, and will continue to execute until the response has come. This makes node a very event-based language (hence its usefulness in the web word, which is fundamentally made up of requests). \n- concurrency here is used liberally, since Node does not run its code concurrently at all. Alas, this is how Node solves the problem, which may be solved using multiple threads (concurrency) at a time. \n\nNode is built on top of [[js.v8]], the engine that converts javascript to native machine code. These core javascript features form the basis of what Node is. Because Javascript is missing native features that server languages typically have, there comes a need of having to fulfill those tasks. Web server functions (such as I/O, networking, streams etc.) are fulfilled by having isolated modules that each have a single responsibility. Node.js is made up of many of these modules, all built on top of the base language of Javascript and its underlying v8 engine.\n- ques:does this mean that all the language features of javascript are also available in node? Is there a global object? what about all of the javascript features that are considered useless, like exec? \n\nNode.js connects the ease of a scripting language (JavaScript) with the power of Unix network programming\n\nNode.js uses an event loop for scalability, rather than processes or threads\n\n## V8\nWhen it was first introduced, javascript was a simple scripting language, and processed commands in real time. This had performance implications, and made it very slow. This was the inspiration for V8, which did just in time compiling, making it a much faster and more capable language.\n\nWith Node, the idea was \"what if we took the V8 engine, and instead of using it in the context of the web, we allowed it to run terminal applications\"\n\n## Command line debugging\nWe can enable debug logs for a specific package/service by prefixing the start command:\n- ex. `DEBUG=ioredis:* npm run start`\n\n## Resources\n### UE Resources\n- [Tutorial: creating a native node module with C++](https://medium.com/@marcinbaraniecki/extending-node-js-with-native-c-modules-63294a91ce4)\n- [Tutorial: creating streams in node](https://www.npmjs.com/package/stream-handbook)\n","n":0.051}}},{"i":755,"$":{"0":{"v":"Types","n":1}}},{"i":756,"$":{"0":{"v":"Stream","n":1},"1":{"v":"\nHTTP requests and responses are streams in Node\n","n":0.354}}},{"i":757,"$":{"0":{"v":"Event Emitter","n":0.707},"1":{"v":"\n# UE Resources\n[Event-emitters](https://www.digitalocean.com/community/tutorials/using-event-emitters-in-node-js)\n","n":0.577}}},{"i":758,"$":{"0":{"v":"Buffer","n":1},"1":{"v":"\nA Buffer is an in-memory collection of raw bytes.\n- The `Buffer` class lets us access these spaces of memory, thereby allowing us to work with binary data.\n\na `Buffer` is similar to an array of integers, but corresponds to a raw memory allocation outside the V8 heap.\n- Unlike arrays, you cannot change the size of a buffer once it is created.\n\nBecause computers store data in bytes, a simple way to think of a Buffer is to think of it as an array of bytes:\n```js\nconst buffer = [11111111, 11011001, 10010001]\n```\n\nBuffers are useful when you’re interacting with binary data, usually at lower networking levels.\n\nThe `Buffer` class can set its encoding so we can put a `String` into it. This enables it to translate from `String` of UTF16 characters into an array of bytes. Once you have a `String` in byte form you can use it in computing communication.\n- In Javascript, a `String` is a collection of characters in UTF16 encoding.\n  - Under UTF16 encoding a single character may consist of multiple bytes (at least one, usually not more than four)\n\nWhen we read from a file with `fs.readFile()`, the data returned to the Promise (or callback) is a `buffer` object.\nWhen we make HTTP requests in Node, they return data streams that are temporarily stored in an internal buffer when the client cannot process the stream all at once.\n\n#### Encoding\nNode supports the following encodings: ASCII, UTF-8, UTF-16, USC-2, Base64, Hexadecimal, binary (ISO/IEC 8859-1)\n","n":0.065}}},{"i":759,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Base64 encode JSON\n```js\nconst valueEncoded = Buffer.from(JSON.stringify(value), 'base64')\n```\n\n### Creating a Buffer object\nThere are 2 ways to create a buffer object:\n1. Create a new buffer object, allocating the size during instantiation\n2. Extract a buffer from existing data\n\n#### Create new buffer\nIf you are going to store data in memory that you have yet to receive, you’ll want to create a new buffer with `Buffer.alloc(1024)`\n\nWhen creating a new buffer with `alloc()`, it is filled by default with binary zeroes to serve as a placeholder.\n\n#### Extract a buffer\nTo create a buffer from pre-existing data, we use `Buffer.from()`\nwe can create buffers from:\n- array of integers (between 0 and 255)\n- an `ArrayBuffer` object, which stored a fixed length of bytes\n- a string\n- another Buffer\n\n### Reading from a Buffer object\nWe can access an individual byte in a buffer or we can extract the entire contents.\n- To access one byte of a buffer, we pass the index or location of the byte we want. Buffers store data sequentially like arrays. They also index their data like arrays, starting at 0. We can use array notation on the buffer object to get an individual byte.\n\nThe buffer object comes with the `toString()` and the `toJSON()` methods, which return the entire contents of a buffer in two different formats.\n\n### Modify a Buffer\nYou can either modify buffer bytes individually using the array syntax, or write new contents to the buffer, replacing the existing data.\n\n# E Resources\nhttps://www.digitalocean.com/community/tutorials/using-buffers-in-node-js\n","n":0.065}}},{"i":760,"$":{"0":{"v":"Serial Port","n":0.707},"1":{"v":"\nSerial Port is a Node library that allows us to connect and communicate with an external device via the device's serial ports\n\n### Raspberry Pi + Roomba/Arduino\nRoomba has a serial port, meaning we can connect it with a Raspberry Pi via the Pi's GPIO pins\n\nOn the Pi, we would install Node SerialPort, and run `npx @serialport/list` to scan the devices that are connected via the GPIO pins.\n- From there, we open a serial port, we open a REPL, and we define a new port, which uses one of the connected devices and specifies some options\n```\n$ npx @serialport/terminal -p /dev/tty.usbmodem14301                                                                    \nOpening serial port: /dev/tty.usbmodem14301 echo: true\n```\n```\n$ npx @serialport/repl                                                                                                  \nport = SerialPort(\"/dev/tty.usbmodem14301\", { autoOpen: false })\nglobals { SerialPort, portName, port }\n```\n","n":0.092}}},{"i":761,"$":{"0":{"v":"package.json","n":1},"1":{"v":"\n## Scripts\nEvery script we define has `pre` and `post` version to it\n- ex. if we define a `install` script, then there is a `preinstall` that is implicitly defined\n\n## Dependencies\n- `^2.2.3` means `>=2.2.3` AND `<3.X`\n\n### Peer Dependencies\npeer dependencies are not automatically installed like dependencies and devDependencies are.\n- Instead, when we list a package as a peerDependency, we expect it to be provided from the host (the host would be our application)\n\npeer dependencies are most likely to be used only when creating npm packages.\n- ex. `react-dom` lists `react` as a peerDependency, since `react-dom` is useless unless we have `react` installed.\n\t- if `react-dom` had listed `react` as a dependency, then effectively we would have 2 different versions of react in our project.\n\t- by listing `react` as a peerDependency, `react-dom` is basically saying \"hey npm, I don't have react as a dependency, but I do need react to work. If someone tries to install me, but they don't have react installed, spit a warning at them, cause I ain't gonna work properly\"\n\n[more info](https://flaviocopes.com/npm-peer-dependencies/)\n\n### Alias package names\nWe can alias our package names however we want to control how we import them in our code:\n```json\n\"pouchdb-core-react-native\": \"npm:@craftzdog/pouchdb-core-react-native@7.2.2\",\n```\n\nHere, we are using the npm protocol to specify which package we'd like to install, and we set it to `pouchdb-core-react-native`.\n- this can be helpful if we are trying to get DefinitelyTyped [[declaration files|ts.declaration-file]] to work with a package where the name differs from the DefinitelyTyped version (which would occur, for example, if we were using a fork of the library, which we are in the above example)\n\n## Resolutions\n`resolutions` is simply a map of package names and the exact versions of those packages that should be kept in the dependency tree\n\n- ex. this configuration will remove all versions of webpack that are not 5.6.0. As long you install webpack@5.6.0 version as a dependency of the project you are working with, this will guarantee that all packages load the same version of webpack.\n```json\n{\n\t\"resolutions\": {\n\t\t\"webpack\": \"5.6.0\"\n\t}\n}\n```\n\n* * *\n\nThe scripts section of package.json has access to the commands provided by node_modules. for instance, we can make a script that calls `jest`. We can't do this in our shell because it doesn't have access to the package by default. we'd have to manually go into node_modules to do that. But the commands are loaded into the shell that is openeed when we run scripts in package.json\n","n":0.051}}},{"i":762,"$":{"0":{"v":"Modules","n":1},"1":{"v":"\nin `node_modules`, we have a folder `.bin/`. This is whe\n- normally these binaries are symlinks to the binaries that are stored within each package's directory in `node_modules`\n- when are within a project and run a command that is not globally installed, it looks in the `.bin/` directory for that executable\n\t- ex. running `jest test` in the project will look for an executable called `jest` within `node_modules/.bin` and execute it.\n\n### How Node resolves modules\nWhen we `import myFunction from 'moduleB'`, the `moduleB` module will be searched for in a `node_modules` directory at the current level. It will search:\n1. `./node_modules/moduleB.js`\n2. `./node_modules/moduleB/package.json` (if it specifies a \"main\" property)\n3. `./node_modules/moduleB/index.js`\n\nif it doesn't find the module here, it will go up one level and continue the search in the same pattern\n1. `../node_modules/moduleB.js`\n2. `../node_modules/moduleB/package.json` (if it specifies a \"main\" property)\n3. `../node_modules/moduleB/index.js`\n\nAnd so on, until the module is either found or there are no more levels to search.\n\nthe [node module resolution algorithm](https://nodejs.org/api/modules.html#modules_loading_from_node_modules_folders) is recursive, meaning when looking for package A, it looks in local `node_modules/A`, then `../node_modules/A`, then `../../node_modules/A`, and so on.\n- unless we use strict versioning (ie. omitting `^` in package.json.dependencies) the version listed in package.json is not the version our package uses. Rather, it defines a range that is allowed to be installed. To see the actual version, check yarn.lock\n\n\n\n### Package.json\n- [Adding comments to package.json (of course, a workaround; not actual comments)](https://stackoverflow.com/questions/14221579/how-do-i-add-comments-to-package-json-for-npm-install)\n\n## UE Resources\n- [Understanding circular dependency issues](https://medium.com/visual-development/how-to-fix-nasty-circular-dependency-issues-once-and-for-all-in-javascript-typescript-a04c987cf0de)","n":0.066}}},{"i":763,"$":{"0":{"v":"Winston","n":1},"1":{"v":"\nWinston is a logger library designed with support for multiple transports (containers for the logs)\n- you can store errors in one transport, and normal logs in another. This is nice if you want to integrate other services. For instance what if we want our database to consume and store these errors, but don't care about storing other log levels?\n","n":0.13}}},{"i":764,"$":{"0":{"v":"Stream","n":1},"1":{"v":"\nA stream is a way for us to incorporate buffering into the process of writing files\n- We may add multiple avenues for this data to go in, releasing us from the restriction of only being able to insert one set of data in a single file\n\t- This gives the ability for a stream to be able to `pipe` its output to both stdout and a file\n\nThere are 4 types of streams in Node.js:\n1. **Writable**: streams to which we can write data. For example, fs.createWriteStream() lets us write data to a file using streams.\n2. **Readable**: streams from which data can be read. For example: fs.createReadStream() lets us read the contents of a file.\n3. **Duplex**: streams that are both Readable and Writable. For example, net.Socket\n4. **Transform**: streams that can modify or transform the data as it is written and read. For example, in the instance of file-compression, you can write compressed data and read decompressed data to and from a file.\n\nIn a Node.js based HTTP server, `request` is a readable stream and `response` is a writable stream\nThe fs module lets you work with both readable and writable file streams\n\nWhenever you’re using Express you are using streams to interact with the client, also, streams are being used in every database connection driver that you can work with, because of TCP sockets, TLS stack and other connections are all based on Node.js streams.\n\n### How to create a readable stream\nWe first require the Readable stream, and we initialize it.\n```js\nconst Stream = require('stream')\nconst readableStream = new Stream.Readable()\n```\n\nNow that the stream is initialized, we can send data to it:\n```js\nreadableStream.push('ping!')\nreadableStream.push('pong!')\n```\n\n#### Async iterator (`for await`)\n[[js.lang.feat.async-iterator]]\n\n#### Pipe vs. Write\n`pipe` is like positioning the pipes that will change the flow of water, before anything has actually gone through those pipes. `write` is like turning on the water.\n\n### Built-in streams\na request to an HTTP server and process.stdout are both stream instances.\nAll streams are instances of EventEmitter.\n\n# UE Resources\n[Async Iterator](https://nodesource.com/blog/understanding-streams-in-nodejs/)\n","n":0.056}}},{"i":765,"$":{"0":{"v":"Staging","n":1},"1":{"v":"\nIf we look in `node_modules/`, we may notice a `.staging/` directory\n\nThis directory is for those dependencies that are getting downloaded. so for the temporary basis it keeps all those dependencies under \".staging\" folder. Once all gets downloaded properly then it will showcase them under node_modules only.\n","n":0.147}}},{"i":766,"$":{"0":{"v":"Path","n":1},"1":{"v":"\n## `Path.join`\n- concatenate each argument to get a full URL\n\t- therefore, it can be relative or absolute (using `__dirname`) depending on what args we pass \n- depending on if the OS is Unix-based or Windows, different delimiters will be used, abstracting this away from us. \n\n## `Path.resolve`\n- attempts to resolve a sequence of paths from RTL, with each subsequence path prepended until an absolute directory is formed. \n\t- if an absolute directory is not able to be formed, then the args will be put on the end of the current working directory. \n- this method will treat the first argument as the root directory \n- when called without arguments, it will return the working directory (which may happen to be equivalent to `__dirname`)\n- it will always result in an absolute URL \n","n":0.087}}},{"i":767,"$":{"0":{"v":"Morgan","n":1},"1":{"v":"\nMorgan is a HTTP request logger middleware for Node.js.\n\n### Token\nWhen morgan logs to the console, the structure of the content that gets logged in determined by the tokens used\n\nA standard way to use morgan is to use the preset tiny:\n`app.use(morgan('tiny'))`, which is equivalent to:\n`morgan(':method :url :status :res[content-length] - :response-time ms');`\n- The part following `:` is the token.\n\n#### Custom Tokens\nWe can create our own tokens with `morgan.token(nameOfToken, callback)`, where the callback returns the value that will stand in for the token name.\n\nTokens can be configured to accept custom arguments:\n```js\napp.use(morgan(':method :host :status :param[id] :res[content-length] - :response-time ms'));\n\nmorgan.token('param', function(req, res, param) {\n    return req.params[param];\n});\n```\n\nMorgan can be combined with Winston to great effect\n","n":0.096}}},{"i":768,"$":{"0":{"v":"Child Process","n":0.707},"1":{"v":"\n### Spawn\nThe spawn function will spawn a new process of git log type. The first argument of the function represents a path for an executable file that should start the process, and the second argument is an arguments vector that will be given to the executable. The returned process object will hold a property for each std type represented as a Stream: .stdin - WriteStream, .stout - ReadStream and finally .stderr - ReadStream.\n```js\nconst { spawn } = require('child_process')\n\nspawn('git', ['log'])\n```\n\nIf we would like to run git log through a Node process and print it to the console we would do something like the following:\n```js\nspawn('git', ['log']).stdout.pipe(process.stdout)\n```\n\nOr we can do:\n```js\nspawn('git', ['log'], {\n  stdio: 'inherit' // Will use process .stdout, .stdin, .stderr\n})\n```\n\n# UE Resources\n[How to launch child processes](https://www.digitalocean.com/community/tutorials/how-to-launch-child-processes-in-node-js)\nhttps://www.freecodecamp.org/news/node-js-child-processes-everything-you-need-to-know-e69498fe970a/\n","n":0.09}}},{"i":769,"$":{"0":{"v":"JS Lang","n":0.707},"1":{"v":"\n## Expression\nan expression is a valid unit of code that resolves to a value.\n\nThere are two types of expressions: \n1. those that have side effects (such as assigning values) \n2. those that purely evaluate.\n\nAny JavaScript expression is also a statement.\n\n## Statement\n\nThe `;` character is used to separate statements in JavaScript code.\n\nThe most basic statement is a block statement, which is used to group statements. \n- Block statements are commonly used with control flow statements (`if`, `for`, `while`).\n\n## UE Resources\n- [Javascript Clean Code](https://github.com/ryanmcdermott/clean-code-javascript)","n":0.11}}},{"i":770,"$":{"0":{"v":"Type","n":1}}},{"i":771,"$":{"0":{"v":"Symbol","n":1},"1":{"v":"\n[[See symbols|general.lang.feat.symbol]]\n\nSymbols can be used to add unique property keys to an object that won't collide with other code that might be added to the object.\n- this effectively enables a weak form of encapsulation.\n\nSymbols are specifically created to be used as Object keys where you fear that key might be overwritten by someone else and lead to trouble (as it can happen with String keys for example).\n- Since Symbols are all unique, the only way to access a property with a Symbol key is to already have that Symbol. This means you either have to be the one who wrote the code, or that Symbol was defined by the standard (e.g. `Symbol.iterator`) and they did that to avoid breaking older code that might have used `obj[\"iterator\"]` for example.\n\n```js\nlet sym1 = Symbol()\nlet sym2 = Symbol('foo')\n// since symbols are guaranteed to be unique, we can create multiple symbols with the same description\nlet sym3 = Symbol('foo')\n\nconsole.log(sym2 === sym3) // false\n```\n\nThe `description` is the value passed to `Symbol()`.\n- can be used for debugging purposes, but not to access the `symbol` itself.\n\n# UE Resources\n[Recommended MDN article on Symbols](https://hacks.mozilla.org/2015/06/es6-in-depth-symbols/)\n","n":0.074}}},{"i":772,"$":{"0":{"v":"Set","n":1},"1":{"v":"\nA `Set` is similar to an array, in that it is a linear non-keyed data structure. \n- Also, like an array the order of data is maintained. \n- Unlike an array, each value in the set may only occur once, making it more like an enum in that sense.\n\nIts main methods are:\n- `new Set(iterable)` – creates the set, and if an iterable object is provided (usually an array), copies values from it into the set.\n- `set.add(value)` – adds a value, returns the set itself.\n- `set.delete(value)` – removes the value, returns true if value existed at the moment of the call, otherwise false.\n- `set.has(value)` – returns true if the value exists in the set, otherwise false.\n    - `set.has` is on average faster than the `Array.prototype.includes` method when an Array object has a length equal to a Set object’s size.\n- `set.clear()` – removes everything from the set.\n- `set.size` – is the elements count.\n\nA key feature is that repeated calls of `set.add(value)` with the same value don’t do anything. That’s the reason why each value appears in a Set only once.\n\nFor example, we have visitors coming, and we’d like to remember everyone. But repeated visits should not lead to duplicates. A visitor must be “counted” only once.\n\nA set can be iterated over with `forEach` or `for..of`\n\n```js\nconst mySet = new Set()\nmySet.add(2)\nmySet.add(5)\nmySet.add(1)\n\n// [2, 5, 1]\n```\n\n### Why and when to use Sets\n- Eliminating Duplicates: Sets are commonly used to remove duplicate values from an array or collection. By converting an array to a Set and then back to an array, you can easily eliminate any duplicate values.\n- Filtering Unique Values: Sets can be used to filter unique values from an array or collection. By adding elements to a Set, you automatically eliminate duplicates, allowing you to obtain a collection of unique values.\n    - tip: if you think in terms of [[math.set-theory]], each element of the set would be what we add to the `Set`. In this example, we are making a `Set` of conversations that satisfy a certain condition. Imagining a venn diagram, one circle would be composed of elements that satisfy this condition, while the second circle would be composed of elements that don't satisfy this condition. In both cases, they are conversations, so the `conversationId` is what we should build the set on.\n```ts\n// return a Set of conversationIds that doesn't include duplicates in cases where multiple messages are associated with a single conversationId\nconst conversationsWhereBusinessResponded: Set<number> = allMessages.reduce((conversations: Set<number>, message: Message) => {\n    if (message.sender === bizOwnerId) {\n        conversations.add(message.conversationId)\n    }\n    return conversations\n}, new Set())\n```\n- Iterating Over Unique Elements: Sets maintain the order of inserted elements, which makes them useful for iterating (e.g. with `forEach()`) over unique elements in the order of insertion.\n- Membership Testing: Sets are efficient for checking whether a particular element exists in a collection. The has() method of a Set can quickly determine if an element is present or not, without the need for iterating through the entire collection.\n- Operations Based on Set Theory: Sets can be used to perform set operations such as union, intersection, and difference. These operations can be useful when dealing with collections that need to be combined, compared, or filtered based on their elements.\n- Checking Array Similarity: Sets can be used to determine the similarity or differences between two arrays. By converting both arrays to Sets, you can easily perform set operations to identify common or distinct elements.\n- Tracking Unique Events: Sets can be employed to keep track of unique events or triggers. As events occur, you can add them to a Set, ensuring that only unique events are stored.","n":0.041}}},{"i":773,"$":{"0":{"v":"Numbers","n":1},"1":{"v":"\nyou should not do javascript math on non-integers. If you want to perform math on decimal numbers, you should first convert all values to an integer, do the math, then convert back\n- reason because javascript does math in binary, there are rounding errors when doing math.\n","n":0.147}}},{"i":774,"$":{"0":{"v":"Map","n":1},"1":{"v":"\nA `Map` is a collection of keyed data items, just like an Object. But the main difference is that Map allows keys of any type (including objects, functions, strings, and symbols), whereas object only allows strings and numbers.\n- ex. with a Map, `true`/`false` can be boolean keys. Even an object can be used as a key\n    - Using objects as keys is one of the most notable and important Map features.\n\nMaps perform better than plain Objects in scenarios involving frequent additions and removals of key-value pairs.\n\nMethods and properties are:\n- `new Map()` – creates the map.\n- `map.set(key, value)` – stores the value by the key.\n- `map.get(key)` – returns the value by the key, undefined if key doesn’t exist in map.\n- `map.has(key)` – returns true if the key exists, false otherwise.\n- `map.delete(key)` – removes the value by the key.\n- `map.clear()` – removes everything from the map.\n- `map.size` – returns the current element count.\n\n```js\nconst myMap = newMap()\n\nmyMap.set('a', 1)\nmyMap.set('b', 2)\nmyMap.get('a') // 1\n\n/*\n{\n    a: 1,\n    b: 2\n}\n*/\n```\n\n### Why and when to use Maps\n- Key Type Flexibility: Maps allow keys of any type, including objects, functions, strings, and symbols. This flexibility can be beneficial when you need to associate values with complex or non-primitive keys that may not be easily represented as strings.\n- Preserving Key Order: Maps maintain the order of key-value pairs based on the insertion order. This is particularly useful when you need to iterate over the entries in the same order as they were added. Regular objects do not guarantee a specific order for their properties.\n- Handling Large Key Sets: Maps are more efficient when dealing with a large number of key-value pairs. The performance of Map operations, such as retrieval and deletion, remains consistent regardless of the size of the map. In contrast, objects may experience performance degradation when the number of properties becomes very large.\n- Iterating Over Key-Value Pairs: Maps provide built-in methods such as `forEach()` and `for...of` loops to iterate over the key-value pairs directly. This simplifies operations that require iteration, transformation, or computation based on each entry in the map.\n- Easy Removal of Entries: Maps offer a straightforward way to remove entries from the collection using the delete() method. With regular objects, removing properties requires more explicit checks and assignments.","n":0.052}}},{"i":775,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Check if key exists\n```js\nusersByBusiness.has(review.businessId)\n```\n\n### Append to an existing `Set`\n```js\nusersByBusiness.get(review.businessId)?.add(review.userId)\n```\n\n### Create new set (value) if doesn't exist; append if it does\n```js\nif (!usersByBusiness.has(review.businessId)) {\n    usersByBusiness.set(review.businessId, new Set<number>())\n}\nusersByBusiness.get(review.businessId)?.add(review.userId)\n// { 2 => { 101, 103}, 3 => { 101, 102, 103 }, 4 => { 101 } }\n```\n\n### Iterating a Map\n```js\nfor (const [key, value] of myMap) {\n  console.log(`${key} = ${value}`);\n}\n```","n":0.132}}},{"i":776,"$":{"0":{"v":"Lexical Scope","n":0.707},"1":{"v":"\nA variable may belong to one of the following scopes:\n- Global scope: The default scope for all code running in script mode.\n- Module scope: The scope for code running in module mode.\n- Function scope: The scope created with a function.\n- Block scope: The scope created with a pair of curly braces (a block).\n    - only applicable to variables initialized with `let` or `const`\n\nLexical scope is scope that is determined by the JS engine before any code has even been executed (ie. it only looks at the source code— known as [lexing time](https://en.wikipedia.org/wiki/Lexical_analysis))\n- when you see lexical, think \"static\"\n- lexical scoping is analogous to prototypical inheritance, in a sense that the engine will walk up the chain to find a variable, if one is not found in the local scope.\n\nblocks within if statements, else statements, while statements, and so on should ideally be one line long. If that is unachievable, then it should be striven for to have all code at a single level of indentation.\n","n":0.078}}},{"i":777,"$":{"0":{"v":"Promises","n":1},"1":{"v":"\nA promise is an object which can be returned synchronously from an asynchronous function. \n- This synchronous-part is reflected in the fact that a promise returns immediately, albeit in Pending state.\n\nA promise can be in one of 3 possible states:\n- *Fulfilled*: onFulfilled() will be c\n- *Rejected*: onRejected() will be called (e.g., reject() was called)\n- *Pending*: not yet fulfilled or rejected\n\nIf it's either fulfilled or rejected, it's said to be *settled*. Otherwise, it's *pending*.\n- Once settled, a promise can *not* be resettled. Calling resolve() or reject() again will have no effect. \n  - The immutability of a settled promise is an important feature.\n\nA promise accepts a callback function as a parameter. The callback function accepts 2 parameters, `resolve` and `reject`. \n- If the task is successfully performed, then it returns resolve. \n- Else it returns the reject.\n\nEvery promise must supply a `.then()` method with the following signature:\n```ts\nthen: (onFulfilled?: Function, onRejected?: Function) => Promise\n```\n\nPromises make some guarantees about their use:\n- Callbacks added with `then()` will never be invoked before the completion of the current run of the event loop. These callbacks will be invoked even if they were added after the success or failure of the asynchronous operation (e.g. fetch for data) that the promise represents.  Multiple callbacks may be added by calling `then()` several times. They will be invoked one after another in a synchronous way.\n\nPromises are [[eager|general.terms.eager-lazy]], meaning that a promise will start doing whatever task you give it as soon as the promise constructor is invoked. \n- If you need lazy, check out observables or tasks.\n\nusing promises effectively abstracts time out of the picture.\n- abstracting away time allows us to better handle race conditions\n\n\n## Components of a Promise\nThere are 2 sides to promises: the `executor` (the one doing the actions) and the `consumer` (the one waiting for the actions to be done so it can consume the result.\nExecutor ex. `(resolve, reject) =>`\nConsumer ex. `.then, .catch, .finally`\n\n```js\n// Executor\nvar p1 = new Promise((resolve, reject) => {\n  resolve('Success!');\n  // or\n  // reject(new Error(\"Error!\"));\n});\n\n// Consumer\np1.then(value => {\n  console.log(value); // Success!\n}, reason => {\n  console.error(reason); // Error!\n});\n```\n\n### Executor\nExecutor is usually defined as part of a library, so it's often the case that this code is already written.\n\nA executor sends the data with `resolve(data)`\n\nExecutor gets run as soon as it's defined. Therefore, its state will either be resolved or rejected\n\n`Promise.resolve()` and `Promise.reject()` are shortcuts to manually create an already resolved or rejected promise respectively. This can be useful at times.\n\n### Consumer\nThink of `.then`, `.catch` and `.finally` as the way that consumers subscribe to the executor\nA consumer receives the data (`stuff`) sent via the executor (ie. `resolve(stuff)`) with `.then(stuff)`\n\n```js\ndoSomething()\n  .then(result => doSomethingElse(result))\n  .then(newResult => doThirdThing(newResult))\n  .then(finalResult => {\n    console.log(`Got the final result: ${finalResult}`);\n  })\n  .catch(onRejected);\n```\n\n#### `.then()`\n`.then` lets us chain (ie. compose) 2+ asynchronous operations.\n\nSignature:\n- note: most often the onRejected is omitted, with responsibility delegated to a final `.catch`\n```js\nconst then = (onFulfilled, onRejected) => Promise\n```\n\nTo avoid surprises, functions passed to `then()` will never be called synchronously, even with an already-resolved promise:\n- Instead of running immediately, the passed-in function is put on a microtask queue, which means it runs later (only after the function which created it exits, and when the JavaScript execution stack is empty), just before control is returned to the event loop\n```js\nPromise.resolve().then(() => console.log(2));\nconsole.log(1); // 1, 2\n```\n\n`.then()` is a method available on a Promise. It doesn't even matter if the Promise is `pending`, `fulfilled`, or `rejected`.\n- if we call `.then` on an already resolved/rejected promise, the `.then` block will be triggered instantly; however, the handler functions will be triggered asynchronously. To illustrate:\n```js\nconst resolvedProm = Promise.resolve(33);\n\nlet thenProm = resolvedProm.then(value => {\n    console.log(\"this gets called after the end of the main stack. the value received and returned is: \" + value);\n    return value;\n});\n// instantly logging the value of thenProm\nconsole.log(thenProm);\n\n// using setTimeout we can postpone the execution of a function to the moment the stack is empty\nsetTimeout(() => {\n    console.log(thenProm);\n});\n\n// logs, in order:\n// Promise {[[PromiseStatus]]: \"pending\", [[PromiseValue]]: undefined}\n// \"this gets called after the end of the main stack. the value received and returned is: 33\"\n// Promise {[[PromiseStatus]]: \"resolved\", [[PromiseValue]]: 33}\n```\n\n`.then` is analogous to adding another subscriber to the mailing list\n\n.then() may be called many times on the same promise. In other words, a promise can be used to aggregate callbacks.\n\n##### Handler functions (`onFulfilled`/`onRejected`)\nif the promise is resolved (ie. when the underlying async operation is completed), `onFulfilled` is called asynchronously (to be scheduled in the current thread loop).\n\nSignature:\n```js\nconst onFulfilled = (fulfillmentValue) => valueOfResolveFunction\n```\n\nHandler functions have some behaviours. If it...\n- returns a value, the promise returned by then gets resolved with the returned value as its value.\n- doesn't return anything, the promise returned by then gets resolved with an undefined value.\n- throws an error, the promise returned by then gets rejected with the thrown error as its value.\n- returns an already fulfilled promise, the promise returned by then gets fulfilled with that promise's value as its value.\n- returns an already rejected promise, the promise returned by then gets rejected with that promise's value as its value.\n- returns another pending promise object, the resolution/rejection of the promise returned by then will be subsequent to the resolution/rejection of the promise returned by the handler. Also, the resolved value of the promise returned by then will be the same as the resolved value of the promise returned by the handler.\n\nThink of async actions atomically, and make each step a `.then()` that returns the input for the next `.then()`\nEx. First .then() returns the JSON data as an obj, second returns a subset of that data, third acts on it\n\n#### Rejecting Promises\n`.catch(onRejected)` is short for `.then(null, onRejected)`\n- If there's an exception, the browser will look down the chain for `.catch()` handlers or `onRejected`.\n\nWhenever a promise is rejected, a [rejectionhandled event](https://developer.mozilla.org/en-US/docs/Web/API/Window/rejectionhandled_event) is sent to the global scope (probably `window`).\n\nJust like `.then`, `.catch` returns a promise, meaning we can chain another `.then` onto `.catch` if we want to \n- this is effectively saying \"do this thing, even if some previous async operation failed\"\n\nspec:when we wrap a function in `Promise(..)`, we are promisifying it\n- The new function that returns this promisified function now returns a Promise that resolves to its original return value\n\n(Inside promises) if a promise is returned (eg. return Promise.resolve('stuff')), the next .then() will execute only when that promise has resolved (with 'stuff').\nIf the return value is anything else besides a promise, then it will be passed immediately to the next .then()\n\n.resolve(value)\nIf the value passed to it is a promise itself, this will automatically \"follow\" that promise chain and wait to pass back the final resolved value.\n- Good to use if you are unsure if a value is a promise or not\n\n`Promise.all` is a server at a restaurant waiting to bring everyone's food at the same time, even though one meal may be ready before the others\n\n### Promises vs Callbacks to achieve asynchrony\nWith the callback method of achieving asynchrony, the idea is to pass a callback into a function and call that function after some event has occurred. \n\nPromises work a bit differently. Essentially, we create this object called a Promise (which if we recall, returns immediately when invoked, albeit with `[Pending]` status). To this object, we attach 2 callback functions: `resolve` and `reject`.\n- we call `resolve` if the async action succeeds\n- we call `reject` if the async action fails\n  - most commonly the `resolve` and `reject` callbacks are asynchronous functions that return a promise. Naturally, they get called upon completion/failure of the async operation (or in the case of a promise chain, they get called when the previous promise resolves.)\n  - both `resolve` and `reject` return void\n\n* * *\n\n## Wrapping Callbacks in Promises\nIn an ideal world, all asynchronous functions would already return promises. Unfortunately, some APIs still expect success and/or failure callbacks, like `setTimeout()`.\n\nLuckily we can wrap `setTimeout` in a promise. \n- Best practice is to wrap problematic functions at the lowest possible level, and then never call them directly again:\n```js\n// Basically, the promise constructor takes an executor function that lets us resolve or reject a promise manually. \n// Since setTimeout() doesn't really fail, we left out reject in this case.\nconst wait = ms => new Promise(resolve => setTimeout(resolve, ms));\n\nwait(10*1000).then(() => saySomething(\"10 seconds\")).catch(failureCallback);\n```\n\n## Promises are only ever resolved once per creation\nIn the following code it seems that we would connect to the database 2 times, but Promises don't work like that.\n- since `databasePromise` is only defined one time, it can by definition only resolve one time.\n\n```js\nconst databasePromise = connectDatabase();\n\nconst booksPromise = databasePromise\n  .then(findAllBooks);\n\nconst userPromise = databasePromise\n  .then(getCurrentUser);\n\nPromise.all([\n  booksPromise,\n  userPromise\n])\n  .then((values) => {\n    const books = values[0];\n    const user = values[1];\n    return pickTopRecommentations(books, user);\n  });\n```\n\n## Sleep example\n- We can use promises make a function who's purpose is to simply wait, before executing further code. We define it as such:\n```ts\nconst sleep = (ms) => new Promise<void>(resolve => setTimeout(resolve, ms));\n```\n- as soon as we call `sleep(1000)`, a promise is returned to us. This means that javascript will say \"ok, since we're waiting on that promise to resolve (or reject), I'm going to go do some other stuff, and once your promise resolves, I'll be back to execute the `.then()` code\".\n\t- This is precicely why promises are said to \"handle asynchronous things synchronously\". It is because promises help us manage 2 different lines of execution at a time\n","n":0.026}}},{"i":778,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Promisify\n```js\nonRequest((request, response) => {\n    const promisifiedFunction = new Promise(resolve => {\n        nonPromisifiedFunction(request.fileToRead, data => resolve(data));\n    });\n    \n    promisifiedFunction\n        .then(data => writeFile(request.fileToWrite, data))\n        .then(status => response.send(status));\n});\n```\n\n### Cancelling promises\nJust reject the promise with “Cancelled” as the reason. If you need to deal with it differently than a “normal” error, do your branching in your error handler.\n\n### `sleep()`/`waitFor()`\n```js\nlet sleep = ms => new Promise(resolve => setTimeout(resolve, ms))\n\nasync function waitFor(fn){\n    while(!fn()) await sleep(1000)\n    return fn()\n}\n\n/* Usage */\n// wait for an element to exist, then assign it to a variable\nlet bed = await waitFor(() => document.getElementById('bedId'))\nif(!bed) doSomeErrorHandling();\n\n// wait for a variable to be truthy\nawait waitFor(() => el.loaded)\n\n// wait for some test to be true\nawait waitFor(() => video.currentTime > 21)\n\n// add a specific timeout to stop waiting\nawait waitFor(() => video.currentTime > 21, 60*1000)\n\n// send an element as an argument once it exists\ndoSomething(await waitFor(() => selector('...'))\n\n// pass it some other test function\nif(await waitFor(someTest)) console.log('test passed')\nelse console.log(\"test didn't pass after 20 seconds\")\n```\n\n### Implementing polling with promises\nie. *“wait for x then do y”*\n```ts\nconst waitFor = async (fn: () => Promise<boolean>, timeoutMs: number) => {\n  return new Promise(async (resolve) => {\n    const startTime = Date.now()\n\n    const poll = async () => {\n      if (await fn()) resolve(true)\n      else if (Date.now() - startTime > timeoutMs) resolve(false)\n      else setTimeout(poll, POLL_FREQUENCY_MS)\n    }\n    await poll()\n  })\n}\n\nasync function isItemInTable(eventId: string) {\n  const { Item } = await dynamoDocumentClient.send(\n    new GetItemCommand({\n      TableName: TABLE_NAME,\n      Key: marshall({ event_id: eventId }),\n    }),\n  )\n  return !!Item\n}\n\nif (await waitFor(() => isItemInTable(eventId), TEST_TIMEOUT_MS)) {\n    // ...\n```\nalternate: http://www.abigstick.com/2019/03/29/pollers_and_promises.html\n\n### Setting a timeout condition with multiple promises\nImagine we want to check multiple shards simultaneously for the same piece of information. We could create a promise for each of the shards we are searching, and each promise will resolve to either true or false depending on if the item was found. In this case, we could use `Promise.race` to return as soon as one promise resolves. This also lends itself nicely to implementing a timeout functionality.\n```ts\nconst promises = Shards.map(shard =>\n    // checkShard returns a promise.\n    checkShard(shard, StreamArn, responsePayloadAsString)\n)\n\nconst timeoutPromise = new Promise((resolve) => setTimeout(resolve, TEST_TIMEOUT_MS))\n\n/* if result is truthy, the record was found and the test passed. If the timer resolves first, then the test is considered to have timed out. */\nconst result = await Promise.race([ ...promises, timeoutPromise ])\n```\n\n### Run 2+ async functions in parallel\n```js\nfunction randomWait() {\n    const seconds = Math.random() * 5;\n  \n    return new Promise(resolve => {\n      setTimeout(\n        () => resolve(`Resolved after ${seconds.toFixed(2)} seconds`),\n        seconds * 1000\n      );\n    });\n}\n\nfunction fn() {\n    const result1 = randomWait()\n    .then(result => console.log('1st call:', result));\n    const result2 = randomWait()\n    .then(result => console.log('2nd call:', result));\n}\n\nfn();\n```\n","n":0.049}}},{"i":779,"$":{"0":{"v":"Async-Await","n":1},"1":{"v":"\nevery function annotated with async returns an implicit promise\n- *\"The async function declaration defines an asynchronous function, which returns an AsyncFunction object. An asynchronous function is a function which operates asynchronously via the event loop, using an implicit Promise to return its result.\"*\n\nevery time we see `await`, it means the promise must resolve before moving on. If it is rejected, then an error is thrown, and it's up to us to `catch` it.\n\nUsing async-await is fantastic when the tasks we’re trying to be accomplished are supposed to be sequential and the asynchronous function calls need to happen in series. If we need true parallelism, we need to go back to vanilla [[Promises|js.lang.promises]] .\n\nThe return value of an async function is implicitly wrapped in `Promise.resolve` - if it's not already a promise itself\n\n`await` must appear directly inside an `async` function. Therefore, we cannot do this:\n```js\nconst myFunc = async () => {\n  const mapResult = dataFromA.map(item => {\n    const dataFromB = await doApiCallHere(item.id)\n\n    return {\n      ...dataFromA,\n      dataFromB,\n    }\n  })\n}\n```\nHere, we are trying to use `await` within a `map` function. \n- spec: since `map` is not asynchronous, this wouldn't make sense.\n\nIf we wanted to do this, we would need to pass an `async` function to map:\n```js\nconst mapResult = dataFromA.map(async item => {\n...\n```\n\n`map` now returns an array of promises, which we can then `Promise.all` over:\n```js\nconst myFunc = async () => {\n  const promises = dataFromA.map(async item => {\n    const dataFromB = await doApiCallHere(item.id)\n\n    return {\n      ...dataFromA,\n      dataFromB,\n    }\n  })\n\n  Promise.all(promises)\n}\n```\n\nAwaiting a non-promise will have the same effect as awaiting a resolved promise that is resolved with the non-promise value. So\n\n```js\nawait abc() // returns undefined\n```\nIs the same as\n\n```js\nawait Promise.resolve(undefined)\n```\n\nThe await will pause the current function (as it always does) then continue running it on the next microtask tick since no additional waiting is needed for a resolved promise and then continue on with the rest of the function.\n\nawait doesn't care if a function is async. It only looks for promises. Non async functions can also return promises though async functions always do. If it doesn't find a promise it pretends like it did by treating the value it's awaiting as a resolved promise.\n\n## How async-await converts to Promises\nThe body of an async function can be thought of as being split by zero or more await expressions. \n- Top-level code, up to and including the first await expression (if there is one), is run synchronously. \n  - In this way, an async function without an await expression will run synchronously. If there is an await expression inside the function body, however, the async function will always complete asynchronously.\n\nFor example:\n```js\nasync function foo() {\n   await 1\n}\n```\n\n...is equivalent to:\n```js\nfunction foo() {\n   return Promise.resolve(1).then(() => undefined)\n}\n```\n\nCode after each await expression can be thought of as existing in a `.then` callback.\n- In this way a promise chain is progressively constructed with each reentrant step through the function. The return value forms the final link in the chain.\n- the promise chain is not built-up in one go. Instead, the promise chain is constructed in stages as control is successively yielded from and returned to the async function. As a result, we must be mindful of error handling behavior when dealing with concurrent asynchronous operations. In the following example, `p2` will not be \"wired into\" the promise chain until control returns from `p1`.\n```js\nasync function foo() {\n   const p1 = await new Promise((resolve) => setTimeout(() => resolve('1')))\n   const p2 = await new Promise((resolve) => setTimeout(() => resolve('2')))\n}\n```\n\n* * *\n\n### Top-level Await (ES2022)\nTop-level await allows us to use await outside of a function marked `async`\n\n- Essentially, our code becomes asynchronous at the module level. This can affect how your module behaves, especially if you have other code that relies on the module's exports being available immediately. Once you use top-level await, all other modules that import your asynchronous module must wait until all of the top-level awaits have been resolved.\n  - Therefore, if you're not loading dependencies for your exports you might not want to be using top-level await.\n\nIn cases where your module's primary purpose is to initialize and configure something essential for the module's operation (e.g., fetching secrets), using top-level await can be a straightforward and efficient approach.\n\nTo use top-level await, you must be using ESModules, not CommonJS (ie. set the package.json field `\"type\": \"module\"`)\n","n":0.038}}},{"i":780,"$":{"0":{"v":"Operators","n":1},"1":{"v":"\n## Operator precedence\nOperators with higher precedence become the operands of operators with lower precedence.\n- ex. `true || false && false` evaluates to `true`, because the `false && false` is evaluated first, which returns `false`, which is then used as the operand for the original expression: `true || false`, which evaluates to `true`\n\nUse parentheses (`()`) to alter precedence","n":0.132}}},{"i":781,"$":{"0":{"v":"Pipeline","n":1},"1":{"v":"\nNote: as of 02/2022, this is still only stage two, so polyfills are needed\n\nUseful any time we want to do something like this:\n```js\nconst result = double(double(increment(double(2))))\n```\n\nWith pipeline operator, we can do this:\n```js\n2 |> double |> increment |> double |> double\n```\n\n","n":0.158}}},{"i":782,"$":{"0":{"v":"Optional Chaining","n":0.707},"1":{"v":"\n`?.` is how we achieve optional chaining\n\nJust as `??` doesn't abide by JS falsy (instead, only `undefined`/`null`), so too does the `?.` operator\n\nExample:\n```js\nconst eligibleRegion = publishingDetails?.salesRights?.[0]?.countriesIncluded?.map\n```\n","n":0.196}}},{"i":783,"$":{"0":{"v":"Logical","n":1},"1":{"v":"\n## Logical AND (`&&`) - Logical Conjunction\nIf one of the operands is falsy, then that first falsy operand will be returned, and the rest of the operands will never be executed (therefore, it is a short-circuit operator)\n- ex. `3 > 0 && -2 > 0` <-- returns `false`\n- ex. `\"\" && \"foo\"` <-- returns `\"\"`\n\nIf all of the operands are truthy, then the last operand will be returned\n- ex. `3 > 0 && 6 > 0` <-- returns `true`\n\nThe value that is returned can always be converted to a boolean primitive by using the double `NOT` operator (`!!`)\n- ex. `!!\"\" && \"foo\"` <-- returns `false`\n\nThe `AND` operator has a higher precedence than the `OR` operator, meaning the && operator is executed before the || operator\n\n## Logical OR (`||`) - Logical Disjunction\nIf one or more of the operands is truthy, then that first truthy operand will be returned, and the rest of the operands will never be executed (therefore, it is a short-circuit operator)\n- if all operands are falsy, then the last operand is returned\n\nThe value that is returned can always be converted to a boolean primitive by using the double `NOT` operator (`!!`)\n- ex. `\"\" || !!\"foo\"` <-- returns `true`","n":0.071}}},{"i":784,"$":{"0":{"v":"Coalesce","n":1},"1":{"v":"\n`??` is the Nullish Coalescing operator \n\nThe nullish coalescing operator can be seen as a special case of the logical OR (`||`) operator\n\nIn Javascript we often reach for `||` when we want to get the RH value if the LH value is falsy\n- ex.\n```js\nconst valA = nullValue ?? \"default for A\";\n```\n\nThis way of doing it works most of the time, but if our business logic considers falsy values like `0` or `''` as legitimate values, then our program will have unexpected behavior. Using `??` gives us more predictable results, and should be used most of the time instead of `||`\n\nPrefer `??` over `&&`.\n- `&&` will return the right-hand side for anything falsy\n- `??` will return the right-hand side for only `null` and `undefined`\n\n### Nullish Coalescing Assignment (`??=`)\nWorks great for defaults\n```js\nconst myFunction = (options) => {\n\toptions.interval??= 10\n\toptions.timeout??= 1000\n\tconsole.log(options)\n}\nmyFunction({ interval: 50 })\n\n```\n\n### Assigning a default value to a variable\nIn the past, when one wanted to assign a default value to a variable, a common pattern was to use the logical OR operator (`||`):\n```js\nlet foo;\n\n//  foo is never assigned any value so it is still undefined\nlet someDummyText = foo || 'Hello!';\n```\n\nHowever, this is kind of hacky when we consider what we are doing. `||` is a boolean logical opertor (ie. it performs \"math\" on booleans). In order to do boolean math on 2 variables, it has to make sure both values are booleans. If they are not, they have to be coerced (Of course, JS is notorious for its sometimes unpredictable coercion behaviour).\n- With boolean logical operators, the only two values that are considered are truthy values and falsy values. This might work most of the time, but imagine our business logic dictates that `0` is valid, and should therefore be considered a truthy value. If this is the case, then\n\nExample:\n\nBelow, `quantityOfApples` will not be `0` as we want. Rather, the default value of `10` is being used, because `0` is considered falsy in the eyes of the boolean logical operator.\n```js\nconst count = 0\n\nconst quantityOfApples = count || 10 // 10 — unexpected!\nconst actualQuantityOfApples = count ?? 10 // 0 — totally expected\n```\n\n### Misc\n- It is not possible to combine both the AND (`&&`) and OR operators (`||`) directly with `??`\n\t- To accomplish this, we need to use parentheses (`(null || undefined) ?? \"foo\";`)\n","n":0.051}}},{"i":785,"$":{"0":{"v":"Objects","n":1},"1":{"v":"\nThe lineage of an object is irrelevant. What matters about an object is what it can do, not what it is descended from.\n\nJavaScript object keys are always coerced to a string, so obj[0] is always the same as obj[\"0\"].\n\nAs of ES6, objects have a predictable order, determined in one of 2 ways:\n1. If *keys are numbers*, The key-value pair being inserted will obey numerical order\n\t- ex. if we have existing keys `4`, `8`, and `33`, then inserting a key-value pair with the key as `1` will put it in first position of the object\n2. If *keys are strings*, The key-value pair being inserted will be appended to the end of the object\n\n### Polymorphism with objects\nImagine we had the idea of a session in our code. Instead of being one type however, we want it to either evaluate to an object, or the id of the object. Therefore, either object or string. This enables us to achieve polymorphism in functions, since we can take in a `session` argument, and depending on its type, we can act accordingly.\n```js\n// code from graphile/starter\nexport const becomeUser = async (\n  client: PoolClient,\n  userOrUserId: User | string | null,\n) => {\n  await becomeRoot(client)\n  const session = userOrUserId\n    ? await createSession(\n        client,\n        typeof userOrUserId === 'object' ? userOrUserId.id : userOrUserId,\n      )\n    : null\n  await client.query(\n    `select set_config('role', $1::text, true), set_config('jwt.claims.session_id', $2::text, true)`,\n    [process.env.DATABASE_VISITOR, session ? session.uuid : ''],\n  )\n}\n```\n","n":0.066}}},{"i":786,"$":{"0":{"v":"Proxy Object","n":0.707},"1":{"v":"\nA proxy object is an object that wraps another object. The proxy object intercepts the fundamental operations of the wrapped object, including property lookup, assignment, function invocations etc\n\t- In other words, we can have a regular object that we interact with normally. We can also wrap that regular object with a proxy object, that will intercept those interactions that we have with the object.\n- Think of a proxy object like an enhanced (wrapped) component in React. It can do everything that the original can, but has some added functionality. This added functionality might be as simple as logging to the console every time a property is read (this would be implemented using a **get trap**. \n- The wrapped object is called the *target*\n- The custom functionality that is added is called the *handler*\n- *trap* - a method defined on the proxy that will intercept some interaction we have with the object (ie. read, write)\n","n":0.081}}},{"i":787,"$":{"0":{"v":"Date","n":1},"1":{"v":"\nDate objects are represented internally by a timestamp, which is milliseconds passed since the epoch (ie. midnight 01 January, 1970 UTC)\n\nThere are always two ways to interpret a timestamp: \n1. as a local time \n    - The local timezone is not stored in the date object, but is determined by the host environment.\n2. as a Coordinated Universal Time (UTC)\n\n### String format\nThe JavaScript specification only specifies one string format to be universally supported: `YYYY-MM-DDTHH:mm:ss.sssZ` \n- `T` is a literal character, which indicates the beginning of the time part of the string\n- `Z` is the timezone offset, which can either be the literal character `Z` (indicating UTC), or + or - followed by HH:mm, the offset in hours and minutes from UTC.\n    - When the time zone offset is absent, date-only forms are interpreted as a UTC time and date-time forms are interpreted as local time (see below for date-only and date-time form explanation)\n\nVarious components can be omitted, so the following are all valid:\n- *Date-only form*: `YYYY`, `YYYY-MM`, `YYYY-MM-DD`\n- *Date-time form*: one of the above date-only forms, followed by `T`, followed by `HH:mm`, `HH:mm:ss`, or `HH:mm:ss.sss`. Each combination can be followed by a time zone offset.\n\n#### Converting a Date object to string\n- `Date.toISOString()` will return a string in the format: `1970-01-01T00:00:00.000Z` \n    - note: this is the same string format that the Date object accepts as input\n- `Date.toString()` will return a string in the format `Thu Jan 01 1970 00:00:00 GMT+0000`\n- `Date.toDateString()` and `Date.toTimeString()` return the date and time parts of the string, respectively.\n- `Date.toLocaleDateString()`, `Date.toLocaleTimeString()`, and `Date.toLocaleString()` use locale-specific date and time formats, usually provided by the Intl API.\n\n* * *\n\nmonths are zero-based, so December is 11\n\n### `Date.now()` vs `new Date()`\n`new Date()` - creates a Date object representing the current (or passed) date/time\n`Date.now()` - returns the number of milliseconds since the epoch \n- we also get this with `new Date().valueOf()` and `new Date().getTime()`\n\nAs a general rule of thumb, use... \n- `Date.now()` in code that deals with intervals (usually subtracting an earlier date from a later date to calculate the time elapsed) \n- `new Date()` for timestamps, e.g. those written to a database.\n","n":0.053}}},{"i":788,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Get elapsed time\n```js\nconst start = Date.now()\ndoSomeExpensiveOperation()\nconst end = Date.now()\nconst timeElapsed = end - start\n```\n\n### Get Date-only (e.g. `2023-03-02`)\n```js\nnew Date().toLocaleDateString()\n```","n":0.224}}},{"i":789,"$":{"0":{"v":"Methods","n":1}}},{"i":790,"$":{"0":{"v":"setTimeout","n":1},"1":{"v":"\n`setTimeout` allows us to run a function *once* after the interval of time.\n\nsetTimeout says \"call *this* function (arg1) after *this many* seconds (arg2)\n\nBoth `setTimeout` and `setInterval` allow us to execute some code at a later point in time (ie. \"schedule a call\")\n\nYou can pass arguments to the function you pass to `setTimeout` as the 3rd+ parameter(s)\n- ex. `setTimeout(executePoll, this.pollIntervalMs, resolve, reject)`. Here, `resolve` and `reject` are parameters of the `executePoll` method.","n":0.119}}},{"i":791,"$":{"0":{"v":"setInterval","n":1},"1":{"v":"\n`setInterval` allows us to run a function repeatedly, starting after the interval of time, then repeating continuously at that interval.\n\nsetInterval says \"call *this* function (arg1) every X seconds (arg2)\n\nTo stop further calls, we should call `clearInterval(timerId)`.\n\nBoth `setTimeout` and `setInterval` allow us to execute some code at a later point in time (ie. \"schedule a call\")\n","n":0.135}}},{"i":792,"$":{"0":{"v":"Console","n":1},"1":{"v":"\n`console.X()` methods return undefined\n\n### `console.table`\nIf we have an array of identically formatted objects, we can print out a nice table to the console, instead of getting a dump of a JSON-like structure, as we would have with a simple console.log\n\n### `console.group`\n\n### `console.dir(obj)`\nDisplays an interactive list of the properies of the passed object\n- \"interactive\" here means that the properties (listed as a hierarchy) can be collapsed and expanded.\n\n### `console.time`/`console.timeEnd`\nWe can use this to get the time of how long something takes to execute:\n```js\nconsole.time('filter array');\nconst visibleTodos = getFilteredTodos(todos, filter);\nconsole.timeEnd('filter array'); // filter array: 0.15ms\n```\n\n* * *\n\n### Console.log an arrow function\nOne of the pain points of implicitly returning arrow functions is that you can't easily stick a `console.log` in, without converting it to an explicit return. To get around this, we can have the function return `console.log(myVal) || <rest of the code>`. The reason this works is because console methods return `undefined`, which causes `<rest of the code>` to continue to be executed.\n\n```js\n// Before: \nconst myFunc = () => doTheThing()\n\n// After:\nconst myFunc = () => console.log(myVal) || doTheThing()\n```\n","n":0.076}}},{"i":793,"$":{"0":{"v":"Iterator","n":1},"1":{"v":"\nThere are two iteration protocols: *iterable protocol* and *iterator protocol*.\n\n### Iterator protocol\nAn object is an iterator when it implements an interface that answers two questions:\n- Is there any element left?\n- If there is, what is the element?\n\nTechnically speaking, an object is qualified as an iterator when it has a `next()` method that returns an object with two properties:\n- `done`: a boolean value indicating whether or not  there are any more elements that could be iterated upon.\n- `value`: the current element.\n\nIf we call `next()` after the last value has returned, we are returned:\n```js\n{done: true: value: undefined}\n```\n\nArray, Maps, and Sets have built-in iterators (ie. the protocol is already implemented)\n- If you have a custom type and want to make it iterable so that you can use the `for...of` loop construct, you must implement the iteration protocols by hand.\n\n### Iterable protocol\nAn object is iterable when it contains a method called `[Symbol.iterator]` that takes no argument and returns an object which conforms to the iterator protocol (ie. has `next()`, and an object `{ value: x, done: false }`).\n- The `[Symbol.iterator]` is one of the built-in well-known symbols in ES6.\n\n### Iterable\nAn iterable is an object that defines its iteration behavior. \n\nCustom iterators and iterables are useful, but are complicated to build, since you need to take care of the internal state. A generator is a special function that allows you to write an algorithm that maintains its own state. They are factories for iterators. A generator function is a function marked with the * and has at least one yield-statement in it.\n\nIterable objects are a generalization of arrays. That's a concept that allows us to make any object useable in a for..of the loop.\n\nThe iterable is an interface that specifies that an object can be accessible if it implements a method who is key is `[symbol.iterator]`.\n\n# E Resources\n[Roll your own iterable object](https://www.javascripttutorial.net/es6/javascript-iterator/)\n","n":0.057}}},{"i":794,"$":{"0":{"v":"Javascript Imports","n":0.707},"1":{"v":"\nModules import one another using a module loader. At runtime the module loader is responsible for locating and executing all dependencies of a module before executing it. Well-known module loaders used in JavaScript are Node.js’s loader for CommonJS modules and the RequireJS loader for AMD modules in Web applications.\n\nIn TypeScript, just as in ECMAScript 2015, any file containing a top-level import or export is considered a module. Conversely, a file without any top-level import or export declarations is treated as a script whose contents are available in the global scope (and therefore to modules as well).\n\nTop-level code is executed only once during module initialization. \n\n# ES Modules (ES6 Import)\nSince imports aren't a part of the Javascript standard yet, imports are just a specification— it is up to the implementation (Babel, Typescript, Metro Bundler(?)) to carry out the operation of linking modules.\n\nESM is for Browsers (though we can use transpilers like Babel to enable its use in Node)\n- ex. Typescript's `tsconfig.json` file has a setting `target` which determined which version of Javascript the code will be transpiled to. Therefore, if we use `target`\n\nESM is tree-shakable\n\nESM are required as they are needed, rather than there being a bundle created beforehand (as with CommonJS)\n\nIf the `type` field of [[package.json|js.node.package-json]] is `module`, then `.js` files will be interpreted as ESModules.\n- Even if `\"type\": \"commonjs\"`, we can still use ESModules by giving our files a `.mjs` extension.\n\nwhen you tell the JS engine to run a module, it has to behave as though these four steps are happening:\n1. Parsing: The implementation reads the source code of the module and checks for syntax errors.\n2. Loading: The implementation loads all imported modules (recursively). (This is the part that isn’t standardized yet)\n3. Linking: For each newly loaded module, the implementation creates a module scope and fills it with all the bindings declared in that module, including things imported from other modules.\n\t- This is the part where if you try to import {cake} from \"paleo\", but the “paleo” module doesn’t actually export anything named cake, you’ll get an error. And that’s too bad, because you were so close to actually running some JS code. And having cake!\n4. Run time: Finally, the implementation runs the statements in the body of each newly-loaded module. By this time, import processing is already finished, so when execution reaches a line of code where there’s an import declaration… nothing happens!\n- when we import a module like `import 'firebase/storage'` (ie. not importing any bindings), it means we are executing the module `firebase/storage`, but not bothering to assign the default export to a variable. In fact, perhaps the module doesn't even export any bindings.\n\t- This implies we are doing side-effects.\n\n- When you `import *`, what’s imported is a *module namespace object*. The properties of this object are the module’s exports:\n```js\n[Module] {\n  default: '[Function]', // the default export\n  first: 'Kyle' // named export `first`\n  last: 'Tycholiz', // named export `last`\n}\n```\n- if we wanted to import the named exports, we could `import { first } from _____`. spec: also, we could import the default by `import default from _____` (or `import { default }`?)\n\n- `import _ from \"lodash\"` is an alias for `import { default as _ } from \"lodash\"`\n* * *\nImported ES6 modules are executed either asynchronously or synchronously, depending on the module loader (ie. the implementation) we use. Therefore, to be safe we must assume async. However, all imports are executed prior to the script doing the importing. This makes ES6 modules different from Node.js modules or `<script>` tags without the `async` attribute\n\n### Importing without name\nex. `import './bootstrap'`\n- this will execute the target module (ie. run the module's code), without importing anything. It will not affect the scope of the active module\n\t- There may be side-effects, such as declaring global variables.\n- This method of importing is described as \"importing a module for its side-effects only\"\n\n### Aggregating modules (Re-exporting)\n- We can import modules and immediately export them again by aggregating the import and export commands:\n```\nexport * from './atoms'\n```\n- If any name exported by “atoms” happened to collide with the other exports, that would be an error, so use export * with care.\n\nUnlike a real import, this doesn’t add the re-exported bindings to your scope, meaning we can't use the exports from \"atoms\" within that file.\n\n# CommonJS Imports\nThis is the Node.js way of handling imports (as of 2022)\n\nModules are copied\n\nImports are synchronous\n\nTree-shaking doesn't work with CommonJS (since this type of thing typically doesn't matter with server code)\n\nIf the `type` field of [[package.json|js.node.package-json]] is `common-js` (or is empty), then `.js` files will be interpreted as CommonJS.\n- Even if `\"type\": \"module\"` (indicating ESModules), we can still use CommonJS by giving our files a `.cjs` extension.\n\n## module.exports\n- `module.exports` is an object that is included in every `.js` file in a Node application.\n\t- `module` represents the current module\n\t- `exports` is an objects that will be exposed as a module\n\t- Therefore whatever we assign to `module.exports` is exposed as a module.\n- like `exports` below, `module.exports` can also be extended by including more properties/methods on the object.\n\nbefore a module's code is actually executed, Node will wrap it in a function that looks something like this:\n```js\n(function(exports, require, module, __filename, __dirname) {\n\t// Module code actually lives in here\n});\n```\nThis gives us the benefit of:\n- scoped variables, rather than global variables.\n- ability to use `module` and `exports` objects.\n- ability to reference the module's absolute filename and directory path with `__filename` and `__dirname`\n\n## exports\n- `exports` is an object that we can attach properties and methods to.\n- when we import a module, we must then call the same property/method:\n```js\n// dependency\nexports.name = 'Kyle'\nexports.phone = '5555774834'\n\n// dependent\nconst person = require('./information')\n\nconsole.log(person.name) // Kyle\n```\n\n# CommonJS vs ES6 Modules\nUnder the hood, we need something like Babel to convert from ES6 modules to CommonJS.\n- [explanation](https://stackoverflow.com/questions/40294870/module-exports-vs-export-default-in-node-js-and-es6)\n\n# Circular Dependencies\n- Not always a problem, but they introduce tight coupling.\n\t- These kinds of modules are harder to understand and reuse, as doing so might cause a ripple effect where a local change to one module has global effects.\n\t- As such, it might indicate lack of a larger context or proper architecture, since a good architecture imposes uni-directional flow between modules and entire layers.\n\nusing `const` over `function` while defining functions prevents function hoisting within a single module and ensures the absence of circular dependencies within that module.\n\nCircular dependencies with Function calls would not cause problems when the cycle is asynchronous, meaning that directly referenced functions are not called immediately.\n- ex. Cycle of function calls when one continues chain through a DOM event listener being async, i.e. waiting for user click.\n\n# Dynamic Imports\n`import(module)` loads the module and returns a promise that resolves into a module object that contains all its exports. `import` can be called from anywhere in the code.\n\nexample\n```js\nimport(modulePath)\n  .then(obj => <module object>)\n  .catch(err => <loading error, e.g. if no such module>)\n```\nor\n```js\n// 📁 say.js\nexport function hi() {\n  alert(`Hello`);\n}\n\nexport function bye() {\n  alert(`Bye`);\n}\n```\nthen\n```js\nlet {hi, bye} = await import('./say.js');\n\nhi();\nbye();\n```\nNote: Although `import()` looks like a function call, it is specified as syntax that just happens to use parentheses (similar to super()). That means that import doesn’t inherit from Function.prototype so you cannot `call` or `apply` it.\n\nDynamic means code that can be executed at runtime. Static means it occurs before compilation and before runtime. \n- ex. dynamic imports. in this case, dynamic means that we can decide the path at runtime, or we can decide which module to import at runtime\n\n# UE Resources\n- https://v8.dev/features/modules\n- [Differences between ESM and CommonJS](https://nodejs.org/api/esm.html#esm_differences_between_es_modules_and_commonjs)\n\n\nre","n":0.029}}},{"i":795,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Alias a module\nImagine we had a function `term`, but we want to create an alias `regex`, so that either could be imported and used:\n```ts\nexport const term = () => {\n    // do stuff\n}\nexport { term as regex }\n```\n","n":0.16}}},{"i":796,"$":{"0":{"v":"Functions","n":1},"1":{"v":"\nIn JavaScript, functions are not only blocks of code but also objects with associated context (the [[this|js.lang.feat.this]] value)\n- By using `.bind`, you effectively create a new function that, when invoked, will maintain the specified context regardless of where it's called. This is especially useful when dealing with asynchronous functions that may be called at different times and contexts.\n\nWhen you pass an object (or array) as a parameter, if the function changes the object's properties (or array's elements), that change is visible outside the function\n\n## Arrow Function\n- Arrow functions do not have their own [[this|js.lang.feat.this]], `arguments`, or `super`. \n- Arrow functions are always anonymous.\n\n## Tagged Template Literals\n```js\n// These are equivalent:\nfn`some string here`\nfn(['some string here'])\n```\n\n* * *\n\nThe rest of the arguments will be the interpolations, in order.\n```js\nconst aVar = 'good'\n\n// These are equivalent:\nfn`this is a ${aVar} day`\nfn(['this is a ', ' day'], aVar)\n```\n\n## Identity function\n```\nfetchBook()\n  .then((book) => formatBook(book))\n  .then((postscript) => print(postscript))\n```\nis equivalent to (verify this)\n```\nfetchBook()\n  .then(formatBook)\n  .then(print);\n```\n\n### Passing in a named function vs Passing in a function that calls the named function\n`setTimeout` will call the passed in function after X milliseconds.\n- in the first case, `resolve` gets called, which resolves the promise\n- in the second case, a function which calls `resolve` gets called. The act of calling this function results in `resolve` being called\n\nTherefore, these are not the same thing, but they attain the same result\n\n```js\nconst sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms))\n// vs\nconst sleep = (ms) => new Promise(resolve => setTimeout(() => resolve('foo'), ms))\n```\n","n":0.064}}},{"i":797,"$":{"0":{"v":"Tagged Template Literals","n":0.577},"1":{"v":"\n[explanation using styled-components](https://mxstbr.blog/2016/11/styled-components-magic-explained/)\n","n":0.577}}},{"i":798,"$":{"0":{"v":"Generator Functions","n":0.707},"1":{"v":"\nCan be exited and later re-entered\n- like closures, variables inside the generator function maintain state.\n- when calling a generator function, an iterator object is returned. When we call `next()` on that object, all the code up until the first `yield` will be executed. Calling `next()` again will then execute all the code up until the second `yield`, and so on.\n\t- The function that calls the generator function is the **iterator**\n- the generator function can pass values to the iterator object (`yield`). Anything that occurs after `yield` gets stored in the iterator's `next()` value\n\t- The generator function can also retrieve values from the iterator object (`next(___)`)\n- `yield` returns execution to outside the generator function (ie. the context from which the gen fn was called), it's possible to use `while(true)`, as long as there is a yield inside\n\t- This way, `next()` can keep getting called\n- spec: `next` is like async/await in the sense that it will execute code up until a point (`yield`), then stop and wait for the availability of that data before continuing on\n- a generator function is a type of *pull* system. (ie. the Consumer determines when it receives data from the data Producer)\n\n# UE Resources\n- [Observable Async flow control (Eric Elliott)](https://medium.com/javascript-scene/the-hidden-power-of-es6-generators-observable-async-flow-control-cfa4c7f31435)\n","n":0.07}}},{"i":799,"$":{"0":{"v":"IIFE","n":1},"1":{"v":"\nWhen async/await was first introduced, attempting to use an `await` outside of an async function resulted in a SyntaxError. Many developers utilized immediately-invoked async function expressions as a way to get access to the feature.\n\n```js\nawait Promise.resolve(console.log('🎉'))\n// → SyntaxError: await is only valid in async function\n\n;(async function() {\n  await Promise.resolve(console.log('🎉'))\n  // → 🎉\n}())\n```\n- note: this IIFE here could be considered to be analogous to the `main` function of a C program (ie. it is the entrypoint— it is what gets called when you run `node filename.js`). Otherwise, we would be declaring the function, then calling it\n\nWithout top-level await, JavaScript developers often used async immediately-invoked function expressions just to get access to await. Unfortunately, this pattern results in less determinism of graph execution and static analyzability of applications. For these reasons, the lack of top-level await was viewed as a higher risk than the hazards introduced with the feature.\n","n":0.082}}},{"i":800,"$":{"0":{"v":"Feat","n":1}}},{"i":801,"$":{"0":{"v":"Math","n":1}}},{"i":802,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Pick random element in array\n```js\nfunction randomPick(array) {\n  let choice = Math.floor(Math.random() * array.length);\n  return array[choice];\n}\n```\n\n#### Generate random number between 1-10\n```js\nMath.floor(Math.random() * 10) + 1\n```\n","n":0.2}}},{"i":803,"$":{"0":{"v":"While","n":1},"1":{"v":"\n### Use `while(true)` and `break`\n- we can make a while loop that will continuously execute code until `break` is reached.\n```js\nwhile(true) {\n\ttry {\n\t\tawait client.query('select true as \"Connection test\";')\n\t\tbreak //if the promise above resolves, then break will be run and we will exit the while-block \n\t} catch(e) {\n\t\tawait sleep(1000)\n\t}\n}\n```\n","n":0.146}}},{"i":804,"$":{"0":{"v":"This","n":1},"1":{"v":"\n`this` only cares about execution context (where the fn was called; ie. call-site). it doesn't care about the scope chain\n\n`this` is essentially an implicit input to a function, thereby negatively impacting function purity. Think of `this` as an implicit argument that gets passed into the function.\n- ie. because of this impure nature, `this` shouldn't be used in [[Functional Programming|paradigm.functional]]\n\n`this` is context-bound and not instance-bound. If, for example, a method is passed on as a callback, it loses its context. If the method is then called like a function, the context will be undefined. As such, in order to avoid this scenario, the `this` context has to be captured in the method. This can be done either by wrapping the method (() => f()), using a field with an arrow function instead (f = () => {}), or using a bound version of it using bind (f = f.bind(`this`)).\n\nThe scope chain encompasses the prototype chain\n- Expl. Imagine 2 functions: inner() and outer(). If a variable is used within inner() and it does not exist within that function's context, it will look at its prototype to see if it exists (prototype chain). If it goes all the way up the chain and still doesn't exist, then the scope of outer() will be considered.\n- Expl. If this were illustrated as 2 for loops, the scope chain would be the `i` iterator, while prototype would be `j`\n\n`return` is a keyword which returns us to the immediate outer execution context (to continue parsing at the point directly after where the function was called)\n\n## Classes\nThe behavior of `this` in classes and functions is similar, since classes are functions under the hood. There are some important caveats:\n- methods of a class can be accessed on `this`.\n\t- We might assume that this means that the method is a property on the `this` object. This is a false assumption. The reality is that the method is a property on the class itself (which is the prototype of `this`). This means that when we are accessing a method like `this.moveUnit`, the `moveUnit` is being accessed by traversing this prototype chain, up from `this` and reaching the class itself, upon which the method is found.\n- if a method is marked `static`, then it is not added to the prototype of `this`.\n\n## Arrow functions\nUntil arrow functions, every new function defined its own `this` value (a new object in the case of a constructor, `undefined` in strict mode function calls, the base object if the function is called as an \"object method\", etc.). \n- This proved to be less than ideal with an object-oriented style of programming.\n\nAn arrow function does not have its own `this`; the `this` value of the enclosing execution context is used.\n\nIn arrow functions, `this` retains the value of the enclosing lexical context's `this`, whereas a normal function's `this` value is whatever the caller's scope is.\n- In global code, it will be set to the global object. Here, the enclosing lexical context (ie. the context of `foo`) is the global context:\n```js\nvar globalObject = this;\nvar foo = (() => this);\nconsole.log(foo() === globalObject); // true\n// `true` returned because the function `foo` is in the global scope\n```\n\nThis logic follows that if we had an arrow function within another function, the arrow functions `this` value would retain the value of the outer functions `this` object.\n```js\nfunction outer() {\n\tconst outerFunctionContext = this\n\tconst inner = (() => this)\n\tconsole.log(outerFunctionContext === inner())\n}\n```\n","n":0.042}}},{"i":805,"$":{"0":{"v":"Spread Syntax","n":0.707},"1":{"v":"\nSpread syntax is like zooming. When you see '...', you should think \"expand\"\n- what's important is the receiver (what we are spreading it out into (eg. [], {}, () )\n- this gives the ability for us to spread it out into an array, object, or function (as args). Notice these all accept iterables\n\n## UE Resources\n- the copying process is shallow\nhttps://zhenyong.github.io/react/docs/jsx-spread.html\nhttps://stackoverflow.com/questions/31048953/what-do-these-three-dots-in-react-do\n","n":0.129}}},{"i":806,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Remove a property from an object\nThis will remove id from the obj:\n```js\nConst { id, ...rest } = obj\n```\n","n":0.229}}},{"i":807,"$":{"0":{"v":"Hoist","n":1},"1":{"v":"\nOnly declarations are hoisted, not initializations\n- ex. `const name` is hoisted, `name = \"Kyle\"` is not.\n- therefore if you access a variable before it's declared, the value is always `undefined`\n\n`var`-declared variables are hoisted, meaning you can refer to the variable anywhere in its scope, even if its declaration isn't reached yet. You can see var declarations as being \"lifted\" to the top of its function or global scope.","n":0.121}}},{"i":808,"$":{"0":{"v":"For","n":1},"1":{"v":"\n### Run loop indefinitely, and stop when a certain condition is met\n```js\nfor (let turn = 0;; turn++) {\n  if (state.parcels.length == 0) {\n    console.log(`Done in ${turn} turns`);\n    break;\n  }\n  // do your stuff here\n}\n```\n\n## `label`\nA `label` allows us to identify a statement, and then later refer to it using a `break` or `continue` statement\n\nAny break or continue that references label must be contained within the statement that's labeled by label. Think about label as a variable that's only available in the scope of statement.\n\nImagine we have 2 nested loops and the inner loop has some condition whereby it should return execution to the outer loop. In this case, what we would do is label the outer loop and create the condition from within the inner loop.\n\n```js\nloop1: for (let i = 0; i < 3; i++) {\n  loop2: for (let j = 0; j < 3; j++) {\n    if (i === 1 && j === 1) {\n      continue loop1;\n    }\n    console.log(`i = ${i}, j = ${j}`);\n  }\n}\n```\n\nWe can also use `break` with labels. In this case, if `break loop1` is encountered from within the inner loop, then the execution of the outer loop is terminated.\n\n## `for..of` vs `for..in`\n`for..of` should be used to loop over iterable objects, like [[Array|js.lang.array]], [[Map|js.lang.type.map]], [[Set|js.lang.type.set]], String, [[Generator|js.lang.functions.generator]]\n- an object is considered iterable if it adheres to the [[iterator protocols|js.lang.iterator]]\n`for..in` should be used with objects, and is used to iterate over the properties of an object\n\n### For..of\nGet index in `for..of`\n```js\nfor (const [i, value] of arr.entries()) {\n  indexMap[i] = value\n}\n```\n\n","n":0.063}}},{"i":809,"$":{"0":{"v":"Do While","n":0.707},"1":{"v":"\nBenefit of a `do...while` over a regular `while` is that `do...while` will always run at least once. A `while` will only run as long as the condition is satisfied. \n\nTherefore,\n```js\ndo {\n    do_work();  \n} while (condition);\n\n// is equivalent to:\ndo_work();\n\nwhile (condition) {\n    do_work();\n}\n```\n\n### Easy retry process with do-while\n```js\n// each time the loop runs, it checks to see if the place equals the address. If did, then there is an automatic retry done, accomplished with the do-while\nVillageState.random = function(parcelCount = 5) {\n  let parcels = [];\n  for (let i = 0; i < parcelCount; i++) {\n    let address = randomPick(Object.keys(roadGraph));\n    let place;\n    do {\n      place = randomPick(Object.keys(roadGraph));\n    } while (place == address);\n    parcels.push({place, address});\n  }\n  return new VillageState(\"Post Office\", parcels);\n};\n```\n","n":0.092}}},{"i":810,"$":{"0":{"v":"Destructuring","n":1},"1":{"v":"\nWith the destructuring technique we can pull out values from an array or object declaratively\n```js\nconst arr = [1, 2, 3, 4]\nconst [ a, , b ] = arr\n// a = 1\n// b = 4\n```\n- when spreading, we are unpacking a variable one level deeper (into an array, object etc)\n    - the `...` is unpacking, the `{}` is putting it into an object\n    - ***ex.*** - if we had an array of objects, the unpacking by itself would result in us having an invalid javascript, since it would just be a group of objects with no \"bucket\" to house them (such as an array or object). This is why `{ ...arrayOfObjects }` will result in those \"stray\" objects having found a home in the surrounding `{}`\n\n```js\nnewObj = {\n    0: {},\n    1: {},\n    ...\n}\n```\n\n```js\n<Component a={obj.a}, b={obj.b} c={obj.c} /> === <Component {...obj} />\n```\nwe can rename the object keys of a destructured object like so:\n- an object is returned from calling `useQuery` with fields `loading` and `error`, which we rename.\n\n```js\nconst { loading: queryLoading, error: queryError, data } = useQuery()\n```\n\nDestructuring creates a shallow copy\n\n# UE Resources\n[advanced destructuring](https://dmitripavlutin.com/javascript-object-destructuring/)\n","n":0.074}}},{"i":811,"$":{"0":{"v":"Coercion","n":1},"1":{"v":"\n# Object::String\nThe default conversion from an object to string is `[object Object]`\n- when we see `[object Object]` somewhere, it would seem to imply that the object is getting coerced into a string somewhere along the line, thus resulting in us seeing `[object Object]` \n- check to see if you didn't properly stringify json\n- output from `console.log` is captured at the application level, meaning that we won't get access to the logs if we are using multiple applications\n\t- ex. if we are using Azure Functions in our app, we are using multiple applications, since our codebase exists in a different application than our Azure Functions. \n","n":0.098}}},{"i":812,"$":{"0":{"v":"Async Iterator","n":0.707},"1":{"v":"\nAn asynchronous iterator allows us to [[iterate|general.patterns.behavioural.iterator]] over data that comes asynchronously, on-demand.\n- ex. download something chunk-by-chunk over a network.\n- in other words, it allows you to access asynchronous data sequentially.\n\nAsync iterators can be used when we don’t know the values and the end state we iterate over. Instead, we get [[Promises|js.lang.promises]].\n\n`Symbol.asyncIterator` is a built-in [[symbol|js.lang.type.symbol]] that enables an object to be async iterable.\n- This enables us to use a `for-await-of` loop with async iterators, which allows us to loop over the async iterable.\n    - ex. Imagine having an array of promises, and each promise represents some data that will arrive in the next chunk. We can loop over each promise, and do something with that data when it arrives. Only once the current promise has resolved will it move onto the next.\n- `Symbol.asyncIterator` is a method that returns the default AsyncIterator for an object. If the `asyncIterator` property is on an object, it is an Async Iterable, and therefore we can use `for-await-of`.\n- recall that at its base, an iterator must know 2 things: where it currently is, and how to get to the next item.\n\n```js\nconst asyncIterable = {\n  [Symbol.asyncIterator]() {\n    return {\n      i: 0,\n      next() {\n        if (this.i < 3) {\n          return Promise.resolve({ value: this.i++, done: false });\n        }\n\n        return Promise.resolve({ done: true });\n      }\n    };\n  }\n};\n\n(async function() {\n   for await (let num of asyncIterable) {\n     console.log(num);\n   }\n})();\n```\n\nAn async iterator is like an iterator except that its `next()` method returns a promise that resolves to the `{value, done}` object. A regular iterator's `next()` method would simply return the object (`{value, done}`).\n\nThere are currently no built-in JavaScript objects that have the `[Symbol.asyncIterator]` key set by default.\n- Streams do, but they are from Node.js. In fact, async iterators are very useful when dealing with streams\n\nA great use case for async iteration is when we need to fetch data from a remote paginated dataset.\n- https://www.nodejsdesignpatterns.com/blog/javascript-async-iterators/\n\nSince the ability of an iterator to be async is determined by the presence of a `[Symbol.asyncIterator]()` property on an object, we can define our own:\n```js\nconst myAsyncIterable = {\n    async* [Symbol.asyncIterator]() {\n        yield \"hello\";\n        yield \"async\";\n        yield \"iteration!\";\n    }\n};\n\n(async () => {\n    for await (const x of myAsyncIterable) {\n        console.log(x);\n        // expected output:\n        //    \"hello\"\n        //    \"async\"\n        //    \"iteration!\"\n    }\n})();\n```\n\nAnother example:\n```js\nconst asyncIterable = [1, 2, 3];\nasyncIterable[Symbol.asyncIterator] = async function*() {\n    for (let i = 0; i < asyncIterable.length; i++) {\n        yield { value: asyncIterable[i], done: false }\n    }\n    yield { done: true };\n};\n\n(async function() { \n    // The for-await-of loop will wait for every promise it receives to resolve before moving on to the next one\n    for await (const part of asyncIterable) {\n        console.log(part);\n    }\n})();\n```\n\n#### Consuming paginated APIs\n- https://blog.risingstack.com/async-iterators-in-node-js/\n","n":0.048}}},{"i":813,"$":{"0":{"v":"Errors","n":1},"1":{"v":"\nWhen the `throw` statement is called, execution of the current function will stop and control will be passed to the first catch block in the call stack.\n- If no catch block exists among caller functions, the program will terminate.\n\nexecuting `throw` generates an exception that penetrates through the call stack.\n\nIn practice, the exception you throw should always be an Error object or an instance of an Error subclass, such as RangeError\n- this is because code that catches the error may expect certain properties, such as [message](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Error/message), to be present on the caught value. For example, web APIs typically throw DOMException instances, which inherit from Error.prototype.\n\n### `try...catch`\nIf any statement within the try block (or in a function called from within the try block) throws an exception, control immediately shifts to the catch block. \n- If no exception is thrown in the try block, the catch block is skipped. \n\n#### `finally`\n- The finally block executes after the try and catch blocks execute but before the statements following the `try...catch` statement.\n- You can use the finally block to make your script fail gracefully when an exception occurs. \n    - ex. you may need to release a resource that your script has tied up (e.g. close db connection, close a file)\n- code within the `finally` will *always* execute. Even if the `catch` block returns a value, a return in the `finally` block will overwrite it.\n    - therefore, if there is a return value in the `finally` block, that will always be the return value of the `try..catch..finally` block.\n","n":0.063}}},{"i":814,"$":{"0":{"v":"Decorator","n":1},"1":{"v":"\nA Javascript [[decorator|general.patterns.structural.decorators]] is an expression which returns a function and can take a `target`, `name` and `property descriptor` as arguments. \n- You apply it by prefixing the decorator with an `@` character and placing this at the very top of what you are trying to decorate. \n- Decorators can be defined for either a class or property.\n\nDecorators get called first before classes are compiled.\n- In Javascript when we define a new `class` with methods, we are really just installing a descriptor on the object's prototype.\n\nA decorator is just an expression that will be evaluated and has to return a function.\n\n### Readonly decorator\n```js\nfunction readonly(target, key, descriptor) {\n  descriptor.writable = false\n  return descriptor\n}\n\nclass Horse {\n  @readonly\n  neigh() console.log('*neigh!*')\n}\n```\n\n### Class decorators\n`@decorator` is a function that returns a decorator.\n- a decorator itself is a function that takes in a class and returns that same class.\n","n":0.084}}},{"i":815,"$":{"0":{"v":"Closures","n":1},"1":{"v":"\nA closure is created when a function is defined inside another function, and the inner function has access to the variables of the outer function even after the outer function has finished execution and has `return`ed.\n- recall that functions return as soon as the body code is executed.\n\nA closure is an expression (most commonly, a function) that can have free variables together with an environment that binds those variables (that \"closes\" the expression).\n\n```js\nfunction outer() {\n  const outerVar = 'I am from outer';\n\n  function inner() {\n    console.log(outerVar);\n  }\n\n  return inner;\n}\n\nconst closureFunction = outer();\nclosureFunction(); // Outputs: \"I am from outer\"\n```\n\nSince a nested function is a closure, this means that a nested function can \"inherit\" the arguments and variables of its containing function. In other words, the inner function contains the scope of the outer function.\n\nWhen a variable is \"captured\" by a closure, it means that the inner function (the closure) maintains access to that variable even after the outer function in which the variable was originally defined has finished executing. \n\nsince the inner function has access to the scope of the outer function, the variables and functions defined in the outer function will live longer than the duration of the outer function execution, assuming the inner function manages to survive beyond the life of the outer function.\n\n```js\n// The outer function defines a variable called \"name\"\nconst pet = function (name) {\n  const getName = function () {\n    // The inner function has access to the \"name\" variable of the outer function\n    return name;\n  };\n  return getName; // Return the inner function, thereby exposing it to outer scopes\n};\nconst myPet = pet(\"Vivie\");\n\nconsole.log(myPet()); // \"Vivie\"\n```\n\nAn object containing methods for manipulating the inner variables of the outer function can be returned:\n```js\nconst createPet = function (name) { // the name variable is accessible to the inner functions\n  let sex;\n\n  const pet = {\n    // setName(newName) is equivalent to setName: function (newName)\n    // in this context\n    setName(newName) {\n      name = newName; // The inner variables of the inner functions act as safe stores for the outer arguments and variables, and are \"persistent\" and \"encapsulated\"\n    },\n\n    getName() {\n      return name;\n    },\n\n    getSex() {\n      return sex;\n    },\n\n    setSex(newSex) {\n      if (\n        typeof newSex === \"string\" &&\n        (newSex.toLowerCase() === \"male\" || newSex.toLowerCase() === \"female\")\n      ) {\n        sex = newSex;\n      }\n    },\n  };\n\n  return pet;\n};\n```\n\nconst pet = createPet(\"Vivie\");\nconsole.log(pet.getName()); // Vivie\n\npet.setName(\"Oliver\");\npet.setSex(\"male\");\nconsole.log(pet.getSex()); // male\nconsole.log(pet.getName()); // Oliver\n\n\n## Closures and Classes\nWhen a closure returns an object, it can function as an alternative to a class.\n- the key-value pairs of the closure correspond to the properties and methods of the class\n\nNotice that the following closure can be implemented as a class:\n```js\nconst UserClosure = ({ firstName, lastName, age, occupation }) => {\n\treturn ({\n\t\tdescribeSelf: () => {\n\t\t\tconst msg = `My name is ${firstName} ${lastName}, I am ${age} years old and I work as a ${occupation}`\n\t\t\treturn msg\n\t\t},\n\t\tgetAge: () => {\n\t\t\treturn age;\n\t\t},\n\t\tshowStrength: () => {\n\t\t\tlet howOld = age;\n\t\t\tlet output = 'I am';\n\t\t\twhile (howOld-- > 0) {\n\t\t\t\toutput += ' very';\n\t\t\t}\n\t\t\treturn output + ' Strong';\n\t\t}\n\t})\n}\n```\n\nClosures and classes behave differently in JavaScript with a fundamental difference: closures support [[encapsulation|paradigm.oop.encapsulation]], while JavaScript classes don’t support it.\n- in other words, we can create a closure where individual members of the closure are invisible to the outside world.\n\nWhen opting for closures over classes, closures offer simplicity, since we don’t have to worry about the context that this is referring to.\n- If we are creating multiple instances of an object, classes will best suit our needs. Meanwhile, if we don’t plan to create multiple instances, the simplicity of closures may be a better fit for our project.\n\n* * *\n\n### Simple state-management with closure\n```ts\nfunction makeState<S>() {\n  let state: S\n  function getState() {\n    return state\n  }\n  function setState(x: S) {\n    state = x\n  }\n  return { getState, setState }\n}\n\nconst { getState, setState } = makeState()\nsetState(1)\nconsole.log(getState()); // 1\nsetState(2)\nconsole.log(getState()); // 2\n```\n\nObserve that you can pass a type like so:\n```ts\nmakeState<number>()\n```\n\nThen, when you go to use `getState` and `setState`, the generic `S` will become `number`\n\n* * *\n\n\n### Inner/Outer function illustration\nFrom the context of an inner scope, there is a: local scope, any number of closure scopes, and a global scope. The closure scopes represent the different scopes of the surrounding code. If our current scope is nested 3 levels deep then there are 2 closure scopes. Within these scopes, there may exist variables.\n\nbecause of how lexical scope works, when we call a function that accesses a variable from outside its scope, it will capture it at the very time the function is created. This means even if that value changes in the future, the value it had *at the time* it was captured will be used.\n\n```js\nvar outer = () => () => {}\nvar innerFunc = outer()\n```\n- above, `innerFunc` causes `outer()` to execute, returning a function and setting its value to it. `innerFunc` has access to the local variables of its containing object (normally a containing function). Therefore, these \"sibling\" local variables are changeable from outside the function.\n- Think of a closure as the lifeline that an inner function extends to the variables (that the inner function has used) defined in the outer function. They continue to exist because the closure exists. In other words, the inner function closes over (ie. captures/remembers) the variables defined in the outer function.\n- Conceptually (but not actually), the closed over function (`outer`) has all of its variables put into an object. That is how `inner` is able to access those values. Something like this is happening:\n```js\nfunction outer() {\n    var x = 1;\n\n    return function inner(){\n        return x;\n    };\n}\n```\nmakes 2 objects like this:\n```js\nscopeOfOuter = {\n    x: 1\n}\n\nscopeOfInner = {};\n```\nthen `scopeOfOuter` is set as the prototype of `scopeOfInner`, so when we try to access the value of x with `return scopeOfInner.x`, we see that `scopeOfInner` doesn't have an `x` property, so it goes up the prototype chain and finds an `x` property on `scopeOfOuter`\n\n```js\nObject.setPrototypeOf( scopeOfInner, scopeOfOuter );\n```\n\n- Conceptually, the structure of a closure is not mutable. In other words, you can never add to or remove state from a closure\n- closures are a subset of lambdas\n\n### How scope enables closures to happen\nIn JS, a scope is created by a function or code block.\n- When we have 2 separate functions at the same level of the code, both can use the same variable names and not have collisions. But what happens when one fn (`inner`) goes within another (`outer`)?\n\nIn the following example, `myInnerFunc` is an instance of `innerFunc`, with the enhanced benefit of having access to `outerVar`\n- The reason it has access is because of lexical scope, which (importantly) is defined before any javascript code has run (ie. analyzed just by the source code)\n![dad83091392736b4fc218299b2073d6d.png](:/8e496538fa28463a9e908e9164c39882)\n- Therefore, a closure is a function that has access to its lexical scope, *even though* that function was executed from outside of that lexical scope.\n\t- Simpler, the closure is a function that remembers the variables from the place where it is defined (and not where it was executed)\n- A rule of thumb to identify a closure: if you see in a function an alien variable (not defined inside the function), most likely that function is a closure because the alien variable is captured.\n\n### Analogy\nImagine a magical paintbrush with an interesting property. If you paint with it some objects from real life, then the painting becomes a window you can interact with.\n\nThrough this window, you can move the painted objects with your hands.\n\nMoreover, you can carry the magical painting anywhere, even far from the place where you’ve painted the objects. From there, through the magical painting as a window, you can still move the objects with your hands.\n\nThe magical painting is a closure, while the painted objects are the lexical scope.\n![c40e3c1034d769d6bc5aa8f2100a83e7.png](:/cee3345f41c44ceca614faea5e5cc400)\n\n### Stale closures\n- stale closures capture variables that have outdated values.\n```js\nfunction createIncrement(i) {\n  let value = 0;\n  function increment() {\n    value += i;\n    console.log(value);\n    const message = `Current value is ${value}`;\n    return function logValue() {\n      console.log(message);\n    };\n  }\n\n  return increment;\n}\n\nconst inc = createIncrement(1);\nconst log1 = inc(); // logs 1\nconst log2 = inc();             // logs 2\nconst log3 = inc();             // logs 3\nlog1();             // logs \"Current value is 1\"\nlog2();             // logs \"Current value is 2\"\nlog3();             // logs \"Current value is 3\"\n```\n- `log{1,2,3}()` are stale closures, because it has already captured the value *at the time* that `inc()` was called. What's important to note here is that `inc()` is called 3 times. Every time it is called, it runs through the `increment` function that was closed over. It then returns that value, and holds it (within a function called `logValue` that prints out the held value). In other words, it does not get updated with each subsequent call of `inc()`. It has already held onto that value, and there is nothing it can do to change that fact.\n- Therefore, if we want to capture the freshest value, we have to figure out which closure it is that has those freshest variables.\n\t- Here, that variable would be the *latest* call of `inc()`.\n\n### Closures vs Objects\nclosure offers granular change control and automatic privacy.\nobject offers easier cloning of state\n\nClosures are made every time we create an event handler, a promise, setTimeout, and even within `useEffect` in React.\n\n### Node Debugger\nclosure scope is outside of local scope\nthere are multiple layers of closure state\n\n![](/assets/images/2021-10-26-09-37-54.png)\n","n":0.026}}},{"i":816,"$":{"0":{"v":"Classes","n":1},"1":{"v":"\n## Constructor\nA constructor's purpose is to allocate you an object and then return immediately.\n\n## Class-related Keywords\n### `super`\nThe `super` keyword is used to access and call functions on an object's parent.\n- ex. If we have `Dog` extend from `Mammal`, then calling `super.get` in `Dog` will call the `get` method of the `Mammal` class.\n\nspec: When used in a constructor, calling `super` calls the `constructor` method of the parent.\n- the `super` keyword appears alone and must be used before the `this` keyword is used.\n\nNote that a rectangle is more generic than a square. Therefore, we can create a `Square` class that extends a `Rectangle` class, and simplify the interface of the `Square`.\n```ts\nclass Rectangle {\n    constructor(height, width) {\n        this.height = height;\n        this.width = width;\n    }\n}\nclass Square extends Rectangle {\n    constructor(length) {\n        super(length)\n    }\n}\n```\n\nWe can call `super` on methods that have the `static` modifier.\n","n":0.085}}},{"i":817,"$":{"0":{"v":"Callbacks","n":1},"1":{"v":"\nA callback is a function you provide as an argument to another function. When that other function has completed its task, it will execute the provided function. In the meantime, however, the code coming after the request will be executed regularly.\n- spec: we see that `bcrypt.genSalt` takes in salt as its argument. given its position, this tells us that the function it is a part of will ultimately return that item. looking at the inner one, `hash` is now in that position. that function `bcrypt.hash` returns a hash.\n```js\nbcrypt.genSalt(10, (err, salt) => {\n    if (err) return next(err)\n    bcrypt.hash(user.password, salt, (err, hash) => {\n      if (err) return next(err)\n      user.password = hash\n      next()\n    })\n  })\n```\n\nThis is also the same value that will be used as the parameter taken in with `.then` when working with promises\n```js\n  bcrypt.genSalt(10)\n    .then(salt => {\n      return bcrypt.hash(user.password, salt)\n    })\n    .then(hash => {\n      user.password = hash\n      next()\n    })\n    .catch(err => {\n      console.error(err)\n      return next(err)\n    })\n```\n\nA callback is a function that happens after we call another function, but the catch is, they’re coupled.\n- However, being coupled in this way doesn’t mean we can’t reuse the callback function.\n\nCallback hell -like structures like below can be seen as a pattern similar to piping. See the same callback-hell code written with async-await:\n```js\n// callbacks\nreadFile(request.fileToRead, data => \n  writeFile(request.fileToWrite, data, status =>\n    response.send(status)\n  )    \n);\n\n// async-await\nconst readFileData = await readFile(request.fileToRead);\nconst writeStatus = await writeFile(request.fileToWrite, readFileData);\nresponse.send(writeStatus);\n```\n","n":0.066}}},{"i":818,"$":{"0":{"v":"Bitwise","n":1},"1":{"v":"\n### NOT operator (`~`)\nthe tilde `~` Bitwise NOT operator is commonly used right before an indexOf() to do a boolean check (truthy/falsy) on a string.\n\n```js\n// prior to ES7\n~foo.indexOf(\"w\")\n\n// As of ES7\nfoo.includes(\"w\")\n```","n":0.18}}},{"i":819,"$":{"0":{"v":"Array","n":1},"1":{"v":"\nArrays technically aren't a type in Javascript.\n\nSince everything in Javascript is an object, `Array` doesn't follow the same limitations as an actual [[array|general.lang.types.array]], such as being of fixed length.\n- for example, the `length` property returns the size of the internal storage area for indexed items of the array. \n\nin JavaScript, array indexes are coerced to strings by an implicit `toString()` call.\n\n## When to use Arrays for lists of data\nIf you have a list of data that you are going to be transforming somehow, or mapping over it somehow, you should try to structure that data as an array\n- expl: if you had an object of keys, and each key was a list item, then to operate on that might get a little messy and imperative. Imagine if we stored taxes in an object like this:\n```js\nconst taxes = {\n    GST: 0.05,\n    PST: 0.07\n}\n```\nWe can loop over this with `Object.entries` without problem, but now imagine that we want to map over each entry, and display it in the UI in React. That might look something like this:\n```jsx\nObject.entries(taxes).map(tax =>\n    <>\n        <p>Tax type: {tax[0]}</p>\n        <p>Amount: {tax[1]}</p>\n    </>\n)\n```\n\nAlternatively, imagine we structured taxes like this:\n```js\nconst taxes = [\n    {\n        jurisdiction: 'PST',\n        percentage: 0.05,\n    },\n    {\n        jurisdiction: 'GST',\n        percentage: 0.05,\n    },\n]\n```\n\nAlthough this is less succinct, there is a benefit to doing it this way. This is how we would handle that React task:\n```js\ntaxes.map(tax => {\n    <>\n        <p>Tax type: {tax.jurisdiction}</p>\n        <p>Amount: {tax.percentage}</p>\n    </>\n})\n```\n\nThis is yet another example of why you should go from a data-consumption perspective (in other words, consider how it will be used; think of the end-user of that code.)\n\n* * *\n\n### Extra commas in Array literals\n```js\nconst fish = [\"Lion\", , \"Angel\"];\nconsole.log(fish);\n// [ 'Lion', <1 empty item>, 'Angel' ]\n```\n- note that the second item is \"empty\", which is not exactly the same as the actual `undefined` value. When using array-traversing methods like `.map`, empty slots are skipped. However, index-accessing fish[1] still returns `undefined`.\n- naturally, if the final element is followed by a comma, that gets removed.","n":0.055}}},{"i":820,"$":{"0":{"v":"Set Theory","n":0.707},"1":{"v":"\n### Union ($\\cup$)\nGet elements that are in either array\n- see [[union Dendron node|math.set-theory.op.union]]\n```js\nfunction getUnion(arr1, arr2) {\n  const mergedArray = arr1.concat(arr2);\n  const unionSet = new Set(mergedArray);\n  return Array.from(unionSet);\n}\n```\n\n### Intersection ($\\cap$)\nGet elements that both arrays share in common\n- see [[intersection Dendron node|math.set-theory.op.intersection]]\n\n```js\nlet intersection = arrA.filter(x => arrB.includes(x))\n```\n\n### Difference\nGet elements from `arrA` that are not in `arrB`\n```js\nlet difference = arrA.filter(x => !arrB.includes(x));\n```\n\n### Symmetrical Difference\nGet elements that are in `arrA` but not in `arrB`, *and* elements that are in `arrB` but not in `arrB`\n```js\nlet difference = arrA\n    .filter(x => !arrB.includes(x))\n    .concat(arrB.filter(x => !arrA.includes(x)));\n```","n":0.106}}},{"i":821,"$":{"0":{"v":"Method","n":1},"1":{"v":"\nOperating on arrays offers `O(1)` search speed if the index is known, but adding/removing an element is slow since size of array cannot change once it's created\n- note: is this relevant for Javascript?\n\n`forEach` is almost always used for side-effects at the end of a chain\n","n":0.149}}},{"i":822,"$":{"0":{"v":"Filter","n":1}}},{"i":823,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Filter on multiple factors\n```js\nreturn restaurants\n    .filter(resto => {\n        if (veganFriendly && resto.veganFriendly === false) {\n            return false\n        }\n        if (maxPrice < resto.price) {\n            return false\n        }\n        if (maxDistance < resto.distance) {\n            return false\n        }\n        return true\n```","n":0.164}}},{"i":824,"$":{"0":{"v":"Sort","n":1},"1":{"v":"\nRemember, sort modifies the original array. If you need to sort a `readonly` array, you must first copy it\n\nThe ES6 `sorted` method takes in a `comparator` callback.\n\n### Comparator callback\neach array element is sorted according to the return value of the callback.\n- `undefined` values are just popped onto the end\n\nIf we omit the comparator callback, then the sort order is ascending/alphabetical (ie. as a string). Therefore, an array of numbers becomes an array of strings, then the strings are compared alphabetically, making `[1, 10, 100, 3, 5, 7, 9]` perfectly sorted.\n\nIf possible, prefer to use Lodash's `sortBy`, as it provides more sensible defaults\n","n":0.099}}},{"i":825,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Sort array of numbers (asc)\n```js\nconst sorted = [...positions].sort((a, b) => a - b)\n```\n\n### Sort array of strings (alphabetically)\n```js\nlist.sort(function (a, b) {\n    return a > b ? 1 : -1;\n})\n```\n\n### Sort by multiple properties\n```js\nlist.sort((a, b) => {\n    if (b.ratings !== a.ratings) {\n        // first sort by ratings (desc)\n        return b.ratings - a.ratings\n    } else {\n        // then sort by id (desc)\n        return b.id - a.id\n    }\n})\n```\n\n### Sort by date (as string)\n```js\narray.sort((a,b) => new Date(b.date).getTime() - new Date(a.date).getTime();\n);\n```","n":0.114}}},{"i":826,"$":{"0":{"v":"Slice","n":1},"1":{"v":"\n- `splice()` changes the original array \n- `slice()` doesn't change the original array.\n\n- `splice()` returns the items that were removed from the array (often, this is simply discarded in favor of the effect of changing the original array via mutation)\n- `slice()` returns a new subarray that is constructed by using the original as a reference (therefore immutable)\n","n":0.132}}},{"i":827,"$":{"0":{"v":"Reduce","n":1},"1":{"v":"\nWhen calling reduce you specify a function that describes how the current array element will alter the accumulator. \n- It's like a functional version of a forEach loop with an additional variable that you change in the loop and return at the end.\n\nThe `reduce()` method executes a user-supplied “reducer” callback function on each element of the array, \n- the return value of the calculation on the previous element (the \"accumulator\") gets passed as input. \n\n`reduce` could just as easily have been named `transform`, for its ability to transform one data structure into another.\n\nAnytime you need to do filter and map on same array, use reduce\n\nAnytime you need to use `reduce` to get uniqueness out of an array of objects, use `Set`\n\nAnytime you need to skip over an iteration of the reducer function, simply return the accumulator\n\nThe final result of running the reducer across all elements of the array is a single value (whether that single value is an array, object, integer etc.).\n\nThe accumulator (ie. reducer function) is the hero of `.reduce()`. It is a story about how the accumulator changes at each step of the way (ie. at each array element). The way that the accumulator changes is described by the reducer function. The value returned from that reducer function describes what that accumulator looks like at that particular iteration.\n- ex. Take for instance a reducer function that adds up all elements in the array. The accumulator starts out as the array's first element (assuming no `defaultValue` passed). As we iterate through the array, the Accumulator is modified in a way that is described by the return value of the reducer function. Whatever value this function returns becomes the value of the Accumulator for that iteration.\n\nA reducer function takes the `accumulator` and `currentValue` arguments.\n- *accumulator* - the value resulting from the previous call to the callback.\n- *currentValue* - the value of the current element\n\n### `initialValue`\nIf initialValue is specified, that also causes `currentValue` to be initialized to the first value in the array. \nIf initialValue is not specified, `previousValue` is initialized to the first value in the array, and currentValue is initialized to the second value in the array.\n\n### Signature\nSignature\n```ts\nfunction reduce<Element, ReturnType>(arr: Element[], initialValue?: ReturnType) {\n    (acc: ReturnType, val: Element) => ReturnType\n}\n\n```\n","n":0.052}}},{"i":828,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Convert array of objects into Object\n```js\n// before\nconst data = [\n  { key: 1, name: \"A\", condition: true },\n  { key: 4, name: \"B\", condition: false },\n]\n\n// after\nconst data = {\n  1 : { key: 1, name: \"A\", condition: true },\n  4 : { key: 4, name: \"B\", condition: false },\n]\n```\n\n```js\nconst arrayToObject2 = (array, key) =>\n    array.reduce((obj, item) => ({\n            ...obj, [item[key]]: item\n        }), {}\n    )\n```\n\n### Return an object\n```js\nconst group = (collection, grouper) => {\n    return collection.reduce((acc, val) => {\n        const objectIndex = grouper(val)\n        if (acc[objectIndex]) {\n            acc[objectIndex].push(val)\n        } else {\n            acc[objectIndex] = [val]\n        }\n        return acc\n    }, {});\n}\n\nconsole.log(group([6.5, 4.2, 6.3, 6.8, 4, 3, 1], Math.floor))\n// { '4': [4.2], '6': [6.5, 6.3] }\n```\n\n### Remove duplicates from array\nThis shows how we can introspect on the array/object as it's being built up\n```js\nconst arrayWithNoDuplicates = myArray.reduce((acc, val) => {\n  if (acc.indexOf(val) === -1) {\n    acc.push(val)\n  }\n  return acc\n}, [])\n```\n\n### `.map()` implemented in `.reduce()`\n```js\nif (!Array.prototype.mapUsingReduce) {\n  Array.prototype.mapUsingReduce = function(callback, initialValue) {\n    return this.reduce(function(mappedArray, currentValue, currentIndex, array) {\n      mappedArray[currentIndex] = callback.call(initialValue, currentValue, currentIndex, array)\n      return mappedArray\n    }, [])\n  }\n}\n```\n","n":0.076}}},{"i":829,"$":{"0":{"v":"Map","n":1},"1":{"v":"\n### Async map\n```js\nconst deleteUser = async (user) => {\n    return await user.delete({\n        where: { id: user.id }\n    })\n}\nconst deleteUsers = async () => {\n    return Promise.all(users.map((user) => deleteUser(user)))\n}\n\n// later you can chain aync maps like this\ndeleteTweets().then(() => {\n    deleteUsers()\n})\n```\n","n":0.16}}},{"i":830,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Execute array of callbacks\n```js\nconst iterate = (count, callback) =>\n  [...Array(count)].map((_, i) => callback(i))\n```\n\n#### Generate empty array\n```js\nconst emptyArray = Array(5).fill(null)\n```\n\n#### Surgically remove one item from an array\n```js\n    return [\n        ...bigArray.slice(0, itemToRemoveIndex),\n        ...bigArray.slice(itemToRemoveIndex + 1)\n    ]\n```\n\n#### Surgically replace one item in an array\n```js\n    return [\n    ...bigArray.slice(0, itemToReplaceIndex),\n        itemToInsert //if item is not object\n        { ...array[itemToReplaceIndex], ...itemToInsert } //if item is object\n        ...bigArray.slice(itemToReplaceIndex + 1)\n    ]\n```\n\n#### Move elements within array\n```js\nfunction swapInArrayByIndex(arr, i1, i2){\n    let t = arr[i1];\n    arr[i1] = arr[i2];\n    arr[i2] = t;\n}\n\nfunction moveBefore(arr, el){\n    let ind = arr.indexOf(el);\n    if(ind !== -1 && ind !== 0){\n        swapInArray(arr, ind, ind - 1);\n    }\n}\n\nfunction moveAfter(arr, el){\n    let ind = arr.indexOf(el);\n    if(ind !== -1 && ind !== arr.length - 1){\n        swapInArray(arr, ind + 1, ind);\n    }\n}\n```\n\n#### Create a new, shallow-copied Array instance from an iterable or array-like object.\n```js\nconst eventsMap: Map = {\n   \"2023-03-02\": [1, 2, 3] \n}\nconst eventsArray = Array.from(eventsMap, ([date, ids]) => ({ date, ids }));\n// [{ date: 2023-03-02, ids: [1, 2, 3] }]\n```","n":0.079}}},{"i":831,"$":{"0":{"v":"JSON","n":1},"1":{"v":"\n### Pretty print an object\n```js\nJSON.stringify(geofencePayload, null, 2)\n","n":0.378}}},{"i":832,"$":{"0":{"v":"Event Loop","n":0.707},"1":{"v":"\nWhen you create a new click handler on an HTML element, that click handler is being registered with the event loop, meaning that the event loop is now listening for that handler to be called. Every time it witnesses the event to happen, it causes a certain snippet of code to be executed.\n\nAn event loop must exist because of the fact Javascript is an asynchronous language. What happens is that we put what we need to do in the queue (eg. fetch data), and tell it \"when you're finished, do this\". The result is non-blocking code.\n\nThe event loop is a [[thread|os.thread]]. Javascript only has a single thread which listens for events and executes user specified functions when the event occurs.\n- although the application appears to run on a single thread from the programmer's perspective, the runtime internally uses multiple threads to handle tasks. The main difference is that the programmer does not have to deal with these internal threads and the challenges of coordination between them. All the programmer has to do is specify callback functions to be executed on the main thread when those background tasks have completed.\n\n# UE Resources\n- [High quality recommended resource for understanding Event Loop](https://www.youtube.com/watch?v=8aGhZQkoFbQ)\n","n":0.071}}},{"i":833,"$":{"0":{"v":"Event Bubbling","n":0.707},"1":{"v":"\nWhen you click on a button, you are inadvertently clicking on things that inside the button, as well as things that contain the button\n- ex. when you click on a strong tag, the event happens on that strong tag. But if nothing happens with that event, then it will bubble up to the surrounding element (ex. a Button). If the button wasn't listening for that specific event, then it will keep bubbling up onto the next surrounding element. \n- This is necessary, because if we have a `<strong>` tag inside a `<button>` and we click on that `<strong>` (which doesn't have a click handler), then our Button needs some way of being notified of that click event.\n- `event.target` is a reference to the object onto which the event was dispatched.\n    - ex. if we click a button, then `event.target` is the `<button>` tag.\n\nWe can think of this as the \"clicked element\".\n\nEvent capturing\n- the process of figuring out what got clicked. Happens after the bubbling phase\n- ex. when you click on a `span`, the browser starts at the top: \"user clicked on the `window` > `html` > `body` > `div` > `button` > `span`\"\n\nWhen we add an event listener, we can specify that it triggers on the capture phase or bubble phase\n- bubble phase is default, and is what we want 99% of the time.\n\n### currentTarget vs target\n`currentTarget`\n- what actually got clicked, prior to any bubbling happening\n- ex. we have a `span` inside a `button`. If the user clicks the `span`, then `span` is the `currentTarget` that got clicked, and the `button` is the `target`\n\n`target`\n- what you listened for a click on (what the bubbling ended up at)\n\n# UE Resources\n- [Event Delegation](https://javascript.info/event-delegation)","n":0.06}}},{"i":834,"$":{"0":{"v":"Doc Block","n":0.707},"1":{"v":"\n```\n/**\n * __useCreateOrUpdateAutomationMutation__\n *\n * To run a mutation, you first call `useCreateOrUpdateAutomationMutation` within a React component and pass it any options that fit your needs.\n * When your component renders, `useCreateOrUpdateAutomationMutation` returns a tuple that includes:\n * - A mutate function that you can call at any time to execute the mutation\n * - An object with fields that represent the current status of the mutation's execution\n *\n * @param baseOptions options that will be passed into the mutation, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options-2;\n *\n * @example\n * const [createOrUpdateAutomationMutation, { data, loading, error }] = useCreateOrUpdateAutomationMutation({\n *   variables: {\n *      createOrUpdateAutomationInput: // value for 'createOrUpdateAutomationInput'\n *   },\n * });\n */\n ```\n","n":0.094}}},{"i":835,"$":{"0":{"v":"Javascript Bundler","n":0.707},"1":{"v":"\nBundling is process of taking all of your Javascript code (including `node_modules`) and reducing it down to a single file.\n- the remaining bundle is as small as possible.\n\nBundling is normally a concept not found in [[Node.js|js.node]] code, since the size Node.js code on a server doesn't really matter.\n- however, there are use-cases for bundling Node.js code. For instance, if we are building multiple Lambdas in a single project using [[serverless-framework]], there is a benefit to bundling, since smaller Lambdas are downloaded from [[aws.svc.S3]] faster, reducing cold start times.\n  - in this case, use ESBuild\n\nThink of bundling as a [[MapReduce|general.patterns.map-reduce]] operation that maps over all source files and \"reduces\" them into a bundle.\n\nBefore ES modules were available in browsers, developers had no native mechanism for authoring JavaScript in a modularized fashion. This is why we are all familiar with the concept of \"bundling\": using tools that crawl, process and concatenate our source modules into files that can run in the browser.\n\n### Tree-shaking\nThe bundler looks through the code and uses only the imported modules (third-party and your own), thereby elminimating all the modules that aren't a part of the project.\n\n## UE Resources\n- [Building a JS bundler](https://cpojer.net/posts/building-a-javascript-bundler)\n","n":0.072}}},{"i":836,"$":{"0":{"v":"Jest","n":1},"1":{"v":"\nJest sets `NODE_ENV=test` by default\n\nThe data entering a component via props is the data, whose stability we want to test during unit tests. For instance, if we have a `<Table />` component, we are chiefly concerned with the data coming in and how it impacts `<Table />`. In this case, it would be: How long is the array of data that comes in (which ultimately translates to amount of rows)? What is the 'title' prop that we are expecting (which renders the title on the Table), etc. Given that the data is what we are interested in, begin to think of ways that data can fail on us. What happens if we get no data? Well, then I guess we expect the amount of `<Row />` components to be 0. This would mean a passing test, meaning that paticular piece of logic is guarded, and will remain robust unless the test breaks.\n\nKeep in mind we are forming our tests around: what is expected given this circumstance? In tests, an error case and happy-path are both equivalent in terms of value. They are simply scenarios that may happen, and we want to just clarify it for the record that \"this is what should happen, given this circumstance.\"\n\n- ex. in `const wrapper = shallow(<FeedbackSummary />);`, imagine we are the parent, and `FeedbackSummary` is the child we are testing. In other words, `FeedbackSummary` is the input, and `wrapper` is the output. In jest, we are basically saying \"hey FeedbackSummary, I'm going to give you some props and render you. The output will be called wrapper, and I will take my magnifying glass (aka enzyme library) and take a close look to ensure that the everything looks like it should\"\n\n## Transformation\nJest runs our code as plain Javscript. That means that any part of our code that isn't plain js (e.g. jsx, TypeScript types) needs to be transformed into plain js.\n- A transformer is just a synchronous function that transforms source files into plain js.\nsee [transform configuration option](https://jestjs.io/docs/configuration#transform-objectstring-pathtotransformer--pathtotransformer-object)\n\n### Implementations\n[ts-jest: a Jest transformer with source map support allowing us to use Jest to test TypeScript code](https://kulshekhar.github.io/ts-jest/)","n":0.053}}},{"i":837,"$":{"0":{"v":"Jest with TypeScript","n":0.577},"1":{"v":"\n\n### Problem with Jest mocks and Typescript\nMocking functions happens at runtime. At compile-time however, the Typescript compiler doesn't know that you have mocked those functions. During compilation, the TSC will do a type-check and it will complain that `mockReturnValueOnce` does not exist on that function.\n- To solve this, we have to extend our function so it both have the original types and the type of a Jest Mock function:\n```ts\njest.mock(\"../models/events\");\nimport { getEvents } from \"../models/events\";\nconst mockedGetEvents = getEvents as jest.MockedFunction<typeof getEvents>\n```\n\n# ts-jest\nts-jest is a jest transformer so that we can transform our source Typescript code into Javascript code for the purposes of our test.\n\n## API\n### `mocked(module, deep)`\nThe purpose of this function is to resolve typing errors\n- when we declare:\n```ts\nconst mockedFoo = mocked(foo, true)\n```\n\nall of the types within the `mockedFoo` module will be known to our jest test, and we will get no typing errors as a result.\n\n## Resources\n- [mocking classes](https://kulshekhar.github.io/ts-jest/docs/guides/mock-es6-class/)\n","n":0.082}}},{"i":838,"$":{"0":{"v":"Throw","n":1},"1":{"v":"\n`toThrow` executes a function passed in expect, and verifies that it throws an error.\n\nIn this case, you're not passing expect a function. You're executing a function that throws an error. toThrow is never executed, and even if it were, it would throw an error, telling you that it should receive a function, but received something else instead.\n\n`expect(someFunctionThatThrows())` is essentially the same as `expect(throw new Error())`.\n\n## Resources\nhttps://jestjs.io/docs/expect#tothrowerror\n","n":0.123}}},{"i":839,"$":{"0":{"v":"Mock","n":1},"1":{"v":"\nWhen we mock a function, its return value becomes of type `jest.Mock`\n\nMock functions provide benefits:\n- allows us to test the links between different units by erasing implementation details of a function\n- allows us to peek into the mock (via the `.mock` property), giving us valuable information on it like:\n    - if it was called or not \n    - which parameters were passed to it\n    - what the return value was\n- allows us to change the implementation of the original function\n- allows us to set return values\n\nWe can mock functions in 2 different ways:\n1. creating a mock function to use in test code\n  - `jest.fn` - mock a function\n  - `jest.mock` - auto-mock the functions of an entire module (e.g. axios), causing them to return mocks\n  - `jest.spyOn` - spy or mock a function\n2. writing a [manual mock](https://jestjs.io/docs/manual-mocks) to override a module dependency\n  - involves making a `__mocks__` directory\n\nMock functions return undefined by default, but we can inject return values into it:\n```js\nconst myMock = jest.fn();\nconsole.log(myMock());\n// > undefined\n\nmyMock.mockReturnValueOnce(10).mockReturnValueOnce('x').mockReturnValue(true);\n\nconsole.log(myMock(), myMock(), myMock(), myMock());\n// > 10, 'x', true, true\n```\n\nIn order to mock properly, Jest needs `jest.mock('moduleName')` to be in the same scope as the import statement (ie. top level).\n\n### Dependency Injection\nIdeally, we could just use [[general.patterns.dependency-injection]] to erase the implementation details of our dependencies so we can test the unit in isolation:\n```js\nconst doAdd = (a, b, callback) => {\n  callback(a + b);\n};\n\ntest(\"calls callback with arguments added\", () => {\n  const mockCallback = jest.fn();\n  doAdd(1, 2, mockCallback);\n  expect(mockCallback).toHaveBeenCalledWith(3);\n});\n```\n\nHowever, this requires us to write our source code in a manner that supports DI.\n\n### Remaking `jest.fn`\n- [source](https://www.pluralsight.com/guides/how-does-jest.fn%28%29-work)\n```js\n// 1. The mock function factory\nfunction fn(implementation) {\n  // 2. The mock function\n  const mockFn = function(...args) {\n    // 4. Store the arguments used\n    mockFn.mock.calls.push(args);\n    return implementation(...args); // call the implementation\n  };\n  // 3. Mock state\n  mockFn.mock = {\n    calls: []\n  };\n  return mockFn;\n}\n```\n\n### Axios example\nImagine we are testing a module (`Users`) which internally calls `axios.get`: \n```js\nclass Users {\n  static all() {\n    return axios.get('/users.json').then(resp => resp.data);\n  }\n}\n```\n\nOf course, we don't want our `Users.test.js` unit test to actually make a GET request. Therefore, we need to mock it with `jest.mock`","n":0.054}}},{"i":840,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Mocking a named import\n```ts\nimport { getTime } from './time';\n\njest.mock('./time', () => ({\n  getTime: () => '1:11PM',\n}));\n```\n\n### Mocking only the named import (and leaving other imports unmocked)\n```ts\nimport { getTime, isMorning } from './time';\n\njest.mock('./time', () => ({\n  ...jest.requireActual('./time'), \n  getTime: () => '1:11PM',\n  // isMorning will return its true value\n}));\n```\n\n### Mocking a default import\n```ts\nimport getDayOfWeek from './time';\n\njest.mock('./time', () => () => 'Monday');\n```\n\n### Mocking default and named imports\nIf you want to mock default and named imports, you’ll need to remember to use `__esModule: true`\n\n```ts\nimport getDayOfWeek, { getTime } from './time';\n\njest.mock('./time', () => ({\n  __esModule: true,\n  default: () => 'Thursday'\n  getTime: () => '1:11PM',\n}));\n```\n\n### Changing what the mock returns per test\nIf you wanted to have `getDayOfWeek` to return a different value per test, you can use `mockReturnValue` in each of your tests\n- If you only wanted to change what the mocked function returned for just one test, you need to use `mockReturnValueOnce`, otherwise you will change the mock for all other subsequent tests.\n```ts\nimport getDayOfWeek from './time';\njest.mock('./time', () => jest.fn());\n\ntest('App renders Monday', () => {\n  getDayOfWeek.mockReturnValue('Monday');\n});\n```\n\n### Mocking Third Party library (e.g. Axios)\n```ts\nimport * as axios from 'axios'\n\njest.mock('axios')\n\ntest('good response', () => {\n  axios.get.mockResolvedValue({ status: 200, data: {...} })\n})\ntest(\"bad response\", () => {\n  axios.get.mockRejectedValue({ ... });\n})\n```","n":0.071}}},{"i":841,"$":{"0":{"v":"API","n":1},"1":{"v":"\n### `jest.fn()`\nAllows us to mock a function\n\nUsed when we want to mock a function, and don't really care about its original implementation (often just mocking the return value)\n- returns a `spy` (which remembers each call that was made on it)\n- creates a stub\n- useful in removing dependencies to some backend (eg. server, API)\n\nIf we pass an argument, then it means we are passing a mock implementation:\n\n```js\nconst queryMock = jest.fn(() => Promise.resolve({ data: 1 })\nconsole.log(queryMock()) // Promise { pending }\n```\n\n### `jest.spyOn(object, methodName)`\nCreates a mock function like `jest.fn`, but also tracks calls to the method, allowing us to spy on them\n- this allows to ensure that the functions are called as expected (e.g. with correct arguments)\n\nspies still call the original code, they aren’t complete mocks. They are called spies because they \"spy\" on the implementation code.\n\n```ts\njest.spyOn(AutomationsDataSource.prototype, 'deleteAutomation').mockResolvedValue()\n```\n\nOr, we can add conditional logic:\n```ts\njest.spyOn(AutomationsDataSource.prototype, 'getAutomation').mockImplementation((id) => {\n  let automation: Automation\n  switch (id) {\n    case a1.id:\n      automation = a1\n      break\n    case a2.id:\n      automation = a2\n      break\n    default:\n      automation = undefined\n  }\n  return Promise.resolve(automation)\n})\n```\n\nWe can also mock as many methods as we want:\n```ts\njest.spyOn(AutomationsDataSource.prototype, 'getAutomation', 'deleteAutomation').mockImplementation((id) => {\n// ...\n```\n\n### `jest.mock()`\n`jest.mock` allows us to mock an entire module\n\nIf we call `jest.mock` with only 1 arg,\n- the module we pass will be replaced simply with `jest.fn`\n\nIf we call `jest.mock` with 2 args,\n- the module we pass will be replaced with the return value of the function provided (arg2).\n    - in other words, if there is 2 args, `jest.mock` says \"replace all occurrences of arg1 with arg2\"\n\nThis code tells jest, \"any time `calculateAge` is called, stick in 42 as its return value\"\n- we say \"any time you import `calculateAge` into some other module, replace its value with `jest.fn(() => 42)`\n```js\njest.mock('../calculateAge', () => {\n  return jest.fn(() => 42);\n});\n\n// This runs the function specified as second argument to `jest.mock`.\nconst calculateAge = require('../calculateAge');\ncalculateAge(); // Will return '42';\n```\n\n#### Example\n```ts\njest.mock('../components/box.jsx', () => 'Box')\n```\n- this is saying \"for the component we are testing, take all instances of the `Box` component and replace them with the text `Box`\"\n- what's happening underlying is that since components are just functions, we are saying \"replace our component function with the function `() => 'Box'`\"\n- we do this when we don't really care about all the props that come with box, but are just interested in the structure. `<Box />` doesn't really matter, since that isn't what we are testing.\n- we do this basically when our proptypes fails, because we aren't supplying data in the way proptypes would expect (ex. passing to `Box` margin as `25px` rather than the expected `25`)\n\nWhen we do `jest.mock('./my-file.js')`, it will turn all functions into Jest mock functions and all objects will have more properties on them like, `mockReturnValueOnce`\n\nspec: Mocks get hoisted, so if there is a variable you define *after* the mock, you probably won't be able to use it\n\n```js\njest.mock('./isValidCoupon', () => ({\n    // isValidCoupon: async () => true,\n    isValidCoupon: jest.fn().mockImplementation(async () => true),\n}))\n```\n\n### Mocking individual functions\nImagine our implementation code file is made up of individually exported functions, and we want to mock one of them. We can use `jest.requireActual` to import the original implementation, but mock individual functions:\n```ts\njest.mock('./automationVersions', () => {\n  const origModule = jest.requireActual('./automationVersions')\n  return {\n    ...origModule,\n    doesDraftExist: jest.fn(),\n  }\n})\n```\n\nThen, in our unit test we can mock the return value:\n```ts\nconst mockedDoesDraftExist = jest.mocked(doesDraftExist)\nmockedDoesDraftExist.mockReturnValue(true)\n```\n","n":0.043}}},{"i":842,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Testing that a class's method was called\n[ref](https://stackoverflow.com/questions/50091438/jest-how-to-mock-one-specific-method-of-a-class#answer-50108368)","n":0.354}}},{"i":843,"$":{"0":{"v":"Async Jest","n":0.707},"1":{"v":"\nBy default, Jest tests complete once they reach the end of their execution. Therefore, async tests will not work by default\n\n## Callbacks\nThis code doesn't work, because `fetchData` returns before `callback`\n```js\n// Don't do this!\ntest('the data is peanut butter', () => {\n  function callback(data) {\n    expect(data).toBe('peanut butter')\n  }\n\n  fetchData(callback);\n})\n```\n\nWe can solve this with the `done` arg\n```js\ntest('the data is peanut butter', done => {\n  function callback(data) {\n    try {\n      expect(data).toBe('peanut butter')\n      done()\n    } catch (error) {\n      done(error)\n    }\n  }\n\n  fetchData(callback)\n})\n```\nNow, Jest will wait until the `done` callback is called before finishing the test.\n- if never called, it will fail with a timeout error, which is the desired way of handling this.\n-  If we want to see in the test log why it failed, we have to wrap expect in a try block (as shown above)\n","n":0.087}}},{"i":844,"$":{"0":{"v":"Jekyll","n":1},"1":{"v":"\nJekyll is a static site generator, a la [[Nextjs|nextjs]] or Gatsby, but built in the Ruby ecosystem.\n\nWe can add a `.nojekyll` file to the root of the `pages` branch of our repo as a way to inform Github Pages that our static site is not a Jekyll site.\n- This should only be necessary if your site uses files or directories that start with underscores since Jekyll considers these to be special resources and does not copy them to the final site.","n":0.111}}},{"i":845,"$":{"0":{"v":"Java","n":1},"1":{"v":"\n# Commands\n### Compile & Run\n```sh\n# compile\n$ javac HelloWorld.java\n\n# run\n$ java HelloWorld\n```\n\nIt provides software for working with Java applications. Examples of included software are:\n- the virtual machine\n- a compiler\n- performance monitoring tools\n- a debugger\n\nrun the binary `/usr/libexec/java_home` to know where in the system Java is installed\n\n# Commands\n### List Java versions installed\n- `$ /usr/libexec/java_home -V`\n\n* * *\n\n## Resources\n- [VisualVM](https://visualvm.github.io/)\n  - provides a visual interface for viewing detailed info about a Java application as it is running in the JVM","n":0.114}}},{"i":846,"$":{"0":{"v":"Variables","n":1},"1":{"v":"\nInstance variables always get a default value.\n- integers, chars - 0\n- floating points - 0.0\n- booleans - false\n- references - null\n\n*Local variables* exist within a method.\n- they do not get default values\n\n## \"Global\" variables\nWhile there isn’t a concept of 'global' variables and methods in a Java, marking a method as `public` and `static` makes it behave much like a 'global'. \n- Any code, in any class of your application, can access a `public static` method.  \n  - this is exactly how the `Math.pi` property and `Math.random()` methods are declared.\n- And if you mark a variable as `public`, `static`, and `final`, you have essentially made a globally-available constant. \n- these static (global-like) things are the exception rather than the rule in Java. They represent a very special case, where you don’t have multiple instances.\n\n## Primitive vs Object Reference Variables\nVariables can either be a *primitive* or a *reference*\n\n### Primitive\nPrimitive variables hold fundamental values (think: simple bit patterns) including integers, booleans, and floating point numbers. They are the most basic building blocks.\n- int, long, float, double, boolean, char, byte, short\n\n#### Numbers\n(all are signed)\n\n- Byte - 8 bits\n- Short - 16 bits\n- Int - 32 bits\n- Long - 64 bits\n\n- Float - 32 bits with decimal\n- Double - 64 bits with decimal\n\n\n### Reference\nReference variables hold a reference to the object.\n- that is, the object itself does not go into the variable; the variable only holds a pointer to it.\n\nThey are based on a class made up of primitives\n- String, Scanner, Random, Die, int[], String[]\n\nanal: think of a *reference* as a variable that holds a remote control. The remote control knows how to control the object that it refers to.\n- Using the dot operator (`.`) on a reference variable is like pressing a button on the remote control to access a method or instance variable.\n\n![](/assets/images/2023-01-18-19-59-12.png)\n\nAnything that is not a primitive is an object.\n\nthe `==` operator is used only to compare the bits in two variables\n- if we are using the `==` operator to compare 2 objects, what we are really doing is comparing the object references. If both variables reference the same object, then we will get `true`\n- therefore:\n```java\nint a = 3;\nbyte b = 3;\nif (a == b) /* true */\n```\n\nWhen an object is created, the following steps happen:\n1. The JVM allocates space for a reference variable, and gives it a name and a type\n2. The JVM allocates space for the new object on the heap\n3. The reference variable is linked to the object\n\nBecause an object reference is simply a pointer, they are all the same size\n\n#### Null Pointer Exceptions\nReference variables can be set to `null`, which means \"I am referencing nothing\"\n- If you try to dereference one of these variables (either explicitly by you or through Java automatically) you get a `NullPointerException`\n\nThe `NullPointerException` (NPE) typically occurs when you declare a variable but did not create an object and assign it to the variable before trying to use the contents of the variable.\n\n```java\npublic void doSomething(SomeObject obj) {\n   // Do something to obj, assumes obj is not null\n   obj.myMethod();\n}\n\n// if the method gets called with a null value, we will get a NPE\ndoSomething(null);\n```\n\n## Garbage Collection\nEach time an object is created, it goes into an area of memory known as The Heap.\n- it’s not just any old memory heap; the Java heap is actually called the Garbage-Collectible Heap. \n- When you create an object, Java allocates memory space on the heap according to how much that particular object needs. \n  - ex. An object with, say, 15 instance variables, will probably need more space than an object with only two instance variables. \n- When the JVM can see that an object is no longer referenced by a variable, that object becomes eligible for garbage collection","n":0.04}}},{"i":847,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n### Typecasting\n```java\n// typecast to integer\nint x = (int) 24.6; \n```","n":0.316}}},{"i":848,"$":{"0":{"v":"Object","n":1},"1":{"v":"\nWhen you write a class, you’re describing how the JVM should make an object of that type.\n\n### Array\n- quite primitive compared to those found in other languages (like Javascript)\n- fixed size once created\n- memory location for the array is contiguous\n- can create an array with specific size with `int array[] = new int[size];` \n- no `indexOf()` method\n\n### ArrayList\n- dynamic in size\n- capable of storing different types in the same list\n- requires more memory to store the elements as well as more time to iterate\n\nusage:\n```java\nArrayList myList = new ArrayList();\nmyList.get()\nmyList.Add (\"Delhi\");\n```\n- create with ``\n- elements accessed with ``\n- elements added with ``\n","n":0.1}}},{"i":849,"$":{"0":{"v":"JS Equivalents","n":0.707},"1":{"v":"\nJavascript -> Java\n- `.env` file -> `.properties` file\n- `Promise` -> `Future`","n":0.302}}},{"i":850,"$":{"0":{"v":"Jenv","n":1},"1":{"v":"\nJava Environment (ie. Java Version Manager)\n\nYou can configure the JDK at 3 different levels, switching the JDK version that is used when running a `java` command:\n- Global – Switch the JDK version globally\n- Local - Switch the JDK version in the current directory only\n- Shell - Switch the JDK version in the current shell instance only\n\n## Steps\n1. install the particular of Java you want\n```sh\n$ brew install openjdk@17\n```\n\n2. Add that version to jenv\n```sh\n$ jenv add /opt/homebrew/opt/openjdk@17\n```\n\n3. Get versions of Java registered with jenv\n```sh\n$ jenv versions \n* system\n  17.0\n  17.0.7\n  openjdk64-17.0.7\n```\n\n4. Set version of Java\n```sh\n$ jenv local 17.0\n```\n\n## Use with Gradle/Maven\nMaven and [[gradle]] both use the system JDK to run. Therefore, we have to enable their respective plugins:\n```sh\njenv enable-plugin gradle\n# or\njenv enable-plugin maven\n```\n\nthis plugin will ensure that the `JAVA_HOME` variable is set correctly.\n\n## Resources\n- https://www.baeldung.com/jenv-multiple-jdk","n":0.087}}},{"i":851,"$":{"0":{"v":"Java Environment","n":0.707},"1":{"v":"\n## Java Development Kit (JDK)\n- implements the Java Language Specification (JLS) \n- implements the the Java Virtual Machine Specification (JVMS)\n- provides the Standard Edition (SE) of the Java API\n\nIt provides software for working with Java applications. Examples of included software are:\n- the virtual machine\n- a compiler\n- performance monitoring tools\n- a debugger\n\nrun the binary `/usr/libexec/java_home` to know where in the system Java is installed\n\ncheck version:\n```sh\n$ javac -version\n```\n\n### javac\n- the main compiler included within the JDK\n- converts source code into Java bytecode\n\nDue to *dynamic binding*, the compiler will not catch `ClassCastExceptions`\n- dynamic binding is the Java feature of being able to include new objects that weren't even known to the original programmer\n\n### Classpath\nClasspath is a parameter in the JVM or the Java compiler that specifies the location of user-defined classes and packages.\n- it is a set of paths where the java compiler and the JVM will find needed classes to compile or execute other classes.\n- classes that the JVM needs for compilation and/or executing other classes can be found at these paths.\n- naturally, we wouldn't expect the JVM to search through every folder on our computer (and nor would we even want that), so we provide a classlist so it knows where to search.\n- A given Java program isn't in a single file, it has access to everything in the classpath\n\nIn Java, we make other classes available to our classes by `import`ing them at the top of our source file:\n```java\n// single class import\nimport org.javaguy.coolframework.MyClass;\n// bulk import\nimport org.javaguy.coolframework.*;\n```\n\nBecause of this `import` declaration, the JVM will know where to find the compiled class (ie. `.class` file).\n- ex. if our project has a `output/` directory, and our class is located at `output/org/javaguy/coolframework/MyClass.class`, then `output/` is as deep as we need to go when specifying the path in the classpath. The rest of the directory path is obtained from the actual `import` statement.\n\nThe classpath can also contain `.jar` files\n\n\n## Java Runtime Environment (JRE)\nThe JRE is included in the JDK, though is can also be installed independently of the JDK\n- end-users need to install the JRE to run the Java program, while the whole JDK is for developers\n\nThe JRE consists of:\n- a Java Virtual Machine (called HotSpot) \n- all of the class libraries present in the production environment, \n- additional libraries only useful to developers, such as the internationalization libraries and the IDL libraries.\n\n\ncheck version:\n```sh\n$ java -version\n```\n\n* * *\n\n## Files\n### JAR (Java Archive) file\nJAR files exist so that we don't have to send an end-user all 100 different classes of our application and tell them to run the app with `gradle` or `maven`. \n- Instead, we can bundle them into a JAR (basically just a zip format) and just get them to run it as an executable\n- in the JAR file, we include a *manifest* which defines which class in that JAR holds the `main()` method\n\n### WAR (Web Application Resource, or Web Application Archive) file\na file used to distribute a collection of JAR-files, JavaServer Pages, Java Servlets, Java classes, XML files, tag libraries, static web pages (HTML and related files) and other resources that together constitute a web application.\n\n## Other\n### JavaBeans (a.k.a Beans)\nBeans are classes that encapsulate one or more objects into a single standardized object (the bean)\n- This standardization allows the beans to be handled in a more generic fashion, allowing easier code reuse and introspection.\n- Essentially, beans are just data objects with getters/setters with little to no logic in them. \n- Your typical `Car` class can be considered a Java bean. Java beans are typically more or less 1:1 mapped to database tables.","n":0.041}}},{"i":852,"$":{"0":{"v":"iOS","n":1},"1":{"v":"\n### Podfile\n- holds our ios-written dependencies.\n\n## UE Resources\n- [Good resource for deploying ios apps](https://www.raywenderlich.com/books/ios-app-distribution-best-practices/v1.0/chapters/5-internal-distribution)","n":0.267}}},{"i":853,"$":{"0":{"v":"Deployment","n":1},"1":{"v":"\nUnlike Android, you can't install any app on an iOS device. It has to be signed by Apple first.\n\n## Test Deploy\nThere are two ways to distribute a test version of your app for iOS: TestFlight and Ad Hoc.\n- TestFlight involves submitting a build to App Store Connect where it can be downloaded by other users.\n\n## App signing\nSigning your app allows the iOS system to identify who signed your app and to verify that your app hasn't been modified since you signed it. \n- before an app can be installed on a device (or even submitted to the App Store), the developer must sign the `.ipa` executable (code signing) by using the certificate (spec: the `.p12` file) procured from Apple's developer portal.\n  - when an app is signed with a certificate, Apple can be reasonably assured that the signer is also the person whom the certificate was issued to.\n- The Signing Identity consists of a [[public-private|crypt.public-key]] key pair that Apple creates for you.\n  - remember, certificate is another name for public key.\n\nProcess:\n- Create a Certificate Signing Request (CSR) through the Keychain Access application.\n- Keychain Access will create a private key (to be stored in the keychain) and a `certSigningRequest` file which you'll then upload through Apple's web portal.\n- Apple will proof the request and issue a certificate for you. The Certificate will contain the public key that can be downloaded to your system. After you downloaded it you need to put it into Keychain Access by double clicking it. The Certificate will be pushed into the Keychain and paired with the private key to form the Code Signing Identity.\n- Finally, as the app is being installed on the device, the private key used to sign the app matches the public key in the certificate. If it fails, app is not installed.\n\n## Terms\n### Provisioning profile (or simply Profile)\nThe provisioning profile acts as a link between the device and the developer account. \n- therefore the certificate and the device are the 2 main parts of the provisioning profile.\n    - getting a certificate involves going to the Apple portal to get a development certificate, which involes registering your device.\n\nDistribution Profiles are used to submit an app to the App Store for distribution. After the app is reviewed by apple they sign in the app with their own signature that can run on any device.\n\nWe use the profile for testing our app prior to sending it to Apple for approval.\n\nA provisioning profile is downloaded from your developer account on Apple's web portal and embedded in the app bundle, and the entire bundle is code-signed.\n\nA Development Provisioning Profile must be installed on each device on which you wish to run your application code.\n- once made, we can download it, then drag it onto the Xcode icon of the MacOS Dock\n- doing this will create a `mobileprovision` file in `~/Library/MobileDevice/Provisioning Profiles`. The filename takes the form `<ProfileUUID>.mobileprovision`\n\nWe must generate provisioning profiles in the `developer.apple.com` portal.\n\nProvisioning profiles are not stored with the project. Xcode has a common area and the profile is pulled when you build and bundled with the `ipa` (the binary file of your app).\n\nEach provisioning profile contains:\n- Development Certificates — development certificate. These are for developers who want to test the app on a physical device while writing code.\n- Unique Device Identifiers (List of devices that the app can run on)\n- an App ID — An App ID is a two-part string used to identify one or more apps from a single development team.\n    - this can include a `*` wild card to be used for many applications with similar bundle identifiers\n\nWhen you install the application on a device the following things happen:\n- the provisioning profile in the Mac goes to the developer certificate in your key chain.\n- xcode uses the certificate to sign the code.\n- device's UUID is matched with the IDs in the provisioning profile.\n- AppID in the provisioning profile is matched with the bundle identifier in the app.\n- The entitlements required are associated with the App ID.\n- The private key used to sign the app matches the public key in the certificate.\n- the signed binary is sent to the device and is validated against the same provisioning profile in the app and finally launches.\n\n![](/assets/images/2022-04-25-09-15-11.png)\n\nIn simple terms, Provisioning profiles say, “applications with this Identifier signed with this Certificate’s private key are okay to run on these devices.”\n- Therefore a provisioning profile is tied to a certificate\n\nAn App ID is a two-part string used to identify one or more apps from a single development team. The string consists of a Team ID and a bundle ID search string, with a period (.) separating the two parts. The Team ID is supplied by Apple and is unique to a specific development team, while\nThe bundle ID search string is supplied by the developer to match either the bundle ID of a single app or a set of bundle IDs for a group of apps.\n\nThe bundle ID uniquely defines each App. It is specified in Xcode. A single Xcode project can have multiple Targets and therefore output multiple apps. A common use case for this is an app that has both lite/free and pro/full versions or is branded multiple ways.\n\nspec: is provisioning profile a private key?\n\n### Archive\nAn archive is a bundle that includes your app binary along with symbol information. \n- Archive is the format our app is in when we distribute it for testing or validate and submit it to App Store Connect.\n  - in other words, to upload our app to App Store, we must first archive it.\n\nThe archive contains debugging information.\n- While testing the app through TestFlight, keep the archive with the debugging information to make it easier to interpret crash reports later.\n\nArchives for each version of our app should be saved.\n\nUsing Archives:\n- Archives can be found in *Window > Organizer*\n- Archives can be made in *Product > Archive*\n\n### Symbolication\nSymbolication deals with translating how a computer sees code at runtime (that is, machine code dealing with low-level processes) and how we as developers see code.\n- without symbolication, we would be reading machine code debug logs.\n\n### Asset Catalog\nAsset catalogs simplify access to app resources by mapping between named assets and one or more files targeted for different device attributes.\n- ex. depending on whether the user is using an iPad or iPhone, we want to serve up different versions of each Asset. It is a waste to download iPhone 12 optimized images for an iPad user of our app.\n\nAsset Catalog is found in a `Assets.xcassets` or `Images.xcassets` directory\n\nContents.json files encode the attributes for elements represented by folders in the hierarchy. Each folder can contain one Contents.json file that encodes the attributes for the asset or group it contains.\n\nspec: Android parallel: Drawable Folder\n\n## Resources\n- https://abhimuralidharan.medium.com/what-is-a-provisioning-profile-in-ios-77987a7c54c2\n- [deploying iOS as github action without Fastlane](https://zach.codes/ios-builds-using-github-actions-without-fastlane/)","n":0.03}}},{"i":854,"$":{"0":{"v":"iOS Cmds","n":0.707},"1":{"v":"\n# Simulator\n- open developer menu - `cmd+d`\n- enable on-screen keyboard - `cmd+shift+k`\n","n":0.289}}},{"i":855,"$":{"0":{"v":"Intellij","n":1},"1":{"v":"\n### Quick `System.out.print`\n- `sout<TAB>`\n- `soutv<TAB>`","n":0.447}}},{"i":856,"$":{"0":{"v":"HTTPS","n":1}}},{"i":857,"$":{"0":{"v":"SAN Certificate","n":0.707},"1":{"v":"\nSAN certificates allow multiple hostnames to be protected by a single certificate.\n\nSAN stands for *Subject Alternate Name*\n\n### SSL Termination\nThe process of decrypting SSL-encrypted data. In other words, it is the final step in the SSL process, where the receiving end can actually read what has been sent to it.\n","n":0.143}}},{"i":858,"$":{"0":{"v":"Let's Encrypt","n":0.707},"1":{"v":"\n## Plugins\nCertbot supports 2 types of plugins for obtaining and installing certificates: authenticators and installers\n- some plugins can do both, such as the Apache and Nginx plugins\n\n### Authenticator\n- Authenticators are plugins used with the `certonly` command to obtain a certificate, validating that we own the domain we are requesting a certificate for. It then obtains the certificate for that domain, and places the certificate in the `/etc/letsencrypt` directory on your machine\n\t- The authenticator does not install the certificate (it does not edit any of your server’s configuration files to serve the obtained certificate)\n- If we list multiple domains to authenticate, they will all be included in a single certificate by default.\n\n### Installer\n- Installers are Plugins used with the `install` command to install a certificate.\n- These plugins modify the webserver's configuration in order to server the site over HTTPS \n\n## Certificates\n- All generated keys and certificates can be found on the host that serves the application. \n\t- found in `/etc/letsencrypt/live/$domain` if using Let's Encrypt\n- note: `pem` is a type of encoding\n\n### privkey.pem\nThis is the private key for the certificate \n- This is what Apache needs for `SSLCertificateKeyFile`, and Nginx for `ssl_certificate_key`\n\n### fullchain.pem\nThis is the full list of certificates, including the server certificate (a.k.a Leaf Certificate or End-Entity Certificate)\n- the server certificate is the first one listed. It is followed by intermediary certificates. \n- This is what Apache needs for `SSLCertificateFile`, and what Nginx needs for `ssl_certificate`.\n\n## Concepts\n### ACME\n- ACME is a communications protocol for automating interactions between CAs and their users' webservers.\n\t- This allows automated deployment of public key infrastructure.\n- Certbot is an example of an ACME client\n\n### Challenge\nChallenges are a way for the Let's Encrypt servers to validate that you own the domain.\n\nThere are 2 types: HTTP-01 Challenge, DNS-01 Challenge\n- We only need one.\n\nHTTP-01\n- The webserver proves it controls the domain by provisioning resources on its filesystem. The ACME server then challenges the webserver to provision a file at a specific path. If the webserver is able to do that, it is proof that the domain is under the webserver's control.\n- When our webserver gets a token from Let's Encrypt, the webserver creates a file at `http://<YOUR_DOMAIN>/.well-known/acme-challenge/<TOKEN>`\n\t- this file also includes a thumbprint of your account key\n- Once our webserver tells Let’s Encrypt that the file is ready, Let’s Encrypt tries retrieving it. If successful in doing so, then we are able to issue the certificate.\n- This is the most common type of challenge.\n\nDNS-01\n- Ownership of the domain is proven by adding a DNS record.\n- This type of challenge is useful if we want to avoid a short period of time where our site doesn't have SSL enabled, which would happen if using HTTP-01.","n":0.047}}},{"i":859,"$":{"0":{"v":"TLS","n":1},"1":{"v":"\nTLS consists of two phases: \n1. secure connection establishment \n2. the use of that encrypted channel for further communication.\n\nA TCP handshake and connection must be established before messages to create a TLS connection are exchanged.\n- the handshake is done in one round trip. \n- after the first two steps, all messages are encrypted.\n\nTLS involves public-key cryptography to establish a shared secret that is then used to encrypt future communication\n\nTLS (Transport Layer Security) replaced SSL (Secure Sockets Layer), which is deprecated\n\nspec:TLS is an agreement (protocol) between 2 IP addresses (your own and the web server you are connecting to).\n\nCertificates are bound to domain names instead of IP addresses, so the \"Not Secure\" (`ERR_CERT_COMMON_NAME_INVALID`) warning will still appear if you connect via an IP address.\n\n### SSL Termination\n- the act of data reaching the end of the SSL chain and getting decrypted (or offloaded) so the recipient can read the data.\n\t- happens at the server end of an SSL connection\n- SSL termination helps speed the decryption process and reduces the processing burden on backend servers.\n![](/assets/images/2021-03-09-09-46-03.png)\n\n### Wildcard SSl Certificate\na single ssl certificate that lets us have SSL on any `*.mydomain.com`\n\n### UE Resources\n- [self-signed certificates](https://medium.com/@jonatascastro12/understanding-self-signed-certificate-in-chain-issues-on-node-js-npm-git-and-other-applications-ad88547e7028)\n- [cloudflare guide on SSL](https://developers.cloudflare.com/ssl/)\n\n### E Resources\n[setting up a private CA](https://www.digitalocean.com/community/tutorials/how-to-\n","n":0.071}}},{"i":860,"$":{"0":{"v":"TLS Certificates","n":0.707},"1":{"v":"\n### Server certificate\nIn order to establish a secure connection between client and server, the server must provide a digital certificate, proving their identity to the client.\n\nThe client then conducts *certification path validation*, ensuring that:\n  1. The subject of the certificate matches the host name (not to be confused with the domain name) to which the client is trying to connect.\n  2. A trusted certificate authority has signed the certificate.\n\n- the certificate path starts with the `Subject` certificate, and proceeds through a number of intermediate certificates up to a trusted root certificate, typically issued by a trusted certificate authority (CA).\n\nOnce the client has validated successfully, a secure connection is established between it and the server.\n\nThe certificate has a `Subject` field which identifies the primary host name of the server as the *Common Name*\n- a certificate may specify more than one hostname (e.g. a domain and its subdomains), which are called *Subject Alternative Name (SAN) certificates*, which contain a `Subject Alternative Name` field\n\n### Client Certificate\nClient certificates authenticate the client connecting to a TLS service.\n- ex. To connect to Zscaler VPN, we must have a certificate issued by Zscaler in our filesystem.\n\nex. Zscaler?\n\n### Certificate chain\nA certificate chain is an ordered list of certificates, containing an SSL/TLS server certificate, intermediate certificate, and [[Certificate Authority|https.TLS.certificates.certificate-authority]].\n- There are 1 or more intermediate certificates, and act as middlemen between the protected root certificates and the server certificates issued out to the public.\n\nWhen we install the TLS certificate, we will also be sent an intermediate root certificate.\n\nWhen a browser downloads our website’s TLS certificate upon arriving at our homepage, it begins chaining that certificate through the intermediate certificates, all the way back to the trusted root certificate.\n- if it can't be chained back to a trusted root, the browser will issue a warning about the certificate.\n\nTo view the certificate chain for google.com, run:\n```sh\nopenssl s_client -connect google.com:443 -servername google.com\n```\n\nExample output:\n```sh\n0 s:/C=US/ST=California/L=Mountain View/O=Google Inc/CN=*.google.com\ni:/C=US/O=Google Inc/CN=Google Internet Authority G2\n1 s:/C=US/O=Google Inc/CN=Google Internet Authority G2\ni:/C=US/O=GeoTrust Inc./CN=GeoTrust Global CA\n2 s:/C=US/O=GeoTrust Inc./CN=GeoTrust Global CA\ni:/C=US/O=Equifax/OU=Equifax Secure Certificate Authority\n```\n\nUnderstanding the output:\n- the numbered lines are:\n    - 0: server certificate\n    - 1: intermediate\n    - 2: root\n- s: indicates the certificate subject\n- i: indicates the issuing certificate’s subject.\n\nThe certificate chain is valid if:\n- Subject of each certificate matches the Issuer of the preceding certificate in the chain (except for the server certificate).\n- Subject and Issuer are the same for the root certificate.\n \nIn other words:\n- The Subject of the intermediate certificate matches the Issuer of the server certificate.\n- The Subject of the root certificate matches the Issuer of the intermediate certificate.\n- The Subject and Issuer are the same in the root certificate.\n\n### Self-signed Certificate\nA self-signed certificate refers to a certificate that is created and signed by the same entity whose identity it certifies \n- ex. we create a certificate from our Synology NAS, and it is also the same NAS that certifies the machine's identity.\n\nSelf-signed certificates are signed with the private key generated by the entity\n\nBecause self-signed certificates are not issued by third-party certificate authorities, they provide less proof of the identity of the server and are usually only used to secure channels between the server and a group of known users.\n\n* * *\n\n## Server Certificate\nUsed to authenticate the identity of a server to the client that is trying to connect to it.\n- this is the certificate that is issued to the specific domain the user is needing coverage for\n\nWhen a server certificate gets installed on a website, HTTPS gets enabled, and the certificate chain gets created, the result of which vouches for the authenticity of the website.\n- When we hop on to our computers and type in a website URL, the server certificate ensures that the data flow between our client browser and the domain we’re trying to reach stays secure\n\nAlso known as SSL/TLS certificates\n\nBased on PKI\n\n## Client Certificate\nUsed to validate the identity of a client.\n- In this way, it serves as a password, as in theory, no one should be able to produce that certificate but the true client.\n- Client certs don't encrypt any data.\n\nClient certificates exist because passwords aren't that secure.\n\nClient certificates use the Public Key Infrastructure (PKI) for authentication (just as server certificates do).\n- Difference is that client certificates don't encrypt any data; they are just for validation purposes.\n\nContain \"Issued to\" and \"Issued by\" fields\n\nBased on PKI\n\n","n":0.037}}},{"i":861,"$":{"0":{"v":"CA (Certificate Authority)","n":0.577},"1":{"v":"\n## Overview\nThe primary role of the CA is to digitally sign and publish the public key bound to a given user. This is done using the CA's own private key, so that trust in the user key relies on one's trust in the validity of the CA's key.\n- When the CA is a third party separate from the user and the system, then it is called the Registration Authority (RA), which may or may not be separate from the CA.\n\nThe CA is an entity responsible for issuing digital certificates that verify identities on the internet.\n- ex. there is a CA in our personal machine.\n\nThe root certificate belongs to the CA, and comes pre-downloaded in most browsers and is stored in what's called a \"trust store\".\n- The root certificates are closely guarded by CAs.\n\na CA is an organization that stores public keys and their owners, and every party in a communication trusts this organization (and knows its public key)\n- When we navigate to a trusted site, the browser will have already had the public key of the CA, which it can then use to verify the signature, implying that the certificate and its contained public key are trustworthy\n\nIf you communicate with HTTPS, FTPS or other TLS-using servers using certificates in the CA store, you can be sure that the remote server really is the one it claims to be.\n\nThe CA certificate file is usually called `ca.pem` or `cacerts.pem`\n\n### Purpose\nCA's are used to sign certificates that enable HHTPS\n- In essence, the CA is responsible for saying \"yes, this person is who they say they are, and we, the CA, certify that\"\n- CA servers can manage certificate enrollment requests from customers, and are able to issue and revoke digital certificates\n- CA servers support various certificate templates, such as SSL (both server and client), email signing/encryption, etc\n- to authenticate the recipient of the certificate, HTTPS servers typically use a technique called \"domain validation\"\n\t- This is where the domain name of the applicant is validated by proving that it owns and has control over a DNS domain. After this validation, a Domain Validated Certificate (DV) is issued.\n- A CA issues digital certificates that contain a public key and the identity of the owner\n\t- the certificate being issued is confirmation (by the CA) that the public key within the certificate belongs to the entity noted in the certificate.\n\t- spec: the CA has the private key\n- the CA market is very fragmented, and government-related activities (govt forms that verify identity) will have their own CA. For commercial use, the CA market is dominated by a few players (Let's Encrypt, GoDaddy, VeriSign are two of them)\n- ex. when using DigitalOcean managed Postgres, we are given a `ca-certificate.crt` from DigitalOcean. This is the certificate we must pass from our application server. The reason this works, is because the certificate is signed by one of the CAs that is trusted by the DigitalOcean server.\n\n### Chain of Trust\n- example chain:\n```\nGlobalSign → Google CA → neverforgetapp.co\n```\n\n- The idea of Chain of Trust is that the Root CA can \"trust\" other Intermediate CAs (ICA) to issue certificates (which are signed by the Root CA). Though the ICAs themselves are not trustworthy, they are trusted by a trustworthy entity (the Root CA), so the certificates they issue are also considered trustworthy.\n- The Root CA is kept behind many layers of security\n\t- if private keys are compromised, then all certificates based on the Root CA are compromised as well. For this reason, we use an Intermediate CA\n- In the browser, if we click on the lock to the left of the URL, we can see the certificate chain\n\t- The one at the bottom (leaf node) of the tree is the SSL certificate (`*.neverforgetapp.co`), which was issued by the ICA directly above it (`Google CA`)\n\t\t- The SSL certificate is signed with the private key of the ICA (`Google CA`). Once this is done, it is sent back to `neverforgetapp.co`.\n\t\t- All certificates contain the public key of the entity that the certificate is for, and the private key of the entity that signed the certificate.\n\t\t\t- ex. the SSL certificate (the end of the chain) contains the public key of `neverforgetapp.co`, and is signed by Google CA; Google's CA certificate contains Google's public key and is signed by GlobalSign.\n\t- In the browser's Certificate Manager, we can see that GTS CA 101 is not listed as a trusted CA. Therefore, this Intermediate CA needs to be verified by a Root CA, which it is (GlobalSign). In the Certificate Manager, we can see that this is indeed trusted. This is the chain of trust.\n- In our example chain, there are 2 certificates issued: the Google ICA certificate, and the SSL certificate (to `neverforgetapp.co`). These both get sent to `neverforgetapp.co` and are installed on the webserver.\n\t- When someone visits `neverforgetapp.co` with HTTPS, both certificates are sent back to the browser. It starts by verifying the certificate that is higher up in the chain (closer to the root), which is the Google ICA certificate. It sees that GlobalSign has signed this certificate, whom it recognizes as a trusted Root CA.\n\t\t- The browser has the public keys (and the certificates, of which they are a part) of all Root CAs, so it can unencrypt the Google ICA certificate. This proves that this ICA is trustworthy.\n\t\t- the browser trusts all Root CAs in its list. The fact that a Root CA in that list has signed the Google ICA certificate means that the browser automatically trusts that ICA.\n\t\t\t- As a consequence of unecrypting the certificate, the browser gets the public key from Google ICA.\n\t\t- Now that Google ICA has been considered trustworthy, the browser then looks to the `neverforgetapp.co` SSL certificate. It uses the public key issued by Google ICA to decrypt the certificate. It sees that Google ICA has signed this certificate, making it trustworthy.\n\t\t- Now, the browser gets the public key from `neverforgetapp.co`, generates a symmetric key, encrypts it (using the public key), and sends it to the `neverforgetapp.co` webserver.\n\t\t\t- since the `neverforgetapp.co` webserver has the private key, it is able to decrypt it, which yields the same symmetric key. Now, both the browser and the `neverforgetapp.co` webserver have the same symmetric key that is used to exchange information securely.\n\n### Private CA\n- There are 2 versions, public and private CA\n\t- public are for verifying the identity of websites and other services that are provided to the general public\n\t- private are for closed groups and private services.\n- spec:While in production we will use a public CA, we may want to configure a private CA in development and staging environments so those environments match the production environment, and also have HTTPS connections.\n- we need to build a private CA when our programs require encrypted connections between client and server.\n\t- with the private CA, we can issue certificates to users, servers, or individual programs and services within your infrastructure.\n\t\t- ex. OpenVPN would use its own private CA\n\t\t- ex. we can configure our web server to use certificates issued by a private CA to make development and staging environments match production servers that use TLS to encrypt connections.\n\n### Bank Example\nWhen a user logs into an HTTPS enabled bank, they receive a public key. The public key is used to create a temporary shared symmetric encryption key, which is used to encrypt all messages sent from the client to the server. These messages are enciphered with the bank's public key in such a way that only the bank has the private key required to decrypt them. For the rest of the session, the (disposable) symmetric key is used to encrypt messages from client to server.\n\n## E Resources\n- https://www.howtouselinux.com/post/exploring-unable-to-get-local-issuer-certificate\n","n":0.028}}},{"i":862,"$":{"0":{"v":"ALPN Protocol","n":0.707},"1":{"v":"\nALPN is a TLS extension that allows the application layer to negotiate which protocol should be performed over a secure connection\n- A benefit of this is that it inherently avoids additional round trips\n\nALPN is sent on the initial TLS handshake (Client Hello), and it shows which protocols the client (eg. browser) supports\n","n":0.139}}},{"i":863,"$":{"0":{"v":"HTML","n":1},"1":{"v":"\nHTML has what’s called complex elements. This is like a drop down, i.e. something that is made up of other elements. If you are to inspect the drop-down element invert browser, you will see the shadowDOM, Which is in accessible to users\n- ex. Input, Select, checkbox, radio button\n\n## Semantic HTML\nHTML should be coded to represent the data that will be populated and not based on its default presentation styling.\n- presentation should *solely* be the responsibility of [[css]].\n\nSemantic HTML is important because...\n- It impacts SEO\n- Makes it navigable to screen readers","n":0.105}}},{"i":864,"$":{"0":{"v":"Tags","n":1},"1":{"v":"\n### `<script />`\nJavascript loaded through script tags should be located before the closing `</body>` tag.\n- This is to stop JavaScript loading from blocking content loading. So, the content is loaded and the scripts are loaded next.\n\nAn alternative to this is to use async in the script tag, like so:\n```html\n<script src=”main.js” async></script>\n```\nthis way it will not block the main thread and hence will not block the content from loading.\n\n### `head`\nput links to CSS style sheets in `<head></head>`\n- this way, the browser styles the HTML as it loads. If style sheets are put at the bottom of the HTML, the browser will have to restyle and render the whole page from the top, which can cause a performance bottleneck.\n\n### `<nav>`\nThe `<nav>` element represents a section of a page whose purpose is to provide navigation links, either within the current document or to other documents.\nex. menus, tables of contents\n\n### `<iframe>`\nSites like Facebook and Twitter use iframes to display content (plugins) on third party websites.\n- think: sometimes you see tweets on other people's website, which has the same styling that you'd find on Twitter.com.\n","n":0.075}}},{"i":865,"$":{"0":{"v":"Homebrew","n":1},"1":{"v":"\n### Services\nHomebrew has a services aspect to it. We can run `brew services list` to see the services that are currently running from Homebrew. We can run `brew services start` to start a service.\n- ex. Redis, Postgres, vault, caddyserver \n\n### Cask\nbrew cask is an extension to brew that allows management of MacOS graphical applications\n\n### Tap\na Git repository of Formulae and/or commands\n\nWhen you *tap* a repository, you are essentially informing Homebrew about the existence of this external repository so that it can be used to install packages or formulas from that repository.\n- Homebrew is designed to be modular and lightweight. By default, it doesn't come with every package or formula available. Instead, it provides the core functionality and allows users to extend it by tapping additional repositories.\n\n### Keg\nthe installation prefix of a Formula\n\n* * *\n\n### Installation directories\n`/opt/homebrew/Cellar/`\n\n## CLI\n#### Search available packages\n`brew search _____`\n\n#### See where package is installed\n`brew info postgresql`","n":0.082}}},{"i":866,"$":{"0":{"v":"Hardware Components","n":0.707},"1":{"v":"\n### What happens when we run an executable?\n1. as we type the characters `./hello` into the shell program, each letter is read into a register (in the CPU), and then stores it in memory\n2. When we hit `<enter>`, the shell loads the executable `hello` file by executing a sequence of instructions that copies the code and data in the `hello` object file from disk to main memory (included is the string of characters `\"hello, world\\n\"`)\n3. the data then travels directly from disk to main memory, without passing through the processor (a technique known as *direct memory access*)\n5. Once the code and data of the `hello` file are loaded into memory, the processor begins executing machine-language instructions in the `hello` program's `main` function.\n\t- These instructions copy the bytes in the `\"hello, world\\n\"` string from memory to the register file (CPU), and from there to the display device (monitor).\n","n":0.082}}},{"i":867,"$":{"0":{"v":"Sound Card","n":0.707},"1":{"v":"\nA computer program sends input and receives output of audio signals to and from the sound card\n- as a provider of output: the sound card provides data to the speakers\n- as a receiver of input: the sound card may receive analog data from a microphone, which it then converts into digital data.\n\nSound cards use a digital-to-analogy converter, converting recorded digital signal data into an analog format to be played over speakers.\n\nThe sound card may have direct memory access to transfer samples to and from main memory. Normally the digital data is only saved to long term storage once the program has received a request from the user indicating the desire.\n\nA sound card provides input and output audio signals to and from a computer\n\nA soundcard is able to process and output multiple independent voices/sounds simultaneously.\n- ex. 2.0 (stereo), 2.1 (stereo+subwoofer), 5.1 (surround)\n\nAn [[music.engineering.audio-interface]] is normally described as a *\"professional sound cards\"*","n":0.082}}},{"i":868,"$":{"0":{"v":"RAM (Random Access Memory)","n":0.5},"1":{"v":"\nRAM works by means of fast-switching pointers. This is analogous to how information retrieval from a database is so efficient. The engine doesn't really care what the data is it's retrieving— it only cares about getting it out.\n\n**overlaying** is the process of transferring a block of program code or other data into main memory, replacing what is already stored.\n- with this technique, entire programs (such as large computer games) can be larger than the computer's main memory, because only certain parts are loaded in at a time.\n\n### Main memory (RAM)\n- the temporary storage device that holds both a program and the data it manipulates while the processor is executing the program.\n- physically, main memory consists of a collection of DRAM chips.\n- logically, memory is organized as a linear array of bytes, each with its own array index starting at 0.\n- ex. Imagine every spot of RAM having its own address.\n\t- If your computer is 32 bit (4,294,967,296), then our computer can't talk to any address that is higher than that.\n\t\t- This much memory is commonly known as 4gb.\n\n### Virtual RAM\nSometimes, the physical RAM runs short for actively running programs. When this happens, a block of space on the hard drive can be configured by the OS to pretend to be memory.\n\nOf course, virtual RAM is a lot slower, so the more your computer is forced to rely on it, the less performant it will be\n","n":0.065}}},{"i":869,"$":{"0":{"v":"Interfaces","n":1}}},{"i":870,"$":{"0":{"v":"Port","n":1}}},{"i":871,"$":{"0":{"v":"Serial Port","n":0.707},"1":{"v":"\nWith this type of port, information transfers in or out sequentially one bit at a time\n![](/assets/images/2021-03-20-19-15-42.png)\n\nThroughout most of the history of personal computers, data has been transferred through serial ports to devices such as modems, terminals, various peripherals, and directly between computers.\n- Modern consumer PCs have largely replaced serial ports with higher-speed standards, primarily USB\n- serial ports are still frequently used in applications demanding simple, low-speed interfaces, such as industrial automation systems, scientific instruments, point of sale systems and some industrial and consumer products.\n- Server computers may use a serial port as a control console for diagnostics\n\n## UE Resources\n[Connect Pi to Arduino with Serial Ports (and using Node's serialport.js)](https://medium.com/@machadogj/arduino-and-node-js-via-serial-port-bcf9691fab6a)\n","n":0.096}}},{"i":872,"$":{"0":{"v":"Parallel Port","n":0.707},"1":{"v":"\nMultiple bits transfer in or out simultaneously\n\nNetworking hardware (such as routers and switches) commonly use serial console ports for configuration, diagnostics, and emergency maintenance access\n- Today, the parallel port interface is virtually non-existent because of the rise of Universal Serial Bus (USB) devices\n\nContrasted with [[Serial Ports|hardware.interfaces.port.serial]]\n","n":0.147}}},{"i":873,"$":{"0":{"v":"CPU (Central Processing Unit)","n":0.5},"1":{"v":"\nThe CPU is the engine that interprets (or executes) instructions that are stored in main memory.\n- at its core, there is a word-sized storage device (or register) called the program counter (PC).\n\t- At any given point in time, the PC points at (contains the address of) some machine-language instruction in main memory.\n- From the time the system is powered on until it is powered off, the processor blindly and repeatedly performs the same basic task over and over again:\n\t1. reads the instruction from memory pointed at by the PC\n\t2. interprets the bits in teh instruction\n\t3. performs some simple operation, as per the instruction\n\t4. updates the PC to point to the next instruction (which may or may not be in contiguous memory) \n- Different CPU architectures understand different instruction types, which is why we can't take a program from one architecture (ex. x86) and run it on another (ex. ARM)\n\nTypically a CPU communicates with devices via a [[bus|hardware.bus]].\n\n- A socket is the physical socket where the physical CPU capsules are placed\n- Cores are the number of CPU-cores per CPU capsule\n- some CPUs can run more than one parallel thread per CPU-core\n- If you multiply the number of socket, cores and threads, then you get the number of \"CPUs\": 24.\n\t- These aren't real CPUs, but the number of possible parallel threads of execution your system can do.\n\nThe CPU reads instructions in, does the instruction, and then reads the next instruction. These instructions need addresses (RAM address) to know where to go to get data, or put data. But it also needs to know what to do with that data. These two parts compose a basic \"instruction\", usually divided into op-codes(operation code) and addresses.\n- So a 32 bit CPU has 32 bits for an instruction, while a 64 bit CPU has 64 bits to hold an instruction. This means we can have many more opcodes, and also have many more places in memory we can address.\n- ex. Let's say numbers 1 through 20 are addresses in RAM. If we have, for example, a 3 bit processor, an instruction would look like so: S-1-2 (something like subtract 1 from 2, assuming S is subtract op-code). Now what if we want to subtract something from memory address 17? We can't, we don't have enough room, since we only have 3 bits(1 for opcode, one for first address, one for second address). There are ways to work around this but for simplicity, it's impossible.\n\n### Hardware Acceleration\n","n":0.049}}},{"i":874,"$":{"0":{"v":"Register","n":1},"1":{"v":"\nA register is quickly accessible storage available to the computer's processors\nmost computers load data from a larger memory into registers where it is used for arithmetic operations\n- manipulated data is then stored as RAM\nregisters are at the top of the memory hierarchy, meaning they provide the fastest access to data\nregisters measure how much data they can hold in terms of bits (ex. 64-bit register)\n- ARM instructions operate only on registers with a few instructions for loading and saving data from / to memory while x86 can operate directly on memory as well.\n- when discussing 32bit or 64-bit architectures, we are referring to the size of the register\n","n":0.097}}},{"i":875,"$":{"0":{"v":"CPU Clock","n":0.707},"1":{"v":"\nCPUs have electronic clocks in them, in fact it’s a fundamental part of how they operate as the electronics within a CPU have to operate in a synchronised way.\n\nThe clock is a crystal that oscillates a predictable number of times each second when electricity is passed through it. Counting these oscillations allows the computer to measure the passing of time.\n- these atomic clocks use the vibrations of atoms to keep time.\n\nClocks are not perfectly accurate: it *drifts* (ie. runs faster or slower than it should). As a result, one machine may be faster or slower than another machine.\n- This adds an interesting wrinkle to [[distributed|deploy.distributed]] systems, and is a reason we can't depend on the clock to determine the ordering of events.\n- It is possible to synchronize clocks to some degree: the most commonly used mechanism is the Network Time Protocol (NTP), which allows the computer clock to be adjusted according to the time reported by a group of servers. The servers in turn get their time from a more accurate time source, such as a GPS receiver.\n- Clock drift varies depending on the temperature of the machine.\n\n## Why use them?\nThe accuracy of an atomic clock matters in computing because time synchronization is critical for many computing tasks, such as \n- coordinating distributed systems, \n    - Without accurate timekeeping, distributed systems may not be able to coordinate their activities effectively, leading to issues such as data corruption, incorrect results, or system crashes.\n- scheduling tasks, \n- maintaining data consistency. \n\n## UE Resources\n- https://onezero.medium.com/the-largely-untold-story-of-how-one-guy-in-california-keeps-the-worlds-computers-on-the-right-time-a97a5493bf73","n":0.063}}},{"i":876,"$":{"0":{"v":"CPU Architecture","n":0.707},"1":{"v":"\n## 32 bit vs 64 bit architecture\n- At a high level, a higher bit CPU allows the machine to run faster by having advanced operations supported by opcodes(rather than using simple opcodes to build up to a complex algorithm) and utilize more memory by having access to more address spaces(it can save stuff in RAM instead of hard-disk, which is slower than RAM, even if it is flash memory also).\n- ex. In legacy versions of Excel, the max number of rows is 65,537, which corresponds to 16 bits (`1111111111111111`)\n- ex. in the original zelda, the max number of rupees was 255, which corresponds to 8 bits (`11111111`)\n- anal: imagine a paper letter. More bits means more possible addresses, which would correspond to a bigger envelope. The other side of the equation is how much RAM we have. Going from a 32-bit system to a 64-bit system would be like going from a standard sized envelope to an envelope that wraps around the world 3 times (since: 2<sup>64</sup> is more than 4 billion times larger than 2<sup>32</sup>).\n- a 32-bit system would only be able to understand 4GB worth of RAM, making any RAM beyond that amount redundant on a 32-bit system.\n\t- This is not entirely true because of the existence of PAE (Physical Address Extension), which allows a 32-bit system access more than 4GB of RAM. However it is still true that individual programs are limited to the 4GB.\n\nA memory address that's 32 bits long can only refer to 4.2 billion unique locations (i.e. 4 GB).\n- anal: we can count on our fingers up to 10 in the decimal system. Since computers are binary, a bit is the number of \"digital fingers\" that the computer can count on.\n\t- ex. If I said a computer has 4 bits, the biggest number I could put in it is 4 places long, or 1111.\n- the biggest number a 32 bit system could handle is `11111111111111111111111111111111` (4,294,967,296)\n\t- this would be the theoretical highest number that our computer could do calculations with (though there are workaround tricks that are slower)\n\t- The practical use of this \"largest number\" is in determining how much memory we can have.\n\n## ARM vs. x86\nARM and x86 are 2 different families of CPU architecture\n- ARM is more efficient than regular x86 processors, which is made possible by a simplified instruction set, and having a stripped down hardware.\n- unlike x86, ARM wasn't made with backward compatability in mind, so it doesn't repeat a lot of the inefficient designs that were made by its antecedents.\n- known as *reduced instruction set computing* (RISC), while x86 uses *complex instruction set* (CISC)\n\nanal: imagine that we have 2 general-purpose factories that can make a wide array of things: the x86 Factory, and the ARM Factory\n\t- **x86 Factory** - There is a receptionist at the front that receives mail (requests) one at a time. Each time he gets a piece of mail, he passes it on to a group of people who open (decode) the messages to figure out what action should be taken. Each person in this group can figure out his own request, and figure out where it should go next. Also, these requests can vary wildly in size and purpose. For instance, one request might be \"take data from this warehouse and put it in bucket X (load to register)\". Another might be \"take data from this warehouse and put it on the conveyor to this trucking station (bus to I/O)\". Another might be \"take 3 different pieces of data from the warehouse, add them together, then put them back in a different spot of the warehouse\". Since each request varies in the amount of time is takes to process, the factory gets more efficient if more people are hired to handle these requests. This is why this type of factory can be so expensive: to get more efficient, more decoders must be hired.\n\t- **ARM Factory** - There is a receptionist at the front that receives mail (requests) **and** opens them. These requests differ from the ones accepted by the x86 Factory, in that they are more atomic and simpler to understand. The implication of this is that the receptionist knows exactly which department to send the request to, without needing another department of decoders to figure everything out. One type of request the receptionist might receive is \"load this one piece of data from the warehouse into this bucket\". Another might be \"add these two buckets together and store the result in the first bucket\". However, this means that the compiler (which converts the program to machine code) needs to write out all of these smaller instructions. Instead of the x86 method of decoding large instructions into smaller ones on the fly, the ARM method is to do it all ahead of time.\n\nSince the ARM architecture has less hardware doing things, it is a lot cheaper to design and make, and is more energy efficient.\nOriginally, ARM was meant for small embedded systems. This meant that there was little incentive to improve performance. Since ARM chips have expanded to phones, performance is a more recent focus of ARM chips.\n","n":0.034}}},{"i":877,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Get info about CPU, including architecture\n`less /proc/cpuinfo`\n","n":0.354}}},{"i":878,"$":{"0":{"v":"Circuit Board","n":0.707},"1":{"v":"\nThere are four fundamental electrical circuit element \n- [[resistors|hardware.circuit-board.resistor]] \n- [[capacitors|hardware.circuit-board.capacitor]]\n- inductors.\n- memory resistor (memrister)","n":0.258}}},{"i":879,"$":{"0":{"v":"Transistor","n":1},"1":{"v":"\nA transistor amplifies charge.\n- transistors have 3 terminals\n- transistors can regulate current or voltage flow while also acting as a switch for signals of the electronic variety\n- Semiconductors make up three layers of a transistor.\n\t- Therefore, each layer can carry current\n","n":0.156}}},{"i":880,"$":{"0":{"v":"Transformer","n":1},"1":{"v":"\nTransformers converts high AC voltage like 230VAC to 24VAC low voltage to be used by bridge rectifier (a.k.a. [[hardware.circuit-board.diode]] bridge)\n","n":0.224}}},{"i":881,"$":{"0":{"v":"Resistor","n":1},"1":{"v":"\nA resistor controls the electric current as it passes through them. \n- The colour-coded lines are used to determine their value.\n\nResistors have many uses including: \n1. reduce current flow\n2. adjust signal levels\n3. divide voltages. This happens when resistors are lined up in series with each other. \n\nResistors dissipate electrical power as heat\n- For bigger applications like a motor, this is exactly what they are used for.\n![](/assets/images/2021-03-22-22-24-31.png)\n","n":0.123}}},{"i":882,"$":{"0":{"v":"Microchip (Integrated Circuit)","n":0.577},"1":{"v":"\nIC is contrasted with Discrete Circuit (such as a PCB)\n- an IC is a set of electronic circuits on one small flat piece (or \"chip\") of semiconductor material (normally silicon)\n- The integration of large numbers of tiny MOS transistors into a small chip results in circuits that are orders of magnitude smaller, faster, and less expensive than those constructed of discrete electronic components.\n- ICs can be seen as a building block approach to integrated circuit design \n\t- This has resulted in discrete transistors being far more rare on the PCB itself\n- a modern chip (year 2020) may have many billions of MOS transistors in an area the size of a human fingernail\n","n":0.094}}},{"i":883,"$":{"0":{"v":"Diode","n":1},"1":{"v":"\nDiodes only allow current to flow in a single direction\n- This is done by having zero resistance in one direction, and very high resistance in the other.\n\nDiodes have 2 terminals, each of which is an electrode: an *anode* and a *cathode*\n- anodes allow current to flow in from outside a circuit\n- cathodes allow current to flow out of a polarized device.\n![](/assets/images/2021-03-22-22-26-11.png)\n","n":0.128}}},{"i":884,"$":{"0":{"v":"Capacitor","n":1},"1":{"v":"\nA capacitor is basically a tiny battery\n- potentially carries just enough energy to keep something like memory chip running for a few second\n\t- this is why when we restart devices to try and fix an issue, we need to leave it off for about 10 seconds. that timespan will ensure that the capacitors are fully drained and the memory chips are all wiped. \n\n![](/assets/images/2022-02-03-14-05-06.png)\n","n":0.125}}},{"i":885,"$":{"0":{"v":"Bus","n":1},"1":{"v":"\nA bus carries bytes of information back and forth between the hardware components of a computer system (or even between computers, if connected physically)\n- Buses are normally designed to transfer fixed-sized chunks of bytes known as *words*. The number of bytes in a word (word size) is a fundamental system parameter that varies across different systems.\n\t- ex. word size could be 4 bytes long, while others could be 8. Typically smaller systems (such as embedded controllers) have word sizes of 1 or 2.\n\n### I/O Device\n- An I/O device is the system's connection to the outside world\n- ex. keyboard and mouse for input, monitor for output, and disk drive for long-term data storage.\n- Each I/O device is connected to the I/O bus by either a controller or adapter.\n","n":0.089}}},{"i":886,"$":{"0":{"v":"Battery","n":1},"1":{"v":"\n### Why batteries drain faster when network connection is bad\nPower output at the receiver antenna (such as the one found in a phone) drops by the inverse square of the distance from the transmitter. Thus, doubling the distance between the antennas decreases the power output by four times. To ensure that the signal strength doesn’t wane, more power is required at the transmitter, which comes at a cost to the battery.   \n","n":0.118}}},{"i":887,"$":{"0":{"v":"ALU (Arithmetic Logic Unit )","n":0.447},"1":{"v":"\nMade up of the Arithmetic Unit and the Logic Unit \n- Arithmetic handles numeric operations, like add/subtract, increment\n\nthe numbers operated on are in binary format \n","n":0.196}}},{"i":888,"$":{"0":{"v":"GPU","n":1},"1":{"v":"\nHistorically speaking, the GPU's principle job is to render triangles onto the screen\n- It is more performant than the CPU, but also more specific, so cannot do the wide-range of jobs that can be performed by a CPU\n\nTheir use has extended past this initial purpose, and they are now also used for ML neural networks and mining cryptocurrencies.\n- using GPU like this (ie. not triangle drawing) is known as General Purpose GPU (GPGPU)\n\nGPUs are optimized for throughput at the cost of latency\n- GPUs need to sustain 3.3GB/s just for texture samples for a shader running at 1280x720 resolution\n- this level of throughput is only possible if the memory of the GPU is very tightly integrated with the cores\n\na GPU can be a completely separate card with its own memory chip, so you control it via a so-called command buffer (or command queue), which is a chunk of memory that contains encoded commands for the GPU to execute.\n- The command buffer is also the hook for the [[driver|os.kernel.driver]] or [[operating system|os]] to let multiple applications use the GPU without them interfering with each other. When you queue up your commands, the abstraction layers below will inject additional commands into the queue to save the previous program’s state and restore your program’s state so that it feels like no one else is using the GPU.\n\nGPUs are great at numerical operations, meaning they lend themselves well to the fields of machine learning and blockchain\n\n### GPU Cores\nGPUs have many cores (from 100's to 1000's) that allow for a high degree of parallelization.\n- however, each core is not as independent as the ones we'd find in a [[CPU|hardware.cpu]]\n\nGPU cores are grouped hierarchically\n\n### Use in Machine Learning\nThough originally GPUs were built for graphics, Nvidia provides a software platform called CUDA (Compute Unified Device Architecture) which is an API that enables software (like Pytorch) to use GPU for general purpose computing tasks.\n\n<!-- Refine this explanation -->\nGPUs are so fast because they are so efficient for matrix multiplication and convolution\n\nCPUs are latency optimized while GPUs are bandwidth optimized. You can visualize this as a CPU being a Ferrari and a GPU being a big truck. The task of both is to pick up packages from a random location A and to transport those packages to another random location B. The CPU (Ferrari) can fetch some memory (packages) in your RAM quickly while the GPU (big truck) is slower in doing that (much higher latency). However, the CPU (Ferrari) needs to go back and forth many times to do its job (location A $\\rightarrow$ pick up 2 packages $\\rightarrow$ location B ... repeat) while the GPU can fetch much more memory at once (location A $\\rightarrow$ pick up 100 packages $\\rightarrow$ location B ... repeat).\n\nSo, in other words, the CPU is good at fetching small amounts of memory quickly (5 * 3 * 7) while the GPU is good at fetching large amounts of memory (Matrix multiplication: (A*B)*C). The best CPUs have about 50GB/s while the best GPUs have 750GB/s memory bandwidth. So the more memory your computational operations require, the more significant the advantage of GPUs over CPUs. But there is still the latency that may hurt performance in the case of the GPU. A big truck may be able to pick up a lot of packages with each tour, but the problem is that you are waiting a long time until the next set of packages arrives. Without solving this problem, GPUs would be very slow even for large amounts of data. So how is this solved?\n\nIf you ask a big truck to make many tours to fetch packages you will always wait for a long time for the next load of packages once the truck has departed to do the next tour — the truck is just slow. However, if you now use a fleet of either Ferraris and big trucks (thread parallelism), and you have a big job with many packages (large chunks of memory such as matrices) then you will wait for the first truck a bit, but after that you will have no waiting time at all — unloading the packages takes so much time that all the trucks will queue in unloading location B so that you always have direct access to your packages (memory). This effectively hides latency so that GPUs offer high bandwidth while hiding their latency under thread parallelism — so for large chunks of memory GPUs provide the best memory bandwidth while having almost no drawback due to latency via thread parallelism. This is the second reason why GPUs are faster than CPUs for deep learning. As a side note, you will also see why more threads do not make sense for CPUs: A fleet of Ferraris has no real benefit in any scenario.\n\nBut the advantages for the GPU do not end here. This is the first step where the memory is fetched from the main memory (RAM) to the local memory on the chip (L1 cache and registers). This second step is less critical for performance but still adds to the lead for GPUs. All computation that ever is executed happens in registers which are directly attached to the execution unit (a core for CPUs, a stream processor for GPUs). Usually, you have the fast L1 and register memory very close to the execution engine, and you want to keep these memories small so that access is fast. Increased distance to the execution engine dramatically reduces memory access speed, so the larger the distance to access it the slower it gets. If you make your memory larger and larger, then, in turn, it gets slower to access its memory (on average, finding what you want to buy in a small store is faster than finding what you want to buy in a huge store, even if you know where that item is). So the size is limited for register files - we are just at the limits of physics here and every nanometer counts, we want to keep them small.\n\nThe advantage of the GPU is here that it can have a small pack of registers for every processing unit (stream processor, or SM), of which it has many. Thus we can have in total a lot of register memory, which is very small and thus very fast. This leads to the aggregate GPU registers size being more than 30 times larger compared to CPUs and still twice as fast which translates to up to 14MB register memory that operates at a whopping 80TB/s. As a comparison, the CPU L1 cache only operates at about 5TB/s which is quite slow and has the size of roughly 1MB; CPU registers usually have sizes of around 64-128KB and operate at 10-20TB/s. Of course, this comparison of numbers is a bit flawed because registers operate a bit differently than GPU registers (a bit like apples and oranges), but the difference in size here is more crucial than the difference in speed, and it does make a difference.\n\nAs a side note, full register utilization in GPUs seems to be difficult to achieve at first because it is the smallest unit of computation which needs to be fine-tuned by hand for good performance. However, NVIDIA has developed helpful compiler tools which indicate when you are using too much or too few registers per stream processor. It is easy to tweak your GPU code to make use of the right amount of registers and L1 cache for fast performance. This gives GPUs an advantage over other architectures like Xeon Phis where this utilization is complicated to achieve and painful to debug which in the end makes it difficult to maximize performance on a Xeon Phi.\n\nWhat this means, in the end, is that you can store a lot of data in your L1 caches and register files on GPUs to reuse convolutional and matrix multiplication tiles. For example the best matrix multiplication algorithms use 2 tiles of 64x32 to 96x64 numbers for 2 matrices in L1 cache, and a 16x16 to 32x32 number register tile for the outputs sums per thread block (1 thread block = up to 1024 threads; you have 8 thread blocks per stream processor, there are 60 stream processors in total for the entire GPU). If you have a 100MB matrix, you can split it up in smaller matrices that fit into your cache and registers, and then do matrix multiplication with three matrix tiles at speeds of 10-80TB/s — that is fast! This is the third reason why GPUs are so much faster than CPUs, and why they are so well suited for deep learning.\n\nKeep in mind that the slower memory always dominates performance bottlenecks. If 95% of your memory movements take place in registers (80TB/s), and 5% in your main memory (0.75TB/s), then you still spend most of the time on memory access of main memory (about six times as much).\n\nThus in order of importance: (1) High bandwidth main memory, (2) hiding memory access latency under thread parallelism, and (3) large and fast register and L1 memory which is easily programmable are the components which make GPUs so well suited for deep learning.","n":0.026}}},{"i":889,"$":{"0":{"v":"Graphql","n":1},"1":{"v":"\n## Overview\nGraphql is a protocol that gets implemented on a server, which gets communicated with from a client. The Graphql protocol defines a language for the client to use. The Graphql server is an expert at locating data. Through its resolvers, it knows exactly where every piece of data resides, and it knows how to get it through the most effective means. But in order to be able to know exactly which data you want, you have to speak to it in the graphql language.\n\nGraphQL is useful when data is relational. Related data can be modeled as a graph and can thus power a GraphQL API. - This is precisely what the GraphQL Engine does.\n\nSince Graphql is just a spec, we need to have an implementation to use it (similar to how SQL is a spec for Postgres, MySQL etc). The 2 components of a GraphQL implementation are: the GraphQL server and the GraphQL client.\n\nGraphql has resolvers, each understanding where a certain resource of data is and how to get it. We might have data in a mongo database or a Postgres database, and as long as the resolver knows how to get it, Graphql doesn't really care. All it cares is that the data is retrievable.\n\nTo teach our Graphql server to be able to retrieve the data, we need to define a type system for it.\n\nGraphql uses `POST` requests for queries and mutations under the hood, and websockets for subscriptions\n\n<!-- TODO: broken image -->\n<!-- ![2bc1e31d73c0da1e8598d766424dd751.png](:/a10e5211520a457fb318c12aec62d1ea) -->\n\n### Phases\nIn GraphQL we have 3 phases:\n1. *Parsing* - A [[document|graphql.documents]] is received by the Graphql engine and is parsed into an [[graphql.ast]], catching any syntax errors and stopping execution.\n2. *Validation* - The Graphql server validates against the schema that we're only querying fields that are allowed, that fragments are applied on the right fields, etc.\n\t- note that this is handled for us, whereas if this was REST, it would be up to us.\n3. *Execution* \n\n## Between the client and app server\nA key thing to understand about GraphQL is that it’s actually agnostic to the way that data is transferred over the network\n- ie. it can work on protocols other than HTTP, like websockets\n\n* * *\n\n## Selection Sets\nA selection set in Graphql is similar to a result set in SQL, except the selection set is where we specify what we'd like to get back, while a result set in SQL is what we *actually* got back. \n\nAs the name suggests, a selection set is a list of Selections (type Selection)\n- a Selection has the following signature:\n```gql\ntype Selection {\n\tField\n\tFragmentSpread\n\tInlineFragment\n}\n```\n\nA selection set might look like:\n```gql\n{\n\tid\n\tfirstName\n\tlastName\n}\n```\nSome fields may describe more complex data or relationships to other data. To allow us to explore this data, a field itself may contain a selection set.\n- To remain unambiguous, the most nested fields must be scalars\n\n### Distributed graphql\nanal: We can have a Postgres database, Mongo database, Redis cache etc, and each is represented by a seed. The orange strand represents the means to access the data from that database. A distributed graphql solution essentially bundles the seeds together, and wraps the orange meat together, giving us a single means to access data from all the databases.\n![](/assets/images/2021-12-02-10-49-11.png)\n\nDistributed graphql can be achieved through schema stitching, [[apollo.federation]]\n\n[more info](https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2)\n\n### UE Resources\n[Graphql concepts visualized](https://www.apollographql.com/blog/the-concepts-of-graphql-bc68bd819be3/#.hfczgtdsj)\n[security (preventing DDOS)](https://www.apollographql.com/blog/securing-your-graphql-api-from-malicious-queries-16130a324a6b/)\n","n":0.043}}},{"i":890,"$":{"0":{"v":"Variables","n":1},"1":{"v":"\nTechnically, we could do string interpolation to make our queries dynamic. However, this wouldn't be a good idea because our client would be tasked with manipulating that string at runtime. Instead, Graphql provides us with first-class variables that allow us to factor-out the dynamic parts of a query (eg. userId, first: 10)\n\nVariables must be either scalars, enums or input object types.\n- Input Object Type: Passing in an object might be a more sensible solution than strings, if there is related data. For instance, imagine we are running a `createUser` mutation. Instead of passing `first_name`, `last_name`, as so on to the mutation, why not just define a type, and pass an object?:\n```gql\nmutation createUser($userInfo: UserInfoInput!) {\n\tcreateUser(userInfo: $userInfo)\t{\n\t\tfirst_name\n\t\tlast_last\n\t}\n}\n```\n","n":0.093}}},{"i":891,"$":{"0":{"v":"Directives","n":1},"1":{"v":"\nUsing variables, Directives (`@`) let us dynamically change the structure and shape of our queries.\n- This allows us to either `@include` or `@skip` over fields.\n- ex. In Never Forget, consider the BrowseNugget screen. Our list component allows us to switch between just showing the nugget name, and showing a summary of the nugget. To determine whether or not we want the summary, we can use a variable:\n```gql\nquery Nugget($details: details) {\n\tnugget {\n\t\ttitle\n\t\tmediaItems @include(if:  $details) {\n\t\t\ttitle\n\t\t}\n\t}\n}\n```\n\nDirectives shouldn’t affect the value of the results, but do affect which results come back and perhaps how they are executed.\n\nDirectives are useful when we find ourselves needing to do string manipulation in order to modify (ie. add/remove fields from) our query structure.\n\nTo be Graphql spec-compliant, a server must implement only the `@include` and `@skip` directives. This leaves each server implementation open to extending their own directives.\n\n### Use with inline fragments\nDirectives can also be applied with inline fragments like so:\n```gql\nquery inlineFragmentNoType($expandedInfo: Boolean) {\n  user(handle: \"zuck\") {\n    id\n    name\n    ... @include(if: $expandedInfo) {\n      firstName\n      lastName\n      birthday\n    }\n  }\n}\n```\n\n### Deprecation\nFields in an object may be marked with the `@deprecated` directive like so:\n```gql\ntype ExampleType {\n  oldField: String @deprecated\n}\n```\n\nThese fields can still be queried (to prevent breaking changes).\n","n":0.071}}},{"i":892,"$":{"0":{"v":"Types","n":1},"1":{"v":"\nIn the same way a schema defines the shape of the total response, the type of an individual field defines the shape of that field's value.\n- The shape of the data we return in our resolver must likewise match this expected shape. When it doesn't, we frequently end up with unexpected nulls in our response.\nGraphQL is based on its type system, and depending on what type the GraphQL server receives, it will determine what it should do next.\n- If we are trying to execute a query to get all nuggets, then the GraphQL server will see we are querying an object (nuggets). Since it is an object (and therefore not a scalar, which would end the execution cycle), GraphQL server knows that it needs to \"fetch\" those fields on the object. It then checks the type of each field. If they are non-scalar, then it checks for more data. This continues until every field is a scalar type. \n\n# User-defined Types\nIn GraphQL we deal with 8 different types:\n- scalars, enums, lists, objects, input objects, interfaces, unions, non-null\n\n## Scalar\nScalar types are:\n1. Int\n2. Float\n3. String\n4. Boolean\n5. ID\n\nA GraphQL object type has a name and fields, but at some point those fields have to resolve to some concrete data. These basic types of data are called scalars, and they are the \"atomic type\" of GraphQL\n- At each type, graphql will keep resolving until it gets to the scalar\n\nScalar types resolve to a scalar object. By definition, they cannot have sub-selections (fields) in the query.\n\nAll Graphql scalars are representable as strings\n\nGraphql provides built-in scalars, but type systems can add additional scalars\n- ex. Imagine making a scalar called `Time`, which we define as conforming to ISO-8601. When we query a field of type `Time`, we can rely on the ability to parse the result with an ISO-8601 parser. On the client, we can use a primitive (such as Date) to represent the value.\n- ex. Imagine a scalar called `Url`. It would still be serialized as a string, but would be guaranteed by the server to be a valid URL.\n\n### ID\nID is a wrapped type of kind `NON_NULL`\n\n## List\nRepresents are sequence of values in Graphql.\n- A list type is a *type modifier*, meaning it wraps another type, found in the `ofType` field. This field defines the type of each item in the list.\n\nWe can request paginated data from list fields with the `first` argument\n\n## Object\nWhile scalars represents the leaf values of a query hierarchy, objects represent the intermediate levels.\n\nRepresent concrete instantiations of sets of fields. the introspection types (e.g. `__type`, `__field`, etc) are examples of objects.\n\n## Input Object\nA composite type used for inputs into queries, defined as a list of named input values.\n- often, an update and create operation on a db record will take in the same inputs (ex. title, mediaitems...).\n- an input type is simply a type that includes all of these same inputs so we can reuse it in multiple places.\n- input types can't have fields that are other objects, only basic scalar types, list types, and other input types.\n```gql\ntype nugget {\n\tid: id!\n\ttitle: string\n\tmediaitems: json\n}\n\ninput nuggetinput {\n  title: string\n  mediaitems: json (?)\n}\n\n// then...\n\ntype mutation {\n  createnugget(input: nuggetinput): nugget\n  updatenugget(id: id!, input: nuggetinput): nugget\n}\n```\n\n## Null\na null result on a Non-Null type bubbles up to the next nullable parent. If this bubbling never stops because everything is of Non-Null type, then the root data field is null.\n- in other words, if we are trying to query a field that is `NONNullable` and the result happens to be `null`, that `null` will bubble, and the parent \"object\" will result in null. If however the field we are querying is `nullable` and the result happens to be `null`, it will remain at that field and not bubble\n- When fields are nullable, you still get partial data: even if you don't have the name, you can still show the age and other fields. When you mark fields as Non-Null, as in the second example, you forfeit your right to partial data.\n[source](http://spec.graphql.org/June2018/#sec-Errors)\n\n## Interface\nSimilar to how fragments allow us to DRY our queries, interfaces can DRY our type definitions.\n\nInterfaces represent a list of named fields and their arguments. These interfaces can then be implemented by objects.\n\n## Union\nUnions represent an object that could be one of many specified types. However, there is no guarantee that any of the fields on each of those types will be provided (whereas interfaces guarantee that a field will be available)\n\nUnlike interfaces, Unions do not implement fields of their own\n\nThey differ from interfaces in that Object types declare what interfaces they implement, but they are not aware of what unions contain them.\n\n```gql\nunion SearchResult = Photo | Person\n\ntype Person {\n  name: String\n  age: Int\n}\n\ntype Photo {\n  height: Int\n  width: Int\n}\n\ntype SearchQuery {\n  firstSearchResult: SearchResult\n}\n```\n\n## Non-null\nGraphql fields are nullable, making `null` a valid response for field type.\n- Like lists, a non-null type is a type modifier (it wraps another type instance in the `ofType` field).\n","n":0.035}}},{"i":893,"$":{"0":{"v":"Tools","n":1},"1":{"v":"\n# Client-side\n[Periqles: Generate a React form from a Graphql mutation](https://github.com/oslabs-beta/periqles)\n\n# Server-side\n## The Guild Libraries\n[Graphql-Tools: Graphql-first mentality to building Graphql Schema](https://www.graphql-tools.com/)\n- graph-tools provides a thin convenience layer on top of graphql.\n- we can get schema validation\n- biggest benefit of using graphql-tools is its nice API for connecting your declarative schema with resolvers\n\n[Graphql-Inspector: Maintain and improve Graphql API thru validation (+receive notifications) (runs in CI/CD pipeline)](https://graphql-inspector.com/)\n\n[Graphql-Scalars: A library of custom scalar types, ready to be imported](https://github.com/Urigo/graphql-scalars)\n","n":0.116}}},{"i":894,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\n### UE Resources\n[mocking graphql](https://graphql.org/blog/mocking-with-graphql/)\n","n":0.5}}},{"i":895,"$":{"0":{"v":"Structures","n":1}}},{"i":896,"$":{"0":{"v":"Edge","n":1},"1":{"v":"\nAn edge describes the relationship between a parent object and the target node. In our graph above, we have a central node (the parent) that points to other objects. The relationship (edge) is that of friendship. The entity on the other end of the line is the node.\n- An edge has metadata about one object in the paginated list, and includes a cursor to allow pagination starting from that object.\n- Each edge has a node and a cursor \n\nAn edge type may look like so:\n```gql\ntype UserFriendsEdge {\n  cursor: String!\n  node: User\n}\n```\nEdges are not just valuable for pagination purposes.\nIn graph theory an edge can have properties of its own, which act effectively as metadata.\nIt has become common to think of the `edges` field as boilerplate, and simply a container for the cursor, but it actually can be quite powerful. When we consider that an edge is a field that represents the relationship between 2 nodes, we can start to realize there is a lot of potentially appropriate fields to describe that connection.\n- ex. In the friend graph above, we can keep metadata about the friendship, such as when it started.\n- ex. Relevancy score when searching (e.g the relevancy of this result to the input search query is 0.7).\n- ex. Distance when searching/sorting by distance from a certain location.\n\nIn the above cases, the field doesn't belong on the node because its value changes depending on the connection parameters. Remember, you should be able to get to the same record (e.g. node) via multiple routes within a single query and the values for its fields should be the same, so contextual data has to go somewhere else -- the edge types.\n\n","n":0.06}}},{"i":897,"$":{"0":{"v":"Connection","n":1},"1":{"v":"\nA connection is a collection of objects with metadata such as `edges`, `pageInfo`...\n- It is effectively the edges, and the metadata associated with their environment.\n- `pageInfo` will contain `hasNextPage`, `hasPreviousPage`, `startCursor`, `endCursor`\n\t- `hasNextPage` will tell us if there are more edges available, or if we’ve reached the end of this connection.\n\nA connection is a paginated field on an object — for example, the friends field on a user or the comments field on a blog post.\n- very similar to [[cursor-based pagination|graphql.pagination]].\n\nConnections are made up of edges, but a connection is more than just that, since it also contains metadata about the group of edges, such as `pageInfo`, which gives us pagination info (ex. what the most recent cursor was, if there is another page or not, etc)\n![](/assets/images/2021-03-09-21-57-34.png)\nIn the above image, the connections would be all of the grey lines together as one. \n\nWe might define our `User` type like this:\n```gql\ntype User {\n  id: ID!\n  name: String\n  friendsConnection(\n    first: Int,\n    after: String,\n    last: Int,\n    before: String\n  ): UserFriendsConnection\n}\n```\n\nThis might lead to a query like so:\n```gql\n{\n  user(id: \"ZW5jaG9kZSBIZWxsb1dvcmxk\") {\n    id\n    name\n    friendsConnection(first: 3) {\n      edges {\n        cursor\n        node {\n          id\n          name\n        }\n      }\n    }\n  }\n}\n```\nA connection is a way to get all of the nodes that are connected to another node in a specific way\n- In this case we want to get all of the nodes connected to our users that are friends. Another connection might be between a user node to all of the posts that they liked.\n\nA connection is by nature an abstract concept, and it is difficult to think about. An edge makes sense, because we can think of a user having a friendship with another user, or we think of a user authoring a post.\n\nA connection type may look like so:\n```gql\ntype UserFriendsConnection {\n  pageInfo: PageInfo!\n  edges: [UserFriendsEdge]\n}\n```\n","n":0.058}}},{"i":898,"$":{"0":{"v":"Graphql Server","n":0.707},"1":{"v":"\nA GraphQL server essentially takes in your API and exposes your GraphQL API via an endpoint. It has two core parts:\n- A Schema, which includes type definitions, and defines:\n\t- what queries clients are allowed to make\n\t- what types of data can be fetched from the server\n\t- what the relationships between these types are\n- [[Resolve functions|graphql.server.resolver]], which hold all functions that define how to get the data\n\n### Type Coercion\nwhen preparing a field of a given scalar type, a GraphQL server must uphold the contract the scalar type describes, either by: \n- coercing the value\n- producing a field error if a value cannot be coerced or if coercion may result in data loss.\n\nA GraphQL server may decide to allow coercing different internal types to the expected return type. Unless the coercion is sensical (ie. no information is lost), the Graphql server will raise a field error.\n- For example when coercing a field of type `Int`, a boolean `true` value may produce `1`. Alternatively, a string value \"123\" may be parsed as base‐10 123.\n\nThe Graphql server will also coerce input values it receives as arguments to fields, as long as those arguments are a scalar type.\n- For example, if we pass a string `\"4\"` or int `4` to an ID input type, the value should be coerced to an ID format (as expected by the Graphql server)\n\n## Examples\n- express-graphql\n- apollo-server\n- graphql-yoga\n\nIn Express, these are nothing but middleware functions that act as glue between the request and GraphQL engine provided by graphql.js (the tool that provides functionality for resolving queries)\n\n- express-graphql has 2 responsibilities:\n\t1. Ensure that the GraphQL query (or mutation) contained in the body of an incoming POST request can be executed by GraphQL.js.\n\t2. Attach the result of the execution to the response object so it can be returned to the client.\n- apollo-server is more generic than express-graphql, so it works in other frameworks, in addition to express.\n\t- it can also be configured to work as FaaS, like AWS Lambda\n- graphql-yoga\n\t- like create-react-app for GraphQL servers\n","n":0.055}}},{"i":899,"$":{"0":{"v":"Resolver","n":1},"1":{"v":"\nResolvers specify how the types and fields in the schema are connected to various backends, making them essentially mini-routers. \n- Through these functions, you are able to answer questions such as “How do I get the data regarding Course Authors?” and “Which backend do I need to call with what arguments to get the data regarding Courses?”.\n\nEach graphql type needs a resolver. The resolver exists to resolve a type from some data layer (field resolvers are the same thing; they just resolve fields on a type)\n\nIf we were converting from a REST API with 20 different endpoints, then we would end up with close to 20 resolvers\n- this mapping is not absolute, but is a good rough guide. For example, maybe we want to implement a resolver `createOrUpdateBook`. RESTful best practice would have us splitting the creation and updating of a book exist as 2 different endpoints, but this would be fine to combine as a single [[mutation|graphql.documents.mutations]]. In this case, we would implement some logic in the resolver \"if an `id` is passed as input to the mutation, update the book. Otherwise, create it\".\n\nA Resolver is a collection of functions whose responsibility is sourcing the data for a particular field. They are responsible for generating a response to the Graphql query.\n- the function is mapped to a schema. In other words, each type in the schema has a corresponding resolver\n- The schema (made up of queries and mutations) says \"here's what you can look at\", and the resolver says \"here's how you get it\"\n\t- If we had a SQL database, the resolver could be configured to call some REST endpoint, and that REST endpoint would ultimately execute the SQL query.\n- The resolvers map to your schema and are what GraphQL actually executes to retrieve each piece of data. The resolvers are like controllers in a regular REST API.\n- a resolver receives 4 args:\n\t1. obj\n\t2. args\n\t3. context\n\t4. info\n\n### `obj`\nthe previous object in the graphql \"tree\" (with the root being `query` or `mutate`). It contains the result returned from the resolver on the parent field.\n- sometimes aka `root`\n- when we are making a resolver function on the root Query type, we probably won't need to use `obj`.\n- All Graphql has to do in order to resolve a query is call the resolvers on the query's fields. This is being done level by level (in other words, from most outdented to most indented; ltr).\n- `obj` argument in each resolver call is simply the result of the previous call\n\t- ex. we are querying `getNuggetById`. `obj` is the `query` type at this point. When the backend receives that request, the resolver executes a db query and returns to us `{ id: 1, title: 'first nugget' }`. With the first field resolved, the value of `obj` on the second iteration is the `nugget` type, since that is what was returned by the first iteration. This is precicely the reason why we don't have to explicitly write resolvers for every single field.\n\t- In fact, if we were to console.log `obj`, on the second iteration we'd have the nugget object.\n```gql\nquery {\n\tnugget(id: $id) {\n\t\tid\n\t\ttitle\n\t}\n}\n```\n\n### `args`\nThe arguments provided to the field in the GraphQL query\n\n### `context`\n![[graphql.server.resolver.context]]\n\n### `info`\nholds field-specific information relevant to the current query as well as the schema details\n- The way you form relationships is by defining custom types in your resolvers\n- In its most basic form, a GraphQL server will have one resolver function per field in its schema\n\t- Each resolver knows how to fetch the data for its field\n\n## How GraphQL resolves fields\nWhile you certainly can write a resolver for every field in your schema, it's often not necessary because the [[graphql.server]] uses a default resolver when you don't provide one.\n- in most cases, the GraphQL library will just omit simple resolvers and will just assume that if a resolver isn't provided for a field, that a property of the same name should be read and returned.\n- what the default resolver does is simple: it looks at the value the parent field resolved to and if that value is a JavaScript object, it looks for a property on that Object with the same name as the field being resolved. If it finds that property, it resolves to the value of that property. Otherwise, it resolves to null.\n\t- This process is the reason why deeply nested queries are more computationally expensive.\n- GraphQL queries always end at scalar values.\n\n# E Resources\n[Good breakdown of how schema/resolvers work](https://www.prisma.io/blog/graphql-server-basics-the-schema-ac5e2950214e)\n","n":0.037}}},{"i":900,"$":{"0":{"v":"Context Object","n":0.707},"1":{"v":"\n`context` is an object that gets passed through the resolver chain that each resolver can read from and write to (basically a means for resolvers to communicate and share information).\n- If we were using Apollo Server, we would set the context when initializing a new `ApolloServer`.\n\nholds info like... \n- currently logged in user, \n- current access to the database (which includes postgres user) etc.\n- access token\n- correlationId\n- [[data sources|general.patterns.data-source]]\n    - `dataSource` is part of the constructor for the `ApolloServer` class. The `dataSource` instances are used by the resolvers via the `context` object.\n\nTherefore, we can use the context to provide access to the database\n","n":0.099}}},{"i":901,"$":{"0":{"v":"Graphql Schema","n":0.707},"1":{"v":"\nSpecifies the capabilities of the API and defines how clients can request the data. It is often seen as a contract between the source of data (ie. server) and its destination (ie. client), and it defines what data can be queried and how it ought to be queried\n- the schema gives the app's backend the ability to say \"here's the data that I want to make available to the client\"\n- when queries come in, they are validated and executed against the schema\n- Our schema, along with the requested query, defines the \"shape\" of the data object in the response returned by our endpoint\n\t- By shape, we mean what properties objects have, and whether those properties' values' are scalar values, other objects, or arrays of objects or scalars\n- Prefer building a GraphQL schema that describes how clients use the data\n\nThe schema is a mode of the data that can be retrieved through the Graphql server. It specifies:\n- It specifies what queries clients are allowed to make\n- what types (scalar, object, query, mutation) of data can be fetched from the server\n- what the relationships between these types are.\n\nThe Schema says \"Here's the data you can look at\". If we were to imagine our database as a complex graph, then the schema would be the graph. The schema defines how we can query and alter the data of the underlying database.\n\n- ex. a simple user and post app `schema.gql`:\n```gql\n// schema may be implicitly defined\nschema {\n\tquery: Query\n\tmutation: Mutation\n}\n\ntype Query {\n\tgetUser(uuid: String!): User\n}\n\ntype Mutation {\n\tcreateUser(input: UserInput!): User\n}\n\ntype User {\n\tUUID: String\n\tName: String\n\tPosts: [Post]\n}\n\ntype Post {\n\tUUID: String\n\tText: String\n}\n\ninput UserInput {\n\tName: String\n\tPosts: [PostInput]\n}\n\ninput PostInput{\n\tText: String\n}\n\n```\n\nA field does not become queryable until it exists on the `Query` type:\n```gql\ntype Query {\n\tmyName: String\n}\n```\n\n## Descriptions\nDescriptions are a first-class way of documentation in Graphql. To implement, we only need to include comments before the definition we are describing. There are 2 different ways to add documentation: Comment blocks for constructs, and inline comments for fields:\n```graphql\n\"\"\"\nA simple GraphQL schema which is well described.\n\"\"\"\ntype Query {\n  \"\"\"\n  Translates a string from a given language into a different language.\n  \"\"\"\n  translate(\n    \"The original language that `text` is provided in.\"\n    fromLanguage: Language\n\n    \"The translated language to be returned.\"\n    toLanguage: Language\n\n    \"The text to be translated.\"\n    text: String\n  ): String\n}\n\n\"\"\"\nThe set of languages supported by `translate`.\n\"\"\"\nenum Language {\n  \"English\"\n  EN\n\n  \"French\"\n  FR\n\n  \"Chinese\"\n  CH\n}\n```\n\n\n","n":0.051}}},{"i":902,"$":{"0":{"v":"Pagination","n":1},"1":{"v":"\n\nAt a high level, cursor-based pagination works like this:\n1. User requests some data (either through initial page load, or clicking some UI pagination button).\n2. Along with the list of data that to display, we also want to get pagination information `pageInfo` that is defined on the connection\n\n```gql\nquery OrganizationForLearningReact {\n  organization(login: \"the-road-to-learn-react\") {\n    name\n    url\n    repository(name: \"the-road-to-learn-react\") {\n      issues(first: 10, after: \"Y3Vyc29yOnYyOpHODAESqg==\") {\n        pageInfo {\n          endCursor\n        }\n        edges {\n          node {\n            author {\n              login\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n3. Get the `id` of the last item in the most previously-fetched data list (known as the `cursor`) \n  - stored in `endCursor` variable\n4. Store the cursor in local state (e.g. React, Apollo Client)\n5. pass as variable on subsequent fetches of data (ex. when user clicks \"Load more\", or scrolls more on infinite page)\n6. use that variable as input to the `after` parameter defined on the Connection (ex. in looking through a list of Github issues, the cursor is defined on the `IssuesConnection`)\n\n[[Edges|graphql.structures.edge]] enable us to perform cursor-based pagination. Since the cursor is (usually) just an `id`, we can use the `after` argument on a list, passing that `id`. This allows us to specify the starting point that data should be retrieved from","n":0.071}}},{"i":903,"$":{"0":{"v":"N+1 Problem","n":0.707},"1":{"v":"\nGraphQL suffers from the N+1 problem\n\n- The number of queries grows exponentially with the depth of the query\n- N1 problem occurs when you have to retrieve the same information multiple times. In graphql, the resolver for that particular field would have to be hit every single time, instead of the system understanding that it's the same result, and being more efficient about it.\n  - ex. We have a query shaped as below. imagine that the article has 5 `comments` and all are written by the same person. In Graphql, that would be 5 times that the resolver got called, instead of once.\n\n```gql\nquery {\n  user(id: \"abc\") {\n    name\n    article(title: \"GraphQL is great\") {\n      comments {\n        text\n        writtenBy {\n          name\n        }\n      }\n    }\n  }\n}\n```\n\n- N1 problem can be solved by DataLoader library\n- [possibly redundant info](https://stackoverflow.com/questions/97197/what-is-the-n1-selects-problem-in-orm-object-relational-mapping)\n- [using a batchloader (like joinMonster) to solve N+1](http://www.petecorey.com/blog/2017/08/14/batching-graphql-queries-with-dataloader/?from=east5th.co)\n","n":0.084}}},{"i":904,"$":{"0":{"v":"Introspection","n":1},"1":{"v":"\nIntrospection is about the links between types and scalars. Introspection tells us what queries are supported\n- The Graphql server is what supports introspection over its schema.\n\nThe type information that is exposed by introspection can be used to build client-side and server-side tooling.\n- The fact that Graphiql exists (ie. that it knows all of our types, how they relate to each other etc) is enabled by introspection.\n\n\n\nTypes and fields required by the Graphql introspection system are prefixed with `__`\n- these types are implicit, and naturally they won't show up when we make queries.\n\nIntrospection means that the server (or the API) that we're building actually knows about the types that it's using.\n- It also exposes them in a standard way.\n\nDetermining which types are available\n```gql\n{\n  __schema {\n    types {\n      name\n    }\n  }\n}\n```\n\n## Introspection Types\n`__Type` is at the core of the type introspection system. It represents scalars, interfaces, object types, unions, enums in the system.\n- All types in the introspection system provide a `description` field to allow type designers (ie. us, the developer) to publish documentation about a type.\n\n### __Field\nRepresents each field in an Object or Interface type.\n\n\n### __InputValue\nRepresents field and directive arguments as well as the inputFields of an input object.\n\nHas fields:\n- `name`— returns string\n- `description`— returns string or null\n- `type`— returns `__Type`, representing the type this input value expects\n- `defaultValue`\n\n## Introspection Queries\nThe schema introspection system is accessible from the meta‐fields `__schema` and `__type` which are accessible from the type of the root of a query operation.\n\nCan be of 3 types:\n```gql\n__schema: __Schema!\n__type(name: String!): __Type\n__typename: String!\n```\n\nThese fields are implicit and do not appear in the fields list in the root type of the query operation.\n\nThe schema of the GraphQL schema introspection system:\n\n```gql\ntype __Schema {\n  types: [__Type!]!\n  queryType: __Type!\n  mutationType: __Type\n  subscriptionType: __Type\n  directives: [__Directive!]!\n}\n\ntype __Type {\n  kind: __TypeKind!\n  name: String\n  description: String\n\n  # OBJECT and INTERFACE only\n  fields(includeDeprecated: Boolean = false): [__Field!]\n\n  # OBJECT only\n  interfaces: [__Type!]\n\n  # INTERFACE and UNION only\n  possibleTypes: [__Type!]\n\n  # ENUM only\n  enumValues(includeDeprecated: Boolean = false): [__EnumValue!]\n\n  # INPUT_OBJECT only\n  inputFields: [__InputValue!]\n\n  # NON_NULL and LIST only\n  ofType: __Type\n}\n\ntype __Field {\n  name: String!\n  description: String\n  args: [__InputValue!]!\n  type: __Type!\n  isDeprecated: Boolean!\n  deprecationReason: String\n}\n\ntype __InputValue {\n  name: String!\n  description: String\n  type: __Type!\n  defaultValue: String\n}\n\ntype __EnumValue {\n  name: String!\n  description: String\n  isDeprecated: Boolean!\n  deprecationReason: String\n}\n\nenum __TypeKind {\n  SCALAR\n  OBJECT\n  INTERFACE\n  UNION\n  ENUM\n  INPUT_OBJECT\n  LIST\n  NON_NULL\n}\n\ntype __Directive {\n  name: String!\n  description: String\n  locations: [__DirectiveLocation!]!\n  args: [__InputValue!]!\n}\n\nenum __DirectiveLocation {\n  QUERY\n  MUTATION\n  SUBSCRIPTION\n  FIELD\n  FRAGMENT_DEFINITION\n  FRAGMENT_SPREAD\n  INLINE_FRAGMENT\n  SCHEMA\n  SCALAR\n  OBJECT\n  FIELD_DEFINITION\n  ARGUMENT_DEFINITION\n  INTERFACE\n  UNION\n  ENUM\n  ENUM_VALUE\n  INPUT_OBJECT\n  INPUT_FIELD_DEFINITION\n}\n```\n\n### __schema\nThe most important field (query) in graphql, as it allows us to fetch the whole schema. It is also the primary source for Graphiql. The name for this query is `__schema`, and its Schema Definition Language (SDL) is:\n```gql\ntype __Schema {\n  types: [__Type!]!\n  queryType: __Type!\n  mutationType: __Type\n  subscriptionType: __Type\n  directives: [__Directive!]!\n}\n```\n\nAs we can see, when we query `__schema`, we get some information about the whole schema, including which operation types we have, and which types and directives we have. The result of that query might look like this:\n```gql\n{\n  __schema {\n    directives {\n      name\n      description\n    }\n    subscriptionType {\n      name\n      description\n    }\n    types {\n      name\n      description\n    }\n    queryType {\n      name\n      description\n    }\n    mutationType {\n      name\n      description\n    }\n    queryType {\n      name\n      description\n    }\n  }\n}\n```\n\n### __type\nAllows us to query for information about the exact type we are interested in. All we need to do is pass in the type as an argument like so:\n```gql\nquery introspectionUserType {\n  __type(name: \"User\") {\n\tname\n\tkind\n\tfields {\n\t\tname\n\t\ttype {\n\t\t\tname\n\t\t}\n\t}\n  }\n}\n```\nThis might return:\n```gql\n{\n  \"__type\": {\n    \"name\": \"User\",\n\t\"kind\": \"OBJECT\"\n    \"fields\": [\n      {\n        \"name\": \"id\",\n        \"type\": { \"name\": \"String\" }\n      },\n      {\n        \"name\": \"name\",\n        \"type\": { \"name\": \"String\" }\n      },\n      {\n        \"name\": \"birthday\",\n        \"type\": { \"name\": \"Date\" }\n      },\n    ]\n  }\n}\n```\n\nWhen the `name` field is `null`, it is because it is a wrapper type (ID, List). If we query for the `ofType` on these fields, we can see what the \"wrapped\" type is\n- ex. a list is a wrapper type, because each item in the list has its own type. The List is just a type that wraps them altogether. The same can be said for Non-null and ID.\n\n### __typename\nOffers type name introspection. `__typename` is a meta-field that allows us to get the type name at any point within a query. It is available through all types when querying.\n\nApollo client makes use of `__typename` to construct the apollo cache. It uniquely identifies an item by `id + __typename`\n","n":0.037}}},{"i":905,"$":{"0":{"v":"Fields","n":1},"1":{"v":"\nEvery field on a GraphQL object type can have zero or more arguments:\n\n```gql\ntype Starship {\n  id: ID!\n  name: String!\n  length(unit: LengthUnit = METER): Float\n}\n```\n\nREST endpoints are similar to GraphQL fields, as they are entry points into the data that call functions on the server.\n- sometimes you can have an entity that is both a field and a type. When I query for one nugget, I am querying on a field `nugget`, and getting back a Nugget type.\n\t- in this case, `nugget` is known as the root field\n\nEach field on each type is backed by a *resolver*\n- When a field is executed, the corresponding resolver is called to produce the next value. If a field produces a scalar value like a string or number, then the execution completes. However if a field produces an object value then the query will contain another selection of fields which apply to that object. \n  - This continues until scalar values are reached. GraphQL queries always end at scalar values.\n\nFields are conceptually functions which return values, and occasionally accept arguments which alter their behavior.\n- These arguments often map directly to function arguments within a GraphQL server’s implementation.\n- you can think of each field as a function of the previous type which returns the next type\n\t- therefore, fields can exist at different levels. Imagine we had a `User` type, and that `user` had an `id` and `name`. We would be looking at 3 fields in total here, since `user` is just a field defined on the `Query` type. \n\nSince fields are conceptually just functions, that means we may be able to pass arguments to them. \n- Arguments can also be passed into scalar values, which would allow the server to do data transformations before sending it to the client (as opposed to the client having to handle that). All we need to do is define an enumeration type:\n\n```gql\ntype Person {\n\tname: String\n\tpicture(size: Int): Url\n}\n\n{\n  person(id: \"1000\") {\n    name\n    picture(size: 600)\n  }\n}\n```\n\n## Alias\nBy default, the response of our query is an object with a key matching the field name(s) we queried\n\nSometimes we want to request data about 2 identical objects. (ex. querying the same field twice, but with different arguments). In cases like this, we need to use aliases, which changes the key of the object we get as our query result.\n\n","n":0.051}}},{"i":906,"$":{"0":{"v":"Graphql Documents","n":0.707},"1":{"v":"\nThere is a distinction between Graphql Documents and Operations:\n- An operation is a [[query|graphql.documents.queries]], [[mutation|graphql.documents.mutations]], or [[subscription|graphql.documents.subscriptions]]\n- A document is one or more operation (including any [[fragments|graphql.documents.fragments]]) stringified in a request body.\n\n`query` types and `mutation` types are the entry point to a graphql query\n- It's important to remember that other than the special status of being the \"entry point\" into the schema, the Query and Mutation types are the same as any other GraphQL object type, and their fields work exactly the same way.\n\nEvery GraphQL query has the shape of a tree — i.e. it is never circular. \n\nScalars are primitive values in GraphQL. If we consider a graphql query as a hierarchical graph, the leaves would be the primitives\n\nGraphQL requires that you construct your queries in a way that only returns concrete data\n- Each field has to ultimately resolve to one or more scalars (or enums). That means you cannot just request a field that resolves to a type without also indicating which fields of that type you want to get back (hence why `graphiql` will auto-complete for us).\n\ntwo operations that occur in GraphQL servers are:\n1. **result coercion**: upholding the contract of a type which we receive from the server (basically upholding the primitive values or object type)\n\t- The type system knows what to expect and will convert the values returned by a resolver function into something that upholds the API contract\n2. **input coercion**: upholding the contract of a type for input arguments that we pass into the GraphQL query or mutation\n\t- if we pass in `5` for the `id` field, the type will be parsed into a string as `\"5\"`\n\n### Arguments\nIn REST, there is one place to pass arguments, which is in query params or in the request body. With Graphql, every field and nested object can get its own set of arguments.\n\n- declarative data fetching where a client can specify exactly what data it needs from an API. Instead of multiple endpoints that return fixed data structures, a GraphQL server only exposes a single endpoint and responds with precisely the data a client asked for.\n- The `GraphQLSchema` object is the core of a GraphQL server\n\t- `GraphQLSchema` consists of 2 parts:\n\t\t1. schema definition (ie. the structure)\n\t\t2. resolver functions that implement the API (ie. the behaviour)\n\t- Query and Mutate are the rootTypes\n- Graphql is completely agnostic to the network layer (usually HTTP) and payload format (normally JSON). In fact, Graphql is not opinionated about the application architecture in general.\n- Graphql allows us to make relational queries that allow us to get all the data needed in one trip, instead of having to make multiple calls.\n","n":0.048}}},{"i":907,"$":{"0":{"v":"Subscriptions","n":1},"1":{"v":"\nSubscriptions offer real-time connection from the client to the server that allows the client to get immediately informed about events happening server-side\n- when a client subscribes to an event, it will hold a steady connection to the server. when this event happens, the server will push that corresponding data to the client\nTherefore, subscriptions are event-based, acting in response to what just happened.\n- We can see this in the subscription nomenclature: `commentAdded`, `paymentMethodAdded`\n- Unlike queries and mutations that follow a typical “request-response-cycle”, subscriptions represent a *stream* of data sent over to the client.\n\nYou should use subscriptions for the following:\n- Small, incremental changes to large objects.\n  - Repeatedly polling for a large object is expensive, especially when most of the object's fields rarely change. Instead, you can fetch the object's initial state with a query, and your server can proactively push updates to individual fields as they occur.\n- Low-latency, real-time updates.\n  - For example, a chat application's client wants to receive new messages as soon as they're available.\n\n```gql\nsubscription {\n  newPerson {\n    name\n    age\n  }\n}\n```\nWhenever a newer mutation is performed that creates a new `Person`, the server sends the information about this person to the client\n","n":0.072}}},{"i":908,"$":{"0":{"v":"Queries","n":1},"1":{"v":"\nEvery GraphQL service has a query type and may or may not have a mutation type\n\n### Understanding how a query works\n```gql\nquery {\n  hero {\n    name\n    appearsIn\n  }\n}\n```\n\n1. We query the special root Query type, and get back an object\n2. We select the hero field on that object\n3. For the object returned by hero, we select the name and appearsIn fields\n","n":0.129}}},{"i":909,"$":{"0":{"v":"Mutations","n":1},"1":{"v":"\nWhile query fields are executed in parallel, mutation fields run in series, one after the other.\n- This means that if we send two `updateNugget` mutations in one request, the first is guaranteed to finish before the second begins\n\n### Input types\noften we will be using the same inputs to different mutations (ex. createNugget, updateNugget). Input types offers us a way to decalare these inputs once, and alias it as a type. \n","n":0.119}}},{"i":910,"$":{"0":{"v":"Fragments","n":1},"1":{"v":"\nFragments are the primary unit of composition in Graphql. They...\n- must specify the type they apply to\n- cannot be specified on any input value (scalar, enumeration, or input object).\n- can be specified on object types, interfaces, and unions.\n\nBelow, the part after `on` is the type we are selecting from\n- so `people` is of type `Person`, and we want the `firstName` and `lastName` fields from `people(id: \"7\")`\n```gql\nfragment NameParts on Person {\n  firstName\n  lastName\n}\n\nquery GetPerson {\n  people(id: \"7\") {\n    ...NameParts\n    avatar(size: LARGE)\n  }\n}\n```\n## Conditional Fragments\n### Type Conditions \n- allow us to conditionally include fields based on their runtime type.\n\nOn Facebook, imagine we have 2 different fields that both specify a `count`: User Friends, and Page Likes. We have set up our schema in such a way that we define a `Profile` type, which can be either a `User` or `Page`. \n\nNow imagine that we want to write a query that gets back both the friends count of Kyle Tycholiz, and it gets back the like count of the Never Forget page. If we want to write this in a single query, we are presented with a problem: while both have a field `count`, the parent field is different, namely `friends` and `likes`. To solve this problem, we must use fragments, which will be applied *depending on* the type of profile that is returned in the query.\n\nWhen we query for this, the `profiles` root field returns a list where each element could be a `Page` or a `User`.\n\n```gql\n{\n\tprofiles(name: [\"kyletycholiz\", \"neverforget\"]) {\n\t\tname\n\t\t...userFragment\n\t\t...pageFragment\n\t}\n\t\nfragment userFragment on User {\n\tfriends {\n\t\tcount\n\t}\n}\n\nfragment pageFragment on Page {\n\tlikers {\n\t\tcount\n\t}\n}\n}\n```\n\n### Inline Fragments\nif querying a field that returns an interface or union type, we need to use inline fragments to access data on the underlying concrete type\n- These can be used to compose fields in a type-dependent way\n    \nThe above Facebook example can be accomplished using inline fragments. \n\nThe Graphql server will determine whether to return `homeAddress` or `address` at runtime, depending on whether the requested object is a `User` or `Business`\n\n```gql\nquery Foo {\n  profile(id: $id) {\n    url\n    ... on User {\n      homeAddress\n    }\n    ... on Business {\n      address\n    }\n  }\n}\n```\n\n#### Practical usage\n- Imagine we had an interface `Character` that represents a character from Star Wars:\n```gql\ninterface Character {\n  id: ID!\n  name: String!\n  friends: [Character]\n  appearsIn: [Episode]!\n}\n```\n- Imagine now we create two types that *implement* this interface:\n```gql\ntype Human implements Character {\n  id: ID!\n  name: String!\n  friends: [Character]\n  appearsIn: [Episode]!\n  starships: [Starship]\n  totalCredits: Int\n}\n\ntype Droid implements Character {\n  id: ID!\n  name: String!\n  friends: [Character]\n  appearsIn: [Episode]!\n  primaryFunction: String\n}\n```\n- As we can see, **Droid** has the field `primaryFunction`, while **Human** does not\n    - This means that if we were to make a query that wanted that field back, we would get an error:\n```gql\nquery HeroForEpisode($ep: Episode!) {\n  hero(episode: $ep) {\n    name\n    primaryFunction\n  }\n}\n// PRODUCES ERROR: cannot query field 'primaryFunction' on type Character\n```\n- To get around this, we need to use an inline fragment:\n```gql\nquery HeroForEpisode($ep: Episode!) {\n  hero(episode: $ep) {\n    name\n    ... on Droid {\n      primaryFunction\n    }\n  }\n}\n```\n","n":0.045}}},{"i":911,"$":{"0":{"v":"Graphql Client","n":0.707},"1":{"v":"\nThe Graphql API has more underlying structure than a REST API. This means there are more things to handle and keep track of, such as batching, caching, and other features. \n","n":0.18}}},{"i":912,"$":{"0":{"v":"Authorization","n":1},"1":{"v":"\nAll authorization should be handled by the business logic layer in the application, not in graphql\n- [source](https://graphql.org/learn/authorization/)\n\nIn a REST API, authentication is often handled with a header, that contains an auth token which proves what user is making this request. Express middleware processes these headers and puts authentication data on the Express request object. \n\nThere are two broad ways of handling authentication in GraphQL APIs:\n1. **Authentication via the GraphQL server**: All users have to be logged in by the GraphQL server before they can query the endpoint—purely GraphQL workflows. Authentication is implemented in the GraphQL Schema\n\t- It involves first getting a JWT token, then passing that token in subsequent requests\n\t- this is the method proposed by Postgraphile (a JWT-based approach). \n2. **Authentication via a Web Server** (ex. Express and Passport): Users can make queries to the GraphQL endpoint once they are logged in. Authentication is implemented with middleware.\n","n":0.082}}},{"i":913,"$":{"0":{"v":"AST (Abstract Syntax Tree)","n":0.5},"1":{"v":"\nIn order to interpret and understand these unique and complex requests, GraphQL employs the use of an Abstract Syntax Tree (AST) to structure the incoming requests.\n- This makes it easier for the back-end gurus & frameworks to parse and construct the required response.\n\nAs a consequence of Graphql being data-layer-independent, libraries such as graphql-js and graphql-tools exist to abstract away the heavy lifting, removing our need to worry about having to interact with the underlying AST.\n- Sometimes however we must be able to perform our own custom operations based on the specifics of the user's request.\n\nWhen a user makes a request, GraphQL combines the query document that the user requested with the schema definition that we defined for our resolver in the form of an AST. This AST is used to determine which fields were requested, what arguments were included and much more.\n\n### Rugby Player Example\nSuppose we wanted to expose a list of rugby players on our Graphql backend. We make a schema definition which models a new Graphql type `RugbyPlayer`, which has 2 fields: `full_name` and `club`:\n```js\nexport const RugbyPlayer = new gql.GraphQLObjectType({\n  name: 'RugbyPlayer',\n  fields: {\n    full_name: {\n      type: gql.GraphQLString,\n    },\n    club: {\n      type: new gql.GraphQLObjectType({\n        name: 'Club',\n        fields: {\n          name: {\n            type: gql.GraphQLString,\n          },\n        },\n      }),\n    },\n  },\n})\n```\nImagine now that a client makes a request for that list of players. From the Graphql server's perspective, the schema knows nothing about the query document (ie. the request). Therefore, it has to parse the query document into AST format (ie. heavily nested objects) before it can traverse it. Now, it can perform any necessary validation, like throwing errors for having the wrong types for fields or arguments.\n- Following this validation, schema types are mapped onto the respective branches of the AST to provide a richer set of metadata.\n\n### What we can do with the AST\nBy understanding the AST, we can craft custom directives and optimise user requests\n- We do this by breaking the traditional Graphql lifecycle and inercepting a request before it gets passed onto another library to generate a response.\n\nBy traversing and augmenting the AST we can implement:\n- Schema stitching\n- Custom directives\n- Enriched queries\n- Layered Abstraction\n- More backend magic!\n\nIn a step to give more control to the front-end client, we often find it useful to implement a range of directives that can transform or filter out fields specified in the AST.\n- ex. In the Rugby example, imagine we want to solve this problem: \"Rugby player's names are being mispronounced too often\".\n\t- What we can do is implement a Graphql directive called `@pronounce`, which can be added onto the `full_name` field when we are querying our `RugbyPlayers` type:\n```gql\nquery {\n  RugbyPlayers {\n    full_name @pronounce\n  }\n}\n```\nlibraries like `graphql-js` can provide methods such as `gql.GraphQLDirective` that allow us to add these directives.\n\nWhenever we see code like this:\n```ts\nconst Type = gql`\n  type ${nameOfType} {\n    str: String\n    int: Int\n  }\n`;\n```\n\nWe can understand `gql` as a function that transforms our human-readable graphql schema language into an AST that the application can understand.\n","n":0.045}}},{"i":914,"$":{"0":{"v":"Graphile Worker","n":0.707},"1":{"v":"\n## Tasks\nWhen we run a task in Node (ex. with `run()`, we are returned a `Runner` object, which has the following methods/properties:\n- stop(): Promise<void> - stops the runner from accepting new jobs, and returns a promise that resolves when all the in progress tasks (if any) are complete.\n- addJob: AddJobFunction - see addJob.\n- promise: Promise<void> - a promise that resolves once the runner has completed.\n- events: WorkerEvents - a Node.js EventEmitter that exposes certain events within the runner (see WorkerEvents).\n\nex. we can add a job to the queue in response to another job being added to the queue:\n```js\nawait runner.addJob(\"testTask\", {\n  thisIsThePayload: true,\n});\n```\n\n### Events\nWe can listen to events like so:\n```js\nrunner.events.on(\"job:success\", ({ worker, job }) => {\n  console.log(`Hooray! Worker ${worker.workerId} completed job ${job.id}`);\n});\n```\n\nTo realize the importance and place of Workers, we have to break asynchronous tasks down into 2 types:\n- async tasks, where the process of execution is dependent on the response to continue on to execute the next lines of code.\n  - ex. When the Express server is making database queries, the it has to wait for the data to return before it can continue on with its operation. This is why we use `async`/`await`, because we need that data\n- async tasks, where the 2 processes are realistically distinct from one another, and can be completely decoupled.\n  - ex. When we want to send registration success emails to users, we don't want to tie up the Express server thread doing this task. It is something that can be handed off to some other service to handle for us.\n","n":0.062}}},{"i":915,"$":{"0":{"v":"Graphile Migrate","n":0.707},"1":{"v":"\n## Tracking Changes\n- GM has an .gmrc file, which allows us to define hooks. We can therefore define a hook that will run `pg_dump` on the shadow db after every migration. If we track this file in git, then we can see a clear history of what each migration has done.\n\n## Development\n- In Development, aside from the main database, there is a shadow db which is used internally by graphile-migrate and is mainly for testing consistency of the migrations, among other minor tasks\n- the shadow database gets reset frequently\n- `commit`, `uncommit`, `watch` and `reset` are development-only commands.\n\n## Flow\n1. We write our new idempotent migration in `current.sql`\n\t- any seed data we have can be placed at the bottom, to be removed prior to committing.\n2. when ready, we run `commit`\n\t- this should only be done immediately before the branch is merged into master (see README###Collaboration). This it to ensure commits are linear.\n\t- all sql in `current.sql` is removed and catalogued into `committed/`\n\t- the shadow database is dropped, and recreated by running all migrations. This is to ensure that the migration works without a hitch.\n\n* * *\n\n### Pitfalls\n- when dropping tables, schemas, and functions, we should use CASCADE\n- use idempotent commands whenever possible.\n\t- ex. DROP ____ IF EXISTS\n\t- ex. CREATE OR REPLACE FUNCTION\n\t- The initial migrations don't need to be idempotent if this migration starts off by dropping all schemas (which implicitly drops all tables attached to those schemas)\n","n":0.065}}},{"i":916,"$":{"0":{"v":"Graphics","n":1},"1":{"v":"\n### Rendering Pipeline\nThe steps taken to \nThere are APIs that can be used to control the underlying hardware and keep the programmer away from writing code to manipulate the graphics hardware accelerators .\n- some examples are Direct3D and OpenGL. The underlying hardware would be AMD/Intel/NVIDIA etc.\n\nspec:(one of?) The most common  type of rendering pipeline implementations is done with a [graphics.rendering.shader]].\n","n":0.129}}},{"i":917,"$":{"0":{"v":"Rendering","n":1},"1":{"v":"\n# UE Resources\n- [Catlikecoding: high quality tutorials and guides on how this stuff works at low level](https://catlikecoding.com/unity/tutorials/rendering/)\n","n":0.243}}},{"i":918,"$":{"0":{"v":"Shader","n":1},"1":{"v":"\nA shader's role is to convert 3D assets into 2D pixels onto the screen.\n\nThe term \"shader\" is a bit of a misnomer in the present day. While their original purpose was to *shade* things, their use has since extended beyond that.\n- originally, a shader was a small piece of code that ran on the GPU to decide for each pixel what color it should be so that you could shade the objects being rendered, achieving the illusion of lighting and shadows. Nowadays, shaders loosely refer to any program that runs on the GPU.\n\nShaders work something like this: You upload a data buffer to your GPU and tell it how to interpret that data as a series of triangles. Each vertex occupies a chunk of that data buffer, describing that vertex’ position in 3D space, but probably also auxillary data like color, texture IDs, normals and other things. Each vertex in the list is processed by the GPU in the vertex stage, running the vertex shader on each vertex, which will apply translation, rotation or perspective distortion.\n- This system of passing data to a vertex shader, then to a fragment shader and then outputting it directly onto the screen is called a pipeline\n\nConsider that a triangle (in 3D space) is made up of three 3D points (vertices), for example:\n```\n(-1.03,  0.00,  0.00)\n( 0.69,  0.00,  1.00)\n( 0.69,  0.00, -1.00)\n```\nTo show a triangle on the screen, we need to convert these 3D points into 2D points onto a virtual screen (called a Viewport)\n- To figure this out, we need to consider the position/rotation of the triangle, as well as the position/rotation of the camera itself.\n- All transformations required to convert a 3D point into a 2D point in the viewport can be encoded in a single 4x4 matrix:\n```\n[0, 0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0]\n```\n\nWhen you write a Shader, you are primarily responsible for writing a *vertex function* and a *fragment function*. The graphics system makes use of these functions, and handles the rest.\n- vertex function is responsible for conversion from 3D to 2D (most of the complicated math can be handled for us by utilizing APIs)\n- fragment function is responsible for determining which color each pixel within the triangle will be (ie. The fragment function returns a color)\n    - this occurs right after rasterization.\n\nVertex function is also responsible for telling the interpolation system what information to interpolate.\n- whatever data is returned from the vertex function will be interpolated for each pixel and sent to the fragment function:\n\nVertex function runs once for each vertex in the triangle (ie. 3 times per triangle)\nFragment function runs once per pixel that is covered by the triangle.\nThe rasterization and interpolation step ties the vertex function and fragment function steps\n\nVertex Function -> position, UV etc -> Interpolation system -> Fragment function -> pixel color to be written to the screen\n\n### Rasterization\nWith a 2D triangle on the screen, rasterization is the process of figuring out which pixels in the screen belong within the triangle.\n\n## Lighting\n","n":0.045}}},{"i":919,"$":{"0":{"v":"3D Modeling","n":0.707},"1":{"v":"\nWith a 3D modeling software like Blender, artists model the shape of a 3D object, **rig** it so that it can be animated, **animate** it to move in a particular way, and **texture** it to give it color and shading. An example of this is shown below. \n![](/assets/images/2021-08-15-16-54-09.png)\nUnity has a quick n' dirty implementation of a 3D modeler called [ProBuilder](https://unity.com/features/probuilder) \n\nThe reason why we use triangles to represent 3D graphics is because no matter where (in 3D space) you put the 3 points of a triangle, it is always 2D. \n- With 4 points, we can make more complex objects, such as a pyramid, which would be more mathematically complex and require higher processing power just to figure out the pixels on the surface.\n\n# Resources\n- [Turbosquid: pre-made 3D modeled assets](https://www.turbosquid.com/)\n- [CG Trader: pre-made 3D modeled assets](https://www.cgtrader.com/)\n","n":0.086}}},{"i":920,"$":{"0":{"v":"Gradle","n":1},"1":{"v":"\n## What is it?\nGradle is a build automation tool\n- It makes as few assumptions as possible with your project, and thus it is built to be flexible enough to support almost any type of software.\n\nGradle only re-runs jobs when inputs/outputs have changed.\n- therefore, caches are utilized\n\nGradle runs on the JVM\n- an implication here is that build logic can use the standard Java APIs.\n\nAt first, we have only a `build.gradle` file. When we run `gradle wrapper`, some files are generated for us:\n- `gradle/wrapper/gradle-wrapper.jar`\n- `gradle/wrapper/gradle-wrapper.properties`\n- `gradlew`\n- `gradlew.bat`\n\n### Build Lifecycle\nGradle evaluates and executes build scripts in three build phases of the Build Lifecycle:\n\n#### Initialization\nSets up the environment for the build and determine which projects will take part in it.\n\n#### Configuration\nConstructs and configures the task graph for the build. Determines which tasks need to run and in which order, based on the task the user wants to run.\n\n#### Execution\nRuns the tasks selected at the end of the configuration phase.\n\n## Terminology\n### Projects\nProjects are the things that Gradle builds.\n\nProjects contain a build script (`build.gradle`)\n- Builds scripts define tasks, dependencies, plugins, and other configuration for that project.\n- A single build can contain one or more projects and each project can contain their own subprojects.\n\n### Tasks\nTasks contain the logic for executing some work—​compiling code, running tests or deploying software.\n\nMost often, we'll use existing tasks.\n- ex. built-in Java `Test` task, which runs tests.\n\nTasks themselves consist of:\n- *Actions*: pieces of work that do something, like copy files or compile source\n- *Inputs*: values, files and directories that the actions use or operate on\n- *Outputs*: files and directories that the actions modify or generate\n\n### Plugins\nPlugins allow you to introduce new concepts into a build beyond tasks, files and dependency configurations\n- ex. most language plugins add the concept of *source sets* to a build.\n\nPlugins provide a means of reusing logic and configuration across multiple projects.\n \n### Build\n\n### shadowJar\nShadowJar is a plugin for the Gradle build system that provides a convenient way to create \"fat\" or \"uber\" JAR files\n- A fat JAR file is a self-contained archive that includes not only your application's compiled classes but also all its dependencies. It allows you to package your application into a single JAR file that can be easily distributed and executed, without requiring users to manage separate JAR files for dependencies.","n":0.052}}},{"i":921,"$":{"0":{"v":"Build","n":1},"1":{"v":"\nA build is an execution of a collection of tasks in a Gradle project. \n\nYou run a build via the command line interface (CLI) or an IDE by specifying task selectors.\n\nThe gradle runtime is only necessary to be installed if we don't already have a Wrapper.\n\n\n### Gradle Wrapper\nThe Wrapper is a script that invokes a declared version of Gradle, downloading it beforehand if necessary.\n\nThe Wrapper is the preferred way to execute a Gradle build.\n- purpose is to ensure a reliable, controlled and standardized execution of the build.\n\nPurposes:\n- Standardizes a project on a given Gradle version, leading to more reliable and robust builds.\n- Provisioning a new Gradle version to different users and execution environment (e.g. IDEs or Continuous Integration servers) is as simple as changing the Wrapper definition.\n\nThe generated Wrapper properties file, `gradle/wrapper/gradle-wrapper.properties`, stores the information about the Gradle distribution.\n- The server hosting the Gradle distribution.\n- The type of Gradle distribution. By default that’s the -bin distribution containing only the runtime but no sample code and documentation.\n- The Gradle version used for executing the build. By default the wrapper task picks the exact same Gradle version that was used to generate the Wrapper files.\n- Optionally, a timeout in ms used when downloading the gradle distribution.","n":0.07}}},{"i":922,"$":{"0":{"v":"Go","n":1},"1":{"v":"\nRunning code concurrently is part of the language.\n- It’s faster than scripting languages like PHP, Python, or Javascript.\n\nGo follows an imperative (procedural, structural) paradigm\n\nThe final application is a binary compiled statically without any dependency.\n- You just need to drop the executable on your server to deploy the application.\n\nIn Go, once the `main()` function finishes, the program exits.\n- this can cause unexpected behaviours with concurrency.\n\nGo is often used for building systems that require high performance and low latency, because they offer better control over memory management and are generally faster than higher-level languages like JavaScript.\n\n# UE Resources\n- [Learn Go with Tests](https://quii.gitbook.io/learn-go-with-tests/)\n- [In depth book (for if you ever want to become a Go expert)](https://lets-go-further.alexedwards.net/)\n- [Go tutorials from Flavio Copes](https://flaviocopes.com/tags/go/)\n","n":0.092}}},{"i":923,"$":{"0":{"v":"Types","n":1},"1":{"v":"\n### Integers\nWhen you are working with integer values, you should always use the int data type unless you have a good reason to use the sized or unsigned integer types.\n\n- `int8`: $-128$ to $127$\n- `int16`: $-2^15$ to $2^15$ - 1$\n- `int32`: $-231$ to $2^31 - 1$\n- `int64`: $-263$ to $2^63 - 1$\n- `int`: Platform dependent (e.g. 64 bits wide on a 64-bit system)\n\nhttps://www.callicoder.com/golang-basic-types-operators-type-conversion/\n","n":0.126}}},{"i":924,"$":{"0":{"v":"Struct","n":1},"1":{"v":"\nKey-value storage, where:\n- All values can be of different type.\n- Need to know all the different fields at compile time.\n- Keys don’t support indexing, (spec: so they can't be iterated over)\n\n```go\n// define the struct type\ntype person struct {\n  name string\n  age int\n\n}\n\nfunc main() {\n  // instantiate a person\n  p := person{name: \"Kyle\", age: 30}\n  fmt.Println(p.name) // Kyle\n}\n```","n":0.132}}},{"i":925,"$":{"0":{"v":"Map","n":1},"1":{"v":"\nKey-value storage, where:\n- All key and value are of same type.\n- provides fast lookup and values that can retrieve, update or delete with the help of keys.\n- can iterate over keys (as long as they are indexed)\n- Don’t need to know all the keys at compile time.\n\n```go\n// create a map with keys as strings and values as integers\nvar myMap = make(map[string]int)\n\n// set value of key\nmyMap[\"firstKey\"] = 2\n```","n":0.122}}},{"i":926,"$":{"0":{"v":"Interface","n":1},"1":{"v":"\nAn interface type is defined as a set of method signatures.\n- A type implements an interface by implementing its methods. There is no explicit declaration of intent, no \"implements\" keyword.\n\t- Implicit interfaces decouple the definition of an interface from its implementation, which could then appear in any package without prearrangement.\n\n```go\ntype I interface {\n\tM()\n}\n\ntype T struct {\n\tS string\n}\n\n// This method means type T implements the interface I,\n// but we don't need to explicitly declare that it does so.\nfunc (t T) M() {\n\tfmt.Println(t.S)\n}\n\nfunc main() {\n\tvar i I = T{\"hello\"}\n\ti.M()\n}\n```","n":0.107}}},{"i":927,"$":{"0":{"v":"Array","n":1},"1":{"v":"\n### Array\nArrays hold multiple elements of the same type, but the length is fixed.\n- the length is part of the array's type and therefore cannot be changed.\n```go\n// create an array of integers with length 5\nvar a [5]int\n\n// create an array with initialized values\nvar a [5]int{5, 4, 3, 2, 1}\n```\n\n### Slice\n![[golang.types.array.slice]]","n":0.141}}},{"i":928,"$":{"0":{"v":"Slice","n":1},"1":{"v":"\nSlices are an abstraction over arrays to make them easier to work with, most notably because they do not have a fixed length.\n```go\nvar a []int{5, 4, 3, 2, 1}\n```\n\nIn particular, a slice is a reference to a segment of an array.\n\nWith slices we can do things like append (which doesn't modify the original Slice, but returns a new one)\n- under the hood, Go is creating a new array, copying the contents from the old one, and adding the new value.","n":0.112}}},{"i":929,"$":{"0":{"v":"Go Syntax","n":0.707},"1":{"v":"\n### Declaring variables\n```go\nvar x int = 7\n// or the equivalent short-form\nx := 7\n```\n\n### If statements\n```go\nif x > 6 {\n  fmt.Println(\"yup, bigger than 6\")\n} else if x < 2 {\n  fmt.Println(\"nope, smaller than 2\")\n}\n```\n\n### `defer`\nA `defer` statement defers the execution of a function until the surrounding function returns.\n\nThe deferred call's arguments are evaluated immediately, but the function call is not executed until the surrounding function returns.\n\nDeferred function calls are pushed onto a stack. When a function returns, its deferred calls are executed in last-in-first-out order.\n\n### Type assertions\n```go\nvar i interface{} = \"hello\"\n\ns, ok := i.(string)\n// If i holds a string, then `s` will be the underlying value and `ok` will be true. Otherwise `ok` will be false\n```\n\nThis statement asserts that the interface value i holds the concrete type T and assigns the underlying T value to the variable t.\n- If i does not hold a T, the statement will trigger a panic.\n\n### Error handling\nGo doesn't have exceptions. This absense is relieved by the fact that functions can return multiple values. To get the error, we set one of our return values to the error and check for its existence after running the function.\n```go\nresult, error := getSomeData() \nif error {\n  return fmt.Errorf(\"no data available\")\n}\n```\n","n":0.071}}},{"i":930,"$":{"0":{"v":"Pointer","n":1},"1":{"v":"\n\nIn Go a pointer is represented using the `*` character followed by the type of the stored value.\n\n`*` is also used to \"dereference\" pointer variables. Dereferencing a pointer gives us access to the value the pointer points to. \n\nWhen we write `*xPtr = 0` we are saying \"store the int 0 in the memory location that `xPtr` refers to\". \n- If we try `xPtr = 0` instead we will get a compiler error because `xPtr` is not an `int` it's a `*int`, which can only be given another `*int`.\n\nWe use the `&` operator to find the address of a variable. `&x` returns a `*int` (pointer to an `int`) because `x` is an `int`. This is what allows us to modify the original variable. `&x` in main and `xPtr` in zero refer to the same memory location.\n\nWhen we use the `&` operator in front of a variable `c`, we are talking about the place where `c` is stored. When we use the `*` operator, we are talking about the variable `c` itself.\n\nWe can get the memory address of a variable by prepending it with an `&`\n```go\ni := 7\nfmt.Println(&i) // 0xc420014090\n```\n\nAs expected, the `inc` function wouldn't cause the value of `i` to change, since `i` is copied by value, so `x` in the `inc` function is just a copy, which gets incremented, but then is discarded, since the function doesn't return anything.\n```go\nfunc main() {\n  i := 7\n  inc(i)\n  fmt.Println(i) // 7\n}\n\nfunc inc(x int) {\n  x++\n}\n```\n\nIf we want to modify the value of `i` here, then we can do so by passing a pointer to the variable to `inc`. In this case, the `inc` function receives a value at that memory reference and is able to modify the original version.\n```go\nfunc main() {\n  i := 7\n  // pass the pointer of `i` to the `inc` function\n  inc(&i)\n  fmt.Println(i) // 7\n}\n\n// here, we say \"the function accepts a pointer that, when dereferenced, is an integer\"\nfunc inc(x *int) {\n  // dereference the pointer before incrementing; otherwise we'd be incrementing the memory address (ie. 0xc420014090).\n  *x++\n}\n```\n\n### Use-cases\nPointers give us the ability to define a function that accepts a pointer as an argument so that the original variable can be modified from within the function.\n```go\nfunc zero(xPtr *int) {\n  *xPtr = 0\n}\nfunc main() {\n  x := 5\n  zero(&x)\n  fmt.Println(x) // x is 0\n}\n```\n- this also has the added benefit of not having to copy the value into a local function version on each method call.\n\nPointers are extremely useful when paired with structs.","n":0.049}}},{"i":931,"$":{"0":{"v":"Loops","n":1},"1":{"v":"\n### regular for-loop\n```go\nfor i := 0; i < 5; i++ {\n  fmt.Println(\"hey\")\n}\n```\n\n### while-loop\n```go\ni := 0\nfor i < 5 {\n  fmt.Println(i)\n  i++\n}\n```\n\n### loop over an array (using `range`)\n```go\narr := []string{\"a\", \"b\", \"c\"}\n\nfor index, value := range arr {\n  fmt.Println(\"index:\", index, \"value:\", value)\n}\n\n\nvar myMap = make(map[string]int)\nfor key, value := range arr {\n  fmt.Println(\"key:\", key, \"value:\", value)\n}\n```","n":0.136}}},{"i":932,"$":{"0":{"v":"Functions","n":1},"1":{"v":"\n```go\n// a function that takes in 2 ints and returns an int\nfunc sum(x int, y int) int {\n  return x + y\n}\n```\n\nIn Go, functions can have multiple return values. If we choose to write our functions with 2 return vlaues, then we must be sure to *always* return all of them.\n- notice below that in the error case, we are still returning `0` as our first return value.\n```go\nimport \"errors\", \"math\"\n\nfunc sqrt(x float64) (float64, error) {\n  // logic: square root of a negative number throws an error\n  if x < 0 {\n    return 0, errors.New(\"Cannot get square root of negative number\")\n  }\n  return math.Sqrt(x), nil\n}\n\nsqrtOf16, err := sqrt(16)\nif err != nil {\n  fmt.Println(err)\n} else {\n  fmt.Println(sqrtOf16)\n}\n```\n\nIn Go function, if we specify the result parameters (ie. return value(s)), then we can just specify `return` without any value/expression list following it.\n- the result parameters act as ordinary local variables. They can be modified, and we can `return` (again, with nothing after) and our function will return those same result parameters:\n\n```go\n// short-form\nfunc a()(x float64, err error){\n    x = 7.0\n    // `return` here will return bother a and b, \n    // since that is what is specified above in the result parameters\n    return\n}\n// long-form\nfunc a() (float64,error){\n  x := 7.0\n  var err error\n  return x,err\n}\n```\n","n":0.069}}},{"i":933,"$":{"0":{"v":"Methods","n":1},"1":{"v":"\nHere, we declare the `isAdult` method on the `*Person` type.\n```go\nfunc (p *Person) isAdult bool {\n  return p.Age > 18\n}\n```\n\n\nIn Go, Methods are functions that have a special *receiver argument*:\n```go\n// The Abs() method a receiver argument of type Vertex named v.\nfunc (v Vertex) Abs() float64 {\n\treturn math.Sqrt(v.X*v.X + v.Y*v.Y)\n}\n```\n\nThe receiver argument can be either a *value receiver* or a *pointer receiver*\n- *value receiver* - makes a copy of the type and passes it to the function so that it has its own local version.\n\t- Therefore, the original object will remain unchanged.\n\t- ex. `(v Vertex)` \n- *pointer receiver* - passes the address of a type to the function. The function stack has a reference to the original object. \n\t- Therefore, any modifications on the passed object will modify the original object.\n\t- ex. `(v *Vertex)`\n\nSince methods often need to modify their receiver, pointer receivers (ie. `*x`) are more common than value receivers.\n\n### Pointer Receiver\nIn this case the receiver argument is a [[pointer|golang.syntax.pointer]], as denoted by `*T`:\n```go\nfunc (v *Vertex) Scale(f float64) {\n\tv.X = v.X * f\n\tv.Y = v.Y * f\n}\n```\n- in this case, the `Scale` method has as a receiver argument `*Vertex`.\n- now, we can modify the value to which the receiver points\n\nmethods with pointer receivers take either a value or a pointer as the receiver when they are called\n- this means that you can pass either the value or the pointer to the value (ie. either `v` or `&v`)\n- this is a convenience offered by the Go engine. Under the hood, Go interprets the statement `v.Scale(5)` as `(&v).Scale(5)` since the Scale method has a pointer receiver.\n```go\nfunc (v *Vertex) Scale(f float64) {\n\tv.X = v.X * f\n\tv.Y = v.Y * f\n}\n```\n\nIn general, all methods on a given type should have either value or pointer receivers, but not a mixture of both.","n":0.058}}},{"i":934,"$":{"0":{"v":"Marshalling","n":1},"1":{"v":"\n### Unmarshaling\nTo read data from JSON received from over the internet, we need to Unmarshall it first\n```go\njson.Unmarshal(reqBody, &article)\n```\n\nBefore we unmarshall the JSON, it is represented as an array of bytes.\n\n### Marshal\n`json.NewEncoder` and `json.Marshal` both marshal objects into JSON encoded strings.\n- The difference being that the Encoder first marshals the object to a JSON encoded string, then writes that data to a buffer stream. The Encoder therefore, uses more code and memory overhead than the simpler `json.Marshal`.\n\n- Encoding/decoding JSON refers to the process of actually reading/writing the character data to a string or binary form.\n- Marshaling/Unmarshaling refers to the process of mapping JSON types from and to Go data types and primitives.\n\n- In Go, struct data is converted into JSON using `Marshal()` and JSON data to string using `Unmarshal()` method.","n":0.088}}},{"i":935,"$":{"0":{"v":"Environment","n":1},"1":{"v":"\n## Initializing project\n1. In your new project directory, run `go mod init <URL of remote repo>`\n2. Add packages with `go get <package URL>`\n\n* * *\n\n### Module\nA module is Go’s new dependency management system.\n\nIn a module, you collect one or more related packages for a discrete and useful set of functions. \n- Go code is grouped into packages, and packages are grouped into modules.\n- a module has a `go.mod` file at its root\n  - this file specifies dependency requirements and the module path.\n- ex. you might create a module with packages that have functions for doing financial analysis so that others writing financial applications can use your work.\n\nYour module specifies dependencies needed to run your code, including the Go version and the set of other modules it requires.\n\n#### `GOPATH`\n`GOPATH` is the deprecated way of managing Go projects.\n\nBefore modules, every Go project had to be created in the `GOPATH`\n- run `go env GOPATH` to see\n\n- [More about GOPATH](https://golangr.com/what-is-gopath/)\n\n#### Module Path (Name)\nWhen you create a module, you specify a path from which your module can be downloaded by Go tools.\n- most often this is our Gitlab/Github URL to our repo.\n- The module path becomes the import path prefix for packages in the module, so be sure to specify a module path that won’t conflict with the module path of other modules.\n\nThe module path is typically of the form: `<prefix>/<descriptive-text>`\n- `prefix` might be your repo name. You would do this if you intended for others to use this module.\n  - if the module isn't going to be used by others, then you can just call it by your project name, as long as you are certain it won't be used by others.\n\n### Packages\nA Go project can be broken out into packages that get compiled together.\n- Functions, types, variables, and constants defined in one source file are visible to all other source files within the same package.\n- spec: packages fall along the same lines drawn by [[DDD|general.principles.DDD]], and each package would correspond to each of our domains\n  - ex. when making a blog app, packages might be `users`, `articles`\n\nIn the most basic terms, a package is nothing but a directory inside your Go workspace containing one or more Go source files, or other Go packages.\n- therefore, every Go source file belongs to a package, and we specify a source file as being part of a package by declaring `package <packagename>` at the top.\n\nOther packages can import and reuse the functions or types that are exported from your package.\n\nGo programs start running in the `main` package.\n\nBy convention, Executable programs (the ones with the main package) are called *Commands*. Others are called simply *Packages*.\n\nGo’s convention is that - the package name is the same as the last element of the import path. \n- ex. the name of the package imported as `import math/rand` is `rand`. It is imported with path `math/rand` because it is nested inside the `math` package as a subdirectory.\n```\n.\n├── main.go\n├── strings/\n│   ├── reverse_name.go\n│   └── greeting/\n|       └── texts.go (`package greeting`)\n```\n- above, within `main.go`, we would import `texts.go` with `\"github.com/tycholiz/my-project/strings/greeting\"`\n\nAnything (variable, type, or function) that starts with a capital letter is exported, and visible outside the package.\n- therefore, when you import a package, you can only access its exported names.\n\n- [source](https://www.callicoder.com/golang-packages/)\n\n\n\n* * *\n\n### Golang Runtime","n":0.043}}},{"i":936,"$":{"0":{"v":"Concurrency in Go","n":0.577},"1":{"v":"\n### Goroutine\nEach execution path of our Go program is a *goroutine*. \n- Therefore, we always have at least 1 (the `main` goroutine).\n\nIf we prepend a function invocation with the keyword `go`, that function won't wait to finish executing before moving on to the next line.\n\nThe following will run `count(\"sheep\")` in the background, then immediately execute `count(\"fish\")`, thereby creating a *goroutine*, which runs concurrently.\n```go\ngo count(\"sheep\")\ncount(\"fish\")\n\n// 1 sheep\n// 1 fish\n// 2 sheep\n// 2 fish\n// etc.\n```\n\nimagine both of our `count()` functions are called in a background goroutine:\n```go\nfunc main() {\n  go count(\"sheep\")\n  go count(\"fish\")\n}\n```\n- in this case, we would not see any logs in the console, since the `go` keyword tells the `count()` function to run in the background and move to the next line. Since that was the last line, the `main()` function finishes and our program exits.\n  - if we added `time.Sleep(time.Second * 2)` as the last line in our `main()` function, then we would see logs for 2 seconds before the program exits.\n- a better solution here is to use a *WaitGroup* (which is basically just a counter):\n```go\nimport \"sync\"\n\nfunc main() {\n  var wg sync.WaitGroup\n  // increment the WG by 1 to incicate that we have 1 goroutine \n  // to wait for before `main()` finishes executing\n  wg.Add(1)\n\n  go func() {\n    count(\"sheep\")\n    // decrement the WG when the goroutine finishes.\n    wg.Done()\n  }\n\n  // wait until the counter is 0 (ie. wait until all goroutines have finished)\n  wg.Wait()\n}\n```\n\nGoroutines are very efficient\n- we can make 1000s of simultaneously running goroutines.\n- however, ultimately we are constrained by how many cores our [[CPU|hardware.cpu]] has.\n\n### Channel\nA *channel* is a means for goroutines to communicate with each other.\n- ex. we have a value in one goroutine that we want to pass to the `main` goroutine.\n  - to do this, we can modify our `count()` function to accept a channel as an argument:\n```go\nfunc main() {\n  // make the channel\n  c := make(chan string)\n  go count(\"sheep\", c)\n\n  // iterate over the range (spec: length, as if it were an array) of a channel\n  for msg := range c {\n    fmt.Println(msg)\n  }\n\n  /* The above for-loop is syntactic sugar for the long-form: \n  * for msg := range c {\n  *   // receive the message from the channel and set it to `msg`\n  *   msg, open := <- c\n  *\n  *   // if channel is not open, break out of for loop (stop receiving messages)\n  *   if !open {\n  *     break\n  *   }\n  *\n  *   fmt.Println(msg)\n  * }\n  */\n}\nfunc count(animalType string, c chan string) {\n  for i: 0; i <= 5; i++ {\n    // send the value of `animalType` over the channel\n    c <- animalType\n    time.Sleep(time.Millisecond * 500)\n  }\n\n  // close the channel once the for loop has finished\n  close(c)\n}\n```\n- note: sending and receiving messages through channels are blocking operations; code execution will stop until a value is sent/received through the channel.\n- note: channels have a type (here, `string`), meaning the only messages we can pass through those channels are strings.\n  - we can even type channels as channels (ie. channels that only accept messages with the type `channel`)\n    - spec: `c chan chan`, or `c chan channel` maybe?\n- note: naturally, only senders of messages should close channels, since they are the ones that know whether or not the data flow has finished.\n\nWe can use channels to synchronize goroutines.\n- ex. Imagine we have 2 goroutines, and gr1 depends on gr2 (gr1 receives a message from gr2 via a channel)\n![](/assets/images/2022-12-20-11-25-19.png)\n  - here, execution of gr1 will pause at line 7 as it waits for gr2 to reach its line 4, where it will send a message through the channel.\n\nChannels must have a goroutine ready to receive a message from them *before* anything can be sent through them.\n- in other words, if we are within the `main` goroutine and our code executes `c <- \"hello\"` before it executes `msg := <- c`, our program will exit in error, since we are trying to send something to the `c` channel *before* anything is set up to listen to it (recall: sending/receiving from a channel is a blocking operation until it can be completed)\n\nChannel references can be specified to restrict us from only ever receiving/sending messages:\n- ex. we make a function that takes 2 channels:\n  - 1. a channel of jobs to do (from which we will only ever receive messages; `<-chan`)\n  - 2. a channel to send results to (to which we will only ever send messages; `chan<-`)\n```go\nfunc worker(jobs <-chan int, results chan<- int) {\n  for n := range jobs\n}\n```\n- if we try and send a message to the `jobs` channel, we will get a compile-time error\n\n#### Buffered Channel\nA buffered channel can be filled without a corresponding receiver, and it will not block until that buffer is full.\n- Buffered channels have a fixed capacity set when they are initialized.\n\n```go\nfunc main() {\n  // make a buffered channel of strings with a capacity of 2\n  c := make(chan string, 2)\n  c <- \"hello\"\n  c <- \"world\"\n\n  msg := <- c\n  fmt.Println(msg) // hello\n\n  msg = <- c\n  fmt.Println(msg) // world\n}\n```\n\n#### `select`\nthe `select` keyword allows us to receive a message from whatever channel has one:\n\nex. imagine we have 2 goroutines that each do some calculation then return it to the `main` function where it then gets logged to the console. We set up each function to do the calculation, then send the data through their own respective channel. Back in the `main` function, we create a loop to log to the console each time a new message arrives in the channel):\n```go\nmain() {\n  for {\n    select {\n      case msg1 := <- c1:\n        fmt.Println(msg1)\n      case msg2 := <- c2:\n        fmt.Println(msg2)\n    }\n  }\n}\n```\n\n### Worker Pool\nA worker pool is a queue of jobs to be done, from which multiple concurrent workers can pull jobs and perform them.\n\n```go\nfunc main() {\n  // no real reason why we have a buffer of 100; it's just a nice round and large enough number\n  jobs := make(chan int, 100)\n  results := make(chan int, 100)\n\n  // create a worker as a concurrent goroutine\n  go worker(jobs, results)\n\n  for i := 0; i < 100; i++ {\n    // fill up the jobs channel with numbers from 0-99\n    // since it's a buffered channel, it's not going to block\n    jobs <- i\n  }\n  close(jobs)\n\n  for j := 0; j < 100; j++ {\n    // receive each fibonnaci number from the `results` channel and print to console\n    fmt.Println(<-results)\n  }\n}\n\nfunc worker(jobs <-chan int, results chan<- int) {\n  // as long as there are jobs on the `jobs` channel, the calculation will continue to run\n  for n := range jobs {\n    results <- fibonnaci(n)\n  }\n}\n\nfunc fibonacci(n int) int {\n  if n <= 1 {\n    return n\n  }\n\n  return fibonacci(n - 1) + fibonacci(n - 2)\n}\n```\n\n\n* * *\n\n[[see: Concurrency main article|general.concurrency]]\n","n":0.03}}},{"i":937,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n### Run the Go program\n`go run main.go`\n\n### Initialize a Go module\n`go mod init <URL_of_repo>`\n- URL of repo is the convention, but not a requirement as long as it's unique\n\n### Compile the code into an executable\nrun `go build` in the project's directory\n\n### Compile code into an executable and put into `bin/` directory\n`go install`\n\n- note: if our project uses 3rd party dependencies (ie. not from standard lib), then it would compile and cache those dependencies into a package directory.\n- note: the `bin/` directory here is a sibling directory to the project's `src/` directory","n":0.105}}},{"i":938,"$":{"0":{"v":"GitLab","n":1},"1":{"v":"\n### Project Groups\nMultiple repos with some common business-level relation can be placed in a *group*\n- ex. Imagine we have a [[microservice|general.arch.microservice]] architecture with 1000+ repos. It is expected groups would form out of these, based on their underlying domains.\n\nWe can have up to 20 levels of subgroups.\n\n[docs](https://docs.gitlab.com/ee/user/group/)\n","n":0.146}}},{"i":939,"$":{"0":{"v":"Gitlab CI-CD","n":0.707},"1":{"v":"\n## Pipelines\n![[gitlab.CI-CD.pipelines]]\n\n## Stages\n![[gitlab.CI-CD.stages]]\n\n## Jobs\n![[gitlab.CI-CD.jobs]]\n\n* * *\n\nGitlab CI can be used with external Git hosts, like Github.\n- [docs](https://docs.gitlab.com/ee/ci/ci_cd_for_external_repos/)\n","n":0.243}}},{"i":940,"$":{"0":{"v":".gitlab-ci.yml File","n":0.707},"1":{"v":"\nIn the .gitlab-ci.yml file, you can define:\n- The scripts you want to run.\n- Other configuration files and templates you want to include.\n- Dependencies and caches.\n- The commands you want to run in sequence and those you want to run in parallel.\n- The location to deploy your application to.\n- Whether you want to run the scripts automatically or trigger any of them manually.\n\n## Keywords\n### Extends\n`extends` can be used to reuse configuration\nextends supports multi-level inheritance, but we should try and limit it to around 3 levels.\n\nIf you want to exclude a key from being inherited, give it value of `null`\n```yml\ntest4:\n  extends: .base\n  variables: null\n```\n\nYou can use `extends` to merge hashes but not arrays.\n- The algorithm used for merge is “closest scope wins,” so keys from the last member always override anything defined on other levels. For example, here `.in-docker` overwrites values from `only-important`, if there is overlap.\n\n### !references\nOnly one level of nesting is supported for `!reference` tags. That is, we cannot `!reference` if the thing we are referencing has already been referenced.\n```yml\n.vars:\n  variables:\n    URL: \"http://my-url.internal\"\n    IMPORTANT_VAR: \"the details\"\n\ntest-vars-1:\n  # the following expands both URL and IMPORTANT_VAR\n  variables: !reference [.vars, variables]\n```\n\n* * *\n\nThe following runs in order:\n- Pipeline starts.\n- job A runs.\n- before_script is executed.\n- script is executed.\n- after_script is executed.\n```yml\nbefore_script:\n  - echo \"Hello\"\n\njob A:\n  stage: build\n  script:\n    - mkdir vendor/\n    - echo \"build\" > vendor/hello.txt\n  cache:\n    key: build-cache\n    paths:\n      - vendor/\n  after_script:\n    - echo \"World\"\n```\n\n# Resources\n[.gitlab-ci.yml Keyword Reference](https://docs.gitlab.com/ee/ci/yaml/)\n[.gitlab-ci.yml Lint Tool](https://docs.gitlab.com/ee/ci/lint.html)\n","n":0.065}}},{"i":941,"$":{"0":{"v":"Gitlab CI Variables","n":0.577},"1":{"v":"\n## Types of Variables\nThere are 4 types of environment variables we can define, differentiated by *where* they are defined:\n- in the `gitlab-ci.yml` file\n- project (repo)-level CI/CD variables. Done in the project settings of the UI.\n- [[group|gitlab#groups]]-level CI/CD variables. Done in the project settings of the UI.\n- instance (ie. self-hosted) CI/CD variables\n\n* * *\n\n### Predefined Variables\nGitlab provides predefined environment variables [(docs)](https://docs.gitlab.com/ee/ci/variables/predefined_variables.html)\n\nExample variables\n- current stage, current job, if and how the pipeline was triggered, commit details (message, author, branch name, SHA)\n- AWS variables like AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION\n\nnote: some variables are available in some pipeline types but not others (ie. some may be available for branch pipelines, but not merge request pipelines)\n- merge request pipelines get their own set of variables.\n\n### Using variables\nThere are two places defined variables can be used. On the:\n- GitLab side, in .gitlab-ci.yml.\n- The GitLab Runner side, in config.toml.\n\n[docs](https://docs.gitlab.com/ee/ci/variables/where_variables_can_be_used.html)\n\n#### Expansion mechanisms\nVariables can be expanded (ie. resolved) via 3 different methods\n- GitLab\n- GitLab Runner\n- Execution shell environment\n\nVariables used in `.gitlab-ci.yml` can be expanded with `$variable`, or `${variable}` or `%variable%`\n- Each form is handled in the same way, no matter which OS/shell handles the job, because the expansion is done in GitLab before any runner gets the job.\n\nGitLab expands job variable values recursively before sending them to the runner. This means the following works:\n```yml\n- BUILD_ROOT_DIR: '${CI_BUILDS_DIR}'\n- OUT_PATH: '${BUILD_ROOT_DIR}/out'\n- PACKAGE_PATH: '${OUT_PATH}/pkg'\n```\n","n":0.068}}},{"i":942,"$":{"0":{"v":"Stages","n":1},"1":{"v":"\nThe order that we list our stages defines the execution order for jobs:\n- Jobs in the same stage run in parallel.\n- Jobs in the next stage run after the jobs from the previous stage complete successfully.\n\nIf stages is not defined in the .gitlab-ci.yml file, the default pipeline stages are:\n- `.pre`\n- `build`\n- `test`\n- `deploy`\n- `.post`\n\nIf a job does not specify a stage, the job is assigned the `test` stage.\n\nTo make a job start earlier and ignore the stage order, use the `needs` keyword.\n\n","n":0.11}}},{"i":943,"$":{"0":{"v":"Runner","n":1},"1":{"v":"\nThe `tags` keyword can be used to restrict which runners should pick up the job.\n","n":0.258}}},{"i":944,"$":{"0":{"v":"Executor","n":1},"1":{"v":"\nAn executor is essentially the environment where the job will be executed.\n\nEach [[runner|gitlab.CI-CD.runner]] will define at least one executor. \n\nif you need Node.js to run a job, this needs to be installed on the machine where the runner is installed.\n- Since everything needed is already available on runtime, the jobs are executed very fast.\n\nGitlab allows us to use the following different executors:\n- Shell\n    - run the jobs directly on the machine where the runner has been installed.\n    - use this when you need a native-run environment (ex. you need to ensure that your software runs on a specific OS or hardware).\n    - since we are just running in a shell, there will be left-overs from previous jobs. This also means that it's harder to manage dependencies (e.g. if we need node 12 for one job and node 14 for another)\n- SSH\n- VirtualBox\n- Parallels\n- Docker\n    - gives us a clean environment for each job. Therefore, this should be the default executor we use.\n    - however, there is added overhead since we need to pull a Docker image for each job.\n- Docker Machine\n- Kubernetes\n    - use this if we already have kubernetes set up.\n    - the GitLab Runner will run the jobs on a Kubernetes cluster. Therefore, each job will have its own pod.\n    - this executor can help us scale our build infrastructure.\n\n# Resources\nhttps://medium.com/devops-with-valentine/a-brief-guide-to-gitlab-ci-runners-and-executors-a81b9b8bf24e\nhttps://docs.gitlab.com/runner/executors/","n":0.067}}},{"i":945,"$":{"0":{"v":"Review App","n":0.707},"1":{"v":"\nWe can configure a Review App to be deployed to a seperate environment each time a commit is pushed to a branch\n\nA Review App is a mapping of a branch with an environment.\n\n![](/assets/images/2021-12-24-13-45-50.png)\n\n[docs](https://docs.gitlab.com/ee/ci/review_apps/)\n","n":0.174}}},{"i":946,"$":{"0":{"v":"Pipelines","n":1},"1":{"v":"\nPipelines are the top-level component of continuous integration, delivery, and deployment.\n\nA pipeline is a set of instructions (ie. [[gitlab.CI-CD.jobs]]) that are executed in an order that we define\n\nA merge request can have many pipelines, and each pipeline must belong to one merge request.\n\nNormally a pipeline is created when you push to a branch on Gitlab\n\nPipelines comprise:\n- Jobs, which define what to do. For example, jobs that compile or test code.\n- Stages, which define when to run the jobs. For example, stages that run tests after stages that compile the code.\n\nIf all jobs in a stage succeed, the pipeline moves on to the next stage.\n\nIf any job in a stage fails, the next stage is not executed and the pipeline ends early.\n- we can set a job to `allow_failure: true` if we want to override this behaviour.\n\nA typical pipeline might consist of four stages, executed in the following order:\n- A test stage, with two jobs called unittest and lint.\n- A build stage, with a job called compile.\n- A staging stage, with a job called deploy-to-stage.\n- A production stage, with a job called deploy-to-prod.\n\n## Types of Pipeline\nPipelines come in various configurations\n\n### Basic pipelines (ie. Branch Pipelines)\nrun everything in each stage concurrently, followed by the next stage. Run the pipeline each time changes are pushed to a branch.\n\nBasic pipelines have some characteristics:\n- they run when you push a new commit to a branch.\n- they have access to some predefined variables.\n- they have access to protected variables and protected runners.\n\n### Merge Request Pipelines\nRun only when commits associated with an MR are pushed (rather than for every commit).\n- Merge Request jobs run in a separate pipeline from Commit jobs.\n\nPipelines for merge requests run when you:\n- Create a new merge request.\n- Commit changes to the source branch for the merge request.\n- Select the Run pipeline button from the Pipelines tab in the merge request.\n\nThis means that if you use a squash-based workflow, jobs in the MR pipelines (ie. those associated with each commit to that MR) will have run on a different commit than the commit that actually gets merged into the `main` branch.\n- remember that in a squash-based workflow, all the commits that went into that MR will not make its way into the repo's history. Consider that if we have a job that depends on a git commit SHA (e.g. a `contract-test` job that records the publication of the contract by the commit SHA), we cannot perform this job in the MR pipeline alone, because the contract testing framework (here, [[Pact|testing.method.contract.pact]]) will only be aware of the commit SHA-related contracts that are associated with those commits that will never make its way into the main commit history line.\n\nMerge Request Pipelines don't run by default; they *must* be configured with `rules` or `only/except` (old method)\n\nJobs of a merge request pipeline only run on the contents of the source branch, ignoring the target branch.\n\nMerge Request Pipelines also have access to additional [pre-defined variables](https://docs.gitlab.com/ee/ci/variables/predefined_variables.html#predefined-variables-for-merge-request-pipelines) \n\nMerge Request Pipelines display the `merge request` tag:\n\n![](/assets/images/2022-08-25-09-54-07.png)\n\n### Git Tag Pipelines\n- note: \"git tag pipeline\" is not an official type of pipeline\n\nA pipeline that runs once a git tag has been created\n![](/assets/images/2022-11-16-04-26-39.png)\n\n```yml\n.git_tag_trigger:\n  only:\n    variables:\n      - $CI_COMMIT_TAG\n```\n\n### Merged Requests Pipeline\n- note the merge**d** request**s**\n\nmerge request pipelines that act as though the changes from the source branch have already been merged into the target branch.\n\n### Merge Trains \nMerge trains use pipelines for merged results to queue merges one after the other.\n\n### Directed Acyclic Graph Pipeline (DAG) \npipelines are based on relationships between jobs and can run more quickly than basic pipelines.\n- we achieve this with the `needs` keyword, which allows us to be explicit about the job's dependencies \n    - ex. a post-install stage *needs* an install stage to have succeeded first\n\n### Multi-project pipelines \ncombine pipelines for different projects together.\n\n### Parent-Child pipelines \nbreak down complex pipelines into one parent pipeline that can trigger multiple child sub-pipelines, which all run in the same project and with the same SHA. This pipeline architecture is commonly used for mono-repos.\n","n":0.039}}},{"i":947,"$":{"0":{"v":"Jobs","n":1},"1":{"v":"\n### What is a Job?\nA job is an atomic unit that is picked up by the runner.\n- If we were acting as the runner, we would simply run a script, like `npm run test` or `npm run lint`. These are 2 different jobs, and they are picked up by the runner.\n- jobs are executed in the environment of the runner.\n\nMultiple jobs in the same stage are executed in parallel\n\nJobs are defined at the top-level of the `gitlab-ci.yml` file.\n- you might have jobs called Lint, Test, Deploy Preview, etc\n\nWhat is important is that each job is run independently from each other.\n\nwith rules, we can modify the order that jobs within a stage will be run.\n\nGitlab loads everything from top to bottom, allowing us to override any job, just by changing the sequential order.\n\nThe `rules` or `only/except` keywords are what determine whether or not a job is added to a pipeline.\n- if a job is not added to the pipeline, the issue is probably related to this.\n- `only/except` is the older way to do it\n\n### Non-zero exit codes\nWhen script commands return an exit code other than zero, the job fails and further commands do not execute.\n\nStore the exit code in a variable to avoid this behavior:\n\n```yml\njob:\n  script:\n    - false || exit_code=$?\n    - if [ $exit_code -ne 0 ]; then echo \"Previous command failed\"; fi;\n```\n\n### Job Templates\nJobs prepended with a `.` will be hidden, and will not be processed by Gitlab CI/CD.\n- You can use hidden jobs as templates for reusable configuration with `extends` keyword or YAML anchors.\n\n### Running jobs conditionally (`rules`)\n```yml\ndefault-job:\n  script:\n    - yarn test\n  rules:\n    - if: $CI_COMMIT_BRANCH\n```\n\n### Groups of jobs\n```yml\nbuild ruby 1/3:\n  stage: build\n  script:\n    - echo \"ruby1\"\n\nbuild ruby 2/3:\n  stage: build\n  script:\n    - echo \"ruby2\"\n\nbuild ruby 3/3:\n  stage: build\n  script:\n    - echo \"ruby3\"\n```\n\nEach job can run in a separate isolated [[Docker container|docker.containers]]\n- [docs](https://docs.gitlab.com/ee/ci/docker/using_docker_images.html)\n\n### Resource Group\nAllows us to make sure only one job per resource group is working at a time\n- ex. consider 2 different pipelines of the same repo that both deploy to the same place. Of course, deployments should happen sequentially, so we put both of those jobs in the same resource_group, guaranteeing that they will run one after the other.\n\nA job can be specified as part of a `resource_group` which ensures a job is mutually exclusive across different pipelines for the same project.\n- if multiple jobs that belong to the same resource group are queued simultaneously, only one of the jobs starts. The other jobs wait until the resource_group is free.\n- ex. `resource_group=prod` puts a limitation that only one job in the `prod` resource group may run at a time.\n- Resource groups behave similar to semaphores in other programming languages.\n\n### Service\nWhen we use services, we specify 2 keywords:\n- services\n- images\n\nA job can define a Docker image\n\n* * *\n\nYou can’t use these keywords as job names:\n- image\n- services\n- stages\n- types\n- before_script\n- after_script\n- variables\n- cache\n- include\n","n":0.046}}},{"i":948,"$":{"0":{"v":"Cache","n":1},"1":{"v":"\n[[Jobs|gitlab.CI-CD.jobs]] can be configured to cache directories/files, which can be used by subsequent jobs and even subsequent pipelines.\n- Subsequent jobs in the same pipeline can use the cache, if the dependencies are identical.\n\nUse cache for dependencies, like packages you download from the internet. Cache is stored where GitLab Runner is installed \n- it is also uploaded to [[S3|aws.svc.S3]] if distributed cache is enabled.\n\nCaching happens after `after_script` has run.\n\nIn the following pipeline, these things happen in order:\n- the build-cache is checked for the `vendor` directory (perhaps jobA cached it)\n  - If nothing is found, we move onto the next step and output \"Hello world\" to the console. Once all steps are done (ex. all before_script, script and after_script steps), the `vendor/` directory is zipped into cache.zip\n  - If the `build-cache` directory is found in the cache.zip, then the cache is extracted, and we move on to the subsequent steps.\n\nThis above flow shows how the cache is extracted at the start if it contains the key, and it is filled at the end if the data hasn't been cached yet.\n\n```yml\njobB:\n  cache:\n    key: build-cache\n    paths:\n      - vendor/\n  after_script: \n    - \"echo Hello world\"\n```\n\nJobs that have caches with the same key will share the cache.\n\nTag your runners and use the tag on jobs that share the cache.\n\nAll caches defined for a job are archived in a single cache.zip\nThe runner configuration defines where the file is stored.\n- By default, the cache is stored on the machine where GitLab Runner is installed.\n    - [exact paths](https://docs.gitlab.com/ee/ci/caching/#where-the-caches-are-stored)\n\nFor runners to work with caches efficiently, you must do one of the following:\n- Use a single runner for all your jobs.\n- Use multiple runners that have distributed caching, where the cache is stored in S3 buckets. Shared runners on GitLab.com behave this way. These runners can be in autoscale mode, but they don’t have to be.\n- Use multiple runners with the same architecture and have these runners share a common network-mounted directory to store the cache. This directory should use NFS or something similar. These runners must be in autoscale mode.\n\nYou can have a maximum of four caches:\n```yml\ntest-job:\n  stage: build\n  cache:\n    - key:\n        files:\n          - Gemfile.lock\n      paths:\n        - vendor/ruby\n    - key:\n        files:\n          - yarn.lock\n      paths:\n        - .yarn-cache/\n  script:\n    - bundle install --path=vendor\n    - yarn install --cache-folder .yarn-cache\n    - echo Run tests...\n```\n\nTo have jobs in each branch use the same cache, define a cache with the key: $CI_COMMIT_REF_SLUG:\n- note: this pattern can be followed to define any business-logic surrounding which objects should share a cache.\n    - per-job and per-branch caching: `\"$CI_JOB_NAME-$CI_COMMIT_REF_SLUG\"`\n    - per-stage and per-branch caching: `\"$CI_JOB_STAGE-$CI_COMMIT_REF_SLUG\"`\n\n```yml\ncache:\n  key: $CI_COMMIT_REF_SLUG\n```\n\n# Resources\n[Special consideration to caching Node.js dependencies](https://docs.gitlab.com/ee/ci/caching/#cache-nodejs-dependencies)\n","n":0.048}}},{"i":949,"$":{"0":{"v":"Artifact","n":1},"1":{"v":"\nArtifacts come in 2 flavors: Job Artifacts and Pipeline Artifacts\n\n## Job Artifacts\nJobs can output an archive of files and directories. This output is known as a job artifact.\n\nOften our [[jobs|gitlab.CI-CD.jobs]] generate files which are used by subsequent jobs/stages.\n- ex. our deploy stage might generate libraries and files which are then used to deploy the application. These files are collectively known as the artifact\n    - this can give us insight into how something was deployed\n\nArtifacts can be uploaded and browsed by using the `artifact` property on the job.\n- similar to a [[cache|gitlab.CI-CD.cache]], it is then available to other jobs (though they must be in the same pipeline).\n\nYou may want to create artifacts only for tagged releases to avoid filling the build server storage with temporary build artifacts. For example, use rules to create artifacts only for tags:\n\n```yml\ndefault-job:\n  script:\n    - yarn test\n  rules:\n    - if: $CI_COMMIT_BRANCH\n\nrelease-job:\n  script:\n    - yarn package -U\n  artifacts:\n    name: \"$CI_COMMIT_REF_NAME\"\n    paths:\n      - target/*.war\n  rules:\n    - if: $CI_COMMIT_TAG\n```\n\nYou can use wildcards for directories too. For example, if you want to get all the files inside the directories that end with xyz:\n```yml\njob:\n  artifacts:\n    paths:\n      - path/*xyz/*\n```\n\nSave all Git untracked files and files in `binaries/`:\n```yml\nartifacts:\n  untracked: true\n  paths:\n    - binaries/\n```\n\nImagine we are using [[Serverless Framework|serverless-framework]] to create 3 [[lambdas|aws.svc.lambda]]. Building this will result in us having 3 different directories, each with its own `node_modules/`. We may want to create this artifact (which is in effect a `dist/` folder with 3 subdirectories).\n\n## How to view + download Artifacts\n[docs](https://docs.gitlab.com/ee/ci/pipelines/job_artifacts.html#download-job-artifacts)\n\n## Pipeline Artifacts\nPipeline artifacts are files created by GitLab after a pipeline finishes. \n- Pipeline artifacts are different to job artifacts because they are not explicitly managed by `.gitlab-ci.yml` definitions.\n\nex. Pipeline artifacts are used by the [test coverage visualization feature](https://docs.gitlab.com/ee/user/project/merge_requests/test_coverage_visualization.html) to collect coverage information. It uses the artifacts: reports CI/CD keyword.\n\nThe latest pipeline artifacts are kept forever. Once a new one comes in, old ones will live on for 7 days after creation date.\n","n":0.056}}},{"i":950,"$":{"0":{"v":"GitHub","n":1}}},{"i":951,"$":{"0":{"v":"PR","n":1}}},{"i":952,"$":{"0":{"v":"Github Pages","n":0.707},"1":{"v":"\n[Subdirectory of master branch at root of gh-pages branch](https://gist.github.com/cobyism/4730490)\n","n":0.333}}},{"i":953,"$":{"0":{"v":"Hub CLI","n":0.707},"1":{"v":"\n##### Open a PR template with Vim\n`hub pull-request`\n","n":0.354}}},{"i":954,"$":{"0":{"v":"Code Review","n":0.707},"1":{"v":"\n# E Resources\n- [Good guidelines for CR etiquette](https://github.com/thoughtbot/guides/tree/main/code-review)\n- [Showing empathy during CR](https://thoughtbot.com/blog/empathy-online)\n\n# UE Resources\n- [Implementing a strong code review culture: A recommended video](https://www.youtube.com/watch?v=PJjmw9TRB7s&t=1s)\n","n":0.209}}},{"i":955,"$":{"0":{"v":"Cmds","n":1}}},{"i":956,"$":{"0":{"v":"Searching Github Repos","n":0.577},"1":{"v":"\nsearch all code bases with `@json`\n- `\"json;\"`\n\nexclude results from repository WatermelonDB\n- `-repo:Nozbe/WatermelonDB`\n\ncheckout a branch from github (ex. for CR)\n- `hub pr checkout <issue number>`\n","n":0.204}}},{"i":957,"$":{"0":{"v":"Github Actions","n":0.707},"1":{"v":"\nA Github Action is established as soon as we have a `.github/workflows` directory committed to our repo on Github.\n\n![](/assets/images/2021-04-15-21-56-11.png)\n\n## Terminology\n\n### Workflow\n![[github.actions.workflow]]\n\n### Job\n![[github.actions.job]]\n\n### Step\n![[github.actions.step]]\n\n### Action\n![[github.actions.action]]\n\n### Runner\n![[github.actions.runner]]\n\n* * *\n\n### Secrets\n`GITHUB_TOKEN` is passed to the runner when a workflow is triggered.\n- Aside from this, no other secrets are passed by default, and must be configured manually.\n\nTo provide an action with a secret (either as input or as an env variable), use the `secrets` context.\n- This allows us to access secrets we've created in our repo.\n- [more info](https://docs.github.com/en/actions/reference/context-and-expression-syntax-for-github-actions)\n\nSecrets should not be passed between processes via the command line, as these commands might be visible to other users with the `ps` command.\n- This is why we use `STDIN` or env variables to pass secrets.\n\n# UE Resources\n[Actions V2 features](https://jasonet.co/posts/new-features-of-github-actions/)\n","n":0.09}}},{"i":958,"$":{"0":{"v":"Workflow","n":1},"1":{"v":"\nA workflow is an automated procedure that exists as a part of our repo.\n- Workflows are made up of a collection of jobs which run is response to some event (on PR, on push etc)\n\nA Github Workflow can be used to build, test, package, release, or deploy a project on GitHub.\n\nWorkflows can be configured to run in response to a webhook call, so we can trigger workflows in response to events happening outside Github.\n\n# Resources\n[Workflow Syntax Guide](https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions)\n","n":0.114}}},{"i":959,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Issue labeler\n```yml\nname: \"Issue Labeler\"\non:\n  issues:\n    types: [opened, edited]\n\njobs:\n  triage:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: github/issue-labeler@v2.2\n        with:\n          repo-token: \"${{ secrets.GITHUB_TOKEN }}\"\n          configuration-path: .github/config/issue-labler/labler.yml\n          not-before: '2020-02-21T00:00:00Z'\n          enable-versioned-regex: 1\n          versioned-regex: 'issue_labeler_regex_version=(\\d+)'\n```\n\n`labler.yml`\n```yml\narea.onboard:\n  - '- \\[ ?(?:x|X) ?\\] Onboard.'\narea.create:\n  - '- \\[ ?(?:x|X) ?\\] Create.'\n# ...\n```\n","n":0.154}}},{"i":960,"$":{"0":{"v":"Step","n":1},"1":{"v":"\nA step is an individual task that can run commands in a job\n\nA step can either be:\n- a shell command\n- an action\n\nEach step in a job executes on the same runner, allowing the actions in that job to share data with each other.\n\nChanging directories in one step does not carry through to the next step:\n```yml\n# workflow.actions.yml\n    - name: cd into publishable\n      run: |\n        cd publishable \n        pwd\n\n    - name: does the cd carry thru?\n      run: |\n        pwd\n```\n\nlog:\n```\nRun cd publishable\n/home/runner/work/Digital-Garden/Digital-Garden/publishable\n\nRun pwd\n/home/runner/work/Digital-Garden/Digital-Garden\n```\n","n":0.112}}},{"i":961,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Run shell command\n```yml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - name: run a script\n      run: |\n        echo \"here it is boys!\"\n        ls main\n```\n\n#### `run` a command in a specific directory\n```yml\n- name: Clean temp directory\n  run: rm -rf *\n  working-directory: ./temp\n```","n":0.16}}},{"i":962,"$":{"0":{"v":"Runner","n":1},"1":{"v":"\nA runner is a server that has the GitHub Actions runner application installed\n- meaning we could use the managed service from Github, or self-host the Runner.\n\nA runner listens for available jobs, runs one job at a time, and reports the progress, logs, and results back to GitHub\n\nGitHub-hosted runners are based on Ubuntu Linux, Microsoft Windows, and macOS\n","n":0.132}}},{"i":963,"$":{"0":{"v":"Problem Matchers","n":0.707},"1":{"v":"\nProblem Matchers are a way to scan the output of actions for a specified regex pattern and surface that information prominently in the UI.\n\n# UE Resources\n[Docs](https://github.com/actions/toolkit/blob/main/docs/problem-matchers.md)\n","n":0.196}}},{"i":964,"$":{"0":{"v":"Job","n":1},"1":{"v":"\nA job is a set of steps that execute on a single runner\n\nA workflow with multiple jobs will run those jobs in parallel by default (configurable)\n\nEach job in a workflow runs in a fresh virtual environment\n","n":0.167}}},{"i":965,"$":{"0":{"v":"Examples","n":1},"1":{"v":"\n[Action to send Slack messages](https://github.com/Brymastr/slack-action)\n","n":0.447}}},{"i":966,"$":{"0":{"v":"Env Vars","n":0.707},"1":{"v":"\nThe runner automatically creates a `GITHUB_TOKEN` secret to authenticate in your workflow\n\nhttps://docs.github.com/en/actions/learn-github-actions/environment-variables\n","n":0.289}}},{"i":967,"$":{"0":{"v":"Action","n":1},"1":{"v":"\nActions are standalone commands that are combined into steps to create a job\nActions are the smallest portable building block of a workflow. You can create your own actions, or use actions created by the GitHub community\n\nFor instance, we have the `checkout@v2` action, which allows the runner to checkout a repo. In fact, we can checkout any SHA from any repo.\n","n":0.129}}},{"i":968,"$":{"0":{"v":"Git","n":1},"1":{"v":"\n# Resources\n- [SQL queries for Git](https://askgit.com/)\n- [A set of Git Katas (practice)](https://github.com/eficode-academy/git-katas)\n- [Remove bad files from Git history](https://rtyley.github.io/bfg-repo-cleaner/)\n","n":0.236}}},{"i":969,"$":{"0":{"v":"Hooks","n":1}}},{"i":970,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### pre-commit hook to check for existence of `TODO` in code\n```sh\n#!/bin/sh\n\n. git-sh-setup  # for die \nif git-diff-index -p -M --cached HEAD -- \\\n| grep '^+' \\\n| grep TODO; then\n    die Blocking commit because string @todo detected in patch\nfi\n```","n":0.162}}},{"i":971,"$":{"0":{"v":"Submodule","n":1},"1":{"v":"\nGit submodules solve the problem: \"what if we need to use another project from within our project repo?\"\n- ex. we have a module (e.g. a React UI library) that we want to use in multiple projects\n\nOne reason that we'd include a submodule package instead of just including it as an npm package it is that in the latter case, we need to run `npm install` in order to get the latest changes.","n":0.118}}},{"i":972,"$":{"0":{"v":"Rewriting History","n":0.707}}},{"i":973,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Remove a file permenantly from the history\ntip: check the resulting SHAs and compare the commit messages of before and after. Do the SHAs differ from what you would have expected? Between the two, are SHAs of \"same\" commits different BEFORE (ie. earlier in git history) the file you are removing (e.g. here `.env`)\n`git filter-repo --force --invert-paths --path .env`\n","n":0.13}}},{"i":974,"$":{"0":{"v":"Refspec","n":1},"1":{"v":"\nMap a branch in local repo to branch in remote repo.\n- Allows us to manage remote branches using local Git commands\n\nSpecified as `<src>:<dst>`\n","n":0.209}}},{"i":975,"$":{"0":{"v":"Ref","n":1},"1":{"v":"\nA ref is anything pointing to a commit.\n- ie. branches (local/remote) and tags, mostly.\n- unlike objects, they are mutable and constantly change.\n    - In Git, the immutable parts are the expensive ones (ex. an entire blob), while the mutable parts are just references, and therefore cheap (ex. branch, remote, HEAD)\n- Think of it like a user-friendly alias for commit SHA\n    - ex. `my-feature-branch` is actually an alias for the branch. In reality, the branch name is a SHA\n\nIn a world where all we have is [[blobs|git.plumbing.objects.blob]], [[trees|git.plumbing.objects.tree]] and [[commit objects|git.plumbing.objects.commit]], we would have to remember the commit SHAs so we have a starting point for our history. Instead it would be easier if we had a file where we could store that SHA under a simple name, so we could use the name instead of the SHA. These simple names are called *refs*.\n\n- `git show-ref`\n\t- ex. if we look at `.git/refs/heads/master`, we will see a commit SHA, which is the latest commit on this branch. If this wasn't done automatically for us, we could manually create this file with the SHA, which would effectively be saying \"hey Git, when we are on master branch, the latest commit will be this SHA\"\n\t\t- Therein lies basically what a branch is: a simple pointer to the latest commit\n- stores pointers to commit objects that we would consider to be significant\n\t- ie. the ref is the location of the commit \n    - The head commit of each branch that exists (`refs/heads` directory)\n    - The head commit of each branch on the remote repos (mostly just `origin`)\n    - stash commits\n    - tags\n- if commits point to trees, and trees point to blobs, then refs conceptually point to commits\n\n### Relative Refs\nwe can use relative refs to move around.\n","n":0.059}}},{"i":976,"$":{"0":{"v":"Git Plumbing","n":0.707},"1":{"v":"\n*\"Plumbing\"* refers to the analogy of a bathroom fixture, where there is a porcelain part and a plumbing part. The Porcelain part is what we interact with on a daily basis, and the plumbing part is what happens under the hood.\n\nGit was initially designed as a toolkit for a version control system rather than a full user-friendly VCS. These low-level commands were designed to be chained together UNIX-style or called from scripts.\n\n### How git data is stored\nGit's database is built on references. It has models of [[commits|git.plumbing.objects.commit]], [[trees|git.plumbing.objects.tree]], [[blobs|git.plumbing.objects.blob]].\n\nEssentially, it is just one giant acyclic graph\n- here, acyclic means that we can pick one commit and traverse its parents in one direction and there will never be a chain that begins and ends with the same commit; no cycle of commits can be formed.\n\nCommits are snapshots, not diffs\n- Git takes snapshots (via commits), and stores it in something of a mini-filesystem. When Git stores a new version of a project, it stores a new tree (a bunch of blobs and a collection of pointers) that can be used to expand back out into the whole directory of files and directories.\n- ex. if we change a single line in a single file, that is technically a new version of the project. However, Git is smart and it realizes that there were no changes in 99.9% of the files in the codebase. This \"new version\" of the project does in fact have its own tree, but because all (except one) of the blobs in this tree are just references to other blobs, this version of the project is extremely lightweight (in fact the only \"weight\" is the content of the new file). This way of handling snapshots is a core reason why Git is so efficient, yet so powerful.\n\nEach time we create a file and begin tracking it, git compresses it and stores it into its own structure\n\nGit stores content similar to how a UNIX filesystem does. All content is stored as tree and blob objects\n- trees would correspond to directories\n\t- Like UNIX filesystem, all a directory is is a file with its contents listed. (The relationship between parent-child is made via reference, not the child actually being inside parent)\n- blobs would correspond to file contents or inodes\n\nA single tree object contains one or more entries, which is a SHA of a sub-tree or blob (ie. directory or file)\n- ex. imagine we had the following project structure:\n```sh\n.\n|-- README\n`-- lib\n    |-- inc\n    |   `-- tricks.rb\n    `-- mylib.rb\n```\n- if we made a commit, we would create a tree object for each directory, and a blob object for each file. It would be represented like this:\n![](/assets/images/2021-03-06-16-14-27.png)\n\nThe reason we can represent the history of the codebase as a tree is because each snapshot (commit) specifies its parent\n\nThe object database is content-addressable, meaning that we can retrieve the data based on its content, rather than its location.\n- This is a major reason why Git is so performant\n\n* * *\n\nAt the core of Git is a simple key-value store, with the value being the content, and the key being a SHA representing that content.\n\n### Demo\n- With a fresh git repo, upon adding `foo.txt` to the index, a new object (in `.git/objects`) is created. If we run `git cat-file` on that object, we will see the contents of the file.\n- Upon committing, 2 more objects will be created:\n\t1. a tree object, which makes reference to `foo.txt`, which points to a blob object SHA\n\t\t- if we `git cat-file` that SHA, we will see the contents of `foo.txt`.\n\t\t\t- What this demonstrates is that we have a tree object which references a filename, which is associated with the content in the file. None of these things are glued together, but they merely make references to each other.\n\t2. a commit object, with reference to the tree (the same tree that was just created)\n\t\t- when we run `git commit`, the SHA of that commit object is output into the console\n\t- We have just demonstrated that a commit points to a tree, and a tree points to a blob. First a blob is created (when adding to index), then committing will create a commit and tree object.\n- Now with a clean tree, we create a new file `bar.txt`, and edit the contents of `foo.txt`. We add both those files to the index.\n\t- two more blob objects are created\n- With those 2 files in the staging area, we commit, and 2 more objects are created: a tree object and a commit object.\n\t- Since this is the second commit, it also references its parent commit\n\t- The tree object will reference both `foo.txt` and `bar.txt`\n- Again with a clean tree, we create a new file `baz.txt`, add it and commit it.\n\t- Interestingly, the tree object that just got created will reference all `foo`, `bar` and `baz`, even though only `baz` was changed in this commit.\n\t\t- This is because the commit object maintains the entire state of every file (and every version of the file) by SHA at that point in time.\n\t\t\t- This is precisely why we can cherry pick a commit and put it on disk\n\n### How Git handles renaming files\nThere is no data structure in Git that stores a record that a file was renamed during a commit. Instead, Git attempts to detect renames during the dynamic diff calculation. \n- There are 2 stages to this rename detection\n\t1. exact renames— here, OIDs (of the blobs) are identical, meaning that the contents of the files are the same; just with a different name.\n\t2. edit-rename— here, Git must iterate through each added file and compare it against each deleted file. From there, Git calculates a **similarity score** as a percentage of lines in common (anything >50% counts as a potential edit-rename).\n\nAfter computing the diff, Git inspects it to discover which paths were added/deleted\n- naturally, a file moved from directory `A/` to directory `B/` would appear as a file being deleted in `A/` and a file being added in `B/`. Git attempts to match these deletions/additions to create a set of *inferred* renames.\n- The first\n\nWhat about if we rename a file then change some content in it?\n- Git will track these changes as first a rename, then a modification to that renamed file.\n","n":0.031}}},{"i":977,"$":{"0":{"v":"Object","n":1},"1":{"v":"\nGit is built around manipulating the structure of the 4 object types (blob, tree, commit, tag). In a way, it is like a mini-filesystem that sits upon the machine's filesystem\n- Nearly all git commands manipulate either the working tree, the staging area, or the commits.\n- objects are immutable\n- A git repo is a collection of objects, with each having its own hash\n    - A commit is a hashed object\n- Each object consists of 3 things:\n    - *type* - what type of object it is (blob, tree, commit, tag)\n    - *size* - the size of the contents\n    - *content*\n\n### SHA of Objects\nInside `.git/objects`, we see many hexadecimally named directories. The names of these are actually the first 2 digits of the SHA\n- ex. `.git/objects/77/a54737` is the commit with SHA `77a54737...`\n\nThe commits themselves are encrypted, but we can see the contents of the object with `git cat-file -p <SHA>`\n- use `-t` to see what type the object is \n\nThe below image is a representation of our workflow. Here we have 3 commits (with the first commit on the left). It's important to note how the same blob is referred by different trees. This is because that file didn't change between commits, therefore the blob is identical \n\n\n![](/assets/images/2021-03-06-16-16-37.png)\n![](/assets/images/2021-03-06-16-17-02.png)\n","n":0.07}}},{"i":978,"$":{"0":{"v":"Tree","n":1},"1":{"v":"\nA tree is Git's representation of snapshots, meaning they store the state of a directory at a given point (without notion of time/author). \n- To tie trees together into a coherent history, Git wraps each one in a commit object and specifies a parent commit. By following the parent of each commit, we can walk through the entire history of the project.\n- Each commit refers to only one tree object.\n\nA tree can contain many trees and many blobs.\n\nMultiple trees may point to the same blob\n- this happens when a file's contents (ie the blob) do not change between a commit.\n\t- therefore, most blobs are referenced by many trees.\n\n![](/assets/images/2022-04-08-22-01-55.png)\n\nThe Tree holds pointers to filenames and other trees, effectively allowing us to group files together (which is essentially what a directory is)\n- A tree object contains 1+ entries. Each of which is either a pointer to a blob or a pointer to a subtree.\n\n![](/assets/images/2022-04-08-22-01-00.png)\n\nThe tree object is what associates the filename (or directory name) with its content.\n- We can confirm this by running `git cat-file` on the tree object. It will give us back a list of blobs and their associated filenames\n- We can use a plumbing command `update-index`, which effectively allows us to associate an existing blob with a filename:\n\t- `git update-index --add --cacheinfo 100644 83baae61804e65cc73a7201a7252750c76066a30 test.txt`\n\t\t- add a file to the index (`--add`), get it from the object database (`--cacheinfo`)\n\t\t- upon executing this command, we have `test.txt` added to the staging area.\n\t\t- `100644` represents the file permissions on disk, showing that trees determine Unix file permissions for each blob\n\t\t\t- only cares about `100644` (normal file), `100755` (executable), `120000` (symbolic link)\n\nThe tree is normally made by examining the state of the staging area\n\nThe tree itself doesn’t know where it exists within the repository, that is the role of the objects pointing to the tree. The tree referenced by `<ref>^{tree}` is a special tree: the root tree. This designation is based on a special link from your commits.\n\nThe fact that git doesn't really about folder names is precicely the reason why you cannot commit an empty directory. Often on Github you will notice that people put a blank `.keep` file. This naming is meaningless to Git, but it is just a convention, as if to say \"hey, I just put a placeholder here so we can still get the empty directory to be part of our tree.\"\n","n":0.05}}},{"i":979,"$":{"0":{"v":"Tag","n":1},"1":{"v":"\nSimply a way to mark specific commits as special in some way (ex. tag a specific commit as a specific release, or something along those lines)\n- tags and branches are similar, in that they both point to a specific commit (ie. they are pointers to snapshots). Theoretically, branches could perform the role of tags. We could just keep around a branch called *Release 2.0*. Instead, Git allows us to separate concerns.\n    - A branch is a moveable pointer to a snapshot, while tags point at a single snapshot and never move. Also, tags are actually stored as objects, while branches are not.\n- if we are doing something a little risqué (like undoing a rebase), it's good to make a backup `git tag BACKUP`. Then if we ever need to go back to it, run `git reset --hard BACKUP`\n\n### Annotated vs. Lightweight Tags\nThere are 2 types of tag, lightweight and annotated\n- *lightweight* - a reference to a commit that never moves\n\t- this type of tag does not create a tag object\n- *annotated* - when we create an annotated tag, Git creates a tag object and then writes a reference point to it (rather than directly to the commit).\n\t- we usually want this one.\n\n### Inner\n- The fourth (and less integral) object functions similar to a commit object, in that it contains a tagger, a message, a date, and a pointer.\n\t- The main difference is that a tag normally points to a commit rather than a tree. In this sense it is similar to a branch reference, but it never moves (ie. it always points to the same commit).\n","n":0.061}}},{"i":980,"$":{"0":{"v":"Commits","n":1},"1":{"v":"\nA commit object stores metadata about the commit, like who authored it, when it was made, what the previous commit is etc.\n- while a [[tree|git.plumbing.objects.tree]] represents a particular directory state of a working tree, a commit represents that state in \"time\", and explains how to get there.\n\n- a commit points to a single tree, marking it as what the project looked like a certain point in time\n\t- Therefore, the commit object is what gives a project its sense of history in Git\n- While the tree and blob objects are the content of the Git, the commit objects allow us to use the data in a user-friendly way. Technically, Git could be used without commit objects. However, we would have to remember every SHA to recall the snapshots. Also, we wouldn't have important information like who saved the snapshots, when they were saved, and who saved them. These benefits are what commit objects bring to us.\n- When we run `git commit`, a commit object is created, and the parent commit is specified as the commit that the HEAD file points to (recall that a branch is just a pointer to a commit)\n\n### Boundary Commit\nA boundary commit is the commit that limits a revision range but does not belong to that range. For example the revision range `HEAD~3..HEAD` consists of 3 commits (`HEAD~2`, `HEAD~1`, and `HEAD`), and the commit `HEAD~3` serves as a boundary commit for it.\n- This is related to the concept of inclusive/exclusive ranges. In this comparison, the boundary commit would be an exclusive boundary.\n\nMore formally, git processes a revision range by starting at the specified commit and getting at other commits through the parent links. It stops at commits that don't meet the selection criteria (and therefore should be excluded) - those are the boundary commits.\n[Source](https://stackoverflow.com/questions/42437590/what-is-a-git-boundary-commit)","n":0.058}}},{"i":981,"$":{"0":{"v":"Merge Commit","n":0.707},"1":{"v":"\nWhen git creates a merge commit it will also by default append a list of files that had conflicts to the commit message:\n```\nConflicts:\n    src/foo-service.c\n    src/bar-client.c\n```\n\nThis is a piece of useful information for us when poring over the history to find out what went wrong\n- Even if your merge conflict was trivial, there is always a non-zero chance of introducing a bug when resolving a conflict, and seeing those lines in the merge commit message could be valuable information. They are basically a hint saying “Still confused? Maybe you should be extra careful when reviewing the changes in these files”.\n","n":0.101}}},{"i":982,"$":{"0":{"v":"Blob","n":1},"1":{"v":"\nA blob is the content encapsulated within a single file.\n- note: this does not include the file itself. Therefore, a blob has no notion of filename nor permissions (handled by the [[tree|git.plumbing.objects.tree]] object).\n\nThe blob is stored internally by the hash of its content\n- the \"hash of its content\" part of this last point means that as long as 2 different pieces of content are identical (ie. file contents are identical), the resulting hash will also be identical. This also illustrates the fact that blobs are decoupled from a (file)name, as it has no bearing on the produced hash.\n\nTo get the id of a blob, git will take the content of the blob and add a prefix, then pass it into a SHA1 algorithm to compute the hash.\n- the prefix is of the format `<type> <length>\\0 <content>\"\n\t- ex. `blob 11\\0my content`\n\t- git does this prefixing for us automatically when we call `git hash-object`\n- This demonstrates how each object is identified with its hash, and the type of the object is baked into that hash\n\nIf we write 2 different files with the same content to the object database, only one SHA will get recorded, since the same content hashed will always produce the same result.\n- the same can be said for a tree object, but not for a commit object, since the author and time of commit makes their content always unique.\n\nThe object is created as soon as it is known to git (ie. when added to index)\n\n","n":0.064}}},{"i":983,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\nNote: you should never need to run any of these commands directly. Their use should be restricted to learning only.\n\n#### `cat-file`: display contents of an object\n```sh\n# show object type\ngit cat-file -t <SHA>\n\n# show object content\ngit cat-file -p <SHA>\n\n# show the tree object associated with a revision\ngit cat-file -p main^{tree}\n```\n\n#### `hash-object`\n```sh\n# -w to write to object database\necho 'hello' | git hash-object -w --stdin\n```\n\n#### `ls-tree`: list the tree contents for any given revision\n```sh\ngit ls-tree <SHA> .\n```\n\n#### `rev-parse`: see which SHA-1 a branch points to\n\n#### `write-tree`: write the contents of the index to a tree object\n\n#### `commit-tree`: create a commit object from a specified tree and a parent\n```sh\ngit commit-tree \n```","n":0.097}}},{"i":984,"$":{"0":{"v":"Packfiles","n":1},"1":{"v":"\nPackfiles can be thought of compressed [[blobs|git.plumbing.objects.blob]].\n\nSince different versions of the same file create multiple blobs, it would be inefficient if the only difference between these versions was a single line. What would be great is if we could store one version of the file, and just store the delta (ie. diff) of the other with the first.\n- Git does exactly this using packfiles.\n\nThe initial format in which Git saves objects on disk is called a \"loose\" object format. Occasionally Git packs up several of these objects into a single binary file called a *packfile* for the purpose of being more efficient.\n\nPackfiles are created under 3 circumstances:\n1. there are too many loose objects around (~7000)\n2. `git gc` is run manually\n3. we push to a remote repo \n\nWhen we run `git gc`, we will notice that many of the objects in our object database disappear and are replaced by packfiles. The objects that remain are those blobs that aren't pointed to by any commit (in other words, the blobs were never added to any commits). Because of this, those blobs are considered dangling and as a result were never packed up into the new packfile.\n- The other files are the packfile and an index. The packfile contains the contents of all the objects that were removed from the filesystem. The index can be thought of as the index of a textbook, which helps us locate specific objects quickly.\n\t- if we inspect this packfile index with `git verify-pack`, we can see that 2 versions of the same file will have one version referencing the other, showing that we use deltas to be efficient. \n\t\t","n":0.061}}},{"i":985,"$":{"0":{"v":"Git LFS (Large File Storage)","n":0.447},"1":{"v":"\nThere are multiple criteria to determine whether to store a file using Git LFS.\n- The size of the file. IMO if a file is more than 10 MB, you should consider storing it in Git LFS\n- How often the file is modified. A large file (based on the users intuition of a large file) that changes very often should be stored using Git LFS\n- The type of the file. A non-text file that cannot be merged is elligible for Git LFS storage\n\nDepending on how often the file changes, any size of file from say 1MB to 50MB and above can benefit. Consider the case where you do 100 commits editing the file every time. For a 20MB file that can be compressed say to 15 MB, the repository size would increase by approximately 1.5GB if the file is not stored using Git LFS.\n\nWhy not store all files in LFS? Because files in LFS cannot easily be diffed, basically breaking an important part of the version control system.\n- hence, why Git LFS is ideal for binary files\n\n## Setting up\n1. Once it has been installed, in your repo run `git lfs track \"*.jpg\"` to start tracking a certain filetype with git lfs\n    - see all tracked files in `.gitattributes` file\n2. Add `.gitattributes` file to version control.\n3. Operate as you normally would.\n\n","n":0.068}}},{"i":986,"$":{"0":{"v":"HEAD","n":1},"1":{"v":"\nHEAD is the pointer that points to the latest commit on the currently checked out branch.\n- Therefore the HEAD is what lets Git know which commit will be the parent for the next commit.\n\n- spec: While a branch simply points to a commit, a HEAD simply points to a branch (or a commit). If it points to a commit, it is because the commit is not at the tip of its branch, resulting in a detached HEAD.\n\t- Recall that when HEAD does not coincide with the tip of any branch, the result is a detached HEAD. From Git's point of view, this simply means that HEAD doesn't contain a local branch.\n\nUnder normal circumstances, HEAD points to the branch we currently have checked out. However, we can also checkout a commit. Interestingly, we can checkout the same commit that our branch points to (which would be a detached head). In this case, both working trees would be identical. \n\nNormally, the HEAD file is a symbolic reference to the currently checked out branch. We can see this by logging out the contents of `.git/HEAD`\n- here, symbolic reference means that it contains a pointer to another reference.\n","n":0.072}}},{"i":987,"$":{"0":{"v":"Gitignore","n":1},"1":{"v":"\nWhen git comes across a file in your repo that is untracked, it will report it to you (in `git status`). However, if that file has been added to `.gitignore`, it will suppress it.\n- once a file is known to git (ie. it has been in the index), adding the file to `.gitignore` has no effect (ie. the file will continue to be tracked)\n\nif you need to stop tracking a file (for example, we've already added the file to the index, but now want to start ignoring it), we need to run `git rm --cached <file>`\n- for folders, `git rm -r --cached <folder>`\n","n":0.099}}},{"i":988,"$":{"0":{"v":"Formulas","n":1},"1":{"v":"\n<!-- This file should probably be refactored so that we include each recipe along with whichever submodule it belongs to (ex. deleting commits should probably be in git.cli.rebase.cook) -->\n<!-- Is this the most logical place that we'd expect to find this information? -->\n\n### Rewording/deleting commits\n- imagine we want to change a commit message from 2 commits ago, or imagine we want to delete it altogether\n\t1. We can run `git rebase -i HEAD~2`\n\t\t- meaning \"I want to operate on the last 2 commits\"\n\t2. vim will open and will list all of the commits that we have asked to change.\n\t3. change the command from `pick` to `reword` (or `drop` to delete) of the relevant commit, then save and close the file.\n\t4. change the message, save and close.\n\n### Changing contents of commit/Splitting commits\n1. run `git rebase -i HEAD~n`, where `n` is the commit immediately preceding the commit we want to edit.\n3. change the rebase command of the commit we want to edit from `pick` to `edit`\n4. run `git reset --mixed HEAD^`, which leaves the working directory unchanged, but reverses the commit\n\t- `HEAD^` is the parent of our current commit\n6. do our normal operations, adding files, deleting files, or modifying them as we please. `git add` them, `commit` them as per normal workflow.\n7. run `git rebase --continue`\n\n### Reverting commits\n- think of `git revert` as an inverse operation to `git commit`. Effectively, this command creates a new commit that undoes all of the changes introduced by a certain commit.\n- Imagine we made a commit that simply changed which port our app connects to. Later on down the line, imagine that we want to \"undo\" that, and go back to the original port. We could change that port in the code and commit it, or we could simply run `git revert <SHA>` to make a new commit, known as a `revert commit` whose sole purpose is to reverse the changes that that particular commit actually made.\n\n### Squashing commits\n- Imagine we made 3 commits that should logically only be one.\n\t1. run `git rebase -i HEAD~3`\n\t2. leave the commit that everything will get squished into alone, but change the commits that will be thrown away from `pick` to `fixup`\n\t\t- listed by oldest to newest\n\t\t- fixup and squish are similar, but fixup will discard the commit messages of the discarded commits.\n\t\t- if we want to retain the commit messages, then use `squash` instead of `fixup`\n- If we compare the git logs before and after, we will notice that the SHA of the commit with the same message will be different, showing that we are in fact rewriting history\n\n### Splitting commits\n- Imagine we have made a commit that realistically should actually be split into 2 commits (\"add navbar and fix bug\")\n\t1. run `git rebase -i HEAD~2`\n\t2. change the commit we want to split from rebase operation `pick` to `edit`\n\t3. after saving the file, git will put us onto a special rebasing branch\n\t4. unstage all files that were added during that commit by running `git reset HEAD^`\n\t5. perform normal workflow, by adding commit1 changes and committing, then adding commit2 changes and committing\n\t6. run `git rebase --continue`\n\n## Undoing Work\n- this will discard everything permanently\n\n### Restoring to the state of the last commit\n- if just a single file, we can simply run `git checkout HEAD <file>` to blow away all changes, and restore the file to what it was in the most recent commit.\n- if we want to blow away all changes and get the exact state of the last commit, we can run `git reset --hard HEAD`\n\t- This tells Git to replace the files in your working copy with the \"HEAD\" revision\n\t- we can replace HEAD with any SHA to go back to that previous version.\n\t- note: this will not produce any new commits (like revert), nor will it delete any old ones. Instead, it works by resetting your current HEAD branch to an older revision (also called \"rolling back\" to that older revision)\n\t- note2: since this doesn't rewrite history, the commits we \"erased\" are still available\n","n":0.039}}},{"i":989,"$":{"0":{"v":"Git Flow","n":0.707},"1":{"v":"\n### Rebase Workflow\nWith an interactive rebase workflow, the idea is to create small commits as you go along, atomic enough that you can describe it in one line. For a given PR you might end up with 30 commits. If along the way you make mistakes, for instance you forget to rename something during an earlier commit when you renamed a variable in multiple places, you should just make a `fix` commit. Later on when you are ready to put your code up for PR, you can do an interactive rebase and move commits around, such as squashing the missed rename into the first commit that was supposed to do that.\n\n[Gitlab flow](https://about.gitlab.com/topics/version-control/what-is-gitlab-flow/)\n- involves having main, production and stable branches.\n","n":0.092}}},{"i":990,"$":{"0":{"v":"Extras","n":1},"1":{"v":"\n### Notable commands\ngit standup\ngit obliterate\ngit missing\ngit reset-file\ngit delta\ngit browse\ngit rename-branch\n\nhttps://github.com/tj/git-extras/blob/master/Commands.md#git-standup","n":0.316}}},{"i":991,"$":{"0":{"v":"Deploy","n":1},"1":{"v":"\n## Centralized vs Distributed Git\nWe can run Git in a centralized system, where we have a git repo on a central server. All contributors must push and pull to the same repo for progress on the project to be made.\n- A concern with this is that for anyone to be able to contribute, they must have access to the entire project. Therefore you cannot maintain control with a centralized workflow.\n\t- This may not be a concern, if there are 2 or 3 people working on a project.\n\nWith an integrator workflow, everyone has their own private repo and public repo. When a contributor wants to add their changes, they push to their own public repo. From there, we can pull their changes into our local repo to ensure everything works as expected. If everything is good, we merge them into our local branch, then push that branch to the main repo. From there, everyone can pull those changes.\n\t- with this workflow, everyone pulls from a single official repo, but pushes to their own public repo.\n\nCentralized:\n![](/assets/images/2021-03-11-15-39-35.png)\nDistributed:\n![](/assets/images/2021-03-11-15-39-48.png)\n","n":0.076}}},{"i":992,"$":{"0":{"v":"Conventional Commits","n":0.707},"1":{"v":"\n## About\nConventional Commits are designed to dovetail with SemVer\n- the CC `fix` corresponds to SemVer `PATCH`\n- the CC `feat` corresponds to SemVer `MINOR`\n- a `!` after the CC type/scope corresponds to SemVer `MAJOR` (ie. breaking change)\n\n## Types\n- fix\n- feat\n- build - changes that affect build components like build tool, ci pipeline, dependencies, project version etc.\n- ops - changes that affect operational components like infrastructure, deployment, backup, recovery\n- chore\n- ci\n- docs\n- style - changes to the code to do with formatting (white-space, semi-colons etc). Consistent with changes that a linter makes\n- refactor - a rewrite/restructure to the code that does not change any behaviour\n\t- perf - a subtype of `refactor` that improves performance.\n- revert\n- test\n\n[ref](https://github.com/commitizen/conventional-commit-types/blob/master/index.json)\n[explanation](https://news.ycombinator.com/item?id=19706037)\n\n## Structure:\n```\n<type>[optional scope]: <description>\n\n[optional body]\n\n[optional footer(s)]\n```\n\n### Subject (required)\nThe subject contains a succinct description of the change.\n- Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\n- Don't capitalize the first letter\n- No dot (.) at the end\n\n### Body (optional)\nThe body should include the motivation for the change and contrast this with previous behavior.\n- Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\n- This is the place to mention issue identifiers and their relations\n\n### Footer (optional)\nThe footer should contain any information about Breaking Changes and is also the place to reference Issues that this commit refers to.\n- optionally reference an issue by its id.\n- Breaking Changes should start with the word `BREAKING CHANGES:` followed by space or two newlines. The rest of the commit message is then used for this.\n\n## Example:\n```\nfeat(stripe)!: grant the ability to users to determine a primary payment method\n\nThis feature builds on database work that was previously done, to allow users to select their primary card from the payment-methods page.\nreference: JIRA-1337\n\nBREAKING CHANGES:\n```\n\ncommmit\nfeat(stripe): create `book_order` when user signals intent to buy.\n\nThis commit introduced a bit of a functionality change to how payments are processed through our backend and Stripe. Previously, rows in the invoices, invoice_items, and book_orders tables were only created after Stripe had verified our purchase (ie. by calling our webhook). Now, a row in the book_orders table will be inserted at the point that the user signals their intent to buy (ie. by entering their address info and hitting the 'Continue' button). At this point, our Express server makes an API call to get taxes (future implementation, though a placeholder solution included in this commit), and it is included on the Stripe PaymentIntent object. We retrieve this metadata in our webhook to insert it into the invoice table.\n\n## Generate Release Candidate Notes\nhttps://about.gitlab.com/topics/version-control/what-is-gitlab-flow/\n\n\nall","n":0.049}}},{"i":993,"$":{"0":{"v":"Commit","n":1},"1":{"v":"\nA commit is the immutable state of a folder structure at a given point in time. It is a snapshot— not a diff\n- There is a misconception that Git stores diffs, since we see diffs when using github, or using common commands like `git diff`. However, these diffs are dynamically generated, given 2 different commit snapshots. In fact, we can see a diff of any 2 commits. If Git stored diffs, we wouldn't be able to see a diff between 2 commits that are far apart from each other.\n    - Under the hood, Git is comparing the root [[trees|git.plumbing.objects.tree]] of each commit, and taking note of the paths that have different Object Ids\n        - ex. if we have 2 commits and the only different is an edit made in the README, Git is able to get that diff by introspecting on the trees, and noticing that the objects representing that README file have different OIDs. When the OIDs do not match, it means they have different contents\n\nYou tell Git you want to save a snapshot of your project with `git commit` and it basically records a manifest of what all of the files in your project look like at that point\n    - Most of the git commands interact with those manifests in some way\n\nWe can think about Git as a tool for storing, comparing and merging snapshots of our project\n- We can get a snapshot of an individual file with `git checkout <Commit SHA> <filename>`\n- A snapshot is basically a commit\n- Snapshot is to a repository as screenshot is to a video, since it represents one moment of time in that video\n\n## Ahead/Behind\n- If your current branch is 3 commits *ahead* of master, it means the current branch has 3 commits that don't exist on master\n- If the current branch is 3 commits *behind* master, it means there are 3 commits on master that don't exit on the current branch\n- The ahead number tells you roughly how much impact the current branch will have on the base branch should it be merged.\n- The behind number tells you how much work has happened on the base branch since this branch was started.\n    - If the number is high, it's an indication that there will not be a clean merge. This would be a good time to merge master (or other base branch) into the current branch, which would bring the \"behind\" number to 0\n- In the follow diagram:\n    - A is 2 commits behind and 0 commits ahead of B\n    - B is 0 commits behind and 2 commits ahead of A\n    - C is 1 commit behind and 2 commits ahead of A\n    - C is 3 commits behind and 2 commits ahead of B\n\n![](/assets/images/2021-03-06-16-45-56.png)\n\n## Detached State\n- Occurs when we check out a commit or a remote branch, as opposed to a local branch.\n    - Put another way, any ref that does not originate from your line of commits (and would thus be unable to trace any sort of history with the code you'd been working on)\n- If we were to develop in detached mode then try to merge it into master, git would complain to us, because by definition, detached mode means there is no path to get back into master. (ie. there is no way to reference that feature \"branch\")\n\n## Fast-Forward commit\n- Occurs when we tell git to merge our feature branch into master, but git realizes that there are no 2 branches to merge, but instead the master just needs to be brought up to speed to the current feature branch. This is known as *fast-forwarding*\n- effectively what is happening is Git engine is moving the current branch tip up to the target branch tip.\n- If the tip of master hasn't moved since we branched off, no merge commit will be made. The HEAD will simply move to the most recent commit of the feature branch.\n\t- In a ff merge scenario, since the master has not changed since we branched, a \"merging/melding\" of branches is not relevant here.\n\n### Forcing a FF Merge\nThis ff may not be desirable. Imagine we want to merge in a branch called *oauth*. This is a pretty significant feature, and we'd like to keep the branch in our history. This is a scenario where we can merge with `--no-ff`.\n\n![240d716cd6708b64cbefb47f30773132.png](:/5edfa6a1c2a4442faff1e60705f87d32)\n\n- ***non fast-forwarding*** (a.k.a *3 way merge*) ex. - we create a feature branch from master. We do some work on feature, then decide to merge into master. Since we had branched off from master, other feature branches have been merged into master. This means the git engine has to figure out how to *merge* the master branch (which is now different from how our feature remembers it) and our feature branch.\n    - If this were a *fast-forward merge*, then there would have been no merges into master during the time that our feature branch existed.\n","n":0.035}}},{"i":994,"$":{"0":{"v":"CLI","n":1}}},{"i":995,"$":{"0":{"v":"Tag","n":1}}},{"i":996,"$":{"0":{"v":"Tag","n":1},"1":{"v":"\n###### List tags\n`git tag`\n\n###### List tags given a pattern\n`git tag -l \"v1.8.5*\"`\n\n###### Create annotated tag\n`git tag -a v1.4 -m \"my version 1.4\"`\n\n###### Create lightweight tag\n`git tag v1.4-lw`\n\n###### See tag data\n`git show v1.4`\n","n":0.177}}},{"i":997,"$":{"0":{"v":"Switch","n":1},"1":{"v":"\nDesigned to replace the branch-switching functionality of `git checkout`\n","n":0.333}}},{"i":998,"$":{"0":{"v":"Subtree","n":1},"1":{"v":"\n## Subtree Push\n- `git subtree push` allows us to cause a sub-directory of the current branch to be the root-level directory of another branch\n- ex. `git subtree push --prefix docs origin gh-pages`\n\t- this pushes just the `docs` directory to the `gh-pages` branch.\n\n## UE Resources\nhttps://www.atlassian.com/git/tutorials/git-subtree\n","n":0.151}}},{"i":999,"$":{"0":{"v":"Stash","n":1},"1":{"v":"\nA stash is simply a list of patches, that you can apply wherever you want.\n- Take all modified tracked files (that are unstaged) and staged changes, and save them onto a stack of unfinished changes\n\nWhen you run git stash (alias of `git stash save`), git makes two commits that are not on any branch. \n1. One commit holds the state of the index, \n2. the second commit holds state of the work tree.\n\n`git stash` takes uncommitted changes, stores them internally, then runs `git reset --hard` to give us a clean working directory.\n- This means that stashes can be applied to any branch, useful if we ever discover that we were developing on the wrong branch.\n\n### Apply vs Pop\n- `pop` will delete the stash after it is applied, while `apply` keeps it around for future use\n\t- this is why the below trick to revert the stash does not work if `pop` is used\n","n":0.081}}},{"i":1000,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\nDiff between stash and HEAD (ie. view the changes tied to the stash)\n- `git stash show -p stash@{1}`\n\nRetain staged work of stashes\n- run `git stash pop --index` so that staged files return as staged when you pop the stash\n\nReverting `stash apply`\n- `git reset --hard`\n\t- assuming you had everything in a clean state when you started\n\nPopping stash onto a new branch\n- `git stash branch <branch-name> stash@{0}`\n\nAdding a message\n- `git stash save \"<message>\"`\n\nIncluding untracked files with stash\n- `git stash --include-untacked`\n\nDeleting a stash\n- `git stash drop stash@{1}`\n","n":0.109}}},{"i":1001,"$":{"0":{"v":"Show","n":1}}},{"i":1002,"$":{"0":{"v":"Show","n":1},"1":{"v":"\n##### Show differences introduced by a commit\n`git show <SHA>`\n\n##### List the files changed in a commit\n`git show <SHA> --name-only`\n","n":0.229}}},{"i":1003,"$":{"0":{"v":"Rm","n":1},"1":{"v":"\nRemove a file from the index as well as working directory so they will stop being tracked.\n- with `--cached`, only remove it from index.\n","n":0.204}}},{"i":1004,"$":{"0":{"v":"Restore","n":1},"1":{"v":"\nRefreshes the current working version to be the HEAD version. No modifications are made to the staging area.\n- We can also pass `-s <branch-name>` to restore in the version on the specified branch\n\nDesigned to replace version-checking out functionality of the `checkout` command\n","n":0.154}}},{"i":1005,"$":{"0":{"v":"Reset","n":1},"1":{"v":"\nIn general, git reset is used to move branch tips around (likely to another commit)\n- `reset` is the opposite of `add`\n\nIf we pass a filename to `git reset`, then the staging area will be updated to match the given commit instead of the working directory (the branch pointer does not move).\n- ex. if we have 3 files staged, we can remove one of them with `git reset HEAD index.js`, making `index.js` match the version found in HEAD. The working directory and current branch are left alone. The result is a staging area that matches the most recent commit and a working directory that contains the modified `index.js` file.\n\t- Put another way, we are unstaging the file.\n\n![8f6b8c503feeeceab0f2175850b7acbd.png](:/78999c9edf5c4f129b389d40acc423cc)\n\n## Soft\n- moves the HEAD to the provided `<SHA>`, while keeping working tree and staging area intact.\n- *soft* means the commit is canceled and moved before the `HEAD`\n- `git reset --soft HEAD^` - undo last commit\n\n## Mixed\n- moves the HEAD to the provided `<SHA>`, while keeping working tree intact.\n\t- Move the HEAD backward `<n>` commits, but don’t change the working directory.\n- reset the staging area\n- `git reset <file>` is the opposite of `git add <file>`\n- ex. if we are on c3 and do `git reset c1`, we will go back to c1, and the working directory and index will remain unchanged\n    - This is the default type of reset\n\n## Hard\n- moves the HEAD to the provided `<SHA>`, keeping neither the working tree nor staging area intact.\n\t- Move the HEAD backward `<n>` commits, and change the working directory to match\n- the only version of `reset` that actually results in a changed working directory file.\n- ex. if we are on c3 and do `git reset --HARD c1`, we will go back to c1 (i.e. our head will point to c1) and c2 and c3 will be \"destroyed\", and working directory wiped.\n","n":0.057}}},{"i":1006,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Reset all files in a directory\n`git restore --source=HEAD --staged --worktree -- modules/client-reader/colibrio`\n- restore all files in the `modules/client-reader/colibrio` to their HEAD version (in other words, unstage them)\n","n":0.189}}},{"i":1007,"$":{"0":{"v":"Remote","n":1},"1":{"v":"\n# Remote\n- a pointer to a branch on a copy of the same repository.\n\t- remote simply means a copy of the repo on someone else's machine.\n- *origin/master* means \"the master branch of the origin remote\"\n- When you clone a repository, Git automatically adds an origin remote pointing to the original repository, under the assumption that you’ll probably want to interact with it down the road\n\t- run `git remote -v` to see what origin is\n- we can run `git branch -r` to see the remote branches available to us. If there are none, then we can run `git fetch <remote-name>` to copy them over.\n- Checking out a remote branch takes our HEAD off the tip of a local branch, resulting in a detached HEAD:\n![efafe6e1a14641006f80ee5a895572b2.png](:/287b1c5127484a3b99356dfdfa60acbc)\n\n## Upstream\n- Imagine we forked a repo remotely, then forked it locally. `Upstream` would be the original repo that we forked, and `origin` would be the remote repo of our forked version\n- upstream means \"towards the trunk\" (ie. towards the single source of truth)\n- By default, `origin/master` is set as the upstream branch of `master`, so `git pull/push` will default there.\n- `git branch -vv` <-- show upstream branch of local version.\n\n## Tracking Branch\n- The local branch that is connected to a remote branch.\n- ***ex.*** `master` **==>** `origin/master`\n- checking out a remote branch from the local repo will create that branch.\n- `git branch --remotes`\n- `git remote -v` - list all remote repos you've connected to\n\n#### Get URL of remote\n`git remote get-url origin`\n","n":0.064}}},{"i":1008,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Set your local branch to track a remote branch\n`git branch -u origin/dev`\n\n#### push to new upstream\ntip: use with `--force` to overwrite everything remotely\n`git push -u origin dev`\n\n#### Get origin URL\n`git config --get remote.origin.url`\n\n#### Add remote URL\n`git remote add origin git@github.com:USERNAME/REPOSITORY.git`\n\n#### Remove remote\n`git remote rm origin`\n","n":0.147}}},{"i":1009,"$":{"0":{"v":"Reflog","n":1},"1":{"v":"\nThe Reflog gives a chronological account of how HEAD has moved. Each time it moves, there is a new entry in the Reflog.\n\nThe reflog is a list of chronological hashes, which represent where you have been during commits, all without regard to branches.\n- Since branches don't matter, we are able to recover dangling commits.\n- Each time a branch is updated to point to a new reference (ie. HEAD changes), an entry is written in the reflog to say where you were.\n- Since the branch is updated whenever you commit, the git reflog has a nice effect of storing your local developer's history.\n\nIt serves as Git's safety net, recording every change made in the repo (regardless of whether it was committed or not)\n- The listed commit hash represents the HEAD after that action.\n\nAlongside the caret (`^`) and tilde (`~`) operators, git provides the `@` for referencing reflog entries in a relative way.\n- `git show HEAD@{5}` gets us the value of HEAD 5 movements ago.\n- we can also see where the HEAD was by day:\n\t- `git show HEAD@{yesterday}`\n\t- `git show HEAD@{1.day.ago}`\n\nWhile `git log` shows a history of the commits, we can think of `git reflog` as showing us a history of everything (what branch we checked out, what commands we ran etc.)\n\nBecuase it keeps a full history, even if we were to `git reset`, we can still access the commit SHA\n- ex. we made some commits, then reset. If we decide that we want to *undo* the reset, all we need to do is checkout the commit we want with `git checkout HEAD@{1}`\n```\nad8621a HEAD@{0}: reset: moving to HEAD~3\n298eb9f HEAD@{1}: commit: Some other commit message\nbbe9012 HEAD@{2}: commit: Continue the feature\n9cb79fa HEAD@{3}: commit: Start a new feature\n```\n- This puts us in detached head state. All we need to do is create a new branch, and continue working on our feature.\n\n#### Data Recovery\n- Most of the time, the reflog is our friend in the circumstance where we want to recover data that has been \"lost\"\n\t- we can either run `git reflog`, or use `git log -g` which gives us normal log output for the reflog\n- Imagine we hard reset a number of commits, effectively \"erasing\" them from the git log. All we need to do is find that commit with `git log -g`, create a new branch with its SHA (`git branch recover-branch <SHA>`)\n- If the data we are looking for is not in the reflog, we can try using `git fsck --full`, which will list all objects that aren't pointed to by another object.\n","n":0.049}}},{"i":1010,"$":{"0":{"v":"Rebase","n":1},"1":{"v":"\nFrom a feature branch (FB), if we rebase master, conceptually we are rewinding back to the point where the FB split from master and updating that point with master and then “replaying” the FB commits on top of that\n- Therefore, rebase is an alternative way to get commits from a feature branch onto a master branch\n\t- with `git merge`, we take the diff between the feature branch and the master branch, and try and jam them together in order to make a new commit (the merge commit).\n\t- with `git rebase`, (assuming rebase on top of master) we take all of the commits that are different between FB and master and move them on top of another. \n\t\t- This involves rebasing first, then doing a regular merge (which will be fast-forward, since the base commit will now be a direct ancestor)\n\t\t- When we run `git rebase master` from our feature branch, we say \"hey master, I want to take all the work I've done so far and make it look like it was built directly on top of all of the work that *you* have\"\n\nIf you're `rebase`ing, you will always have to `--force` when you push, because rebase rewrites history— That's simply how it works. Rewritten history isn't going to fast-forward on top of that same history by definition.\n- only the commits that are being replayed are rewritten (and thus, new SHAs). If we are on FB and rebasing on top of master, the commits from master will remain unchanged (ie. history isn't being written here). However, the commits that only existed on FB will be rewritten, and thus will have new SHAs.\n- While collaborating with others using the \"rebase workflow\", you should always `git pull --rebase` to avoid the circumstance where a merge commit is made. If one person rebases, and then pulls that code with merge commits, it will be a difficult rebase (spec:) due to the rewritten history.\n\nAs the name suggests, rebase exists to change the base of a branch (ie. the origin commit). We do this by replaying a series of commits on top of a new base.\n- This is mostly needed when a series of local commits is deemed to start from an obsolete base (put another way, our local master is very out of sync with origin/master)\n\n- behind the scenes, git is duplicating the commits of the feature branch, putting them on top of the master branch, and then blowing away those original feature branch's commits (hence why they are greyed out in the following image).\n\t- therefore, in a sense it is rewriting history, as evidenced by the fact that the duplicated commits have a different SHA than the originals\n\nBecause rebase rewrites history, it's important that we pull all remote changes to our local master branch before rebasing, so that we are reanchoring our feature branch's commits to the current version of the code.\n- ex. we are on branch `about`, which has diverged from `master`. We want to incorporate changes from master into `about`. From `about`, we run `git rebase master`:\n\nOne should not rebase another branch onto a shared branch.\n- ex. while on shared `development` branch, running `git rebase main`\n\n## Process\nGit always squashes a newer commit into an older commit or “upward” as viewed on the interactive rebase todo list, that is into a commit on a previous line.\n- This means if commit1 is a `WIP` commit, and commit2 is the one we want to keep (along with changes from commit1), then we must actually `squash` commit2 into commit1. Doing so will allow us modify the commit message (now a combination of the messages from commit1 and commit2) before rewriting the history.\n\n\n### Behind the Scenes\n1. Git will checkout the upstream branch and copy all the changes you've done since you last merged, placing them on the tip of the upstream branch.\n\t- ex. in the above image, to an outside observer it would seem that you had checked out the upstream branch from ***a***b and then done your changes.\n\t- note: here upstream most likely is origin/master or simply master, but it could be any branch we are \"merging\" into.\n2. Git produces a series of patch files of your work and starts applying them to the upstream branch.\n\t- consider that these commits are actual copies with different commit SHAs\n\n### Process\n1. when finished with feature branch, pull all remote changes onto master\n\t- if local master === origin/master, step 2 can be skipped, since it would have no effect anyway\n2. from feature branch, run `git rebase master`, which will cause our feature branch commits to be anchored against the updated master branch\n\t- consider that when we checkout a new branch, we have a common base with the branch which we checked out from. Rebasing master here ensures that the remote changes that happened and got merged into master (remotely) are included as part of that anchor.\n3. from master branch, run `git rebase feature-branch`, copying and placing the commits of the feature branch onto the main branch.\n\n#### Conflicts\n- say we are rebasing 8 commits onto the new branch — each one could cause a conflict, and we can resolve the conflicts introduced by each commit one by one.\n\t- fix the file, run `git add`, then run `git rebase --continue` (which moves us on to the next patch, until all are completed)\n\n### When to use rebase\n1. Any time we are working on a long term branch that needs to stay somewhat up to date with master. It is better to keep it as up to date as possible, rather than staying diverged for a long time.\n\t- ex. Imagine working on an experimental branch and getting blocked at some point. This is a scenario that would cause your branch to diverge from master more and more over time. When it does come time to pick up work again, it would be a good time to rebase to the master branch. The result is that it would look just like you started from there.\n2. Consider that in a perfect world, my coworker and I would have a linear commit history (even though we are developing asynchronously, it makes more sense looking back if we have a straight line of commits). In this ideal world, I would be developing my work off the base of my coworker's work, and vice versa.\n3. Imagine we have a *quick-fix* branch that we don't want muddying up the history. If master has not been touched since we branched, the ff merge is automatic. However, if master has indeed changed, then we need a way to tweak *quick-fix* so it becomes a direct descendent of *master* again.\n\t- In this scenario, we want our local master to have the same tip as origin/master. This would allow us to do a ff merge, thus avoiding muddying up history.\n\n### Drawbacks to rebase\n- doesn't play too well with open source projects, since it becomes hard to trace changes introduced to a codebase.\n- doesn't work well when working on a shared branch, since commits are rewritten.\n- only rebase when working on a local branch prior to pushing, or on remote repos where you are the only contributor (ex. for backup purposes).\n\t- In the second scenario, we'll need to force push (since we replaced its commit history with a fresh one).\n\t- Issues arise when other people pull in objects that were orphaned by the rebase process.\n\n#### Shared branches\nRebase is not a great candidate for shared branches. Because `git push --force` is a fact of life to the \"rebase-way\" of Git workflow, we would have to be careful to check if someone else has pushed to the remote branch first. This is why we should use `--force-with-lease`, so that we cannot overwrite commits that have been pushed already to that remote branch. If there have been, we will get errors, and we can `git pull --rebase` to incorporate those changes, before force pushing again.\n\n## Long-lived Feature Branches (LLFB)\nas the LLFB gets periodically rebased off master its commits get rewritten, so we end up with different SHAs for \"the same\" content of the commit.\n\n# E Resources\n[VS Code tip: Interactive rebase editor from the GitLens extension](https://m.youtube.com/watch?v=P5p71fguFNI)\n","n":0.027}}},{"i":1011,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### See the patch of the current commit\n`git rebase --show-current-patch`\n\n##### Status of rebase\nNumber of commits in this rebase\n`cat .git/rebase-apply/last`\n\nOut of the number of commits in this rebase, which are we on?\n`cat .git/rebase-apply/next`\n\nWhich commit is currently being applied?\n`cat .git/rebase-apply/original-commit`\n\n#### Cherry-Pit: Remove a specific commit\n`git rebase -p --onto SHA^ SHA`\n\n##### alias cherry-pit\nusage: `git cherry-pit <SHA-to-remove>`\n\n#### Include some changes as part of a previous commit\nImagine we realized that we should have included a change (perhaps deleting some old comments) as part of a previous \"cleaning commit\"\nOf course, we will have to change history to do so, with `rebase`:\n\n##### Stash method\nStrategy: stash the desired changes, reset back to a commit, pop those changes and amend commit, then complete rebase\n\n1. Use `git stash` to store the changes you want to add.\n2. Use `git rebase -i HEAD~10` (or however many commits back you want to see).\n3. Mark the commit in question (a0865...) for edit by changing the word pick at the start of the line into edit. Don't delete the other lines as that would delete the commits.\n4. Save the rebase file, and git will drop back to the shell and wait for you to fix that commit.\n5. Pop the stash by using `git stash pop`\n6. Add your file with `git add <file>`.\n7. Amend the commit with `git commit --amend --no-edit`.\n8. Do a `git rebase --continue` which will rewrite the rest of your commits against the new one.\n9. Repeat from step 2 onwards if you have marked more than one commit for edit.\n\n##### Autosquash method\n1. Stage the changes that we want to include in a previous commit\n2. Create a new commit with an identical message of the original commit with the following command\n`git commit -c <ORIGINAL-COMMIT-SHA>`\n\t- pro-tip: use `HEAD~2` syntax for relative\n3. With the editor open, prepend the name of the new commit with `fixup! ` (or `squash! ` if we want to edit the commit message)\n`fixup! refactor: clean up payment methods`\n4. run `git rebase -i --autosquash <SHA>`, where `<SHA>` is the commit immediately before the original commit we want to amend.\n5. Git will detect the `fixup! ` directive, and will look for the commit with the same message. With this information, Git will perform the squash directive (combining the commits), and then continue to rebase the rest of the commits\n\n##### fixup (manual)\n1. make the commit `git commit -m fixup`\n2. run `git rebase -i`\n3. move the commit, and squash it into an earlier one\n\n##### git fixup alias\n```\nfixup = \"!fn() { git commit --fixup ${1} && GIT_EDITOR=true git rebase --autosquash -i ${1}^; }; fn\"\n\n// then add staged changes to commit 3 before HEAD:\ngit fixup HEAD~3\n\n```\n","n":0.048}}},{"i":1012,"$":{"0":{"v":"Push","n":1},"1":{"v":"\n- `git push origin master` moves the HEAD of the central repo:\n\t- This has the exact same result as if we went on the central repo and ran a fetch and fast-forward merge.\n![cbbedd3aff8b9bae62000af2a996468e.png](:/1f2474a7a067490f91335052cb37b7cf)\n\n## Avoiding race conditions with force push\nWhat if we want to force push, but don't want to run into problems if someone else has pushed to that branch in the meantime?\nTo get a warning when trying to force push to a branch that has been committed to in the meantime, run:\n`git push --force-with-lease`\n\n## Force push\n- Force pushing to feature branches is a fact of life. Force pushing to a `master` branch should be considered with extreme care.\n\t- Force pushing to feature branches allows us to have a clean history of commits on that feature branch. If we embrace `rebase`-`merge` instead of just `merge`, then we will encounter lots of scenarios where we must force push to that feature branch.\n\t\t- Note: this strategy should NOT be taken if multiple developers are working on the same branch. Rebase is not a good candidate for shared branches.\n","n":0.075}}},{"i":1013,"$":{"0":{"v":"Prune","n":1},"1":{"v":"\nDelete lost or \"orphaned\" objects (ie. those that are unaccessable by any ref). Any commit that cannot be accessed through a branch or tag is unreachable.\n- prune is a garbage collection command, and considered a child command of `git gc` (in other words, `gc` runs `prune`)\n","n":0.147}}},{"i":1014,"$":{"0":{"v":"Merge","n":1},"1":{"v":"\nTo undo a merge, simply run `git reset --hard HEAD`\n- by default, only the index will be reset, meaning the partially merged files will remain in the working directory.\n- If we've merged and committed already but want to revert the merge, we can discard the commit with `git reset --hard ORIG_HEAD`\n\nFrom a DX standpoint, the big difference between rebase and merge is that `merge` will preserve the \"railroad\" switch that was our branch when we look at the history, whereas `rebase` will show it linear.\n- \"Should this branch remain visible in the graph?\"\n\n[git merge with multiple parents](https://softwareengineering.stackexchange.com/questions/314215/can-a-git-commit-have-more-than-2-parents)\n\n## When to use Merge\n1. If master remained untouched while we were working on our feature branch (in which case a fast-forward merge would be automatic).\n2. When we have work that is related to an agile ticket or bugfix (which would allow us to look back in history and clearly see it)\n\n## Terminology\nThe `ours` `theirs` terminology is from the perspective of the \"already existing branch\". Put another way, if we are on main and merging in a feature branch, main is `ours` and feature is `theirs`. Also in the same way (and using a different paradigm), `current change` would refer to main, and `incoming change` to feature.\n\nThis is also the same as with rebase. if we are rebasing main onto feature, then `ours` will refer to main (`git rebase main` from feature branch)\n","n":0.066}}},{"i":1015,"$":{"0":{"v":"Merge Strategies","n":0.707},"1":{"v":"\nGit has several different methods to find a base commit, these methods are called \"merge strategies\".\n- Once Git finds a common base commit it will create a new \"merge commit\" that combines the changes of the specified merge commits.\n- Technically, a merge commit is a regular commit which just happens to have two parent commits.\n\nwhen merging (and pulling), we can pass the `-s` flag to specify which merge strategy we want to use.\n- When not specified, Git will select the most appropriate based on the provided branches.\nmerges can be either explicit or implicit\n- *explicit* means a new merge commit is created.\n- *implicit* means no new merge commit is made, and any evidence of a branch having existed is erased from history\n\t- This is accomplished either by fast-forward merges or rebases.\n\n## Strategy types\nSpecified with `-s` flag\n- ex. `git merge -s ours feature-branch`\n\n### Recursive\nOperates on 2 heads. Therefore, it is the default when pulling/merging one branch.\n\nHas additional sub-operation options, which allow us to auto-resolve conflicts, by accepting all changes in one HEAD version\n- *ours* - accept all \"ours\" versions of the HEADS\n- *theirs* - accept all \"theirs\" versions of the HEADS\n- *patience* - This option spends extra time to avoid mis-merges on unimportant matching lines. This options is best used when branches to be merged have extremely diverged.\n\n### Resolve\noperates on 2 heads using 3-way merge algorithm.\n\n### Octopus\ndefault when there are more than 2 heads.\n\n### Ours\nThe output merge result is always that of the current branch HEAD. The \"ours\" term implies the preference effectively ignoring all changes from all other branches. It is intended to be used to combine history of similar feature branches.\n- note: this should not be confused with the `ours` option of the Recursive Merge Strategy\n","n":0.059}}},{"i":1016,"$":{"0":{"v":"Log","n":1},"1":{"v":"\nWe can pass a filename to `git log` to only show the history of a given file\n- also, we can pass a commit SHA to see the revision history up until that point\n\t- This fact shows how by default, `git log` will run with `HEAD`, showing the revision history from the current branch's tip\n- We can run `git log -n 4` to show only the last 4 commits from HEAD\n\t- equivalent to `git log HEAD~4..HEAD`\n- running `git log master..origin/master` checks what exists on origin that doesn't on our local master. The opposite, `git log origin/master..master` checks what exists on our local master that doesn't on origin. If the output is empty, it means that our branches have not diverged.\n","n":0.092}}},{"i":1017,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### See commits that exist on a given branch\n`git log feature-branch`\n\n##### See commits that exist on a branch, while excluding ones that live on another\n`git log feature-branch ^main`\n- equivalent to `git log main..feature-branch`\n\n##### See the history of a file across this rename\n`git log --follow -- /path/to/file`\n\n##### See only commits where files were deleted\n`git log --diff-filter=D --summary`\n","n":0.134}}},{"i":1018,"$":{"0":{"v":"Fetch","n":1},"1":{"v":"\nRunning fetch will pull all refs and objects that we don't already have from the remote repo, and will put them into the object database. \n- fetch is opposite of push, in the sense that fetch will import branches, while push will export them\n\t- when we run `git push origin master`, we are exporting our master branch to origin. If we were to run `git branch` on origin, we would see our local branch that we just pushed, listed as a local branch.\n\t\t- ex. Imagine we had a repo called `foo`, then cloned in locally into a different directory on our machine and called it `bar`. On `bar`, origin would default to `foo` (since it was copied from `foo`). `foo` would not have an origin by default, since it did not come into existence by being copied. We could manually add `bar` as a remote. Now, if we made a branch on `foo` and pushed it, that branch would be available locally on `bar`.\n","n":0.078}}},{"i":1019,"$":{"0":{"v":"Diff","n":1},"1":{"v":"\n### Patch file\nThe default output of `git diff` is a valid patch file, meaning we can pipe its output into a file, give that file to someone else, and they can apply it with the `patch` command.\n- ex. `git diff master..experiment > experiment.patch`. The recipient can then run `patch -p1 < ~/experiment.patch`\n\nEach patch represents a full commit, complete with metadata like author and date.\n- ex. If we have made 2 commits since master, then running `git format-patch master`\n\n### range-diff\n`git range-diff` allows us to inspect how two commit ranges are different\n\n## Under the hood\nto compare 2 commits, Git starts by looking at the root trees of each commit (which are almost always different). Then, Git initiates a depth-first-search on the subtrees by following pairs when paths for the current tree have different OIDs.\n\n* * *\n\n- `git diff` - show changes between index and working tree\n- `git diff --staged` - show changes between index and HEAD (ie. last commit)\n- `git diff master..feature-branch` - show changes between master and feature-branch \n\t- `git diff f733ed..` - show changes between a commit and HEAD\n- `git diff -- package.json` - show only changes in a specific file.\n\n","n":0.072}}},{"i":1020,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Match a pattern only in UNSTAGED hunks\n`git diff -U1 | grepdiff 'TODO' --output-matching=hunk`\n\n#### Match a pattern only in STAGED hunks\n`git diff -U1 --staged | grepdiff 'TODO' --output-matching=hunk`\n\n#### List the files that match a pattern in a given git range\nHere, look for the pattern `TODO` in all commits from `main..HEAD`\n`git grep TODO -- $(git diff --name-only main..HEAD)`\n","n":0.132}}},{"i":1021,"$":{"0":{"v":"Commit","n":1},"1":{"v":"\n## Operators\n\n### ~ Operator\n- allows us to reach parent commits\n\t- ex. `git show HEAD~2` shows the grandparents of HEAD\n- if the commit has more than 1 parent (ie. merge commit), then the first parent of the commit will be used by default.\n\t- Need to use `^` to specify a different commit\n\n### ^ Operator\n- allows us to specify which parent we want to refer to\n\t- spec: therefore, if there is only one parent, then `HEAD~1` refers to the same commit as `HEAD^1`\n![](/assets/images/2021-03-07-22-45-05.png)\n","n":0.111}}},{"i":1022,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Find out which branch a commit took place on\nIf within last 90 days (ie. before gc), check [[reflog|git.reflog]]\n`git reflog show --all | grep d7f32e`\n\n`git name-rev --name-only d7f32e`\n","n":0.189}}},{"i":1023,"$":{"0":{"v":"Amend","n":1},"1":{"v":"\nAmend the most previous commit, allowing us to add/remove files to that commit, or even to simply change the commit message.\n\nif we forgot to stage some changes as part of the last commit, simply add them to the working tree (should be clean, considering we just committed), then run `git commit --amend` \n","n":0.137}}},{"i":1024,"$":{"0":{"v":"Clean","n":1},"1":{"v":"\nRemove untracked files and any patchfiles.\n","n":0.408}}},{"i":1025,"$":{"0":{"v":"Cherrypick","n":1},"1":{"v":"\n## Overview\nCherry-pick works by copying a commit diff (between it and its parent) onto the current branch\n\ncherry-picking results in the creation of a new commit that has an identical diff to the diff between the specified commit and its parent. When we run cherry-pick, the changes that were introduced in a single commit will be applied on top of HEAD.\n\nInstead of working bottom-up (or past to the present) like rebase, cherry-pick works top-down (or present back to the past)\n\nWhen you cherry-pick a commit, you also specify which parent commit to consider, with the `-m` parent-number argument. The cherry-pick command then generates a diff against that parent, so that the resulting diff can be applied now.\n\nShould you choose to cherry-pick a non-merge commit, there is only one parent, so you don't actually pass `-m` and the command uses the (single) parent to generate the diff. But the commit itself is still a snapshot, and it's the cherry-pick command that finds the diff of commit^1 (the first and only parent) vs commit and applies that.\n\nIf you want something else done aside from applying the patch between a commit and it’s parent, then you don't want the cherry-pick command. For instance, if you just want a particular snapshot's version of some file, use `git checkout <revspec> -- <path>`, or `git show <revspec>:<path>`\n\nIt's crucial to recognize that cherry picking does not involve moving a commit on top of HEAD. Rather, it involves copying the diff produced by 2 commits, and applying that patch on top of HEAD. This process results in a new commit SHA.\n\nuse cherry-pick when you want to rebase, but have more power over what exactly you want to bring over from one branch to another\n\n### Resetting and Cherrypicking\nThis lends itself to a strategy that is similar conceptually to `rebase`\n\nImagine we have 2 branches that have a common base, but have diverged. We want to get a series of commits to be placed onto the tip of an existing branch (much like rebase)\n1. from feature-branch, `git reset main --hard` to the HEAD of the master branch. These branches are now identical.\n- Before doing this, we need to know which commits we want. It's probably even a good idea to duplicate the branch as a backup just in case.\n2. cherry pick the range of commits on top of feature-branch.\n- as a result, the only conflicts will be in the feature-branch commits.\n","n":0.05}}},{"i":1026,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Take commits from 6f3e5a (inclusively) until fe85ab, and plop it on top of `HEAD`\n`git cherry-pick 6f3e5a^..fe85ab`\n- `^` gets the parent, to make the range inclusive\n","n":0.196}}},{"i":1027,"$":{"0":{"v":"Checkout","n":1},"1":{"v":"\nThe act of switching between different versions of a file, commit or branch\n- Think of it as switching between snapshots\n- When checking out a different branch, Git makes your working directory look like that branch. Any checked in content that is in your working directory but is not in the new tree will be removed. This is why Git only lets us checkout another branch if everything is checked in (ie. no uncommitted modified files).\n\t- The reason for this is that Git will remove files that are not necessary in the branch we are checking out.\n- if we add a file path to `git checkout`, only the specified file will be checked out, and the branch pointer will not be updated.\n\t- ex. `git checkout HEAD index.js` will check out the most recent (HEAD) version of `index.js`\n![0a0ed0120ebc145ec651db96de7b73c4.png](:/a7a7bd8f7ee646d4a308c17366095fad)\n\nChecking out a file\n- running `git checkout <file>` is similar to running `git reset <file>`, except checkout updates the working directory, while reset updates the staging area.\n\t- This has a similar effect to `git revert`, with an important difference: `revert` only undoes changes introduced the commit, while `checkout` undoes all changes *since* that commit.\n\t- ex. what if we want to change the file in the working tree to what it was 2 commits ago. We can run `git checkout HEAD~2 <file>`.\n- while running `checkout` on a branch/commit will move the HEAD reference, running `checkout` on a file will not, meaning we don't change branches.\n","n":0.065}}},{"i":1028,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Pull files from one branch to current branch\n- If we have a base branch that we want to keep, but several files from another branch that we want to pull over, from the base branch we can run:\n    - `git checkout <other-branch-name> -- file1.js file2.js`\n","n":0.147}}},{"i":1029,"$":{"0":{"v":"Branch","n":1},"1":{"v":"\nA Branch is simply a pointer to a specific commit\n\t- Therefore, creating a branch is nothing more than writing 40 characters (a SHA) to a file.\n- The key difference is that it is moveable\n- It’s important to realize that Git uses the tip of a branch to represent the entire branch. That is to say, a branch is actually a pointer to a single commit—not a container for a series of commits\n\t- this is why Git diagrams show commits pointing to other commits.\n- If we make a new branch from master, that branch will point to the same commit as master. Once we commit, this new branch will then point to the new commit. Therefore, the branches will have diverged.\n- When we create a new branch, Git knows which commit to use because of the HEAD file.\n\n## Slashes in branch names\nImagine we had a branch named `stripe` already, and we wanted to make a new branch called `stripe/saved-methods`. This would cause an error, because as far as git knows, we have a file named `stripe`, and we are now trying to create a directory named `stripe`.\n- Because they are just directories, deleting `stripe/saved-methods` branch will result in the `stripe` directory still existing.\n\nIt's probably a good convention to stick to at most one level. Consider if we had a branch `wip/stripe`, and we then wanted to create `wip/stripe/saved-methods`, we would get an error because we are trying to create a directory `stripe`, when there is already a file called `stripe`\n\nWhen we have a branch with slashes in it, it gets stored as a directory hierarchy under `.git/refs/heads`.\n","n":0.061}}},{"i":1030,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Create new branch from given commit\n`git checkout -b new-branch a67de0`\n- Equivalent to checking out a commit, then creating a new branch\n\n`git checkout -b new-branch HEAD~3`\n\n### Stop tracking a remote branch\n`git branch --unset-upstream`\n\n### Set local branch to track remote branch\n`git branch --set-upstream-to=origin/my-feature my-feature`","n":0.152}}},{"i":1031,"$":{"0":{"v":"Bisect","n":1},"1":{"v":"\n### Flow\nFrom the commit that is not working,\n1. run `git bisect start`\n2. run `git bisect bad` to indicate the current commit is \"bad\"\n3. run `git bisect good <SHA>` on a commit we know works\n    - git will checkout a commit in the middle of the good-bad range\n4. run `git bisect good`/`git bisect bad`, depending on if this middle commit works or not\n    - repeat until we find the commit that introduced the issue\n6. run `git bisect reset` to clean up bisection state, returning to original HEAD\n\n* * *\n\nyou can pass git-bisect a script which programmatically tests a tree for the presence of a bug and avoid false positives\n\n\n","n":0.096}}},{"i":1032,"$":{"0":{"v":"General","n":1}}},{"i":1033,"$":{"0":{"v":"Tool","n":1},"1":{"v":"\n- [Minute Inbox](https://www.minuteinbox.com/)\n    - Generate 10 minute emails. useful for when you need to test features that involve sending an email\n- [Mockturtle](https://mockturtle.net/)\n    - UI-based site for generating large sets of data by a certain schema","n":0.169}}},{"i":1034,"$":{"0":{"v":"Time","n":1},"1":{"v":"\nTimestamps are often recorded in GMT.\n- GMT is 8 hours ahead of PST.\n\n# Overview\nwe need to express both a time and a location whenever we want to place some event\n- The basic problem is that we assume spatial coordinates more than we assume temporal coordinates\n\t- Consider how timezones really are. Everyone has a concept of 3:00 in the afternoon. It doesn't matter which timezone you are in, because 3:00 will have a certain feel. We ignore the timezone (spatial coordinate) when it comes to conceptualizing the time. It falls secondary to the time (temporal coordinate).\n- This problem results in people having a poor conception of how their timezone relates to others. This is why `-07:00` is a pretty abstract concept, and thus meaningless to most people.\n- When talking about time, people almost always leave out some crucial information, leaning on the context of what they are saying to help communicate.\n\t- ex. to denote a year people often shorten to 2 digits ('04).\n\n### Other issues\n- Scientific time naturally has origin 0, as usual with scientific measures, even though the rest of human time notations tend to have origin 1\n\n#### Unix time\nThe Unix-time way of solving time-related problems is to set time 0 equal to 1970-01-01 00:00:00 UTC. This approach works well as long as the rules for converting between relative and absolute time are stable. As it turns out, they are not.\n- Some have used local time as the point of reference, some use decoded local time as the reference, and some use hardware clocks that try to maintain time suitable for direct human consumption. Without consistency, problems arise.\n\n#### Political Time\nOn the surface, answering the question \"how long does it take for a clock to show the same value?\" is an easy one to answer. The answer is 24 hours, right? Well, this question gets complex when we consider daylight savings, since 1/365 days will have 23 hours, and 1/365 days will have 25 hours.\n\nThere is no way you can know from your location alone which time zone applies at some particular point on the face of the earth: you have to ask the people who live there what they have decided.\n\nUnfortunately, political time is what people want their computers to produce when asking for time of day.\n\n## Other\n- `Z` appended to a timestamp is a *zone offset*\n\n# UE Resources\n[Essential reading before working with Timezones](https://zachholman.com/talk/utc-is-enough-for-everyone-right)\n- Jensen recommends using https://date-fns.org/\n\n# E Resources\n[Scientific paper on time and its complexities](http://naggum.no/lugm-time.html)\n","n":0.05}}},{"i":1035,"$":{"0":{"v":"Terms","n":1},"1":{"v":"\n<!-- Maybe this file should be moved to a more sensible place -->\n\n### Resource\nA resource is any computer component of limited supply\n- a [[file descriptor|dendron://tech/unix.streams#file-descriptor-fd]] is a resource because a system can only handle so many\n- a network connection is a resource because there are only so many network sockets\n- memory is a resource because a computer only has so much memory\n","n":0.127}}},{"i":1036,"$":{"0":{"v":"Side Effects","n":0.707},"1":{"v":"\nFunctions that have side effects are often convenient and easy to write. Their effects can in principle be encoded in their names and in the comments. A function called `SetPassword` or `WriteFile` is obviously mutating some state and generating side effects, and we are used to dealing with that. It’s only when we start composing functions that have side effects on top of other functions that have side effects, and so on, that things start getting hairy. It’s not that side effects are inherently bad — it’s the fact that they are hidden from view that makes them impossible to manage at larger scales. Side effects don’t scale, and imperative programming is all about side effects.\n","n":0.093}}},{"i":1037,"$":{"0":{"v":"Indirection","n":1},"1":{"v":"\nIndirection is the ability to refer to something through use of a *reference* (id). Normally, this involves manipulating the variable in question by using a pointer. Pointers exist to provide reference to an object (called indirection node)\n- *\"All problems in computer science can be solved by another level of indirection\"*\n- When the complexity of information becomes too much at one level, we simplify it with a layer of indirection that pushes the bulk of the complexity to a new level.\n\n![](/assets/images/2022-04-08-08-57-51.png)","n":0.112}}},{"i":1038,"$":{"0":{"v":"Idempotency","n":1},"1":{"v":"\nIn mathematics, the an idempotent function is one that will yield the same output, even as its output feeds back as input indefinitely, such as:\n`Math.abs(Math.abs(Math.abs(Math.abs(-2))))`\n`String( String( x ) )`\na way to test mathematical idempotency is to check if the output of a single function is equal to multiple calls of that function:\n```js\ncurrency( -3.1 ) == currency( currency( -3.1 ) )\n// true\n```\nWherever possible, restricting side effects to idempotent operations is much better than unrestricted updates.\n\nThe programming-oriented definition for idempotence is similar, but less formal. Instead of requiring `f(x) === f(f(x))`, this view of idempotence is just that `f(x);` results in the same program behavior (in other words, not just the output. *All* impacts of the function) as `f(x);` `f(x);`. In other words, the result of calling `f(x)` subsequent times after the first call doesn't change anything.\n- Pure functions are idempotent in the programming sense\n\nIf you need to do side-effects, try to make it idempotent\n\nPure functions can reference `free variables` (ie. those that are not defined within the function scope), as long as those variables are sure to not change during the execution of the program (ie. they are constant)\n\nAn example of idempotency is in Postgres, when we say `delete function if exists`, rather than just `delete function`\n- It is idempotent because running the command more than once has no additional effect than simply running it once.\n","n":0.067}}},{"i":1039,"$":{"0":{"v":"Greedy","n":1},"1":{"v":"\nA greedy algorithm always takes the best/cheapest option available at that moment, even if that might not be the best way to go from an overall point of view.\n\nA greedy algorithm is one that picks the best option at the moment, even if doing so means missing better choices later on.\n\nA greedy algorithm is an algorithm that tries to maximize its gain each step of the way, instead of seeking out the overall best solution.\n\nThe opposite of a greedy algorithm is a dynamic algorithm:\n- The *greedy algorithm* would say \"I need to make my way from point A to point Z, so at every step in-between I look for the adjacent vertex closest to Z and move there.\" It's easy to program and will probably get the job done but it doesn't guarantee that you've found the shortest path.\n- *Dynamic algorithm* says \"I go to every node and make a note of the distance to each adjacent node and see if it's shorter than any previously known shortest path to that node. Once I've figured out the shortest path to every node then I can easily lookup the shortest path from A to Z.\" So, for example we see that A-B-C-D-E gets us to E in five steps, and we know that A-F gets us to F in one step, and then we see that E connects to F so we update our records to say the shortest path from A to E is A-F-E instead of A-B-C-D-E. Continue doing this until you have found the shortest paths from any node to any other node.\n\n### Example: Packing boxes in vans\nif your algorithm was about packing differently sized boxes into a van, the algorithm would always choose to put the biggest box that could fit in the van— even if we could achieve a higher cumulative volume by using a combination of smaller boxes.\n\n### Example: Highest altitude\nSay you're dropped off in the the woods and want to find the highest point in the area so you can have a look around. A greedy algorithm would be to look at your immediate surroundings and pick the direction that has the highest slope, then travel that direction for a bit and repeat. This will usually get you to the top of a hill, but you can't be sure it's the highest hill. Mount Everest could be on the other side of the ravine you started next to, but you'll never find it because your algorithm never chooses a direction that descends.\n","n":0.049}}},{"i":1040,"$":{"0":{"v":"Federated","n":1},"1":{"v":"\nTo be federated means to be unified in some way, while still maintaining the independence of each unit that lies underneath. The core outcome of a federated system is a group of loosely coupled components that share a common aim.\n- ex. the idea of [[Apollo Federation|apollo.federation]] is that we have many Graphql endpoints (with their own upstream REST and database layers) tied together to provide the benefit of unity to the person who uses it.\n- ex. [[Federated identity|security.federated-identity]] is the idea that we can link someone's identity across many platforms\n- anal: consider that the United States is a federation. The key here is that each state maintains some degree of independence, yet is connected to the whole in some way.\n\n\"to federate\" means to establish a trusted connection or authentication between two different systems or entities. It typically involves allowing one system to access resources or perform actions on another system using the credentials or authorization provided by the first system.\n- ex. \"Gitlab has a JWT_TOKEN environment variable that's passed from each job which can be used to federate into your AWS account securely\"\n    - here, the JWT_TOKEN from GitLab is being used as a secure method to authenticate and access resources in the AWS account without needing to expose or manage AWS-specific credentials separately.\n\n### Federation in database\nFederation (or functional partitioning) splits up databases by function. For example, instead of a single, monolithic database, you could have three databases: forums, users, and products, resulting in less read and write traffic to each database and therefore less replication lag.\n\n## Resources\n- https://en.wikipedia.org/wiki/Federated_architecture","n":0.062}}},{"i":1041,"$":{"0":{"v":"Entity","n":1},"1":{"v":"\nAn entity is an object that...\n- can be uniquely identified with an ID\n- represent a real world object (generally speaking)\n- generally exist as a row in a database table (note the difference between the concept of an apple, vs an actual apple. If your data was modeled with SQL, the latter would make reference to the former)\n- typically mutable\n- implement some kind of business logic\n- consists of [[value objects|general.objects.value-object]]\n\nAn entity may also refer to a set of units (e.g. `table` in SQL, `collection` in Mongo, `document` in CouchDB)\n- TypeORM defines Entities this way.","n":0.104}}},{"i":1042,"$":{"0":{"v":"Eagerness-Laziness","n":1},"1":{"v":"\nLazy means that the algorithm is only doing the work when it needs to; it is not acting ahead of time.\n\nwith eager evaluation we get the whole list right away, but we have to remember the whole thing too. for the lazy evaluation we create only the smallest part of the list that we care about. we are lazy. we don't evaluate the whole thing, we just get as much of it as you ask for. do as little work as possible.\n\n### Example: addition\nImagine you have a program that has to solve the math problem: 1 + 2 + 3 + 4 + 5\n\nA lazy algorithm would take the first 2 numbers, add them, then ask for the next number and add it to the total, and repeat until there are no more numbers.\nAn eager algorithm would take all the numbers at once and add them at the same time.\n","n":0.082}}},{"i":1043,"$":{"0":{"v":"Duck Typing","n":0.707},"1":{"v":"\nThis is the object design pattern by which objects are assessed based on their ability to respond to particular methods\n- In other words, if it responds to the methods you want, there's no reason to be particular about the type.\n\n```rb\n# Say bye to everybody\ndef say_bye\n  if @names.nil?\n    puts \"...\"\n  elsif @names.respond_to?(\"join\")\n    # Join the list elements with commas\n    puts \"Goodbye #{@names.join(\", \")}.  Come back soon!\"\n  else\n    puts \"Goodbye #{@names}.  Come back soon!\"\n  end\nend\n```\nThe `say_bye` method doesn’t use each, instead it checks to see if `@names` responds to the join method, and if so, uses it. Otherwise, it just prints out the variable as a string. This method of not caring about the actual type of a variable, just relying on what methods it supports is known as “Duck Typing”, as in “if it walks like a duck and quacks like a duck…”. The benefit of this is that it doesn’t unnecessarily restrict the types of variables that are supported. If someone comes up with a new kind of list class, as long as it implements the join method with the same semantics as other lists, everything will work as planned.\n","n":0.073}}},{"i":1044,"$":{"0":{"v":"Abstraction","n":1},"1":{"v":"\nImplementations exist at a given level of abstraction. If the term abstraction in essence refers to how low-level or high-level our context headspace is, then the tools we have at our disposal each operate at a certain level of abstraction.\n- [[redux]] in my React app operates at a certain level of abstraction, and the [[os.kernel]] in my OS operates at a totally different one, but in essence, the Redux application depends on the kernel doing its job properly.\n\nThe root of all abstractions is electricity. From that, a [[stream|general.patterns.streaming]] of 0s and 1s are produced, and so forth. Developers design and build programs that operate at a certain level of abstraction.\n- ex. wires exist at a level of abstraction as much as an [[os.process]] does. When we think about it, wires are technology that has been developed to serve a purpose; no different than any other implementation (or outcome) of code.\n\n### Leaky Abstraction\nA leaky abstraction is one that doesn't provide a clean and complete separation between higher-level and lower-level systems, causing developers to deal with the underlying complexities when they should ideally be shielded from them.\n\nA leaky abstraction occurs when we have built some abstraction, but some of the implementation details of the lower-level things we were supposed to have abstracted leaked through the API and are made available to the user of the abstraction.\n- the problem with this is that it needlessly makes the abstraction more complex\n- also, if those lower-level implementation details are modifiable, it makes the abstraction brittle, since changes to the lower-level details can break modules that depend on it.\n\nIf an abstraction is leaky, it means that the consumer of that API must know *exactly* how it works in order to avoid pitfalls.\n\n#### Example: Abstraction to hide SQL-specific implementations\nImagine we had a module that abstracted away details about whether the user was using MySQL, Postgres or any other SQL engine. Ideally, the user of the abstraction doesn't have to be aware of which engine they are using. We would have a leaky abstraction if the user had to eschew this pretense and do something \"the MySQL way\" in order to solve a problem. ","n":0.053}}},{"i":1045,"$":{"0":{"v":"Refactoring","n":1},"1":{"v":"\nWhen you are refactoring some piece of code, all you need to ask is \"did the signature of what I just refactored change?\" put another way, were the inputs and outputs of the changed code modified, so that code that depends on it would no longer work properly? \n- Any time you are changing code, always be mindful of the code that depends on it. Think of how that depending code consumes the code that you are refactoring\n\nBefore doing any refactoring, it's a good idea to draw out the components that you are refactoring, as well as any existing dependencies that use the code you are refactoring. This is important to know, because if you modify any API, and dependent code is going to also need to be modified.\n\n## Resources\n- [Guide to code smells and associated refactorings](https://refactoring.guru/refactoring)\n- [Refactoring](https://refactoring.com/)","n":0.085}}},{"i":1046,"$":{"0":{"v":"General Programming Principles","n":0.577},"1":{"v":"\nwhen coding a problem, you must first understand what you are coding before you understand how you are coding\n- Put another way, you need to understand the underlying logic/business case that you are implementing with code. Understand the issue, understand how a resolution of that issue feels. The what is the important part. Once that's done, the how will flow naturally\n","n":0.128}}},{"i":1047,"$":{"0":{"v":"Code Maintainability","n":0.707},"1":{"v":"\n### Splitting out a data structure into 2 forms: one for maintainability, and one for functionality\n- [source: Robot Delivery game `buildGraph` function.](https://eloquentjavascript.net/07_robot.html)\nIn this project, it makes sense to have an object of key-value pairs \n- where the key is the name of a node, and the value is an array of adjacent nodes that can be travelled directly to (ie. without having to go through another node to get there):\n```js\nconst graph = {\n    'Alice-House': ['Bob-House', 'Post-office', 'Cabin']\n    // ...\n}\n```\nWe could do this by hand, but then we are looking at a maintainability nightmare. Consider that if we add a new node that connects to 3 other existing nodes, then we need to modify the existing object in 4 places: modify 3 values (ie. the arrays), and add a new key. \n- When building out this code, we can take a strong focus to future-proofing it. Always consider likely maintenance needs, and find a way to make it simpler.\n\nThe way the project goes about future proofing this is by using a more primary way of representing the nodes, which is done through using an array where each element is an edge (ie. the link between 2 nodes):\n```js\nconst roads = [\n  \"Alice's House-Bob's House\",   \"Alice's House-Cabin\",\n  // ...\n]\n```\n\na `buildGraph` function is then made, which takes in the array and outputs the `graph` object. Now, whenever a new node is added to the town, only a single element needs to be added to the `roads` array.\n\n### Immutability in data structures\nImmutable data structures help you to understand your programs, making it a solution to the problem of complexity management. When objects in a system are known to be fixed, stable things, we can consider operations in isolation (in the Robot game above, this might be moving from one location to another; going from location A to location B always produces the same new state). A simpler program lets us build it more ambitiously.\n","n":0.056}}},{"i":1048,"$":{"0":{"v":"Separation of Concerns","n":0.577},"1":{"v":"\nApplying SoC well results in a program that is highly modular\n\n### Room Analogy\nConsider that we separate a space into rooms, so that an activity in one room does not affect people in other rooms, and keeping the stove on one circuit and the lights on another, so that overload by the stove does not turn the lights off. The example with rooms shows encapsulation, where information inside one room, such as how messy it is, is not available to the other rooms, except through the interface, which is the door. The example with circuits demonstrates that activity inside one module, which is a circuit with consumers of electricity attached, does not affect activity in a different module, so each module is not concerned with what happens in the other.\n","n":0.088}}},{"i":1049,"$":{"0":{"v":"Domain-Driven Design","n":0.707},"1":{"v":"\nDomain Driven Design is the concept that we should design our application code with heavy consideration into the business logic.\n- ex. we don't know if it will always be the case that you must be a user to purchase a book. Therefore, we shouldn't design the system in a way where we can't accomplish that easily.\n\nDDD is about thinking about your software in terms of business application, instead of looking at it in terms of technology.\n- ie. we are writing for the needs of the business\n- we achieve this by defining models for our domain, and then writing software that conforms to it.\n\nDDD has nothing to do with writing code in a certain way. Rather, it's about understanding the underlying logic behind the problems you need to solve. It's the thinking about domains that drives the design of the code. In a way, DDD is the opposite of jumping right in with code and trying to solve something without thinking about it beforehand.\n- \"The novice jumps in right away with code. The veteran ponders the problem they are solving, and thinks (in code) about the logic of that problem\"\n\nDDD is all about unifying all of the code in a way that is highly comprehensible to those uninvolved with the code's creation. That is, focus is placed on the high level business logic and domain delineations.\n- At a trivial level, it’s all about the names you use for things.  \n- at the level above that, it’s about the way that you combine and activate things to produce business value.  \n- at the level above that, it’s the causal and relationship (semantic) model that keeps everything cohesive, coherent, and aligned with the business.  \n- this alignment can’t come from an implementation-focused model. you must use a domain model.\n\nWhy is this important?\n- Centralizes (business) domain logic into the software to account for changes in understanding and business needs and to improve longevity \n- Helps us understand where we should be investing (strategic) and how to build it (tactical) - investing in the parts of the business that are core \n- Understanding the problem helps align everyone to produce better solutions and more effective communication\n\nKey ideas:\n1. Collaboratively model the domain so that your software reads like the problem it solves.\n2. Isolate your software and establish context in each part of it. There should be enough previously handled assertions that you have a good idea of what's going to happen while looking at any random line of code. (Not sure how to word this one)\n3. You don't have to model the domain *exactly*. \n    (Use abstraction to get rid of unnecessary details, and sometimes it makes sense to model things as they are used rather than what they actually are)\n\nDDD is not about writing code in a certain way. Rather, it's about understanding the underlying logic behind the problems you need to solve. It's the thinking about domains that drives the design of the code. In a way, DDD is the opposite of jumping right in with code and trying to solve something without thinking about it beforehand.\n- \"The novice jumps in right away with code. The veteran ponders the problem they are solving, and thinks (in code) about the logic of that problem\"\n\nDDD is all about unifying all of the code in a way that is highly comprehensible to those uninvolved with the code's creation.\n\n### Domain Primitives\nThe building blocks of DDD are [[value objects|general.objects.value-object]].\n- They can also be thought of as *domain primitives*.\n```ts\n// without domain primitives\nclass User {\n    name: string\n    email: string\n    mobile: string\n}\n\n// with domain primitives\nclass User {\n    name: Name\n    email: Email\n    mobile: PhoneNumber\n}\n\n```\n\n### Domain Model\na domain model is an abstraction of the subdomain. Not every aspect of the domain can be part of the model. It's the aspects chosen for implementation that constitute the model.\n- this means that we don't just think in terms of what the object can do; we think of it in terms of what we *want* it to do.\n\n### Farmyard Domain analogy\nImagine we wanted to build a house. We would first need to determine what type of house we are building. Detached? Condo? Duplex? Farmhouse?\n\nAfter determining this, we would talk to the domain expert, which would probably be a farmer. \n- Notably, it's not the architect, since they wouldn't have the in-depth domain knowledge required to design everything.\n![](/assets/images/2022-01-10-09-34-42.png)\n\n- Domain: Farmyard\n- Sub-domains: Barn, Stables, Farmhouse, Garden gate, etc.\n- Domain model: The layout/plans for each sub-domain (e.g. the layout of the farmhouse)\n    - analogous relationship:\n        - classes -> objects\n        - domain model -> bounded contexts\n- Ubiquitous language: in the layout, we have rooms like Family room, Bedroom, Bathroom. This is the ubiquitous language of the domain.\n- [[Bounded context|general.principles.DDD.bounded-context]]: The parts circled in blue.\n    - language used in each bounded context is specific. For example, \"caretaker\" of the stables and \"caretaker\" of the farmhouse are two different things.\n\n## Tactical design tools\nTactical design tools are concerned with implementation details of the components inside a bounded context\ntactical design is expected to change over the process of development\n\n# UE Resources\n- [Event Storming: Brainstorming to arrive at DDD conclusions](https://www.youtube.com/watch?v=b6D_NTgzmhs)\n- [Roller Coaster Tycoon Examples](https://github.com/Softwarepark/exercises/blob/master/transport-tycoon.md)","n":0.034}}},{"i":1050,"$":{"0":{"v":"Bounded-Context","n":1},"1":{"v":"\nOne of the core ideas of DDD is Bounded Context: the subsystems of a business (business used loosely here). Consider that a business has many different contexts, and that there is interaction between them at certain interface points, like the sales context and the customer support context\n![](/assets/images/2021-06-05-21-26-21.png)\n\nBecause Bounded Context is a fundamental idea, the whole software project needs to be on board with the idea of DDD.\n","n":0.122}}},{"i":1051,"$":{"0":{"v":"Aggregate","n":1},"1":{"v":"\nAn aggregate is a pattern in DDD whereby we treat a cluster of domain objects as a single unit.\n- ex. an order and its line-items, these will be separate objects, but it's useful to treat the order (together with its line items) as a single aggregate.\n\nIf executing a command related to one aggregate instance requires additional domain rules to be run on one or more additional aggregates, you should design and implement those side effects to be triggered by domain events.\n- ex. When the user initiates an order, the Order Aggregate sends an OrderStarted domain event. The OrderStarted domain event is handled by the Buyer Aggregate to create a Buyer object in the ordering microservice, based on the original user info from the identity microservice (with information provided in the CreateOrder command).\n\nAs shown below, a domain event should be used to propagate state changes across multiple aggregates within the same domain model.\n- this shows how consistency between aggregates is achieved by domain events.\n\n![](/assets/images/2021-11-26-10-37-44.png)\n\nAn aggregate will have one of its component objects be the aggregate root, which acts as the interface to the outside world.\n- The root can thus ensure the integrity of the aggregate as a whole.\n\nAggregates are the basic element of transfer of data storage - you request to load or save whole aggregates. Transactions should not cross aggregate boundaries.\n","n":0.067}}},{"i":1052,"$":{"0":{"v":"Design Pattern","n":0.707},"1":{"v":"\nA design pattern can be one of four general types:\n1. Creational design patterns\n2. Structural design patterns\n3. Behavioral design patterns\n4. Architectural design patterns\n\n### Creational\nThese patterns are used to provide a mechanism for creating objects in a specific situation without revealing the creation method. In absence of the pattern, the approach for creating an object might lead to complexities in the design of a project.\nUsing creational design patterns allow flexibility in deciding which objects need to be created for a specific use case by providing control over the creation process.\n![](/assets/images/2021-10-08-13-42-21.png)\n\n### Structural\nThese patterns concern class/object composition and relationships between objects. They let you add new functionalities to objects so that restructuring some parts of the system does not affect the rest. Hence, when some parts of structure change, the entire system does not need to change.\n![](/assets/images/2021-10-08-13-43-17.png)\n\n### Behavioural\nThese patterns are concerned with communication between dissimilar objects in a system. They streamline the communication and make sure the information is synchronized between such objects.\n![](/assets/images/2021-10-08-13-43-47.png)\n\n### Architectural\nThese patterns are used for solving architectural problems within a given context in software architecture.\n![](/assets/images/2021-10-08-13-44-15.png)\n","n":0.076}}},{"i":1053,"$":{"0":{"v":"Structural","n":1}}},{"i":1054,"$":{"0":{"v":"Proxy","n":1},"1":{"v":"\nA proxy, in its most general form, is a class functioning as an interface to something else (the *subject*).\n- That interface could be anything, including:\n  - a network connection\n  - a large object in memory\n  - a file\n  - some other resource that is expensive or impossible to duplicate\n\nWhen you see \"proxy\" think: *\"something that stands-in for something else.\"*\n\nThe proxy implements the same interface as the real serving object. Therefore, from the client's perspective, using the proxy and using the real object (ie. the *subject*) itself should be the same.\n\na proxy is a wrapper or agent object that is being called by the client to access the real serving object (ie. the *subject*) behind the scenes\n- the proxy may simply forward the request, or it may provide some additional logic.\n  - for example, it may cache data from the serving object, or it may perform some type of check to ensure that the the operation should be permitted (e.g. maybe some business logic must be satisfied before the serving object can be accessed in the first place)\n\n![](/assets/images/2022-04-08-09-04-59.png)\n\nProxies are about providing an extra level of [[indirection|general.terms.indirection]] to support distributed, controlled, or intelligent access.\n\n```ts\nclass Driver {\n  constructor (age) {\n    this.age = age\n  }\n}\n\nclass Car {\n  drive () {\n    console.log('Car has been driven!')\n  }\n}\n\n// this proxy checks to make sure the driver is old enough, then calls the serving object (the one instantiated from Car)\nclass ProxyCar {\n  constructor (driver) {\n    this.car = new Car()\n    this.driver = driver\n  }\n\n  drive () {\n    if (this.driver.age <= 16) {\n      console.log('Sorry, the driver is too young to drive.')\n    } else {\n      this.car.drive()\n    }\n  }\n}\n\n// Run program\nconst driver = new Driver(16)\nconst car = new ProxyCar(driver)\ncar.drive()\n\nconst driver2 = new Driver(25)\nconst car2 = new ProxyCar(driver2)\ncar2.drive()\n```","n":0.059}}},{"i":1055,"$":{"0":{"v":"Decorator Pattern","n":0.707},"1":{"v":"\n## What are they?\nA decorator is a function that can be used to extend (decorate) the functionality of a certain object at run-time, independently of other instances of the same class.\n- This is achieved by designing a new Decorator class that wraps the original class/method.\n- One value is that they read more like plain English, and allow us to discern at a distance what behaviour has been added to a given class/method.\n\nDecorators are helpful for anything you want to transparently wrap with extra functionality. \n- ex. memoization, enforcing access control and authentication, instrumentation and timing functions, logging, rate-limiting etc.\n\nDecorators help us adhere to Single Responsibility Principle, since it allows functionality to be divided between classes with unique areas of concern.\n- We might have solved this problem with [[subclassing|paradigm.oop.inheritance]], but decorators can be more efficient because an object's behavior can be augmented without defining an entirely new object.\n    - subclassing (ie. `extend`ing) happens at compile time, meaning that this binding cannot be changed at run-time.\n  \n### Differences between other structural patterns\nWhile an [[Adapter|general.patterns.structural.adapter]] provides a different interface to its subject, and a Proxy provides the same interface, a Decorator provides an enhanced interface.\n- a Decorator enhances an object's responsibilities. Decorator is thus more transparent to the client. As a consequence, Decorator supports recursive composition, which isn't possible with pure Adapters.\n\nAn [[adapter|general.patterns.structural.adapter]] should be used when the wrapper must respect a particular interface and must support [[polymorphic|paradigm.oop.polymorphism]] behavior.\n\nDecorator and Proxy have different purposes but similar structures. Both describe how to provide a level of indirection to another object, and the implementations keep a reference to the object to which they forward requests.\n\n\n## How are they used?\nDecorators can be thought of as functions.\n- you can think of:\n  ```ts\n  @Decorator\n  class Foo {}\n  ```\nas\n  ```ts\n  Decorator( new Foo() )\n  ```\n\nImagine having a `Decorator` class that implements the interface of the extended (decorated) object transparently by forwarding all requests to it \n\n### Example\nImagine that we had a `@UserValidatorDecorator` whose sole responsibility was to verify that a username adheres to our application's business logic:\n- note: not valid code; just for illustrative purposes\n```ts\nfunction UserValidatorDecorator(user) {\n  if (user.username === \"\") {\n    throw new Error('Invalid!')\n  }\n}\n```\nThen, we can use the decorator to ensure that each time we instantiate, the logic of the decorator is used:\n```ts\n@UserValidatorDecorator\nclass User {\n  username: string = \"\"\n}\n```\n\n### Decorator Factory\nA Decorator Factory is simply a function that returns the expression that will be called by the decorator at runtime.\n\nExample:\n```ts\nfunction color(value: string) {\n  // this is the decorator factory. it sets up the returned decorator function\n  return function (target) {\n    // this is the decorator. do something with 'target' and 'value'...\n  };\n}\n```\n\n### Alternatives to Decorators\n","n":0.048}}},{"i":1056,"$":{"0":{"v":"Adapter Pattern","n":0.707},"1":{"v":"\nAn adapter is a wrapper that converts the interface of an existing class into another interface that would be expected by a different client. It is a way of bridging the gap between incompatible interfaces.\n- It is often used to make existing classes work with others without modifying their source code.\n- ex. the [[browser]] uses an adapter to convert the interface of a [[browser.DOM]] of an XML document into a tree that can be displayed.\n\nAnal: we have adapters for electrical sockets when visiting foreign countries. These bridge the gap between 2 interfaces that are incompatible with one another \n\nAn adapter may also be known as a *wrapper*, though that name may also refer to a [[decorator|general.patterns.structural.decorators]].\n\nAn adapter allows two incompatible interfaces to work together. It does this by converting the interface of one class into an interface expected by the clients.\n- Interfaces may be incompatible, but the inner functionality should suit the need. \n\nThe adapter design pattern solves problems like:\n- How can a class be reused that does not have an interface that a client requires?\n- How can classes that have incompatible interfaces work together?\n- How can an alternative interface be provided for a class?\n\nOften an (already existing) class can't be reused only because its interface doesn't conform to the interface clients require. The adapter design pattern describes how to solve such problems:\n- Define a separate adapter class that converts the (incompatible) interface of a class (adaptee) into another interface (target) clients require.\n- Work through an adapter to work with (reuse) classes that do not have the required interface.\n\nThe key idea in this pattern is to work through a separate adapter that adapts the interface of an (already existing) class without changing it.\n\n","n":0.059}}},{"i":1057,"$":{"0":{"v":"Creational","n":1}}},{"i":1058,"$":{"0":{"v":"Pool","n":1},"1":{"v":"\nA pool is a collection of resources that are kept, in memory, ready to use, rather than the memory acquired on use and the memory released afterwards.\n- In this context, resources can refer to system resources such as file handles, which are external to a process, or internal resources such as objects.\n- A pool client requests a resource from the pool and performs desired operations on the returned resource. When the client finishes its use of the resource, it is returned to the pool rather than released and lost.\n\nThe pooling of resources can offer a significant response-time boost in situations that have high cost associated with resource acquiring, high rate of the requests for resources, and a low overall count of simultaneously used resources.\n\nPooling is also useful when the latency is a concern, because a pool offers predictable times required to obtain resources since they have already been acquired. \n\nPooling is also useful for expensive-to-compute data, notably large graphic objects like fonts or bitmaps, acting essentially as a data cache or a memoization technique.\n\nSpecial cases of pools are [[connection pools|db.strategies.pool]], thread pools, and memory pools.\n\n## Object Pool\nA pool (a.k.a *object pool*) is used to manage object caching. \n- A client can access the pool which will hold objects that have already been instantiated, thus negating the client's need to instantiate a new object itself.\n- The pool is often self-managing, and will instantiate new objects as they are needed, and limit the creation of more to adhere to a pre-defined limit.\n- When a client is done with the object, it is returned to the pool to be used by another client.\n\nThe pool itself is designed to be a singleton (ie. there is only a single instance of it)\n\n```ts\nclass ObjectPool<T> {\n  private objects: T[]\n  public acquire() {\n    // take an object from the pool and give it to the client\n  }\n  public release() {\n    // return an object from the client back to the pool\n  }\n}\n```\n\n## UE Resources\n- https://en.wikipedia.org/wiki/Pool_(computer_science)","n":0.055}}},{"i":1059,"$":{"0":{"v":"Factory Functions","n":0.707},"1":{"v":"\nFactory functions allow us to create objects without having to specify the exact class of the object that will be created.\n- This is accomplished by calling a function, rather than calling a constructor.\n\nThe idea is to define an interface for creating an object, but let subclasses decide which class to instantiate. The Factory method lets a class defer instantiation it uses to subclasses.\n\nIn the absence of factory functions, we may have some added problems, and an object's creation...\n- may lead to a significant duplication of code,\n- may require information not accessible to the composing object,\n- may not provide a sufficient level of abstraction,\n- may otherwise not be part of the composing object's concerns.\n\nThe factory method pattern relies on inheritance, as object creation is delegated to subclasses that implement the factory method to create objects.\n\nThe problem with constructors is that they look just like regular functions, but do not behave like regular functions at all.\n\nThe factory function pattern is similar to constructors, but instead of using new to create an object, factory functions simply set up and return the new object when you call the function.\n\nWith a factory\n```js\nconst personFactory = (name, age) => {\n  const sayHello = () => console.log('hello!');\n  return { name, age, sayHello };\n};\n\nconst jeff = personFactory('jeff', 27);\n\nconsole.log(jeff.name); // 'jeff'\n\njeff.sayHello(); // calls the function and logs 'hello!'\n```\n\nWith a constructor\n```js\nconst Person = function(name, age) {\n  this.sayHello = () => console.log('hello!');\n  this.name = name;\n  this.age = age;\n};\n\nconst jeff = new Person('jeff', 27);\n```\n\nES6 modules are actually very similar to factory functions. The main difference is how they’re created.\n- The concepts are exactly the same as the factory function. The nuance is that instead of creating a factory that we can use over and over again to create multiple objects, the module pattern wraps the factory in an IIFE (Immediately Invoked Function Expression).\n```js\nconst calculator = (() => {\n  const add = (a, b) => a + b;\n  const sub = (a, b) => a - b;\n  const mul = (a, b) => a * b;\n  const div = (a, b) => a / b;\n  return {\n    add,\n    sub,\n    mul,\n    div,\n  };\n})();\n\ncalculator.add(3,5) // 8\ncalculator.sub(6,2) // 4\ncalculator.mul(14,5534) // 77476\n```\n\nabove, the function inside the IIFE is a simple factory function, but we can just go ahead and assign the object to the variable calculator since we aren’t going to need to be making lots of calculators, we only need one.\n","n":0.05}}},{"i":1060,"$":{"0":{"v":"Architectural","n":1}}},{"i":1061,"$":{"0":{"v":"Repository Design Pattern","n":0.577},"1":{"v":"\na repository allows you to populate an application with data that comes from the database in the form of the domain entities. Once the entities are loaded, they can be changed and then persisted back to the database through transactions.\n\nThe repository pattern has 2 purposes:\n1. abstract the data layer\n2. centralize the handling of domain objects. \n\nA repository is a class / component that encapsulates the logic required to access [[data sources|general.patterns.data-source]]\n\nIf executed properly, this pattern will relieve any consuming code of knowing what data layer is being used under the covers. \n- Client objects declaratively build queries and send them to the repositories for answers.\n- a consumer that uses a repository should have no knowledge if the underlying layer is an SQL database, a NoSQL database, or even just an online API that does some further interaction.\n\nOften the data layer logic / knowlege that is hidden is performing some set of CRUD actions. In these cases, a repository might be implemented as a class with methods that provide logic to access the database.\n\nWe should create one repository per [[aggregate|general.principles.DDD.aggregate]]\n- [source](https://docs.microsoft.com/en-us/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/infrastructure-persistence-layer-design#define-one-repository-per-aggregate)\n- the purpose is to achieve [[transactional|db.strategies.transaction]] consistency amongst all objects of the aggregate.\n  - if instead we decided to create an aggregate for each table, we would be making our data-operations less transactional.\n\nConsider the overlap between an [[ORM|sql.ORM]] and a repository. An ORM abstracts away the \"how\" of how we interact with a set of data. An ORM operates at a slightly higher level of abstraction, becuase it abstracts away the logic of how data is actually accessed \n- with an ORM used over SQL, we don't write a `SELECT *`, we write `findAll()`. With a repository, we still have to write the `SELECT *`, but we get the benefits of abstracting that fact away from those who consume the repository.\n\nQuestion: what is the difference between a data source and a repository?","n":0.057}}},{"i":1062,"$":{"0":{"v":"Command-Query-Responsibility-Segregation","n":1},"1":{"v":"\nCQRS is an architectural pattern that separates reading and writing into two different models. This means that every method should either be a Command that performs an action or a Query that returns data. A Command cannot return data and a Query cannot change the data\n- ex. While not CQRS in principle, Redux attempts to follow the spirit of this pattern. Consider that to write to state we must dispatch an action, and to read state we must use selectors. The implementations of reading and writing to state are decoupled.\n\nA common implementation of CQRS occurs in databases with Leader-Follower [[replication|db.distributed.replication]], whereby the read requests are routed to the follower database instances, while the write requests are routed to the leader database instance.\n\nThe CQRS pattern offers high performance and high availability at the cost of stale reads (assuming replication is done asynchronously)\n\nIn CQRS, we use different interfaces to read and update data. This is as opposed to CRUD (Create, Read, Update, Delete), where we use a single interface to perform all the reads and updates.\n\nCQRS is implemented using [[general.patterns.event-sourcing]]\n\nWhile implementing a CQRS system, all the commands are modelled as events. So whenever a user performs any action, a command is dispatched as an event and the event is stored in a store.\n- Whenever we want to read data, we take the data at its initial state and apply all the commands to the data to get the most up to date data.\n\nStoring data is normally quite straightforward if you don’t have to worry about how it is going to be queried and accessed; many of the complexities of schema design, indexing, and storage engines are the result of wanting to support certain query and access patterns. For this reason, you gain a lot of flexibility by separating the form in which data is written from the form it is read, and by allowing several different read views. This is the basis for the idea of CQRS.\n- The traditional approach to database and schema design is based on the fallacy that data must be written in the same form as it will be queried.\n- Debates about [[normalization and denormalization|db.design.normalization]] become largely irrelevant if you can translate data from a write-optimized event log to read-optimized application state: it is entirely reasonable to denormalize data in the read-optimized views, as the translation process gives you a mechanism for keeping it consistent with the event log.\n\nConceptually, you always implement CQRS by using two different models, one for updates and one for queries.\n- You populate a model specifically designed to make updates easy from commands, then serialize it, and you populate a distinctly designed model, designed to make the life of querying clients easy, from data store queries, and the put it on the wire and send it to clients. How you populate each of the two distinct models, whether you use a database view, an in-memory cache, aggregate SQL queries or some other mechanism is irrelevant. As long as you stick to the two distinct models pattern it's CQRS.\n\nYou don't necessarily implement CQRS at the database level, or at the database query level.\n\n\n# UE Resources\n- https://martinfowler.com/bliki/CQRS.html\n- https://docs.microsoft.com/en-us/azure/architecture/patterns/cqrs\n","n":0.044}}},{"i":1063,"$":{"0":{"v":"Streaming","n":1},"1":{"v":"\nStreaming data refers to data that is continuously generated, usually in high volumes and at high velocity\n\nIn general, a \"stream\" refers to data that is incrementally made available over time. \n- The concept appears in many places, such as... \n    - in the stdin and stdout of Unix\n    - programming languages (lazy lists)\n    - filesystem APIs (such as Java’s FileInputStream)\n    - TCP connections\n    - delivering audio and video over the internet\n\nA streaming data source would typically consist of a stream of logs that record events as they happen – such as a user clicking on a link in a web page, or a sensor reporting the current temperature.\n\nex. IoT sensors, Server and security logs, Real-time advertising, Click-stream data from apps and websites\n\nIn all of these cases we have end devices that are continuously generating thousands or millions of records, forming a data stream – unstructured or semi-structured form, most commonly JSON or XML key-value pairs.\n\n### Bounded vs Unbounded Streams\nYou can either organize your data processing around bounded or unbounded streams, and which of these paradigms you choose has profound consequences.\n\n#### Unbounded streams \nWith unbounded streams, we are using the *stream processing* paradigm.\n\n- have a start but no defined end. \n- do not terminate and provide data as it is generated. \n- Unbounded streams must be continuously processed, \n    - i.e., events must be promptly handled after they have been ingested. It is not possible to wait for all input data to arrive because the input is unbounded and will not be complete at any point in time. \n- Processing unbounded data often requires that events are ingested in a specific order, such as the order in which events occurred, to be able to reason about result completeness.\n\n#### Bounded streams \nIn this mode of operation you can choose to ingest the entire dataset before producing any results\n- this means we can do things like sort the data, compute global statistics, or produce a final report that summarizes all of the input.\n\nWhen you process a bounded data stream, we are using the *batch processing* paradigm.\n\n- have a defined start and end. \n- can be processed by ingesting all data before performing any computations. \n- Ordered ingestion is not required to process bounded streams \n    - this is because a bounded data set can always be sorted. \n- Processing of bounded streams is also known as batch processing.\n\n### Buffer\nanal: Imagine adding various amounts of liquid to a big funnel. If we add liquid slowly, or fast, it doesn't change the rate at which the liquid exits the funnel. This is in essence what a buffer does for us. It is an intermediary data object that sits between the server and the client, and receives data at the rate that it is sent by the server.\n","n":0.047}}},{"i":1064,"$":{"0":{"v":"Window","n":1},"1":{"v":"\nA window is a way to chunk data so that it can be processed in a more effective way.\n\n## Window processing patterns\n### Tumbling Windows\nElements are assigned to a window based on their timestamp.\n\n- each window has a specified size \n- windows do not overlap with each other.\n\n![](/assets/images/2023-01-10-13-10-03.png)\n\n### Sliding Windows\n- see: [[algorithm implementation|general.algorithms.patterns.sliding-window]]\n\nLike tumbling windows, each window has a specified size.\n\nSliding windows allow you to configure how frequently a window is started.\n- therefore, sliding windows can be overlapping if the slide is smaller than the window size, causing elements to be assigned to multiple windows.\n- ex. you could have windows of size 10 minutes that slides by 5 minutes. With this you get every 5 minutes a window that contains the events that arrived during the last 10 minutes as depicted by the following figure.\n\n![](/assets/images/2023-01-10-13-21-31.png)\n\n### Session Window\nassigner groups elements by sessions of activity.\n\n- do not overlap\n- do not have a fixed start and end time\n- Instead, a session window closes when it does not receive elements for a specified period of time (ie. when a gap of inactivity occurred).\n  - When this period expires, the current session closes and subsequent elements are assigned to a new session window.\n\n![](/assets/images/2023-01-10-13-28-01.png)\n\n### Global Window\nWith a global window scheme, all elements with the same key will be assigned to the same single global window\n\n![](/assets/images/2023-01-10-13-29-40.png)","n":0.068}}},{"i":1065,"$":{"0":{"v":"State Machine","n":0.707},"1":{"v":"\nA state machine is a directed graph where nodes represent all possible states of a view or of the whole app, and where edges represent possible transitions between the states.\n\nState machines are particularly useful when the state is very complex and involves multiple transitions.\n\nTurnstile:\n![](/assets/images/2021-10-27-09-53-05.png)\n\nAn arrow between two nodes means that it’s possible to go through one state to another via some action. All non-listed transitions are not possible. There can be a meaningful transition from a state to the same state that is marked by a circular arrow.\n\nHere is a state machine for the storage of fetched data that is to be stored in the browser\n![](/assets/images/2021-10-27-09-56-33.png)\n\nThere are four states:\n- empty - fetching has not started yet or has been canceled\n- loading - fetching is in progress\n- withData - fetching was successful\n- error - fetching failed\n","n":0.086}}},{"i":1066,"$":{"0":{"v":"Sink","n":1},"1":{"v":"\nA sink is a function (or class) designed to receive incoming events from another function (or object).\n- commonly implemented as callbacks\n\na sink is often misconstrued with a [[gateway|general.patterns.gateway]].\n\nThe word sink has been used for both input and output in the industry.\n\nAlso known as event sink or data sink\n\nIn Event-based architecture, sinks are on the consumer-side.\n","n":0.135}}},{"i":1067,"$":{"0":{"v":"Shim","n":1},"1":{"v":"\nA shim is a piece of code that intercepts API calls transparently (ie. without the user knowing about it) and either:\n- changes the arguments that are passed\n- handles the operation itself\n- redirects the operation elsewhere\n\nShims are commonly used to support... \n- an old API in a new environment\n    - ex. a common occurrence is that shims are released following the behaviour change of a public API. In these cases, a shim can be made that bridges the gap between the old usage of the API (which clients are still using) and the new API.\n- a new API in an old environment\n    - ex. a polyfill, which is just a shim for a [[browser]] API.\n        - when we use experimental language features via a polyfill, there is a shim in the background that intercepts our usage of the feature and changes it in some way so that it can be understood by the language engine.","n":0.081}}},{"i":1068,"$":{"0":{"v":"Plugin","n":1},"1":{"v":"\n[Creating a plugin system](https://betterprogramming.pub/how-to-design-software-plugins-d051ce1099b2)\n","n":0.5}}},{"i":1069,"$":{"0":{"v":"Messaging Pattern","n":0.707},"1":{"v":"\nA messaging pattern describes how two different parts of an application (or different systems) connect and communicate with each other.\n- In actuality, protocols like REST are indeed messaging protocols, but we don't think of them as such. Typically when we talk about messaging patterns, we are referring to situations where we have this third entity between the provider/consumer of the data.\n\nmessage-passing communication is usually one-way: a sender normally doesn't expect to receive a reply to its messages. It is possible for a process to send a response, but this would usually be done on a separate channel.\n\na message is just a sequence of bytes with some metadata, so you can use any encoding format\n\nTwo types of messaging patterns are available − \n1. point to point \n\t- a particular message can be consumed by a maximum of one consumer only, and it disappears from the queue once a consumer reads it\n\t- The typical example of this system is an Order Processing System, where each order will be processed by one Order Processor, but Multiple Order Processors can work as well at the same time.\n2. [[Pub-Sub|general.patterns.messaging.pubsub]] \n\t- Most of the messaging patterns follow pub-sub.\n\n# Overview\nImagine we have 2 services: OrderService and InvoiceService. In order to pass data from OrderService downstream to InvoiceService, we set up a message queue in the OrderService. Then, InvoiceService would read from it.\n\nIf we need to add more downstream services, then we need to modify the upstream service (ie. OrderService) to instruct it to write a message queue. This creates more complex high-level service relationships that are hard to maintain, since future downstream services now become reliant on that upstream service to update its message queues.\n- This burden is what causes people to go for more of a [[bus|general.arch.SOA.bus]]-based solution\n\nIn a typical message-queueing implementation, a developer installs and configures message-queueing software (a queue manager or broker), and defines a named message queue. Or they register with a message queuing service.\n- An application then registers a software routine that \"listens\" for messages placed onto the queue.\n- Second and subsequent applications may connect to the queue and transfer a message onto it.\n- The queue-manager software stores the messages until a receiving application connects and then calls the registered software routine. The receiving application then processes the message in an appropriate manner.\n\nSome patterns for implementing messages are:\n- Message Queues\n\t- ex. [[SQS|aws.svc.SQS]]\n- [[Pub/Sub|general.patterns.messaging.pubsub]]\n\t- ex. [[SNS|aws.svc.SNS]]\n- Event buses\n\t- ex. [[EventBridge|aws.svc.event-bridge]]\n\n## Fan out\nFan-out is a messaging pattern where messages are broadcast in parallel in a one-to-many arrangement.\n- this pattern can be seen in the [[pub/sub|general.patterns.messaging.pubsub]] messaging system, since a single sender can send messages to many receivers.\n\n![](/assets/images/2022-04-07-21-41-28.png)\n\n## Message Queue\nMessage queues provide some guarantee of reliability, in that the number of incoming messages equals the number of outgoing messages\n\n## Message Broker\nA message broker is an intermediary computer program module that translates a message from the formal messaging protocol of the sender to the formal messaging protocol of the receiver.\n\nMessage brokers are used as follows: one process sends a message to a named queue or topic, and the broker ensures that the message is delivered to one or more consumers of or subscribers to that queue or topic. \n- There can be many producers and many consumers on the same topic.\n\n### Examples\n- [[kafka]], though more of an event-streaming platform\n- RabbitMQ\n- AWS Kinesis\n- Google Cloud Pub/Sub\n- Redis (not a message broker per se, but rather has messaging brokering as one of its capabilities)\n- NATS\n","n":0.042}}},{"i":1070,"$":{"0":{"v":"PubSub Pattern","n":0.707},"1":{"v":"\n## Overview\n- stands for *Publish-Subscribe*\nsenders (publishers) are not programmed to send their messages to specific receivers (subscribers). Rather, published messages are characterized into channels, without knowledge of what (if any) subscribers there may be. Subscribers express interest in one or more channels, and only receive messages that are of interest, without knowledge of what (if any) publishers there are. This decoupling of publishers and subscribers can allow for greater scalability and a more dynamic network topology.\n\nPub-Sub is a paradigm for getting real-time updates to a client. It is unlike webhooks, which would require the publisher of information to explicitly call someone who has \"signed up\" to receive it. Instead, with Pub-Sub the publisher determines that there is a certain set of information that it wants to send out, and it specifies a container (the topic) that it will be sent to. This leaves the topic open to be subscribed to, which a subscriber (a client) may do. The subscriber specifies that it is interested in receiving any new messages that arrive in a specific Topic (by name), and when new messages arrive, they are sent to the client.\n- This topic name would be scoped for a specific user, for instance we might call it `graphql:user:${userId}`\n\nA common example of pub-sub are push notifications on mobile devices, since information preferences are expressed in advance by the user.\n- A client \"subscribes\" to various information \"channels\" provided by a server; whenever new content is available on one of those channels, the server pushes that information out to the client.\n\nA subscriber receives messages either by Pub/Sub pushing them to the subscriber's chosen endpoint, or by the subscriber pulling them from the service. Upon successful receiving of the message, the subscriber informs that it was received successfully.\n\nPub-sub is a messaging pattern whereby senders of messages (publishers) do not program the messages to be sent to the receivers (subscribers). Instead, the publishers categorize published messages into classes, without knowing who the subscribers are that will ultimately consume those messages\n- Similarly, subscribers express interest in one or more classes and only receive messages that are of interest to them (without any knowledge about who the publishers are)\n- This above explanation shows that the Pub-Sub paradigm results in the publisher service and the subscriber service being decoupled from one another.\n\nAn RSS feed might be a good analogy for the Pub-Sub paradigm, if we consider that we don't really care where each post/article came from. We just get them all lumped together, and consume them there, without any real consideration to where they came from. The process is decoupled.\n\nCommunication may be:\n1. 1:many (fan-out)\n\t- or, many clients subscribing to one topic (ie. multiple subscriptions to 1 topic)\n\t- ex. Twitter,whereby one single tweet is sent to all the parties following the person sending the tweet.\n2. many:1 (fan-in)\n\t- or, one client subscribing to many topics (ie. multiple subscriptions to multiple topics)\n3. many:many\n![PubSub many:1 vs 1:many](/assets/images/2021-03-24-10-42-17.png)\n\nThe pub-sub pattern provides greater network scalability and a more dynamic network topology, with a resulting decreased flexibility to modify the publisher and the structure of the published data.\n\nDifferent pub-sub systems have different trade-offs, and when determining which system to use it's often worth asking the questions:\n- What happens if the producers send messages faster than the consumers can process them? \n\t- Broadly speaking, there are three options: \n\t\t1. the system can drop messages, \n\t\t2. buffer messages in a queue, \n\t\t\t- If messages are buffered in a queue, it is important to understand what happens as that queue grows. Does the system crash if the queue no longer fits in memory, or does it write messages to disk? If so, how does the disk access affect the performance of the messaging system\n\t\t3. apply backpressure (also known as flow control; i.e., blocking the producer from sending more messages). \n\t\t\t-\tex. Unix pipes and TCP use backpressure: they have a small fixed-size buffer, and if it fills up, the sender is blocked until the recipient takes data out of the buffer.\n- What happens if nodes crash or temporarily go offline? Are any messages lost? \n\t- As with databases, durability may require some combination of writing to disk and/or replication, which has a cost. If you can afford to sometimes lose messages, you can probably get higher throughput and lower latency on the same hardware.\n\n## Core concepts\n### Topic\n- messages are sent to resources, which we call Topics or Streams.\n- topics have names that uniquely identify them\n\t- ex. if we are subscribing to the event \"user clicks this button\", then the topic might be named `user-clicks-CTA`\n- the topics live on a server controlled by the publisher\n\t- ex. Postgraphile server, Google Pub/Sub\n- a topic can be defined as a group of partitions that all carry messages of the same type.\n\n### Subscription\n- A subscription represents the stream of messages from a single specific topic.\n\n### Relation to Message Queue paradigm\nthe Pub-Sub paradigm is a sibling of the \"Message Queue\" paradigm.\n- Most messaging systems support both the Sub-Pub and Messaging Queue paradigms in their API.\n\n### Relation to Job Queues\nJob Queues only let one \"subscriber\" watch for new \"events\" at a time, and keep a queue of unprocessed events.\n- ex. Celery\n\nIt turns out that Postgres generally supersedes job servers as well. You can have your workers \"watch\" the \"new events\" channel and try to claim a job whenever a new one is pushed. As a bonus, Postgres lets other services watch the status of the events with no added complexity.\n\n### Examples\n- Kafka\n- RabbitMQ\n- NATS\n- Redis PUB/SUB\n","n":0.033}}},{"i":1071,"$":{"0":{"v":"Broker","n":1},"1":{"v":"\nA message broker (also known as a message queue) is essentially a kind of database that is optimized for handling message streams.\n\nThe message broker runs as a server, with both producer and consumers connecting to it as clients.\n- Producers write messages to the broker, and consumers receive them by reading them from the broker.\n- By centralizing the data in the broker, these systems can more easily tolerate clients that come and go (connect, disconnect, and crash), and the question of durability is moved to the broker instead.\n\nUsing a broker is an alternative to using direct network communication between producers and consumers without going via intermediary nodes.\n- forgoing a broker is done in situations where latency is very important, such as stock market feeds.\n\nSince networks are unreliable, forgoing a message broker requires the application code to be aware of the possibility of message loss.\n- even if the protocols detect and retransmit packets that are lost in the network, they generally assume that producers and consumers are constantly online.\n- If a consumer is offline, it may miss messages that were sent while it is unreachable. Some protocols allow the producer to retry failed message deliveries, but this approach may break down if the producer crashes, losing the buffer of messages that it was supposed to retry.\n\nmost message brokers automatically delete a message when it has been successfully delivered to its consumers\n\nConsumers may crash at any time, so it could happen that a broker delivers a message to a consumer but the consumer never processes it, or only partially processes it before crashing. In order to ensure that the message is not lost, message brokers use *acknowledgments*: a client must explicitly tell the broker when it has finished processing a message so that the broker can remove it from the queue.\n- If the connection to a client is closed or times out without the broker receiving an acknowledgment, it assumes that the message was not processed, and therefore it delivers the message again to another consumer.\n\nmessage brokers often support some way of subscribing to a subset of topics matching some pattern.\n\nWhen multiple consumers read messages in the same topic, two main patterns of messaging are used:\n1. Load balancing - In an effort to parallelize the processing, each message is delivered to one of the consumers so the consumers can share the work of processing the messages in the topic.\n2. Fan-out - Each message is delivered to all of the consumers.\n\nThe two patterns can be combined: for example, two separate groups of consumers may each subscribe to a topic, such that each group collectively receives all messages, but within each group only one of the nodes receives each message.","n":0.047}}},{"i":1072,"$":{"0":{"v":"Dead Letter Queue","n":0.577},"1":{"v":"\nThe Dead Letter Queue (DLQ) is a service that stores messages that meet one or more of the following criteria:\n- message is sent to a [[queue|general.lang.data-structs.queue]] that doesn't exist\n- queue/message length limit exceeded\n- message is not processed successfully\n- and others\n\nDead letter queue storing of these messages allows developers to look for common patterns and potential software problems\n\nDLQs are incorporated out of the box with [[EventBridge|aws.svc.event-bridge]], [[SQS|aws.svc.SQS]], [[kafka]], RabbitMQ, and others\n- [[EventBridge|aws.svc.event-bridge]] uses [[SQS|aws.svc.SQS]] as the DLQ\n","n":0.115}}},{"i":1073,"$":{"0":{"v":"Map Reduce","n":0.707},"1":{"v":"\nMapReduce is a concept that solves problems by applying a two-step process: the map phase and the reduce phase.\n- The map phase looks at all documents separately one after the other and creates a map result.\n- The reduce phase takes in the results from the map phase and reduces them to produce an end result.\n\nMapReduce is an abstraction on top of a distributed filesystem.\n\nimplementing a complex processing job using the raw MapReduce APIs is actually quite hard and laborious\n- various higher-level programming models (Pig, Hive, Cascading, Crunch) were created as abstractions on top of MapReduce.\n\nMapReduce is a [[batch|general.patterns.batching]] processing algorithm.\n\nEach MapReduce job is independent from every other job.\n\nMapReduce is very robust: you can use it to process almost arbitrarily large quantities of data on an unreliable multi-tenant system with frequent task terminations, and it will still get the job done (albeit slowly). On the other hand, other tools are sometimes orders of magnitude faster for some kinds of processing.\n\nMapReduce is designed to tolerate frequent unexpected task termination, making it more appropriate for larger jobs— jobs that process so much data and run for such a long time that they are likely to experience at least one task failure along the way.\n\nMapReduce has no concept of [[indexes|db.strategies.index]], at least not in the usual sense.\n- this is because there is usually no need to find individual items— we normally use MapReduce to aggregate data in some way.\n\nUsing the MapReduce programming model has separated the physical network communication aspects of the computation (getting the data to the right machine) from the application logic (processing the data once you have it). This separation contrasts with the typical use of databases, where a request to fetch data from a database often occurs somewhere deep inside a piece of application code\n- Since MapReduce handles all network communication, it also shields the application code from having to worry about partial failures, such as the crash of another node: MapReduce transparently retries failed tasks without affecting the application logic.\n\nMapReduce is a bit like [[unix]] tools, but distributed across potentially thousands of machines. \n- Like Unix tools, it is a fairly blunt, brute-force, but surprisingly effective tool. \n- A single MapReduce job is comparable to a single Unix process: it takes one or more inputs and produces one or more outputs.\n- While Unix tools use stdin and stdout as input and output, MapReduce jobs read and write files on a distributed filesystem.\n  - ex. In Hadoop’s implementation of MapReduce, that filesystem is called HDFS (Hadoop Distributed File System). HDFS conceptually creates one big filesystem that can use the space on the disks of all machines running the daemon.\n- Unlike pipelined Unix commands, MapReduce can parallelize a computation across many machines, without you having to write code to explicitly handle the parallelism.\n  - The mapper and reducer only operate on one record at a time; they don’t need to know where their input is coming from or their output is going to, so the framework can handle the complexities of moving data between machines.\n\nit is very common for MapReduce jobs to be chained together into workflows, such that the output of one job becomes the input to the next job.\n- ex. Workflows consisting of 50 to 100 MapReduce jobs are common when building recommendation systems\n\nThe pattern of data processing in MapReduce is very similar to this example:\n1. Break files into records: Read a set of input files, and break it up into records. \n  - In the web server log example, each record is one line in the log (that is, `\\n` is the record separator).\n2. Map: Call the mapper function to extract a key and value from each input record. \n  - In the preceding example, the mapper function is `awk '{print $7}'`: it extracts the URL ($7) as the key, and leaves the value empty.\n3. Sort: Sort all of the key-value pairs by key. \n  - In the log example, this is done by the first sort command.\n  - sorting is implicit, because the results from map are sorted before they are given to the reduce function.\n4. Reduce: Call the reducer function to iterate over the sorted key-value pairs. If there are multiple occurrences of the same key, the sorting has made them adjacent in the list, so it is easy to combine those values without having to keep a lot of state in memory. \n  - In the preceding example, the reducer is implemented by the command `uniq -c`, which counts the number of adjacent records with the same key.\n\n### Mapper\nThe mapper is called once for every input record\n- job is to extract the key and value from the input record. For each input, it may generate any number of key-value pairs (including none). \n- It does not keep any state from one input record to the next, so each record is handled independently.\n\n### Reducer\nThe MapReduce framework takes the key-value pairs produced by the mappers, collects all the values belonging to the same key, and calls the reducer with an iterator over that collection of values. The reducer can produce output records (such as the number of occurrences of the same URL).\n\nOne way of looking at this architecture is that mappers \"send messages\" to the reducers. When a mapper emits a key-value pair, the key acts like the destination address to which the value should be delivered. Even though the key is just an arbitrary string (not an actual network address like an IP address and port number), it behaves like an address: all key-value pairs with the same key will be delivered to the same destination (a call to the reducer).\n\n\n# UE Resources\nhttps://timilearning.com/posts/mit-6.824/lecture-1-mapreduce/\n","n":0.033}}},{"i":1074,"$":{"0":{"v":"Long Polling","n":0.707},"1":{"v":"\nLong polling is a technique where the server elects to hold a client’s connection open for as long as possible, delivering a response only after data becomes available or a timeout threshold has been reached.\n- Long polling is essentially a more efficient form of the original polling technique. The difference is that we don't need to make repeated requests.\n\nImplementing long polling is a server-side concern, meaning that if it is a third party application we plan to long poll, they must have built the functionality in.\n\nThe only difference to basic polling, as far as the client is concerned, is that a client performing basic polling may deliberately leave a small time window between each request so as to reduce its load on the server, and it may respond to timeouts with different assumptions than it would for a server that does not support long polling.\n- With long polling, the client may be configured to allow for a longer timeout period (via a Keep-Alive header) when listening for a response – something that would usually be avoided seeing as the timeout period is generally used to indicate problems communicating with the server.\n\nLong polling existed as the conventional technique to implementing (pseudo) real-time communication prior to the advent of WebSockets.\n","n":0.069}}},{"i":1075,"$":{"0":{"v":"Ingress","n":1},"1":{"v":"\nGenerally an ingress (or ingression) is a layer that handles the directing and flow of data into another entity.\n\n### Examples\n- [[Kubernetes Ingress|k8s.objects.ingress]]\n- An event bus can act as an ingression layer for data analysis.\n","n":0.171}}},{"i":1076,"$":{"0":{"v":"Hooks","n":1},"1":{"v":"\nHooks are a type of function that allows your system to call some other module.\n- It is a broad way to describe this pattern where some piece of code (the hook) can trigger in response to data being sent between software components.\n\n- ex. keylogging\n\t- Basically, our application code says \"ok System, I want to be notified anytime anytime a user presses a key\"\n- ex. Every Chrome (or other browser) extension is possible because of hooks in the browser.\n\t- The extension sets up a hook to be notified when a specific page is navigated to.\n\t- Vimium executes hooks in response to keypresses.\n- ex. Video games with mod support are another example of hooks in action.\n- ex. a browser might use a hook to make it easier for antivirus software to scan downloads.\n\t- When it starts a download, it shouts \"Hey, anyone listening, I'm starting a download!\", and any program listening for that will be able to react and intervene.\n- A screen reader for blind users sets up a system hook to know when another program creates a new window on the screen.\n\n### James Bond paintball mode example\n\nIf you were to try and implement this mod to the game, you could use a hook provided by the game itself. Essentially, when an event happens in the game (ie. when bullets are shot), a hook is executed, which turns the resulting bullet holes colored.\n\n![](/assets/images/2021-11-23-14-35-06.png)\n","n":0.066}}},{"i":1077,"$":{"0":{"v":"Handlers","n":1},"1":{"v":"\nA handler is a routine/function/method which is specialized in a certain type of data or focused on certain special tasks.\n- *handler* a very generic term.\n\nExamples:\n- Event handler - Receives and digests events and signals from the surrounding system (e.g. OS or GUI).\n- Memory handler - Performs certain special tasks on memory.\n- File input handler - A function receiving file input and performing special tasks on the data, all depending on context of course.\n\nTherefore, it is code that's associated with and triggered by the occurrence of a specific event, like an incoming message, a thrown exception, a signal sent to a process, a network I/O request completing, or a mouse click on a user interface element.\n","n":0.093}}},{"i":1078,"$":{"0":{"v":"Gateways","n":1},"1":{"v":"\nA gateway is an object that encapsulates access to an external system or resource. It is therefore an abstraction of the APIs that our application accesses. Thus, all the benefits that an abstraction typically provides apply here.\n- A gateway is typically a simple wrapper. We look at what our code needs to do with the external system and construct an interface that supports that clearly and directly.\n\n![](/assets/images/2021-12-14-14-25-34.png)\n\nThe gateway acts on terms that our system uses. It allows access to data sources, which may be queried in varying ways. The gateway takes all of these odd endpoints and ties them up into a nice little bow that can provide a unified interface that is ready to be queried.\n\nGateways should only include logic that supports this translation between domestic and foreign concepts. Any logic that builds on that should be in clients of the gateway.\n\nThe notion of a gateway fits well with that of the [[Bounded Contexts|general.principles.DDD.bounded-context]]\n- use a gateway when dealing with something in a different context. The gateway handles the translation between the foreign context and your own.\n\t- The gateway is a way to implement an Anticorruption Layer\n\n### The problem\nUsually we depend on third party APIs to build out our applications. These interfaces often seem awkward from the context of our software\n- The API may use different types, require strange arguments, combine fields in ways that don't make sense in our context. Dealing with such an API can result in jarring mismatches whenever its used.\n\nUse a gateway whenever there is a need to access some external software and there is any awkwardness in that external element.\n- Rather than let the awkwardness spread through the code, contain it to a single place in the gateway.\n\nThe question we have to ask ourselves when considering whether or not to make a gateway is \"do we wish to isolate ourselves from the underlying platform?\"\n- In many cases the platform's facilities are so pervasive that it's not worth going through the effort of wrapping it in a gateway.\n\n### Pros\n- Insulates the clients from how the application is partitioned into microservices\n- Insulates the clients from the problem of determining the locations of service instances\n- Provides the optimal API for each client\n- Reduces the number of requests/roundtrips. For example, the API gateway enables clients to retrieve data from multiple services with a single round-trip. Fewer requests also means less overhead and improves the user experience. An API gateway is essential for mobile applications.\n- Simplifies the client by moving logic for calling multiple services from the client to API gateway\n- Translates from a “standard” public web-friendly API protocol to whatever protocols are used internally\n- The API gateway pattern has some drawbacks:\n\n### Cons\n- Increased complexity - the API gateway is yet another moving part that must be developed, deployed and managed\n- Increased response time due to the additional network hop through the API gateway - however, for most applications the cost of an extra roundtrip is insignificant.\n\n\n### Connection Object\nIt's often useful to add a connection object to the basic structure of the gateway. The connection is a simple wrapper around the call to the foreign coded.\n- The gateway translates its parameters into the foreign signature, and calls the connection with that signature. The connection then just calls the foreign API and returns its result. The gateway finishes by translating that result to a more digestible form.\n- The connection can be useful in two ways.\n\t1. Firstly it can encapsulate any awkward parts of the call to the foreign code, such as the manipulations needed for a REST API call.\n\t2. Secondly it acts as a good point for inserting a [[Test Double|testing.test-double]].\n\nThe connection object is where we make the HTTP request.\n\n#### Testing\nUsing a gateway can make a system much easier to test by allowing the test harness to stub out the gateway's connection object.\n- particularly important for gateways that access remote services, as it can remove the need for a slow remote call\n\t- use a gateway here, even if the external API is otherwise okay to use (in which case the gateway would only be the connection object).\n\n* * *\n\nA gateway is usually either an end-point or allows bi-direction communication between dissimilar systems\n- In contrast, sinks are just an event input point.\n\n* * *\n\n### API Gateway\nAPI gateway handles requests in one of two ways. \n- Some requests are simply proxied/routed to the appropriate service. \n- Other requests by fanning out to multiple services.\n\n![](/assets/images/2021-12-20-11-28-44.png)\n\nRather than provide a one-size-fits-all style API, the API gateway can expose a different API for each client\n- ex. the Netflix API gateway runs client-specific adapter code that provides each client with an API that’s best suited to its requirements.\n\n#### Backends for Frontends\nThis is a variation of the API Gateway approach whereby each client gets its own API gateway:\n![](/assets/images/2021-12-20-11-30-09.png)\n\n# E Resources\n[Martin Fowler article](https://martinfowler.com/articles/gateway-pattern.html)\n- includes info on testing with gateway\n[Introduce gateway into existing app](https://martinfowler.com/articles/refactoring-external-service.html)\n[Simple API Gateway service Java code](https://github.com/cer/event-sourcing-examples/tree/master/java-spring/api-gateway-service)\n","n":0.035}}},{"i":1079,"$":{"0":{"v":"Event Sourcing","n":0.707},"1":{"v":"\nA system that allows us to query a database tells us the current state of everything. A system that allows Event Sourcing also tells us how we got here. In other words, the system keeps records of all changes that have happened.\n- Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.\n\nevery change to the state of an application is captured in an event object, and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.\n- To be clear, we are persisting two different things: an application state (ex. DB) and an event log.\n\nThe key to Event Sourcing is that we guarantee that all changes to the domain objects (ex. what is stored in the DB) are initiated by the event objects\n- This shows that when we want to change something in our database, our event system is first updated with the new log. Only once that log has been entered can our database be updated as well.\n\t- An implication of this is that we could dump the whole application state, and rebuild it from the log.\n\t- Another implication is that we can see the state of the application as a timeline, analogous to Git branching.\n\nA common example of an application that uses Event Sourcing is a version control system\n\n[[CQRS|general.patterns.architectural.CQRS]] and event sourcing often go hand-in-hand\n\n# UE Resources\n- https://martinfowler.com/eaaDev/EventSourcing.html\n- https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing\n- [EventStoreDB](https://www.eventstore.com/eventstoredb)\n","n":0.061}}},{"i":1080,"$":{"0":{"v":"Dependency Injection","n":0.707},"1":{"v":"\n# Overview\nDependency Injection is one form of implementing [[Inversion of Control|general.patterns.IOC]]\n- Here, IoC would be achieved by delegating instantiation of dependencies to the IoC container\n  - ex. in [[Nestjs|nestjs]], the IoC container is the Nestjs runtime system.\n\nDependency Injection also naturally flows out of the principle of Modularity\n\n### Goal\nThe goal is to achieve [[separation of concerns|general.principles.SoC]]\n\nThe client should have no concrete knowledge of the specific implementation of its dependencies. It should only know the interface's name and API. As a result, the client will not need to change even if what is behind the interface changes. Because we achieve this by using interfaces, if the interface itself changes, then the client will have to change as well.\n- ex. if the interface is refactored from being a class to an interface type (or vice versa) the client will need to be reworked.\n- This last point creates a potential issue if the client and services are published separately.\n\nHaving more testable code is a big benefit of using dependency injection\n\n### Motivation\nDependency injection separates the creation of a client's dependencies from the client's behavior, which allows program designs to be loosely coupled and to follow the dependency inversion and single responsibility principles.\n\nA client who wants to call some services should not have to know how to construct those services. Instead, the client delegates the responsibility of providing its services to external code (the injector; see below)\n- The client is not allowed to call the injector code; it is the injector that constructs the services\n\nThe injector then injects (passes) the services into the client which might already exist or may also be constructed by the injector. The client then uses the services. This means the client does not need to know about the injector, how to construct the services, or even which actual services it is using. The client only needs to know about the intrinsic interfaces of the services because these define how the client may use the services. This separates the responsibility of \"use\" from the responsibility of \"construction\".\n\n#### Dependency Inversion Principle\nThis principle states two essential things:\n1. High-level modules should not depend on low-level modules. Both should depend on abstractions.\n2. Abstractions should not depend upon details. Details should depend on abstractions.\n\nDIP keeps high-level modules from knowing the details of its low-level modules and setting them up. It can accomplish this through DI. A huge benefit of this is that it reduces the coupling between modules.\n\n### Components\nDependency injection involves four roles:\n1. the service object(s) to be used\n2. the client object that is depending on the service(s) it uses\n3. the interfaces that define how the client may use the services\n4. the injector, which is responsible for constructing the services and injecting them into the client\n    - The injector may be referred to by other names such as: assembler, provider, container, factory, builder, spring, construction code, or main\n\nAs an analogy,\n1. `service` - an electric, gas, hybrid, or diesel car\n2. `client` - a driver who uses the car the same way regardless of the engine\n3. `interface` - automatic, ensures driver does not have to understand details of shifting gears\n4. `injector` - the parent who bought the car for the driver and decided which kind\n\nAny object that may be used can be considered a service. Any object that uses other objects can be considered a client. The names have nothing to do with what the objects are for and everything to do with the role the objects play in any one injection.\n\n## Pros and Cons\n### Pros\n- The client becomes an entity whose behavior is fixed, yet configurable. The client may act on anything that supports the intrinsic interface the client expects\n- Because clients are more independent, they are easier to unit test. They are more like proper modules, that we can test in isolation, using stubs and mocks to simulate that module interacting with other modules.\n- DI can be an effective way to refactor legacy code, because it does not require any change in code behavior.\n\n### Explanation of terminology\nAn \"Injection\" is the basic unit of dependency injection, and it works in the same way that \"parameter passing\" works\n- Referring to \"parameter passing\" as an injection carries the added implication that it is being done to isolate the client from details\n- An injection doesn't care about how the passing is accomplished. For instance, it doesn't care if it is passed by value or by reference.\n- An injection is about what is in control of the passing (never the client)\n\nThe DI pattern involves an object receiving other objects that it depends on.\n- These \"other objects\" are called dependencies\n\nConsider the client-server relationship.\n- Here, the client is the object that receives, and the server is the object that is passed (ie. \"injected\")\n- The code that passes the service to the client is called the *Injector*.\n- Instead of the client specifying which service it will use, the injector tells the client what service to use. The \"injection\" refers to the passing of a dependency (a service) into the object (a client) that would use it.\n\n## Examples\n\n### Desktop/Laptop Analogy\nWithout DI: You have a laptop computer and you accidentally break the screen. And darn, you find the same model laptop screen is nowhere in the market. So you're stuck.\n\nWith DI: You have a desktop computer and you accidentally break the screen. You find you can just grab almost any desktop monitor from the market, and it works well with your desktop.\n\nYour desktop successfully implements DI in this case. It accepts a variety type of monitors, while the laptop does not, it needs a specific screen to get fixed.\n\n### NestJS Constructor Dependency Injection\nThis highlights the fact that `AutomationService` is dependent on httpService: `httpService` is a dependency that is being injected into the class:\n\n```ts\nexport class AutomationService {\n  private capabilityCache: Map<string, Record<string, unknown>>\n  constructor(private httpService: HttpService) {\n    this.capabilityCache = new Map()\n  }\n  // injecting httpService via the constructor gives our AutomationService the ability to call methods on the httpService class\n}\n```\n\n# UE Resources\n- [Expalantion with Nodejs](https://stackoverflow.com/questions/9250851/do-i-need-dependency-injection-in-nodejs-or-how-to-deal-with)\n- [DI with Node](https://khalilstemmler.com/articles/tutorials/dependency-injection-inversion-explained/)\n- [DI in Javascript with Sam](https://sammeechward.com/dependency-injection-in-javascript/)\n","n":0.032}}},{"i":1081,"$":{"0":{"v":"Decoupling","n":1},"1":{"v":"\nTight coupling has many causes:\n- Mutation vs immutability\n- Side-Effects vs purity/isolated side-effects\n- Responsibility overload vs Do One Thing (DOT)\n- Procedural instructions vs describing structure\n- Class Inheritance vs composition\n\nImperative and object-oriented code is more susceptible to tight coupling than functional code.\n- pure functions are less vulnerable to tight coupling by nature.\n\n## Should this code be decoupled?\nWhen considering whether or not code should be decoupled, you have to consider both modules and ask if there is a reasonable likelihood that one could exist without the other. It is easy to fall into the trap of only considering if one module should ever need to exist on its own, while completely ignoring the other one.\n\n### Illustration of this thought\nconsider the case where we had considered adding the is_subscribed argument to the register_user function.\n\nMy first point of view was that \"we should not include this as an argument in the function, because it couples the logic of registering a new account with the logic of subscribing that user to a mailing list. What if we want to register a user without having to decide about subscribing? Why is this logic coupled?\"\n\nThis is how I first reasoned about it, but realistically, I should have looked at the other side of it too: \"is there any place in the code that we'd want to set a user as subscribed, without registering them also?\".\n\nThe lesson is: when considering questions of if code should be decoupled, look at all modules that are being decoupled together, and ask \"will any of these modules need to be used without the other?\"\n\n* * *\n\nCoupled code is not necessarily an anti-pattern. You have to ask if these modules should live together under all circumstances. Would it make sense for one to exist without the other? (again, thinking about it from both angles)\n- ex. webpage data. We can introduce 3 separate hooks to the page, each which gets a different piece of data, or we can wrap these 3 hooks into a separate hook, which couples the 3 hooks together, and indirectly to the page (since this is a very specific set of data, we can now consider coupled to the concept of this particular page, which requires this particular set of data).\n    - anal. Think of how a Cable Management Sleeve is used. We wrap it around multiple cables to \"abstract away\" the need to concern ourselves with each cable. We would only ever wrap cables that we are pretty sure would stay together. If we had only one charging cable for our laptop and had to carry it around wherever we go, then we wouldn't wrap this cable in the sleeve. This analogy helps shows that there are costs and benefits to each strategy.\n\nA common pattern is that software starts out built in a decoupled way, but eventually becomes more decoupled over time.\n- ex. consider 2 services: OrderService and InvoiceService. The OrderService gets called the a user signals intent to buy, and then it calls the InvoiceService. For all intents and purposes, these 2 are decoupled. However, with this architecture the OrderService has the burden of calling the downstream service's API. In the simple case where we have InvoiceService, that is no problem. However, once we add more, the burden gets heavier and heavier. Soon it becomes apparent that there is a tight coupling between OrderService and the other services that sit downstream from it.\n","n":0.042}}},{"i":1082,"$":{"0":{"v":"Data Source","n":0.707},"1":{"v":"\n### What is it\nA data source generally sits on a server, and acts as the broker between the server and the data from the data layer. A data source offers generic methods that declaratively show what information the data source can provide. How the data source actually accesses that data is abstracted away from those who use the data source.\n- you can think of a *data source* as the \"client\" on a server, since it's essentially the interface to the data layer.\n\n### Why use it?\nThe benefit here is that if something changes on the data layer (such as the shape of the data), we only need to make adjustments in the data source. Without a data source, every component that consumes data from the database would need to change how it receives that data.\n\n### Other value-adds\nApollo server has a data source class that can help with caching, [[deduplication|storage.deduping]]","n":0.082}}},{"i":1083,"$":{"0":{"v":"Circuit Breaker","n":0.707},"1":{"v":"\nIf an application server connects to a database 100 times per second and the database fails, the same error will be thrown to the application server over and over. Furthermore, the application server will have to wait for the TCP connection to timeout before receiving that error.\n\nBy implementing the circuit breaker pattern, the application server can check the availability of that database. After implementing this pattern, the circuit breaker will detect failures and will prevent the application from trying to perform an action that is doomed to fail.\n\nThe circuit breaker works by recording the state of the external service on a set interval\n\n## Why use it?\n- *Fault Isolation*: The Circuit Breaker isolates the impact of a failing service, preventing the failure from propagating to other parts of the system and causing a cascading effect.\n- *Fast Failure Detection*: The Circuit Breaker quickly detects and responds to failures, reducing the response time and preventing the system from wasting resources on calls to a failing service.\n- *Graceful Degradation*: By providing fallback behavior, the Circuit Breaker enables the system to gracefully degrade in the face of failures, ensuring that some level of functionality is available to clients even when the primary service is unavailable.\n- *Automatic Recovery*: The Circuit Breaker provides an automatic recovery mechanism. After the service has stabilized, it automatically allows requests to pass through again, restoring normal operation.\n\n## States\nThe circuit breaker implements a [[general.patterns.state-machine]] that can exist in 3 different states: closed, open or half-open\n\n### Closed\nThis is the happy path state, and all requests to the service will pass through without incident\n\n### Open\nThis state is enabled once the number of failures increases beyond the configured threshold\n\nIn this state circuit breaker returns an error immediately without even invoking the services\n\n### Half-open\nThis state is enabled after a set period of time elapses of being in the Open state\n\nIn this state, the circuit breaker allows a limited number of requests from the Microservice to passthrough and invoke the operation.\n- If the requests are successful, then the circuit breaker will go to the closed state. \n- If the requests continue to fail, then it goes back to Open state.","n":0.053}}},{"i":1084,"$":{"0":{"v":"Behavioural Patterns","n":0.707},"1":{"v":"\n\"Behavioral patterns are concerned with algorithms and the assignment of responsibilities between objects. Behavioral patterns describe not just patterns of objects or classes but also the patterns of communication between them. These patterns characterize complex control flow that's difficult to follow at run-time. They shift your focus away from flow of control to let you concentrate just on the way objects are interconnected.\"","n":0.126}}},{"i":1085,"$":{"0":{"v":"Strategy","n":1},"1":{"v":"\nThe strategy pattern (a.k.a policy pattern) allows us to write client code that determines which algorithm to select at runtime.\n- instead of implementing a single algoritm, we implement multiple, and based on some condition, select which algorithm to run.\n- doing this allows the calling code to be more flexible and reusable.\n\nIn the following example, we have some client code which needs to update the fields of a food item. The thing each, there are different classes of food items (those that are organic, those that are non-perishable, etc), and they all need to be handled differently. For instance, organic items degrade twice as fast, while non-perishable items don't degrade at all.\n\n```ts\nexport interface FoodItemStrategy {\n  update(item: FoodItem): void;\n}\n\nexport class PerishableFoodItemStrategy implements FoodItemStrategy {\n  update(item: FoodItem): void {\n    if (item.doesItemImproveWithAge()) {\n      item.increaseQuality();\n    } else if (item.getSellInDaysValue() <= 0) {\n      item.decreaseQuality(2);\n    } else {\n      item.decreaseQuality();\n    }\n    item.decrementSellIn();\n  }\n}\n\nexport class OrganicFoodItemStrategy implements FoodItemStrategy {\n  update(item: FoodItem): void {\n    if (item.getSellInDaysValue() <= 0) {\n      item.decreaseQuality(4);\n    } else {\n      item.decreaseQuality(2);\n    }\n    item.decrementSellIn();\n  }\n}\n\nexport class NonPerishableFoodItemStrategy implements FoodItemStrategy {\n  update(item: FoodItem): void {\n    // no-op\n  }\n}\n\n/* Implementation */\nexport class StoreInventory {\n  constructor(public items: FoodItem[]) {}\n\n  private updateFoodItem(item: FoodItem): void {\n    let strategy: FoodItemStrategy;\n    if (item.isItemNonPerishable()) {\n      strategy = new NonPerishableFoodItemStrategy();\n    } else if (item.isItemOrganic()) {\n      strategy = new OrganicFoodItemStrategy();\n    } else {\n      strategy = new PerishableFoodItemStrategy();\n    }\n    strategy.update(item);\n  }\n}\n```","n":0.067}}},{"i":1086,"$":{"0":{"v":"Observer Pattern","n":0.707},"1":{"v":"\nThe intent behind the observer pattern is to define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically.\n\nThe idea with the observer pattern is that an object (the `subject`) maintains a list of all its dependents. Whenever a piece of state in the subject changes, all of the observers are automatically notified. This is normally accomplished by the subject calling the observer's `update()` method.\n- The sole responsibility of a *subject* is to maintain a list of observers and to notify them of state changes by calling their update() operation. \n- The responsibility of *observers* is to register (and unregister) themselves on a subject (to get notified of state changes) and to update their state (synchronize their state with the subject's state) when they are notified. This makes subject and observers loosely coupled.\n    - Observers can be added and removed independently at run-time. This is very similar to [[pub-sub|general.patterns.messaging.pubsub]]\n\nObserver pattern is used in event handling systems that are generally distributed. In this case, the subject is usually a \"stream of events\", while the observers are \"sinks of events\". The observers themselves are physically separated and have no control over their emitted events from the stream-source (ie. subject).\n\nThe observer pattern is well-suited to a process where data arrives from some input that is not available to the CPU at startup, but instead arrives \"at random\" (HTTP requests, GPIO data, user input from keyboard/mouse/..., distributed databases and blockchains, ...).\n\nThe Observer pattern addresses the following problems:\n\n- A one-to-many dependency between objects should be defined without making the objects tightly coupled.\n- It should be ensured that when one object changes state, an open-ended number of dependent objects are updated automatically.\n- It should be possible that one object can notify an open-ended number of other objects.\n\nThe great strength, and weakness, of observer is that control passes from the subject to the observer implicitly.\n- You can't tell by reading code that an observer is going to fire, the only way you can see what's happening is to use a debugger. \n- As a result of a complex chain of observers can be a nightmare to figure out, change, or debug as actions trigger other actions with little indication why. Therefore, we should tend towards using observer pattern in more simple manners to avoid this.\n","n":0.051}}},{"i":1087,"$":{"0":{"v":"Observable","n":1},"1":{"v":"\nAn observable is a [[stream|general.patterns.streaming]] object, which has methods for interacting with that stream.\n- because it is a stream, the data it represents is naturally asynchronous. Therefore, we interact with the observable in an asynchronous way, and the observable listens to a *producer* of data.\n\nThe problem is this: we can consider an array where we already know all the values as eager, and an array that receives values (ie. increases length) at a set interval (1s) as lazy. Normally, we perform data processing in an eager way, since the data is processed immediately as it's received, which is instant. What if we are in a position where the array grows over time? It would be beneficial if we could call a method on an array value as it enters the array. In this sense, we need to subscribe to the array to execute a method when the array grows.\n\nobservables are conceptually similar to a fancy event emitter.\n\nThere are 2 sides to an observable: producer and consumer\n- **Producer** - adds to the array\n    - ex. button clicks add that click event to the array\n- **Consumer** - calls the function on the new array item\n    - ex. calls `console.log` in response to the new click event\n\nJust as promises abstract time away from our concern for a single asynchronous operation, observables abstract time away from a set of data (eg. array)\n\nIf you combine the functionality of an Observer and an Observable, you get a Subject\n- subject: you can send to it and receive from it.\n- Observable: you can receive from it only.\n\n- each operator on an Observable returns a new Observable, meaning they are chainable (this is known as a *stream*)\n\n- A map(..) on an array runs its mapping function once for each value currently in the array, putting all the mapped values in the outcome array. A map(..) on an Observable runs its mapping function once for each value, whenever it comes in, and pushes all the mapped values to the output Observable.\n\n\n## Characteristics\n- They are time-independent (ie. lazy)\n- They are mostly used in asynchronous data streams, like web sockets or multiple concurrent api calls\n- An `Observer` subscribes (ie. is consumer) to an `Observable`\n  - an observer is a collection of callbacks\n\n### Differences with other async approaches\nThe following table shows what role observables fulfill. When we want to get a single value for a synchronous action, we set the value to a variable. When we want to get a single value for an async action, we use promises. When we want to get the value for an array synchronously, we use an array. Finally, when we want to get the value of an array of values asynchronously, we use observables.\n- [[rxjs]] has a function `lastValueFrom`, which converts an observable to a promise\n  - when we call that function, it will subscribe to the observable. Once it is complete, it resolves the returned promise with the last value from the observed stream.\n\n|          |sync    |async     |\n|----------|--------|----------|\n|single    |variable|promise   |\n|collection|array   |observable|\n\n- while [[promises|js.lang.promises]] handle data processing in an eager way, an observable does it lazily. That is, it sits around waiting for the event to happen, then it will respond.\n- In a traditional workflow, we take a pull approach by calling a function to do something for us. That is, the consumer decides when it's going to get the data. Functions are naturally *pull*, because whoever uses the function is \"pulling\" out a single return value from its call.\n\nObservables are like arrays because they represent a collection of events but are also like promises as they’re asynchronous: each event in the collection arrives at some indeterminate point in the future.\n- This is distinct from a collection of promises (like `Promise.all`) as an observable can handle an arbitrary number of events, and a promise can only track one thing.\n\nObservables are lazy Push collections of multiple values. They fill the missing spot in the following table:\n\n|      | Single   | Multiple   |\n|------|----------|------------|\n| Pull | Function | Iterator   |\n| Push | Promise  | Observable |\n- They *push* with `.next()`\n\nAn observable can be used to model clicks of a button. It represents all the clicks that will happen over the application’s lifetime, but the clicks will happen at some point in the future that we can’t predict.\n```js\nlet myObs$ = clicksOnButton(myButton);\n```\n\n* * *\n\nWhen we create an observable, we add subscribers (ie. observers) to it\n```ts\n{\n  subscriber.next(1);\n  subscriber.next(2);\n  subscriber.next(3);\n  setTimeout(() => {\n    subscriber.next(4);\n    subscriber.complete();\n  }, 1000);\n};\n```\n\nWhile an observable is by design a type of stream, consider the different scales at which a stream like [[kafka]] sits compares to where observables sit.\n- A Kafka stream (ie. topic) has potentially many subscribers to its set of data. Many are consuming the same set.\n- An observable stream is local to one set of data for one user (ie. they are said to be *cold observables*).\n  - ex. Think of movie data of Netflix represented as an observable. When you press play, you are effectively subscribing to an observable. You are the only subscriber of this stream (unicast) and have full control of when it starts/stops. When someone in a different household watches the same movie as you a brand new observable is created and they are subscribed to it.\n\n- If you wanted to subscribe to the reactive way of programming, then you could just \"observable all the things\"\n- Observables are cancellable.\n\n### Example: Weather report\nConsider that if you want to know the weather for the day, you can either look it up to get a single response, or you can turn on your radio and get a constant stream of updates. That is, as long as you are subscribed to it (ie. as long as the radio is turned on), you will get all the latest information about the weather. \n- While the weather itself is observable, in this example the user is actually subscribed to the radio, which is another observable. \n- Behind that, the radio gets its data from a weatherman, which is another observable. \n- Even more beyond that, the weatherman gets his data from a meteorologist report, yet another observable (here we say the data received by the weatherman is a function of the data from the meteorologist report). \n- the meteorologist report gets its data from the instruments (baromoter, wind gauge etc.), which is another observable, \n- finally, the instruments get their data from the weather itself, yet another observable.\n- all in all this whole sequence shows at least 5 observables. The radio is the `output` observable, and the weather itself is the `source` observable. All the observables in between represent the `PIPE FUNCTION`.","n":0.03}}},{"i":1088,"$":{"0":{"v":"Iterator","n":1},"1":{"v":"\nAn iterator gives us the ability to say \"get me the next item in the list\", without us having to know anything about the underlying implementation (ie. whether it's a list, stack, tree etc.). \n- Therefore, the type of list is immaterial (ie. could be an array, linked list, object etc.)\n\nThe iterator is what hands us the next item in the sequence.\n\nThe iterator manages the state of which element of the list we are currently on (known as the `Head`). Because of this `Head`, an iterator is fully aware of its location.\n- More complex iterators may let us move forwards and backwards, but they all must satisfy this simple requirement of knowing where it is.\n\nSince a container is dumb, we need a way to traverse over elements of a collection without going over the same ones over and over.\n- if our collection was based on a list, this might be a simple solution, but this problem gets more complex as we use more complex data structures\n\t- ex. imagine we had a tree and by default we used depth-first traversal. To traverse this, our class implements a `next()` function which gets the next item. Now imagine that something then required us to use breadth-first. Our implementation details of the `next()` function would have to change, and we would wind up muddying up our class with different traversal algorithms. Since different types of collections have different ways of accessing their elements, our only choice would be to couple our code to the specific collection class.\n\nThe purpose of an iterator is to extract the logic of how to traverse a collection into a separate object.\n- We do this by defining different *iterators* (e.g. `DepthFirstIterator`, `BreadthFirstIterator`) which each implement their own methods (e.g. `next()`), with all iterators providing the same interface.\n- this way, if we need to change the way that our collection is traversed, we just need to use a different iterator. No other areas of the code need to be touched.\n\nAn *iterable* is a collection that we can use an *iterator* on.\n\nIterators implement the *open/closed principle* since using them enables us to change how we traverse our collections / change the collections themselves, without having to change any other consuming code.\n\n## Resources\n- [Iterator in Typescript](https://refactoring.guru/design-patterns/iterator/typescript/example)","n":0.052}}},{"i":1089,"$":{"0":{"v":"Batching","n":1},"1":{"v":"\nBatching is the process of taking a group of requests, combining them into one, and making a single request with the same data that all of the other queries would have made. \n- This is usually done with a timing threshold. For example, with a threshold of 50ms, if a query is made from a component, instead of making the query immediately, the client waits 50ms. If any other queries are requested in that 50ms, all of those additional queries are requested at the same time, rather than separately.\n\nIn the batch processing world, the inputs and outputs of a job are files (perhaps on a distributed filesystem).\n\nNaturally, batched operations are always as slow as the slowest operation in the batch.\n- The downside of this comes if one of the underlying services is slower than the rest, the effects may be felt across the whole client app.\n\nAdditionally, batching makes it much harder to debug network traffic.\n\nBecause of its drawbacks, it's not recommended to batch unless there are performance issues that can't be resolved through some other means (like [[deploy.distributed.CDN]] caching)\n","n":0.075}}},{"i":1090,"$":{"0":{"v":"SSR (Server Side Rendering)","n":0.5},"1":{"v":"\nThe idea for SSR is that there is a large chunk of our website (if not all) that is static. It is expensive to ship a bundle of javascript to the client, and expect the client to do all the work of parsing it and converting it to html. The question becomes: \"what if we could just pre-render this react into html beforehand on the server, and give that content to the client when it is requested? That way, the content can be served as fast as if we were using plain HTML/CSS/Javascript.\n\nThe most common use case for server-side rendering is to handle the initial render when a user (or search engine crawler) first requests our app. When the server receives the request, it renders the required component(s) into an HTML string, and then sends it as a response to the client. From that point on, the client takes over rendering duties.\n\n## CSR vs SSR\n### CSR Approach\nUsing CSR, the server will send a mostly empty HTML file to begin with. Through links, it will build up the page through a series of further HTTP requests.\n\n1. Browser requests `example.com/index.html`\n2. Server responds with `<script/>` tag at end of `<body/>`\n3. Browser makes an API request for the content in those script tags\n4. Server responds with that content.\n5. Browser makes async requests for Javascript bundles.\n    - note: these bundles can be cached to speed things up in the future.\n\nCons of CSR:\n- Loading content may take a while because requests have to travel all the way to the server, which can be very far away.\n- SEO takes a hit because all search engines see is an empty HTML file.\n\n### SSR Approach\nIn SSR, the whole web page is compiled on the server. The HTML is completely populated with the content before it is sent to the client.\n\n1. Browser requests `example.com/index.html`\n2. Server makes co-located API requests to populate the HTML \"template\" with the data it needs\n3. Server sends that populated HTML back to the client.\n\nCons of SSR:\n- A page will have to be rendered on the server and reloaded every time a new page on the site is visited, which will lead to full page reloads.\n- The server will receive frequent requests, which can easily lead to the server getting flooded with requests and slowing down.\n\n## Implementations\n\n### with React\nWith simple client-side rendering, we have a root node `<div id=\"root\">` to which the entire React app gets attached to. This node is by default empty.\nWith SSR, this node has some content by default. But, instead of replacing that content, we want React to attach a running application to it.\n\nThis is where hydration comes in. During hydration, React works quickly in a virtual DOM to match up the existing content with what the application renders, saving time from manipulating the DOM unnecessarily. It is this hydration that makes SSR worthwhile.\n\nThere are two big rules to hydrating an application in React.\n1. The initial render cycle of the application must result in the same markup, whether run on the client or the server.\n2. We must call ReactDOM.hydrate instead of ReactDOM.render in order to instruct React to hydrate from our SSR result.\n\n### with Redux\n- When using Redux with server rendering, we must also send the state of our app along in our response, so the client can use it as the initial state. This is important because, if we preload any data before generating the HTML, we want the client to also have access to this data. Otherwise, the markup generated on the client won't match the server markup, and the client would have to load the data again.\n- To send the data down to the client, we need to:\n\n1. create a fresh, new Redux store instance on every request;\n2. optionally dispatch some actions;\n3. pull the state out of store;\n4. and then pass the state along to the client.\nRedux's only job on the server side is to provide the initial state of our app.\n\nOn the server, you will want each request to have its own store, so that different users get different preloaded data.\n\n### with Gatsby\nSometimes Gatsby is referred to as doing SSR. More realistic is that its doing SPG (Static site generation)\n\n# UE Resource\n- [Good breakdown of SSR](https://www.freecodecamp.org/news/react-server-components/)\n","n":0.038}}},{"i":1091,"$":{"0":{"v":"IoC (Inversion of Control)","n":0.5},"1":{"v":"\nIoC means letting other code call you rather than insisting on doing the calling.\n- ex. An example of IoC without dependency injection is the template method pattern. Here, polymorphism is achieved through subclassing (inheritance).\n\nIf you follow these simple two steps, you have done inversion of control:\n1. Separate the *what-to-do* part from the *when-to-do* part.\n2. Ensure that *when-to-do* part and *what-to-do* part know as little as possible about each other\n\n### IoC Container\nAn IoC Container (a.k.a. DI Container) is a framework for implementing automatic dependency injection. It manages object creation and its life-time, and also injects dependencies to the class.\n- ex. [[nestjs]] provides an IoC container. The framework works by enabling us to create [[providers|nestjs.providers]] and register them (via a [[module|nestjs.modules]]) with the IoC container via the `@Module()` decorator.\n\nThe IoC container creates an object of the specified class and also injects all the dependency objects through a constructor, a property or a method at run time and disposes it at the appropriate time. This is done so that we don't have to create and manage objects manually.\n\nThe IoC container must provide easy support for the following DI lifecycle:\n1. *Register*: The container must know which dependency to instantiate when it encounters a particular type. This process is called registration. Basically, it must include some way to register type-mapping.\n2. *Resolve*: When using the IoC container, we don't need to create objects manually. The container does it for us. This is called resolution. The container must include some methods to resolve the specified type; the container creates an object of the specified type, injects the required dependencies if any and returns the object.\n3. *Dispose*: The container must manage the lifetime of the dependent objects. Most IoC containers include different lifetimemanagers to manage an object's lifecycle and dispose it.\n\n## Examples\n\n### Email Example\n#### The Scenario\nImagine we had a module called `FortuneCookieEmailer`, which sends email. It uses some kind of service to do the work of actually sending the emails, `BillsBulkEmailService`. Here, the `FortuneCookieEmailer` code now contains a direct reference to the `BillsBulkEmailService`.\n\nThis works, but imagine our program expands and now `FortuneCookieEmailer` has to sometimes use `KylesSpammingService`. We would have to go into the `FortuneCookieEmailer` module and introduce logic that would make it conditionally use either `BillsBulkEmailService` or `KylesSpammingService`. Now we need to have some kind of config file somewhere, or some kind of extra \"switch\" for our `FortuneCookieEmailer` module. The bad thing is that we are adding the job of \"deciding which to use\" to a module whose sole responsibility should be fortune cookie emails.\n\n#### The solution\nInstead of `FortuneCookieEmailer` going out and getting a reference to the `BillsBulkEmailService` or `KylesSpammingService` itself and then using that, `FortuneCookieEmailer` should be provided with a reference to the desired service by something outside of `FortuneCookieEmailer`.\n\n#### Summary\n- First situation: FortuneCookieEmailer actively goes out and gets the service that it needs.\n- Second (IoC) situation: Someone else gives FortuneCookieEmailer the service that it should use.\n\nThe benefit of the second approach is that we can configure our application to use different email services without having to even touch `FortuneCookieEmailer`\n\n### Text Editor Examples\nsay your application has a text editor component and you want to provide spell checking. Your standard code would look something like this:\n```cs\npublic class TextEditor {\n\n\tprivate SpellChecker checker;\n\n\tpublic TextEditor() {\n\t\t\tthis.checker = new SpellChecker();\n\t}\n}\n```\nWhat we've done here creates a dependency between the TextEditor and the SpellChecker. In an IoC scenario we would instead do something like this:\n\n```cs\npublic class TextEditor {\n\n\tprivate IocSpellChecker checker;\n\n\tpublic TextEditor(IocSpellChecker checker) {\n\t\t\tthis.checker = checker;\n\t}\n}\n```\n\nIn the first code example we are instantiating `SpellChecker` (`this.checker = new SpellChecker();`), which means the `TextEditor` class directly depends on the `SpellChecker` class.\n\nIn the second code example we are creating an abstraction by having the `SpellChecker` dependency class in `TextEditor`'s constructor signature (not initializing dependency in class). This allows us to call the dependency then pass it to the `TextEditor` class like so:\n\n```cs\nSpellChecker sc = new SpellChecker(); // dependency\nTextEditor textEditor = new TextEditor(sc);\n```\n\nNow the client creating the TextEditor class has control over which SpellChecker implementation to use because we're injecting the dependency into the TextEditor signature.\n\n### Analogy to understand IoC\nInversion of Control, (or IoC), is about getting freedom (You get married, you lost freedom and you are being controlled. You divorced, you have just implemented Inversion of Control. That's what we called, \"decoupled\". Good computer system discourages some very close relationship.) more flexibility (The kitchen in your office only serves clean tap water, that is your only choice when you want to drink. Your boss implemented Inversion of Control by setting up a new coffee machine. Now you get the flexibility of choosing either tap water or coffee.) and less dependency (Your partner has a job, you don't have a job, you financially depend on your partner, so you are controlled. You find a job, you have implemented Inversion of Control. Good computer system encourages in-dependency.)\n\nWith the above ideas in mind. We still miss a key part of IoC. In the scenario of IoC, the software/object consumer is a sophisticated framework. That means the code you created is not called by yourself. Now let's explain why this way works better for a web application.\n\nA traditional software framework will be like a garage with many tools. So the workers need to make a plan themselves and use the tools to build the car. Building a car is not an easy business, it will be really hard for the workers to plan and cooperate properly. A modern software framework will be like a modern car factory with all the facilities and managers in place. The workers do not have to make any plan, the managers (part of the framework, they are the smartest people and made the most sophisticated plan) will help coordinate so that the workers know when to do their job (framework calls your code). The workers just need to be flexible enough to use any tools the managers give to them (by using Dependency Injection).\n\nAlthough the workers give the control of managing the project on the top level to the managers (the framework). But it is good to have some professionals help out. This is the concept of IoC truly come from.\n\nDependency Injection and Inversion of Control are related. Dependency Injection is at the micro level and Inversion of Control is at the macro level. You have to eat every bite (implement DI) in order to finish a meal (implement IoC).\n\n\n# UE Resources\n[Martin Fowler on IoC](https://martinfowler.com/bliki/InversionOfControl.html)\n","n":0.031}}},{"i":1092,"$":{"0":{"v":"DTO","n":1},"1":{"v":"\nA DTO is an object that defines how the data will be sent over the network. \n- Its purpose is to carry data between processes in order to reduce the number of method calls.\n\nThe DTO is used to encapsulate data we send from one endpoint to another. Use DTO to define interfaces for input and output for endpoints in your system\n\nWhen we are working inter-process, each call is expensive. As a result, we want to pack more data in each call.\n- we could accomplish this by adding many parameters, but this gets unwieldly fast.\n- instead, we can create a DTO to hold all of the data for the call.\n\nUsually an assembler is used on the server side to transfer data between the DTO and any domain objects.\n\nDTOs are normally implemented as POJOs, and are flat data structures that contain no business logic. \n\n## UE Resources\n- https://www.baeldung.com/java-dto-pattern","n":0.083}}},{"i":1093,"$":{"0":{"v":"Complex Event Processing","n":0.577},"1":{"v":"\n*Event processing* involves tracking and processing streams of data about things that happen (ie. events) and deriving a conclusion from them.\n\n*Complex Event Processing* involves gleaning useful information from the event streams as they arrive.\n- the idea is to identify meaningful events\n  - here, \"meaningful\" means as it relates to the business logic of an application\n- these events may be originating from various layers of an organization, such as sales leads, orders or customer service calls. Or, they may be news items, text messages, social media posts, stock market feeds, traffic reports, weather reports etc.\n\nCEP relies on a number of techniques, including:\n- Pattern recognition of the events\n- Event abstraction\n- Event filtering\n- Event aggregation and transformation\n- Modeling event hierarchies\n- Detecting relationships (such as causality, membership or timing) between events\n- Abstracting event-driven processes\n\n### Example: Church Monitor\nImagine we have a monitoring system in a church. One day, the system receives the following 3 events at around the same time:\n1. church bells ringing.\n2. the appearance of a man in a tuxedo with a woman in a flowing white gown.\n3. rice flying through the air.\n\nCEP as a technique helps discover complex events by analyzing and correlating other events\n- From these events the monitoring system may infer a complex event: a wedding","n":0.07}}},{"i":1094,"$":{"0":{"v":"Objects","n":1}}},{"i":1095,"$":{"0":{"v":"Value Object","n":0.707},"1":{"v":"\nA value object is a small object that represents a simple [[general.terms.entity]]\n\nTwo value objects are considered equal when they have the same value. That is, they don't necessarily have to be the same object, since 2 different objects can have the same value.\n- another way of saying this is \"equality is not based on identity\".\n\nConsider that in [[C|c]], there is no concept of a string; only a [[character array|c.lang.types.array]]. Therefore, strings are abstractions on top of character arrays that serve 2 purposes:\n1. they reduce complexity\n2. they enforce ubiquitous language\n\nThis is precicely what Value Objects do for us. \n\nValue Objects are simply values that exist in our system, and they generally come from the ubiquitous language.\n- generally, applications will use primitives like float for money, string for email... but in DDD, we shouldn't do this. Instead, we should be using value objects.\n- Some say that primitives should not exist in our objects at all.\n\nspec: value objects are made from primitives.\n\nValue objects are among the building blocks of [[DDD|general.principles.DDD]].\n\n### Examples of Value Objects\nobjects representing...\n- money\n- date range\n\n### Features of Value Objects\n- They are fungible\n    - ex. $100 in a system can be replaced with any other $100 without consequence\n- They are immutable.\n    - ex. $100 in a system shouldn't be able to be changed to $50\n    - this is required for the implicit contract that two value objects created equal, should remain equal.\n    - It is also useful for value objects to be immutable, as client code cannot put the value object in an invalid state or introduce buggy behaviour after instantiation\n- Rich in domain logic\n- Auto-validating\n    - the business logic is baked in. For instance, if we have our own `money` type, that is implemented in floats, the value object itself should implement the logic that only 2 decimal places can exist, negative values don't exist etc.\n\n### Implementation\n```java\npublic class StreetAddress\n{\n    public StreetAddress(string street, string city)\n    {\n        Street = street;\n        City = city;\n    }\n\n    public final string Street { get; }\n    public final string City { get; }\n}\n```\n","n":0.055}}},{"i":1096,"$":{"0":{"v":"Learning","n":1},"1":{"v":"\nAs a skilled guitarist, you might watch a 10 minute video and walk away with,\n> “Ah, so one major pentatonic scale cannot be applied universally to a chord progression. To sound good I have to match the scale to the specific chord being played.\n> I can accomplish that by using CAGED patterns to match up with the chords in a progression.”\n\nWhat if you jump into learning about AWS before learning the periphery concepts first? You will never learn this way, because knowledge builds on top of other knowledge.\n\nTo understand... \n- PassportJS - you need to know how auth works, how sessions works, or how/what OAuth is\n- Docker - you need to know what a virtual machine was or why it is useful, otherwise you won't understand containers.\n- AWS - you need to understand what “cloud architecture” so, otherwise you can't understand what it meant to deploy code in the cloud.\n- HTML template engines - you need to understand browsers, http, servers, databases, and client-side vs server-side rendering\n\n# Resources\n[RealWorld Example Apps: The same app (same API) built using different technologies](https://codebase.show/projects/realworld)\n","n":0.075}}},{"i":1097,"$":{"0":{"v":"New Codebase","n":0.707},"1":{"v":"\n1. Understand at a high level what the structure looks like. How is the folder structure laid out? What have the designers of the structure seem to have emphasized in its architecture?\n- This must be understood in the context of the codebase you are analzing. A Java codebase that uses Springboot is fundamentally going to look different than a React+Express app. Understand the main philosophies of the framework/language that forms the context of the codebase.\n- ex. Sveltekit and Next.js apps use filesystem-based routing. This gives a sort of pseudo-sitemap of the site, and gives you a strong starting point to understanding it all at a high level.\n\nCheck out what network requests are sent when you do certain actions\n","n":0.092}}},{"i":1098,"$":{"0":{"v":"Programming Languages","n":0.707},"1":{"v":"\n## Compiled vs Interpreted Languages\n- A compiler figures out everything a program will do, turns it into “machine code” (a format the computer can run really fast), then saves that to be executed later.\n- An interpreter steps through the source code line by line, figuring out what it’s doing as it goes.\n\nTechnically any language could be compiled or interpreted, but one or the other usually makes more sense for a specific language. \n- Generally, interpreting tends to be more flexible, while compiling tends to have higher performance.\n- a lot of language design decisions are affected by the decision to make a language interpreted or compiled \n    - for example, static typing is a big benefit to compiled languages, but not so much for interpreted ones.\n\nInterpreted languages are generally easier design, build and learn\n\n### Writing your own language\nIf you are writing an interpreted language, it makes a lot of sense to write it in a compiled one (like C, C++ or Swift) because the performance lost in the language of your interpreter and the interpreter that is interpreting your interpreter will compound.\n\nIf you plan to compile, a slower language (like Python or JavaScript) is more acceptable. Compile time may be bad, but that isn’t nearly as big a deal as bad run time.\n\n#### From source file to execution\nA programming language is generally structured as a pipeline. The first stage is a string containing the entire input source file. The final stage is something that can be run. This will all become clear as we go through the Pinecone pipeline step by step.\n\n##### Lexer\nThe first stage. The lexer is supposed to take in a string containing an entire files worth of source code and spit out a list containing every token (a token is a unit of the language, like a function name or an operator).\n- also included in tokenization is determining how whitespace is handled, brackets, semi-colons etc. Essentially, we are focused on the atomic units of the language here.\n\nThe lexer provides all details of the source file that are needed by future stages.\n\n##### Parser\nThe parser turns a list of tokens into a tree of nodes. \n- A tree used for storing this type of data is known as an [[AST|general.lang.AST]].\n\n## Terms\n### Expression\nAn expression is a piece of code that will be evaluated to produce a value, or a piece of code that is already at its furthest level of evaluation (string, number etc) \n- ex. `5`\n- ex. `\"hello\"`\n- ex. `1 / 2` evaluates to `0.5`\n- ex. `i++`\n- ex. `'A' + 'string'`\n- ex. `a && b`\n- ex. `a ? b : c`\n- ex. `function() {}`\n- ex. `window.resize()`\n\n### Assignment\n- any time there is a LHS and RHS separated by `=`\n","n":0.047}}},{"i":1099,"$":{"0":{"v":"Types","n":1}}},{"i":1100,"$":{"0":{"v":"String","n":1},"1":{"v":"\n- String is collection of only char datatype.\n- string size can be changed if it is a char pointer.\n- The last character of a string is a null – ‘\\0’ character.\n\nWhen manipulating a string (ex. Algorithm) remember that a string is nothing but a character array\nEx. `['c', 'h'...]`\n\nwhenever we have a string in any platform we have an encoding being used. Even if we didn't specify any encoding:\n- either the default encoding of the operating system will be used\n- or it will be taken from a configuration parameter","n":0.107}}},{"i":1101,"$":{"0":{"v":"Loops","n":1},"1":{"v":"\n### Nested for-loops\nGenerally, they are a bad idea, but not always. They are perfect if they describe the correct algorithm e.g. if we need to access every value in a matrix\n\n#### Tips for avoiding\nDoes sorting the array first help you?\n","n":0.158}}},{"i":1102,"$":{"0":{"v":"If Statements","n":0.707},"1":{"v":"\n### Testing for non-equality\nWhen testing that 2 values are *not* equal to each other, we have to be careful with the logic. Note that checking for non-equality is not the opposite of checking for equality.\n\nConsider that the follow equality test is strict. It will only pass the test if they are exactly the same:\n```js\nif (myVersion.title === yourVersion.title) \n```\n\nOn the other hand, when we test for non-equality, there are additional values that can cause our test to pass.\n- ex. if myVersion is undefined, then we would be testing `(undefined !== yourVersion.title)`, which is `true`. However, this is probably not what we meant to test for. In the case where `myVersion` does not even exist, we'd probably want to exit and handle it differently. Otherwise, the assumption is made that we should handle it as if we were checking non-equality between 2 different version titles.\n```js\nif (myVersion?.title !== yourVersion.title) \n\n// to fix this, check truthiness of `myVersion` first\nif (myVersion && myVersion?.title !== yourVersion.title) \n```\n\n\n# UE Resources\n[Getting rid of nested ifs](https://lawyerdev.medium.com/i-never-write-nested-ifs-e4e91a5440ee)\n","n":0.077}}},{"i":1103,"$":{"0":{"v":"Generator","n":1},"1":{"v":"\n\nA generator can be used to control the iteration behaviour of a loop\nA generator is also an iterator, meaning it can be looped over\n\nA generator is very similar to a function that returns an array. A generator: \n- has parameters \n- can be called \n- generates a sequence of values\n\nHowever, instead of building an array and returning all values at once, a generator yields the values one at a time, requiring less memory, and has the added benefit of allowing the caller to access the first few values immediately, instead of having to wait for the whole array to return.\n- In short, a generator looks like a function but behaves like an iterator.\n","n":0.094}}},{"i":1104,"$":{"0":{"v":"Feat","n":1}}},{"i":1105,"$":{"0":{"v":"Symbol","n":1},"1":{"v":"\nA `Symbol` is a primitive type found in programming languages that are used for specifying a specific resource, due to the fact that an instance of `Symbol` is guaranteed to be unique.\n\nYou can think of a symbol as a lightweight immutable string constant.\n","n":0.152}}},{"i":1106,"$":{"0":{"v":"Interface","n":1},"1":{"v":"\nAn interface fulfills the principle of \"duck typing\"\n\n```ts\ninterface inputObjectI = {\n\tlabel: string\n\toptionalVal: string?\n\treadonly readOnlyVal: number // value is immutable post-declaration\n}\n```\nOnce we define the interface, it can then be used as a type.\n\nFunction interfaces can also be created\n- ex. we have a fn with 2 params of type `String` and `String`, and the function returns `Boolean`\n```ts\ninterface search {\n\t(source: string, substring: string): boolean\n}\n```\n\n* * *\n\n## Different perspectives of Interface\nThe purpose of an interface changes depending on if we are looking at it from an Object-Oriented perspective or a Function perspective.\n\n### Interfaces in Object-Oriented Programming\nIn OOP context, an interface defines a list of methods that a class needs to implement in order to conform to the interface.\n- often used to fulfill the Dependency Inversion Principle from SOLID design principles.\n- The dependency inversion principles says that it’s much better to have interfaces than concrete classes as dependencies.\n\nIn general, the OOP part of interfaces is their ability to express some *required behavior*, as opposed to *data* in the functional paradigm.\n\n### Interfaces in Functional Programming\nIn functional programming context, interfaces are used not to enumerate methods of an object, but to describe the shape of the data contained by the object.\n- In this context, generic interfaces are used to describe a data shape when you don’t know or care about the exact type of some properties of the interface. It often makes sense when the data types contain some value.\n","n":0.066}}},{"i":1107,"$":{"0":{"v":"Generic","n":1},"1":{"v":"\nThink of a generic as a \"type which takes in a parameter\"\n- contrast this with the more familiar \"function which takes in a parameter\"\n\nThink of a generically typed function as a prototype. When you define a generic `map()` function, it is basically saying \"I don't care if you pass in an array of strings, or integers, or objects... just pass it in, and I will make sure the function works with that type\". Conceptually, this is like creating many different versions of a function with different type parameters.\n\nAs exemplified by `map`, functions that take arrays as parameters are often generic functions. If you look at the typings for the lodash library, you will see that nearly all of them are typed as generic functions. Such functions are only interested in the fact that the argument is an array, they don’t care about the type of its elements.\n\nWhen you call a function, you pass it arguments. You can think of a generic as an additional thing you pass to the function. But instead of passing a value, you pass a type to it.\n- This type is inferred through whatever the type actually is during execution.\n\nConsider the `identity` function, which most fundamentally explains the value of generics\n```ts\nfunction identity<T>(arg: T): T {\n    return arg\n}\n```\n\nWe can describe the above function as \"the generic function `identity` takes a type parameter `T`, and an argument `arg` (which is of type T), and returns a value of type `T`.\n\nTo write an algorithm generically is to think of the algorithm in terms of *types-to-be-specified-later*. When we want to use these algorithms, the arguments that we pass *inform* the generic function, and the algorithm's behavior will be tailored.\n- this approach permits writing common functions or types that differ only in the set of types on which they operate when used.\n\nWhat they are - When you declare a generic, E, for example. You are basically saying *\"I don't know what this will be, it can be anything, so I shall simply refer to it as \"the thing\" and your type is a generic type.\"*\n\nsituation: you have some chunk of code that is identical between 2 functions, yet the function returns 2 different types. for instance, we have a createpizza and createhamburger. there is naturally a lot of overlap between these, but we can’t simply use one function between them because one function returns an object of type pizza and the other returns an object of type hamburger. to solve this, we can use a generic, and define a function called makefood:\n```ts\nfunction makefood<typeoffood>(): typeoffood {\n    // do all stuff to make food\n    const madefood = {}\n    return madefood\n}\n```\n\na generic is kind of like a function that returns a type, and that type that is returned is modified by whatever type was “passed in”\n\nEx imagine we have a wrapper around fetch(). In the context of our application, fetch might return Order, Customer, Product etc. \n","n":0.046}}},{"i":1108,"$":{"0":{"v":"Functions","n":1},"1":{"v":"\nA function is a binary relation between 2 sets (ie. collection of values). The input set is mapped to a return value.\n\nA function's type consists of the types of its argument and its return type.\n- ie. function type === function signature\n\nthe indent level of a function should not be greater than one or two.\n- If a function does only those steps that are one level below the stated name of the function, then the function is doing one thing.\n\nFunctions can be either total or partial.\n- A total function is one that will succeed for all possible inputs, whereas a partial function may not.\n\n```js\n/* No matter what numbers you specify, this function will always return a value. You can be confident that under no circumstances\n * will your program crash because of this function (Reality is slightly more complex, but this is close enough)\n * As such, this function is a total function.\n */\nfunction add( x, y ){ return x + y };\n\n// All numbers can be converted to strings, so this is a total function.\nfunction numberToString( num ) { return num.toString() };\n```\n\nWhen a function is called, its memory is allocated on the stack.\n- When a function executes, it adds its state data to the top of the stack; when the function exits this data is removed from the stack.\n    - All data, such as local variables, parameters, return address etc. are included in this single unit that is added to the stack.\n\n### Memory allocation of recursive functions\nA recursive function calls itself, therefore, the memory for a called function is allocated on top of memory allocated for calling function.\n- a different copy of local variables is created for each function call. When the base case is reached, the child function returns its value to the function from which it was called. Then, this child function’s stack frame is removed. This process continues until the parent function is returned.\n\nIf the memory stack was a stack of plates, then each execution of the function would be a plate added to the stack. Plates would continue to be added until the base case was reached. At that point, the top plate would be removed, and its return value would go to the next plate. This would continue until the return value reaches the final plate in the stack, which would return the final value to the initial calling site.\n\n### Function composition\nIntuitively, composing functions is a chaining process in which the output of function f feeds the input of function g.\n\n### Higher-order function\nA higher-order function is such a function that:\n- Has a parameter that has a function type, OR\n- it’s return type is a function type","n":0.048}}},{"i":1109,"$":{"0":{"v":"First-Class Function","n":0.707},"1":{"v":"\nIf a language treats functions as first-class citizens, it means we can treat it like we do other entities (ie. variables).\n- That is, we can assign the function to a variable, we can pass the function as an argument to another function, we can return a function from another function etc.\n\nIn languages such as these, the names of the functions have no special status. They are treated as ordinary variables with a `function` type.\n","n":0.116}}},{"i":1110,"$":{"0":{"v":"Function Best Practices","n":0.577},"1":{"v":"\nThe indent level of a function should not be greater than one or two.\n- If a function does only those steps that are one level below the stated name of the function, then the function is doing one thing.\n\n### Total vs Partial Functions\nFunctions can be either total or partial.\n- A total function is one that will succeed for all possible inputs, whereas a partial function may not.\n\nnote: technically all functions in dynamic languages like Javscript are partial, because the function call will fail if we pass in the wrong type (something not possible in a statically typed language)\n\n#### Total Fn\nNo matter what numbers you specify, this function will always return a value. You can be confident that under no circumstances will your program crash because of this function (Reality is slightly more complex, but this is close enough).\nAs such, this function is a total function.\n\n```js\nfunction add( x, y ){ return x + y };\n\n// All numbers can be converted to strings, so this is a total function.\nfunction numberToString( num ) { return num.toString() };\n```\n\n#### Partial Fn\nThis function has the possibility of crashing. If you specify the divisor as zero, you'll get an \"Divide by zero\" error.  therefore, this function is a partial function.\n\n```js\nfunction divide( number, divisor ) { return number / divisor }\n\n// Not all strings can be converted to numbers, so some inputs to this might fail.\n// therefore, this is a partial function.\nfunction stringTonumber( str ){ .... }\n```\n\nIf a total function calls a partial function, then the total function becomes \"infected\", and therefore becomes a partial function.\n- This makes sense, since now the function call can fail and throw an exception.\n- the way we deal with this is by using a `try`/`catch`. We anticipate errors happening, and we give an alternative result. If our fetch for a user returns no results, we want to show a \"user not found\" notification to the user.\n\t- using `try`/`catch` blocks allows us to inch a partial function closer towards total function status, since we are reducing the probability of a generic error that breaks the program, or otherwise allows it to function in a state that was not intended.\n","n":0.053}}},{"i":1111,"$":{"0":{"v":"Closure","n":1},"1":{"v":"\nA closure is a [[first-class function|general.lang.feat.functions.first-class]] with an environment.\n- The environment is a mapping to the variables that existed when the closure was created. The closure will retain its access to these variables, even if they’re defined in another scope.\n\nA closure is a function that encloses its surrounding state by referencing fields external to its body. The enclosed state remains across invocations of the closure.\n- in the `inner`/`outer` function illustration, the `inner` function is the closure. The fact that it has access to the state of the `outer` function's scope is key to its quality of being a closure.\n\nA closure occurs when the function we are calling returns another function, so the second function lives on, while the initial function dies (because it has already returned). Put another way, a closure is formed any time an inner function is made available from outside of the scope it was defined within.\n\nNormally, when you exit a function, all its variables “disappear”. This is because nothing needs them anymore. But what if you declare a function inside a function? Then the inner function could still be called later, and read the variables of the outer function. In practice, this is very useful! But for this to work, the outer function’s variables need to “stick around” somewhere. So in this case, JavaScript takes care of “keeping the variables alive” instead of “forgetting” them as it would usually do.\n\n# UE Resources\n[Recommended resource: Closures in Ruby](https://reprog.wordpress.com/2010/02/27/closures-finally-explained/)\n","n":0.065}}},{"i":1112,"$":{"0":{"v":"Data Structures","n":0.707}}},{"i":1113,"$":{"0":{"v":"Tree Structure","n":0.707},"1":{"v":"\nTrees are relation-based data structure, which specialize in representing hierarchical structures.\n\nLike a [[linked list|general.lang.data-structs.linked-list]], nodes contain both elements of data and pointers marking its relation to immediate nodes.\n- This means that a BST is a linked data structure.\n\nFor a graph to be a tree, we must be able to take any 2 nodes and be able to draw exactly *one* path between them.\n\nalmost every operation you'll do with trees is implemented most easily with recursion\n\ntrees usually work in `O(log n)`\n","n":0.112}}},{"i":1114,"$":{"0":{"v":"Trie","n":1},"1":{"v":"\nUsed for storing a dynamic set of strings, where the keys share prefixes. \n- commonly used in dictionary and autocomplete applications.\n- [[time complexity|general.algorithms.big-o]] of `O(n)` where `n` is length of searched prefix.\n\n```ts\nclass TrieNode {\n    children: Map<string, TrieNode>;\n    isEndOfWord: boolean;\n\n    constructor() {\n        this.children = new Map();\n        this.isEndOfWord = false;\n    }\n}\n\nclass Trie {\n    root: TrieNode;\n\n    constructor() {\n        this.root = new TrieNode();\n    }\n\n    add(word: string): void {\n        let node = this.root;\n        for (const char of word) {\n            if (!node.children.has(char)) {\n                node.children.set(char, new TrieNode());\n            }\n            node = node.children.get(char) as TrieNode;\n        }\n        node.isEndOfWord = true;\n    }\n\n    get(prefix: string): string[] {\n        const result: string[] = [];\n        let node = this.root;\n        for (const char of prefix) {\n            if (!node.children.has(char)) {\n                return result;\n            }\n            node = node.children.get(char) as TrieNode;\n        }\n\n        this.collectWords(node, prefix, result);\n        return result;\n    }\n\n    private collectWords(node: TrieNode, prefix: string, result: string[]): void {\n        if (node.isEndOfWord) {\n            result.push(prefix);\n        }\n\n        for (const [char, child] of node.children.entries()) {\n            this.collectWords(child, prefix + char, result);\n        }\n    }\n}\n\n// Example usage:\nconst trie = new Trie();\ntrie.add(\"apple\");\ntrie.add(\"app\");\ntrie.add(\"banana\");\n\nconsole.log(trie.get(\"ap\")); // Output: [\"apple\", \"app\"]\nconsole.log(trie.get(\"b\")); // Output: [\"banana\"]\nconsole.log(trie.get(\"c\")); // Output: []\n```","n":0.076}}},{"i":1115,"$":{"0":{"v":"Binary","n":1},"1":{"v":"\nEach node has at most 2 children (referred to ask *left child* and *right child*).\n\nWe can break a binary tree down into the left subtree and the right subtree.\n\n# Binary Search Tree (BST)\nA binary tree is a BST if:\n- the key of each node in the left subtree is *less than* all the nodes in the right subtree\n- the root key is between the *largest* key in the left subtree and the *smallest* key in the right subtree\n    - in other works, the root key serves as the dividing point.\n\nThe following binary tree *is* a BST, because there are no nodes in the left subtree that are greater than any node in the right subtree.\n![](/assets/images/2021-10-07-06-54-40.png)\n\nThe following binary tree is *not* a BST, since there is a node with key 51 in the left subtree of 50 which breaks our invariant.\n![](/assets/images/2021-10-07-06-57-26.png)\n\n## Implementing a BST in Typescript\n```ts\nclass TreeNode<T> {\n  left: TreeNode<T> | null;\n  right: TreeNode<T> | null;\n  data: T;\n\n  constructor(data: T) {\n    this.left = null;\n    this.right = null;\n    this.data = data;\n  }\n}\n\nclass BinarySearchTree<T> {\n  root: TreeNode<T> | null;\n\n  constructor() {\n    this.root = null;\n  }\n\n  insert(data: T) {\n    const newNode = new TreeNode(data);\n\n    if (!this.root) {\n      this.root = newNode;\n      return;\n    }\n\n    let currentNode = this.root;\n    while (true) {\n      if (data < currentNode.data) {\n        if (!currentNode.left) {\n          currentNode.left = newNode;\n          return;\n        }\n        currentNode = currentNode.left;\n      } else {\n        if (!currentNode.right) {\n          currentNode.right = newNode;\n          return;\n        }\n        currentNode = currentNode.right;\n      }\n    }\n  }\n\n  search(data: T): TreeNode<T> | null {\n    let currentNode = this.root;\n\n    while (currentNode) {\n      if (data === currentNode.data) {\n        return currentNode;\n      } else if (data < currentNode.data) {\n        currentNode = currentNode.left;\n      } else {\n        currentNode = currentNode.right;\n      }\n    }\n\n    return null;\n  }\n}\n\nconst bst = new BinarySearchTree<number>();\nbst.insert(5);\nbst.insert(3);\n```\n### Finding a node by key\n1. Compare current node's key with X. If it's equal, we've found the key. All done.\n2. If X is less than node's key, we start looking at node's left subtree. It's because we know that right subtree cannot contain anything greater than X.\n3. If X is greater than node's key, we start looking in the right subtree.\n4. We repeat this process until we find the key or we reach the leaf node. If we reach the leaf node and haven't found the key as yet, we return not found.\n\n### Inserting into tree\nNew nodes are *always* leaf nodes. Following the logic of left children being smaller and right children being larger, we start searching a key from the root until we hit a leaf node. Once a leaf node is found, the new node is added as a child of the leaf node.\n- Therefore, there is only one correct location for the node to be, which is dependent on the key.\n\nAssume we want to insert a node with key `K`. Starting at the root,\n1. Compare current node's key with `k`.\n2. If `k` is less than the current node,\n    - If left child of current node is `null`, we insert `k` as the left child of current node and return.\n    - If the left child is not `null`, the left child becomes the new current node, and we repeat the process from step 1.\n3. If `k` is greater than the current node,\n    - If right child of current node is `null`, we insert `k` as the right child of the current node and return.\n    - If the right child is not `null`, the right child becomes the new current node, and we repeat the process from step 1.\n\n#### Advantages\nBinary trees are useful for accessing nodes based on their key or value. When labelled like this, we can implement a Binary Search Tree (for efficient searching/sorting/min/maxing).\n\nBinary search trees allow binary search for fast lookup, addition and removal of data items, and can be used to implement dynamic sets and lookup tables.\n- with the way that the nodes of a BST are arranged, each comparison skips ~half the remaining tree.\n    - this results in a [[time complexity|general.algorithms.big-o]] that is logarithmic (`O(log n)`), making it much faster than the linear time required to find items in an unsorted array, but slower than finding items in a hash table.\n\n### Traversal types\n- In-order traversals are performed on each node by processing the left subtree, then the node itself, then the right subtree.\n- Pre-order traversal\n- Post-order traversal\n\n### \"Storing\" a BST\nstore both Inorder and Preorder traversals. This solution requires space twice the size of Binary Tree.\n","n":0.037}}},{"i":1116,"$":{"0":{"v":"B-tree","n":1},"1":{"v":"\nUnlike a [[Binary Tree|general.lang.data-structs.tree.binary]], a B-tree allows more than 2 children per node.\n\nThe B-tree is well suited for storage systems that read and write relatively large blocks of data, such as disks. \n- Therefore, it is commonly used in databases and file systems.\n\nB-trees keep key-value pairs sorted by key, which allows efficient key- value lookups and range queries.\n\nB-trees break the database down into fixed-size blocks or pages, traditionally 4 KB in size (sometimes bigger), and read or write one page at a time. \n- This design corresponds more closely to the underlying hardware, as disks are also arranged in fixed-size blocks.\n- Each page can be identified using an address or location, which allows one page to refer to another—similar to a pointer, but on disk instead of in memory. We can use these page references to construct a tree of pages\n- in this example, we are looking for the key 251, so we know that we need to follow the page reference between the boundaries 200 and 300. That takes us to a similar-looking page that further breaks down the 200–300 range into subranges. Eventually we get down to a page containing individual keys (a leaf page), which either contains the value for each key inline or contains references to the pages where the values can be found.\n![](/assets/images/2022-03-07-21-19-53.png)\n\nOne page is designated as the root of the B-tree; whenever you want to look up a key in the index, you start here.\n- The page contains several keys and references to child pages. Each child is responsible for a continuous range of keys, and the keys between the references indicate where the boundaries between those ranges lie.\n\nThe number of references to child pages in one page of the B-tree is called the branching factor. \n- ex. in the above example, the branching factor is six.\n- the branching factor depends on the amount of space required to store the page refer‐ ences and the range boundaries, but typically it is several hundred.\n\n### Writing\nInserting a new key into a B-tree is reasonably intuitive, but deleting one (while keeping the tree balanced) is somewhat more involved\n\nThe basic underlying write operation of a B-tree is to overwrite a page on disk with new data. \n- the overwrite does not change the location of the page\n- You can think of overwriting a page on disk as an actual hardware operation. On a magnetic hard drive, this means moving the disk head to the right place, waiting for the right position on the spinning platter to come around, and then overwriting the appropriate sector with new data.\n\nIn order to make the database resilient to crashes, it is common for B-tree implementations to include a WAL (write-ahead log).\n- When the database comes back up after a crash, this log is used to restore the B-tree back to a consistent state\n\nIf you want to update the value for an existing key in a B-tree, you search for the leaf page containing that key, change the value in that page, and write the page back to disk\n\nIf you want to add a new key, you need to find the page whose range encompasses the new key and add it to that page.\n\nIf there isn’t enough free space in the page to accommodate the new key, it is split into two half-full pages, and the parent page is updated to account for the new subdi‐ vision of key ranges\n\nThe B-tree algorithm ensures that the tree remains balanced: a B-tree with *n* keys always has a depth of `O(log n)`\n- Most databases can fit into a B-tree that is three or four levels deep, so you don’t need to follow many page references to find the page you are look‐ ing for. \n    - A four-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB.\n\nA B-tree index must write every piece of data at least twice: once to the write-ahead log, and once to the tree page itself (and perhaps again as pages are split)\n\n### Implementation\n- [[Postgres|pg]] and [[CouchDB|couchdb]] implement [[indexes|db.strategies.index]] using a B-tree.","n":0.038}}},{"i":1117,"$":{"0":{"v":"Stack","n":1},"1":{"v":"\nStacks are linear data structures in a LIFO (last-in, first-out) order. Now, what does that mean? Imagine a stack of plates. The last plate that you put on top of the stack is the first one you take out. Stacks work that way: the last value you add is the first one you remove.\n- The most common functions in a stack are `push`, `pop`, `isEmpty`, and `peek`.\n\nSince stacks are simply LIFO, retrieving data is very fast, as there is no lookup required.\n- this also has the consequence that the size of the data stored must be known at compile-time.\n\nAlong with queues, stacks are the most the basic list types in computing\n\nA stack has a number of useful applications:\n- Backtracking to a previous state\n- Expression evaluation and conversion\n","n":0.089}}},{"i":1118,"$":{"0":{"v":"Queue","n":1},"1":{"v":"\nQueues are very similar to stacks in that they both are linear data structures with dynamic size. However, queues are FIFO (first-in, first-out) data structures, like a line to a rollercoaster. That is, first come, first serve.\n\nAlong with stacks, queues are the most the basic list types in computing\n\nA queue has a number of useful applications:\n- When a resource is shared by multiple consumers\n- Create directories\n- When data is transferring asynchronously between two resources\n\n","n":0.116}}},{"i":1119,"$":{"0":{"v":"Log","n":1},"1":{"v":"\nA log is an append-only and totally ordered data structure sequence of changes.\n\nLogs are powerful primitives for building [[distributed|deploy.distributed]] systems.\n\nMany RDBMSs use change logs to improve performance, provide point-in-time recovery (PITR) after a crash, and implement [[replication|db.distributed.replication]].\n- ex. Postgres uses Write-ahead Logs (WALs)\n- A database change log also effectively allows the creation of a distributed database via replication of data on additional external hosts.\n\nTo utilize logs to their fullest, we must understand the key principles of ordering, [[deduplication|storage.deduping]], and checkpointing.\n","n":0.112}}},{"i":1120,"$":{"0":{"v":"Linked List","n":0.707},"1":{"v":"\ninstead of using indexes or positions, Linked lists use a referencing system: elements are stored in nodes that contain a pointer to the next node, repeating until all nodes are linked.\n- Therefore, each node in a linked list contains a reference to the next node in the chain, in addition to its own data.\n\nThis system allows efficient insertion and removal of items without the need for reorganization.\n\nBest used when data must be added and removed in quick succession from unknown locations\n\nLinked lists do not use physical placement of data in memory, unlike arrays.\n\neven if you have millions of elements inside a list, the operation of adding a new element in the head or in the tail of the list is performed in constant time.\n\nThe `head` is a reference to the first node in the linked list. The last node on the list points to `null`.\n- Therefore, if the linked list is empty, the `head` points to `null`.\n\n### vs Arrays\nSimplistically, \"Use an Array for storing and accessing data, and LinkedList to manipulate data.\"\n\nNodes can easily be removed or added from a linked list without reorganizing the entire data structure. This is not the case with arrays, and adding/removing an element to any position of the array that is not the head or tail is non-trivial, and the whole array needs to be destroyed and created again. In circumstances such as this, linked lists add a lot of value.\n\nHowever, accessing nodes by position from a linked list is less efficient than with an array. If we consider that an array is just a contiguous row of memory blocks, then if we want to access `Array[5]`, then we just need to count up from the first block allocated to the array. With a linked list however, we need to manually traverse the chain until we reach the 5th node.\n\nAlso, search operations are slow in linked lists. Unlike arrays, random access of data elements is not allowed. Nodes are accessed sequentially starting from the first node.\n\n#### Deleting nodes\nIf we want to remove a non head/tail node from a linked list, the node *behind* the deleted node must update its pointer value to point to the node that the deleted node was previously pointing to:\n\nfirst -> second -> third\nfirst -> <s>second</s> -> third\nfirst -> third\n\nTo delete `second`, we must update `first` to now point to `third`.\n\n### Code\nThis is what the head node (ie. first node) of a 3-element linked list looks like:\n```js\nListNode {\n  data: 'First data',\n  next: ListNode {\n    data: 'Second data',\n    next: ListNode { data: 'Third data', next: null }\n  }\n}\n```\n\nIf a linked node were a class, it would look like this:\n```js\nclass ListNode {\n    constructor(data) {\n        this.data = data\n        this.next = null                \n    }\n}\n```\n\nAnd the LinkedList itself would look like:\n```js\nclass LinkedList {\n    constructor(head = null) {\n        this.head = head\n    }\n}\n```\n\nImplementing it would look like this:\n```js\nconst node1 = new ListNode('First data')\nconst node2 = new ListNode('Second data')\nconst node3 = new ListNode('Third data')\nnode1.next = node2\nnode2.next = node3\n\nconst list = new LinkedList(node1)\nconsole.log(list.head.next.next.data); // 'Third data'\n// notice how we repeat `next` to get to the subsequent node. If we only had it\n// once, it would have returned the second node.\n```\n","n":0.044}}},{"i":1121,"$":{"0":{"v":"Heap","n":1},"1":{"v":"\nHeaps can thought of as regular [[binary trees|general.lang.data-structs.tree.binary]] with two special characteristics:\n1. Heaps must be Complete Binary Trees.\n2. The nodes must be ordered according to the Heap order property. Two heap order properties are as follows:\n    1. Max Heap Property: All the parent node keys must be greater than or equal to their child node keys in max-heaps. So the root node will always contain the largest element in the Heap. If Node A has a child node B, then, `key(A) >=key(B)key(A)>=key(B)`\n    2. Min Heap Property: In Min-Heaps, all the parent node keys are less than or equal to their child node keys. So the root node, in this case, will always contain the smallest element present in the Heap. If Node A has a child node B, then: `key(A) <=key(B)key(A)<=key(B)`\n\nHeaps are useful for sorting and implementing priority queues.\n\n\n","n":0.085}}},{"i":1122,"$":{"0":{"v":"Hash Table","n":0.707},"1":{"v":"\nA hash table is an alternative to storing the type of data that we might find in an array. In a hash table, a large pool of memory is allocated, and a hash function is chosen that always returns values lying somewhere within the available memory. In order to store a value, the key is hashed, and the value is placed at the location given by the hash. To look a value up given a key, you just hash the key and get back the location of the corresponding value.\n\nOne complication of this method is that if two keys hash to the same value, the previous value would be overwritten. This is called a hash collision, and any hash table implementation must have a method of dealing with them.\n\nA hash table is an implementation of an associative array (also called a dictionary, map,symbol table or key-value store).\n\n## Purpose\nA hash table is one particular way of storing items that are associated with a key in such a way that the item can be looked up quickly using the key.\n- The key might be a person's name, an ID number, a directory path or anything else.\n\nLet's say you want to write a program, for example, that calculates how frequently each word is used in a book. You could use the word as the key, and the value associated with the key would be the number of times that word has been seen. If we wanted to access this information fast, we could store the results in a hash table, and make that data available for consumers. This could especially be useful if we need multiple clients need to access the same data.\n\nLet's say you want to store video game information in hash tables (like weapon stats). You could use hash tables to store the data for easy querying.\n- To give an idea of the scope of what could be included in a hash table, we might include weapon stats, the enemy being hit, your buffs, your accessories, etc. You formulaicly concatenate them together, like:\n```\n[weapon][buff1][buff2][accessory1][accessory2][accessory3][enemy][random]\n```\n\nand then you take the hash of that, and look up the resulting damage in a pre-computed table, indexed by that hash. That way, you're not spending in-game computation time adding, multiplying, subtracting, dividing, and etcetera for potentially dozens or hundreds of damage incidents each second, each of which has to be calculated instantly\n\n\nhash tables have been called \"the backbone of most programming\".\n\nHashing and trees perform similar jobs, but have different usefulness:\n- Trees are more useful when an application needs to order data in a specific sequence. \n- Hash tables are the smarter choice for randomly sorted data due to its key-value pair organization.\n\n## Composition\n\nHash tables are made up of two parts:\n1. *Object*: An object with the table where the data is stored. The array holds all the key-value entries in the table. The size of the array should be set according to the amount of data expected.\n2. *Hash function* (or mapping function): This function determines the index of our key-value pair. It should be a one-way function and produce the a different hash for each key.\n    - takes an item’s key as an input, assigns a specific index to that key and returns the index whenever the key is looked up.\n\n*Hashing* is the process of assigning an object into a unique index, known as a key. Each object is identified using a key-value pair, and the collection of objects is known as a dictionary.\n- Hash functions help to limit the range of the keys to the boundaries of the array\n\n## Implementation\n\nA hash table is implemented by storing elements in an array and identifying them through a key. A hash function takes in a key and returns an index, giving us the location of the data in the array.\n- whenever you input the key into the hash function, it will always return the same index, which will identify the associated element.\n\nIn JavaScript, hash tables are generally implemented using arrays as they provide access to elements in constant time.\n\nA hash table has a number of useful applications:\n- When a resource is shared by multiple consumers\n- Password verification\n- Linking file name and path\n\nSome common uses of hash tables are:\n- Database indexing\n- Caches\n- Unique data representation\n- Lookup in an unsorted array\n- Lookup in sorted array using binary search\n\nHash tables provide access to elements in constant time, so they are highly recommended for algorithms that prioritize search and data retrieval operations. \nHashing is ideal for large amounts of data, as they take a constant amount of time to perform insertion, deletion, and search.\n\ntime complexity is `0(1)`\n\nTo properly understand the difference between a map and an object (in JavaScript), we have to understand the difference between a concept and its implementation. For instance, we most think of a POJO when needing to implement a hash map in JavaScript. However, we can also use a Map object, and we can also just use an array. The implementation in all 3 cases are differ, but in the end of each, we end up with a hash map.\n","n":0.034}}},{"i":1123,"$":{"0":{"v":"Graph","n":1},"1":{"v":"\nA Graph is a non-linear data structure consisting of nodes (a.k.a vertices) and edges. \n- the node is the fundamental unit of the graph\n- the edge is used to connect 2+ nodes\n\n![](/assets/images/2023-03-23-09-23-27.png)\n\n## When to use a graph\nGraphs offer benefits as a data structure in situations where you need to represent and analyze relationships between objects or entities.\n\nExamples include:\n- *social networks*, where each node represents a person and each edge represents a connection between two people, such as a friendship or a follower/following relationship.\n- *computer networks*, where each node represents a device and each edge represents a connection between two devices, such as a wired or wireless connection.\n- *transportation systems*, where each node represents a location and each edge represents a route between two locations, such as a road or a rail line.\n- *Recommendation engines*, where each node represents an item and each edge represents a relationship between two items, such as a similarity or a co-occurrence.\n\n## Types of graph\n### Directed graph (a.k.a digraph) vs Undirected graph\nA directed graph is a graph in which the edges have a direction. \n\n### Cyclic graph vs Acyclic graph\nA cyclic graph is a graph that includes at least one cycle, whereby you can start at one node and follow a path to end up back at the same node.\n- A cyclic graph can be either directed or undirected.\n\nExamples of cyclic graphs include:\n- *social networks*. If Alex, Kyle and Elizabeth are all friends with each other, then what results is a cyclic graph\n\nExamples of acyclic graphs include:\n- *decision trees*, whereby each node represents a decision/outcome of a decision, and each edge represents a causal relationship between one decision and the next\n- *dependency systems*, where each node represents a dependency, and each edge represents one dependency depending on another.\n    - assuming no circular dependencies. If there are, then it is cyclic.\n\n## Implementing a graph in Typescript\n```ts\ntype Vertex<T> = {\n  data: T;\n  edges: Edge<T>[];\n};\n\ntype Edge<T> = {\n  source: Vertex<T>;\n  target: Vertex<T>;\n  weight?: number;\n};\n\nclass Graph<T> {\n  private vertices: Vertex<T>[];\n\n  constructor() {\n    this.vertices = [];\n  }\n\n  addVertex(data: T): Vertex<T> {\n    const vertex: Vertex<T> = { data, edges: [] };\n    this.vertices.push(vertex);\n    return vertex;\n  }\n\n  addEdge(source: Vertex<T>, target: Vertex<T>, weight?: number): Edge<T> {\n    const edge: Edge<T> = { source, target, weight };\n    source.edges.push(edge);\n    return edge;\n  }\n\n  getAdjacentVertices(vertex: Vertex<T>): Vertex<T>[] {\n    return vertex.edges.map((edge) => edge.target);\n  }\n}\n\nconst graph = new Graph<string>();\n\nconst a = graph.addVertex('A');\nconst b = graph.addVertex('B');\nconst c = graph.addVertex('C');\n\ngraph.addEdge(a, b);\ngraph.addEdge(b, c);\ngraph.addEdge(c, a);\n\nconst adjacentVertices = graph.getAdjacentVertices(a);\n```\n\n## BFS and DFS\nBFS and DFS are types of graph traversal algorithms\n\n### BFS (Breadth First Search)\nused to find the shortest path in the graph between two nodes.\n\nstart from the top node in the graph and travels down until it reaches the root node\n\nuses a [[queue|general.lang.data-structs.queue]] to remember how to get the next vertex to start a search, in the event that a dead end occurs in any iteration.\n- therefore, FIFO\n\nBFS is slower and requires a large memory space.\n\nBFS is better when target is closer to Source.\n\n```ts\n// represents an adjacency list, where each key represents a vertex and the corresponding value is an array of its neighbors.\ntype Graph = {\n  [key: string]: string[];\n};\n\nfunction bfs(graph: Graph, startNode: string): string[] {\n  // store the vertices that need to be visited.\n  const queue = [startNode];\n  const visited: { [key: string]: boolean } = {};\n  const result: string[] = [];\n\n  // run until the queue is empty\n  while (queue.length > 0) {\n    // remove first vertex in queue and store in `currentNode`\n    const currentNode = queue.shift()!; // `!` assert value is truthy, since `.shift()` can return undefined if queue is empty\n    if (!visited[currentNode]) {\n      visited[currentNode] = true;\n      result.push(currentNode);\n      for (const neighbor of graph[currentNode]) {\n        queue.push(neighbor);\n      }\n    }\n  }\n\n  // returns an array of visited vertices in the order they were visited.\n  return result;\n}\n\nconst graph: Graph = {\n  A: ['B', 'C'],\n  B: ['D', 'E'],\n  C: ['F'],\n  D: [],\n  E: ['F'],\n  F: []\n};\n```\n\n![](/assets/images/2023-03-23-10-17-52.png)\n\n### DFS (Depth First Search)\nstart from the top node and follows a path to reaches the end node of the path.\n\nuses a stack to remember how to get the next vertex to start a search, in the event that a dead end occurs in any iteration.\n- therefore, LIFO\n\nDFS is faster and requires less memory. \n\nDFS is best suited for decision trees.\n\n![](/assets/images/2023-03-23-10-18-57.png)\n\n## Terminology\n\n### Path\nA path in a graph is a finite or infinite set of edges which joins a set of nodes.\n- therefore, a path must have a starting node\n- if the path connects all the nodes of a graph data structure, then it is a *connected graph*, otherwise it is called a *disconnected graph*.\n- There may or may not be path to each and every node of graph. In case, there is no path to any node, then that node becomes an *isolated node*.\n\n\n","n":0.036}}},{"i":1124,"$":{"0":{"v":"Deque","n":1},"1":{"v":"\nA Deque (pronounced \"Deck\") is a Double-ended queue, meaning elements can be added to or removed from either the front (head) or back (tail).\n\nThe basic operations on a deque are enqueue (add) and dequeue (remove) on either end.\n\nDeques are normally implemented with:\n- A modified dynamic array that can grow from both ends\n    - which has time-complexity of `O(1)` for all operations except insertion/deletion in middle, which is `O(n)`\n- A doubly linked list\n    - which has time-complexity of `O(1)`\n","n":0.113}}},{"i":1125,"$":{"0":{"v":"Array","n":1},"1":{"v":"\nAn array is the most basic of all data structures. The implementation of its data storage is low-level, since adjacent memory blocks are used to store the elements. Therefore, to find elements we just need to count by memory blocks.\n\nAn array is a type of data structure that stores elements of the same type in a contiguous block of memory\n- An array size can not be changed.\n- The last element of an array is an element of the specific type.\n\nAn array has a fixed number of cells that are allocated upon creation. This makes it expensive to \"insert\" or \"delete\" values, as the original array (at its original memory location) cannot easily be reused.\n\n### List vs Tuple\nArrays commonly are used to implement Lists and Tuples.\n- A list is where all elements have the same shape and the length is often unknown\n- A tuple has a fixed length and elements can be of different types\n\n","n":0.081}}},{"i":1126,"$":{"0":{"v":"CRDT (Conflict-free Replicated Data Type)","n":0.447},"1":{"v":"\nCRDTs are general-purpose data structures, like [[hash maps|general.lang.data-structs.hash-table]] and [[arrays|general.lang.types.array]], but the special thing about them is that they are multi-user from the ground up.\n\nCRDTs are built with the complexities arising from multiple users on multiple devices in mind. \n- They are also fundamentally local and private.\n\nThe only type of change that a CRDT cannot automatically resolve is when multiple users concurrently update the same property of the same object; \n- in this case, the CRDT keeps track of the conflicting values and leaves the responsibility of resolution up to the application code.\n- Thus, CRDTs have some similarity to version control systems like Git, except that they operate on richer data types than text files.\n\nEvery application needs some data structures to store its document state. For example, if your application is a text editor, the core data structure is the array of characters that make up the document. If your application is a spreadsheet, the data structure is a matrix of cells containing text, numbers, or formulas referencing other cells. If it is a vector graphics application, the data structure is a tree of graphical objects such as text objects, rectangles, lines, and other shapes.\n- If you are building a single-user application, you would maintain those data structures in memory using model objects, hash maps, lists, records/structs and the like. If you are building a collaborative multi-user application, you can swap out those data structures for CRDTs.\n\n[[Concurrent|general.concurrency]] updates to multiple [[replicas|db.distributed.replication]] of the same piece of data often results in conflicts.\n- as a result, the strategy is often to prevent concurrent updates to replicated data.\n- another approach is optimistic replication (a.k.a lazy replication), where replicas are allowed to diverge.\n\nThe pessimistic approach guarantees that all replicas are identical to each other. The optimistic approach is of eventual consistency, meaning that we only expect replicas to converge on an identical state when the system has been quiesced for a period of time (ie. paused until all replicas can catch up).\n- as a result, there is no need for all replicas to be synchronized before making udpates.\n    - this helps concurrency and parallelism.\n\nThe downside to this approach is that each replica may require explicit reconciliation later on, which may be difficult.\n\nThe idea of this approach is that we will re-establish consistency between replicas eventually (ie. achieve eventual consistency) by merging different replicas.\n\nOptimistic replication does not work in many cases, but it works with CRDTs.\n- in fact, it is mathematically always possible to merge or resolve concurrent updates on different replicas of the data structure without conflicts \n\n### Example: boolean flag\nImagine we had a flag in the database that represented whether or not someone has ever done something. For example, whether someone has ever drank alcohol before. With this particular business logic, once `has_drank_booze` goes to `true`, it can never return to `false`. Therefore, we know that if one replica has a `true` value, then that is the source of truth.\n- here the resolution method is \"true wins\"\n\n## Resources\n- https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type\n- https://www.inkandswitch.com/local-first/#crdts-as-a-foundational-technology\n- https://www.youtube.com/watch?v=iEFcmfmdh2w&t=607s","n":0.045}}},{"i":1127,"$":{"0":{"v":"AST","n":1},"1":{"v":"\n## What is an AST?\nThink of an AST as a big nested object that holds metadata about the thing we are describing.\n- ex. we could generate an AST of a single Javascript file. The AST would have a field called `body`, which is an array of parts: \n  - variable declaration \n  - expression statement \n\n### Use cases\n- ASTs are commonly used in compilers (like the one found in [[js.v8]]) to parse the code we write, converting it into a tree structure that we can traverse programmatically.\n- [[Linters|linter]] also make use of ASTs, recognizing certain parts of the code and applying rules that can be picked up by an IDE / command-line tool.\n  - ex. The `no-unused-vars` looks at all `VariableDeclaration` types and ensures that the variable names are used at least once elsewhere in the code.\n- Syntax highlighting is made possible by ASTs\n  \n## Example\nImagine we have an extremely short file `file.js`:\n```js\nconst myname = 'jason'\nconsole.log(myname)\n```\n\nThe resulting AST of this file would be:\n```json\n{\n  \"type\": \"Program\",\n  \"start\": 0,\n  \"end\": 43,\n  \"body\": [\n    {\n      \"type\": \"VariableDeclaration\",\n      \"start\": 0,\n      \"end\": 22,\n      \"declarations\": [\n        {\n          \"type\": \"VariableDeclarator\",\n          \"start\": 6,\n          \"end\": 22,\n          \"id\": {\n            \"type\": \"Identifier\",\n            \"start\": 6,\n            \"end\": 12,\n            \"name\": \"myname\"\n          },\n          \"init\": {\n            \"type\": \"Literal\",\n            \"start\": 15,\n            \"end\": 22,\n            \"value\": \"jason\",\n            \"raw\": \"'jason'\"\n          }\n        }\n      ],\n      \"kind\": \"const\"\n    },\n    {\n      \"type\": \"ExpressionStatement\",\n      \"start\": 23,\n      \"end\": 42,\n      \"expression\": {\n        \"type\": \"CallExpression\",\n        \"start\": 23,\n        \"end\": 42,\n        \"callee\": {\n          \"type\": \"MemberExpression\",\n          \"start\": 23,\n          \"end\": 34,\n          \"object\": {\n            \"type\": \"Identifier\",\n            \"start\": 23,\n            \"end\": 30,\n            \"name\": \"console\"\n          },\n          \"property\": {\n            \"type\": \"Identifier\",\n            \"start\": 31,\n            \"end\": 34,\n            \"name\": \"log\"\n          },\n          \"computed\": false,\n          \"optional\": false\n        },\n        \"arguments\": [\n          {\n            \"type\": \"Identifier\",\n            \"start\": 35,\n            \"end\": 41,\n            \"name\": \"myname\"\n          }\n        ],\n        \"optional\": false\n      }\n    }\n  ],\n  \"sourceType\": \"module\"\n}\n```\n\n# Tools\n[AST Explorer tool](https://astexplorer.net/) —multiple language support\n","n":0.059}}},{"i":1128,"$":{"0":{"v":"Implementations","n":1}}},{"i":1129,"$":{"0":{"v":"State Management","n":0.707},"1":{"v":"\n### Using a class to manage state\nConsider the [Robot delivery game](https://eloquentjavascript.net/07_robot.html#p_md/LJiyP4s), where we have a robot going around delivering parcels to different villages. For state, we need to track:\n1. Robot's current location\n2. The array of parcels (inc. current location, and destination)\n\nTo accomplish this, a class `VillageState` was used, which looks like this:\n```js\nVillageState {\n  place: 'Post Office',\n  parcels: [ { place: 'Post Office', address: \"Alice's House\" } ]\n}\n```\n\nIn taking this approach to state management, the next question to ask is: \"what methods should exist that will result in a new state to be computed?\". The answer here is the action of the robot moving from location to location, so we add a `move` method. The signature of this function is:\n- taking in the desired destination, and returns the new `VillageState`.\n\nTaking this approach lets us think of our program in terms of \"turns\". If two people were competing to see who could deliver all of the parcels in less turns, then conceptually we can think of each new computation of state (ie. an instance of `VillageState`) as being a turn.\n```js\nclass VillageState {\n  constructor(place, parcels) {\n    this.place = place;\n    this.parcels = parcels;\n  }\n\n  move(destination) {\n    if (!roadGraph[this.place].includes(destination)) {\n      return this;\n    } else {\n      let parcels = this.parcels.map(p => {\n        if (p.place != this.place) return p;\n        return {place: destination, address: p.address};\n      }).filter(p => p.place != p.address);\n      return new VillageState(destination, parcels);\n    }\n  }\n}\n```\n","n":0.066}}},{"i":1130,"$":{"0":{"v":"Feature Flags","n":0.707},"1":{"v":"\n### Using Feature Flags while undergoing a DBMS transition\n\"What if we want to go from Mongo to Postgres, without having to make the switch all at once?\"\n- https://featureflags.io/feature-flags-database-migrations\n","n":0.189}}},{"i":1131,"$":{"0":{"v":"Dark Mode","n":0.707},"1":{"v":"\n# UE Resources\n[Dark mode with Next.js](https://www.joshwcomeau.com/react/dark-mode/)\n","n":0.408}}},{"i":1132,"$":{"0":{"v":"CSS","n":1}}},{"i":1133,"$":{"0":{"v":"Auth Roles","n":0.707},"1":{"v":"\n### One-to-many approach\nHave a roles table, which has a many-to-one relationship with users:\n```sql\ncreate table roles (\n  id uuid primary key,\n  name text\n);\n\ncreate table permissions (\n  id uuid primary key,\n  access text,\n  roles_id uuid references roles\n);\n\ninsert into roles (name) values ('admin')\ninsert into permissions (access) values ('canDeleteUsers');\n```\n\n### Many-to-many approach\n```sql\ncreate table roles (\n  id uuid primary key,\n  name text\n);\n\ncreate table user_roles (\n  user_id uuid references users,\n  role_id uuid references roles,\n  unique (role_id, user_id)\n);\n\ninsert into roles (name) values ('admin')\ninsert into user_roles (user_id, role_id) values ('123', 'abc');\n```","n":0.11}}},{"i":1134,"$":{"0":{"v":"Concurrency","n":1},"1":{"v":"\n## What is it?\nConcurrency is the characteristic of a program whereby different parts (or units) of the program can be executed out-of-order or at the same time without affecting the final outcome.\n- this differs from Parallelism, which is the ability to run 2 things at the same time (e.g. what happens on a multi-core [[processor|hardware.cpu]]).\n  - concurrency is one way to achieve parallelism, though parallelism is not the only goal of concurrency. Put another way, a concurrent program is one that *can* be parallelized.\n\nA concurrent system is one where the processes know how to wait their turn to access resources.\n\n## Why?\nOne thing you *never* want in computing is for 2 different programs to be accessing/writing to the same memory location at the same time. This results in file corruption.\n- ex. This could result if 2 different programs are trying to write the same file at the same time.\n- for this reason, Operating systems and programming languages provide various mechanisms to coordinate access to shared memory between different programs or threads. These mechanisms include [[locks|deploy.distributed.locks]], semaphores, mutexes, and other synchronization primitives.\n\nWith a concurrent system, we aren't going to concern ourselves with what is happening at the [[CPU|hardware.cpu]] level and whether something is running on multiple cores or not. Instead, we allow the runtime (e.g. [[golang]] runtime) and [[OS|os]] to take care of it.\n\nBugs arising from concurrency issues are hard to find by testing, because such bugs are only triggered when you get unlucky with the timing.\n\nIt may seem that two operations should be called concurrent if they occur “at the same time”— but in fact, it is not important whether they literally overlap in time. Because of problems with clocks in [[distributed systems|deploy.distributed]], it is actually quite difficult to tell whether two things happened at exactly the same time.\n- For defining concurrency, exact time doesn’t matter: we simply call two operations concurrent if they are running independently and are both unaware of each other, regardless of the physical time at which they occurred.\n  - that is, we say that two processes running on a computer system that are executing independently of each other and are not sharing any data or resources, are concurrent.\n\n* * *\n\n### Resource Starvation\nResource starvation occurs in concurrent computing where a process is denied access to [[resources|dendron://tech/general.terms#resource]] that it needs to process its work\n\nWhen starvation is impossible in a concurrent algorithm, the algorithm is called starvation-free\n- aka *lockout-freed*, or said to have *finite bypass*\n\n### Liveness\nLiveness is the quality of a concurrent algorithm where multiple processes may have to \"take turns\" during the critical sections in order to continue to make progress. In this way, the program does not stall, and progress can still be made.\n\nLiveness guarantees are important properties in operating systems and distributed systems\n\n### Critical Section\nThe parts of a program where a shared resource is being accessed needs to be protected in a way that will prevent concurrent access from being possible. This protected section is called the critical section, and it cannot be executed by more than one process at a time.\n- anal: think of the critical section as the traffic cop at the sensitive areas. It allows each process to know when it's their turn to access the data.\n\nTypically, the critical section accesses a shared resource, such as a data structure, a peripheral device, or a network connection, that would not operate correctly in the context of multiple concurrent accesses.\n\nA critical section is typically used when a multi-threaded program must update multiple related variables without a separate thread making conflicting changes to that data. In other words, each thread knows to wait its turn before manipulating and accessing the data.\n\nWhoever holds *the lock* is the process that is allowed to access the resource (ie. it's their turn)\n\n## Dining Philosophers Problem\nThe dining philosophers is an analogy for a program where the processses must alternate between one task and another (here, eating and thinking)\n\nThe problem was designed to illustrate the challenges of avoiding deadlock, a system state in which no progress is possible\n\n![](/assets/images/2021-07-30-09-00-51.png)\n\n### How it works\nThe philosopher is either in thinking or eating state.\n- eating -> the philosopher has both forks in his hand\n- thinking -> everything else\n\nThe goal is to create a program that would result in no philosopher having starved. This would be the case if the program were to deadlock because the line of execution was stalled.\n- if we are able to achieve this, then we will have created a **concurrent algorithm**\n\nA philosopher having starved represents **resource starvation**\n\nOnce the philosopher has finished their meal, they place down both forks so that they are available to others.\n\nThere are different approaches to solving the dining philosopher's problem\n1. Introduce a waiter who enforces that only one philosopher at a time may pick up 1 or 2 forks, meaning that the philosophers have to request and receive permission from the waiter before picking up the fork. This results in a system where only one philosopher at a time can pick up a fork.\n\n* * *\n\nThe word consistency is overloaded:\n- It can refer to *replica consistency* and the issue of eventual consistency that arises in asynchronously replicated systems (ie. *replication lag*).\n- It can refer to *consistent hashing*, which is an approach to partitioning that some systems use for rebalancing.\n- In the CAP theorem [[deploy.distributed.CAP-theorem]], the word consistency is used to mean [[linearizability|db.acid.consistency.strong#linearizability]].\n- In the context of [[ACID|db.acid]], consistency refers to an application-specific notion of the database being in a “good state\".","n":0.033}}},{"i":1135,"$":{"0":{"v":"Best Practices","n":0.707},"1":{"v":"\nInstead of always trying to follow best practices, it's often a better idea to avoid *anti-patterns*.\n- The term *best practices* seems to imply that there are clear cut paths set out for us, which is only true some of the time. Most of the time, there are exceptions in our specific use-case which throws a wrench in our plans to follow best practices.\n- That being said, anti-patterns, like patterns, are about contextual wisdom. A discussion of anti-patterns needs to talk both about when a behavior is a problem and when that behavior might be acceptable.\n\n### Fault tolerance\nThe best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees.\n- ex. because of the abstraction provided by [[transactions|db.strategies.transaction]], the application can pretend that there are no crashes (atomicity), that nobody else is concurrently accessing the database (isolation), and that storage devices are perfectly reliable (durability).\n\n### Coupling\nSigns that there is too much coupling:\n- Changes in one part of the code cause unexpected changes in other parts\n- High dependency on specific implementations (ie. a module or class has a lot of dependencies on other specific modules or classes)\n- Difficulty in testing (ie. challenging to test a particular module or function in isolation)\n- Difficulty in maintaining (challenging to modify or extend a module or function without making changes to many other parts of the codebase)\n- Tight coupling between business logic and infrastructure: If the business logic of an application is too tightly coupled to infrastructure-related code (e.g., database access or network requests), it may make the code harder to maintain or to reuse in other contexts.\n- Repeated code or functionality: If there is a lot of repeated code or functionality throughout the codebase, it can be a sign of high coupling.\n\nCoupling is a much worse problem than non-DRY code.\n- This is because too much coupling can lead to a codebase that is difficult to modify and maintain, making it harder to add new features or fix bugs. non-DRY code can still be relatively easy to maintain if it is organized well and follows good coding practices\n- Non-dry code is also much easier to refactor than code that is overly coupled.\n\nsee [[Dendron note on decoupling|general.patterns.decoupling]]\n\n### Rules\nIf you are going to make a rule about something, make sure to provide good error handling for the user. Put them in a position where they can get feedback about the rule as quickly as possible. The more seemingly arbitrary the rule, the more important this practice becomes.\n- ex. if you make a rule about Git branch naming that \"branches must not contain a `_` character\", then you should set up the means to have the user warned any time they create a branch with naming like this.","n":0.046}}},{"i":1136,"$":{"0":{"v":"Arch","n":1}}},{"i":1137,"$":{"0":{"v":"Workers","n":1},"1":{"v":"\nTask Queues/Worker Queues are used as a mechanism to distribute work across threads or machines.\nA task queue’s input is a unit of work called a task. Dedicated worker processes constantly monitor task queues for new work to perform.\n\nOften, a task queue will use a message broker to mediate between clients and workers\n- To initiate a task the client adds a message to the queue, the broker then delivers that message to a worker.\n\nTask Queues like Celery require a configured message broker like Redis broker transports or RabbitMQ to be functional\n- Realistically, you could roll your own using SQLite if you wanted to. You just need a service that will take care of the message queue for you.\n- Graphile-worker has this functionality built-in, as it manages its own queue in your Postgres db.\n","n":0.087}}},{"i":1138,"$":{"0":{"v":"Webhooks","n":1},"1":{"v":"\n# Webhook\nA webhook is a callback living on a 3rd party server that sends an HTTP request upon some event.\n- ex. Twilio providing an Express server with SMS as they arrive to the Twilio server\nWebhooks are a way for an app to provide other applications with real-time information\n- ex. with Twilio, whenever their servers receieve an SMS, they will call the webhook endpoint that we provide to them. their webhook is a POST request, that will match up with a POST route that we define on our app server.\n- The key difference with a webhook is the use of callbacks. In traditional APIs, we manually have to poll for data in order to get a real-time experience. \n- Webhooks can be thought of as \"reverse APIs\", as they give you the API spec, and you must design the API for them to use.\n\t- Put another way, the webhook makes an API request to your app, which you must handle.\n- To make the connection, the webhook provider (ex. Twilio) must be provided with the address that it will send the requests to.\n\t- Therefore, your app must have a publicly accessible url for the webhook to work (ie. localhost will not work)\n\nMetaphorically, webhooks are like a phone number that Stripe calls to notify you of activity in your Stripe account\n- The webhook endpoint is the person answering that call who takes actions based upon the specific information it receives.\n\nA webhook on our server says \"hey, I want you to call this number (`/webhook` url) when *this* event happens\"\n","n":0.063}}},{"i":1139,"$":{"0":{"v":"Streaming Architecture","n":0.707},"1":{"v":"\nA streaming architecture is a defined set of technologies that work together to handle [[stream processing|general.patterns.streaming]], which is the practice of taking action on a series of data at the time the data is created.\n\nWhile streaming is an architectural-level concept, there are multiple ways it could be implemented. Most commonly, we can use [[pubsub|general.patterns.messaging.pubsub]], but we can also use source/sink.\n- common sources and sinks are [[Kafka|kafka]], Hadoop\n\nWhenever you build a streaming based architecture, you have to account for anti-entropy in your streams.\n- You have to consider that the sources of data can be unreliable, causing the stream to vary in bandwidth. You have to account for outages in any of the services you're connected to.\n\t- This is usually a problem in cases where we are counting on a fully up-to-date state of the world. In cases like the Facebook News feed, we don't really have this functional requirement.\n\n## Consuming a stream\nBroadly, there are 3 things we can do with the events of a stream once we have it:\n1. You can take the data in the events and write it to a database, cache, search index, or similar storage system, from where it can then be queried by other clients.\n  - this is a good way of keeping a database in sync with changes happening in other parts of the system—especially if the stream consumer is the only client writing to the database.\n2. You can push the events to users in some way\n  - ex. by sending email alerts or push notifications, or by streaming the events to a real-time dashboard where they are visualized. \n    - In this case, a human is the ultimate consumer of the stream.\n3. You can process one or more input streams to produce one or more output streams. Streams may go through a pipeline consisting of several such processing stages before they eventually end up at an output (option 1 or 2).\n\n\n## four key building blocks:\n### The Message Broker / Stream Processor\nThis is the element that takes data from a source, called a producer, translates it into a standard message format, and streams it on an ongoing basis. Other components can then listen in and consume the messages passed on by the broker.\n\n### Batch and Real-time ETL tools\nData streams from one or more message brokers need to be aggregated, transformed and structured before data can be analyzed with SQL-based analytics tools\n\n### Data Analytics / Serverless Query Engine\nAfter streaming data is prepared for consumption by the stream processor, it must be analyzed to provide value.\n\n## DSMS (Data Stream Management System)\nWhile traditional databases have [[DBMSs|db.DBMS]], streaming architectures use a DSMS (data stream management system). \n\nWhile a DBMS holds static data, a DSMS holds dynamic data, the true size of which cannot be known (since it is always changing)\n\nA DBMS also offers a flexible query processing so that the information needed can be expressed using queries. a DSMS executes a continuous query that is not only performed once, but is permanently installed (and therefore continuously executed).\n- Unlike queries executed in a traditional DBMS, which return a result and exit, queries executed in a DSMS do not exit, generating results continuously as new data become available.\n\nWhile data in a DBMS is accessed randomly (ie. random access), data in a DSMS is accessed sequentially\n\nOne of the biggest challenges for a DSMS is to handle potentially infinite data streams using a fixed amount of memory and no random access to the data. \n- there are 2 broad ways to limit the amount of data that needs to be processed at any given time (ie. processing step)\n  1. Synopses - Take the raw data and compress it. This “compression” may be through sampling (ie. taking random data points to represent the whole picture). It may also be through calculating a running average of the values that come through the stream, since to calculate an average we only need the total members, along with the current average.\n    - naturally, this doesn’t give us accurate results. \n  2. Window - only look at a portion of the data at a given time. This approach is motivated by the idea that only the most recent data are relevant. Therefore, a window continuously cuts out a part of the data stream, e.g. the last ten data stream elements, and only considers these elements during the processing. \n    - there are different types of windows, such as sliding windows (similar to FIFO), and tumbling windows. Windows can also be element-based, such as considering only the last 10 elements, or time-based, such as considering only the most recent 10 seconds of data.\n\nMost query languages for a DSMS are based on SQL, but are not standardized, and thus vary from implementation to implementation.\n\n* * *\n\n\n# UE Resources\n- [Kappa Architecture: Arch for processing streaming data](https://hazelcast.com/glossary/kappa-architecture/)\n  - in use by Twitter\n","n":0.035}}},{"i":1140,"$":{"0":{"v":"Offline First","n":0.707},"1":{"v":"\nTo make an app truly offline first, we primarily need to do two things:\n1. Any code and assets used should be available offline\n2. Any data changes should be made locally first and then [[synced|db.distributed.sync]] to the cloud.\n\nThere are 2 categories of offline-first app that has different implications for the app's architecture: single-user and multi-user.\n- single-user is an application like Everdo, where there is only ever one user editing a document at a time\n- multi-user is an application like Trello or GoogleDocs, where multiple users can edit the same document at once\n  - multi-user applications may benefit more from a data structure like [[CRDTs|general.lang.data-structs.CRDT]]\n\nWhen the backend isn't very smart it's not too hard, you can just encapsulate any state-changing action into an event, when offline process the events locally + cache the chain of events, when online send the chain to the backend and have the backend apply the events one by one. Periodically sync new events from the backend and apply them locally to stay in sync. This is what classic Todo apps like OmniFocus do.\n- The problems start when the backend is smarter and also applies business logic that generates new events (for example enriching new entities with data from external systems, or adding new tasks to a timeline etc). Naturally, the new server-generated events are only available later when the client comes back online.\n\n### Offline vs Cloud apps\nIn cloud apps, the data on the server is treated as the primary, authoritative copy of the data; if a client has a copy of the data, it is merely a cache that is subordinate to the server. Any data modification must be sent to the server, otherwise it “didn't happen.” \n\nIn offline-first applications we swap these roles: we treat the copy of the data on your local device — your laptop, tablet, or phone — as the primary copy. Servers still exist, but they hold secondary copies of your data in order to assist with access from multiple devices. \n\nOffline-first apps can use end-to-end encryption so that any servers that store a copy of your files only hold encrypted data that they cannot read.\n\n## Tools\n- [WatermelonDB](https://github.com/Nozbe/WatermelonDB)\n- [RxDB](https://rxdb.info/)\n- [CouchDB+PouchDB](https://medium.com/offline-camp/couchdb-pouchdb-and-hoodie-as-a-stack-for-progressive-web-apps-a6078a985f18)\n- [Dexie](https://github.com/dexie/Dexie.js)\n\n## E Resources\n- [myTunes: using SQLite to create a reactive app](https://riffle.systems/essays/prelude/)\n- [local-first apps](https://www.inkandswitch.com/local-first/)\n- [Downsides of offline-first](https://rxdb.info/downsides-of-offline-first.html)","n":0.052}}},{"i":1141,"$":{"0":{"v":"Multitenancy Architecture","n":0.707},"1":{"v":"\nThe multitenancy architecture permits the use of a single software instance to server multiple distinct user groups.\n- Conversely, single tenancy means that the software instance has only 1 end user (or group of users, e.g. one organization).\n\nA SaaS offering is an example of multitenant architecture.\n\nIn cloud computing, multitenancy is also referred to as \"shared hosting\".","n":0.135}}},{"i":1142,"$":{"0":{"v":"Microservice","n":1},"1":{"v":"\nA microservice architecture is variant of the [[SOA (Service-oriented Architecture)|general.arch.SOA]] structural style\n- arranges an application as a collection of loosely-coupled services.\n- services are fine-grained and the protocols are lightweight.\n\ngoal is that teams can bring their services life independent of others, but it comes at a cost to maintain the decoupling.\n- Interfaces need to be designed carefully and treated as a public API.\n\t- These difficulties are usually solved by either having versioned APIs, or multiple interfaces on the same service (ie. multiple commands that effectively export a document— just done in different ways)\n\nA microservice is a self-contained piece of business functionality with clear interfaces\n- through its own internal components, it may implement a layered architecture\n\nThe architecture essentially follows the Unix philosophy of \"Do one thing and do it well\"\n\nIt lends itself to a continuous delivery software development process.\n- A change to a small part of the application only requires rebuilding and redeploying only one or a small number of services\n\nAdheres to principles such as fine-grained interfaces (to independently deployable services), business-driven development (e.g. domain-driven design).\n\nMicroservice architectures are commonly adopted for cloud-native applications, serverless computing, and applications using lightweight container deployment.\n\nWhen implementing a microservice-based architecture, the most important technology choices are the way microservices communicate with each other (synchronous, asynchronous, UI integration) and the protocols used for the communication (RESTful HTTP, messaging, GraphQL ...). Things like language and infrastructure don't introduce the coupling between services as you'd find in a monolithic app.\n- In a traditional architecture, the choice of language impacts the whole system. This isn't the case in microservices,\n\n## Considerations when to make a service\nHow many people will be working in the service and how frequently will there be changes?\n- more people working in one codebase results in more merge conflicts and competition for the same resources.\n\nServices should be determined along product offering boundaries defined by nouns (e.g. Customer, Sales) or verbs (Search, Add to Cart)\n- If ServiceB depends on ServiceA to do its work, then it's a sign that they should live in the same service. Otherwise, a failing service will cause its dependent services to fail. Finding the offending service becomes like finding the burnt out bulb in a christmas tree.\n- if we need to achieve the separation of team ownership, or if a service is simply getting too large, we may consider writing a simple library.\n\n\n## Pros & Cons\n### Pros\nModularity\n- This makes the application easier to understand, develop, test, and become more resilient to architecture erosion. This benefit is often argued in comparison to the complexity of monolithic architectures.\n- developers also gain freedom of implementation and technology choices, negating the need to subject services to the lowest common denominator implementation\n\nScalability\n- Since microservices are implemented and deployed independently of each other, i.e. they run within independent processes, they can be monitored and scaled independently.\n\nIntegration of heterogeneous and legacy systems\n- microservices is considered as a viable means for modernizing existing monolithic software application. There are experience reports of several companies who have successfully replaced (parts of) their existing software by microservices, or are in the process of doing so. The process for Software modernization of legacy applications is done using an incremental approach.\n\nDistributed development\n- it parallelizes development by enabling small autonomous teams to develop, deploy and scale their respective services independently. It also allows the architecture of an individual service to emerge through continuous refactoring. Microservice-based architectures facilitate continuous integration, continuous delivery and deployment.\n\t- teams working on monoliths need to synchronize to deploy together.\n\nIncreased fault isolation (only when deployed properly)\n\n### Cons\n- Services form information barriers.\n- Inter-service calls over a network have a higher cost in terms of network latency\n- Testing and deployment are more complicated\n- Moving responsibilities between services is more difficult. It may involve communication between different engineering teams, depending on how messages are passed between each microservice.\n- following this paradigm results in a tendency to see the microservice as the atomic unit. This can lead to too many services when the alternative of internal modularization of a more monolithic approach may lead to a simpler design.\n\t- To make this judgement requires understanding the overall architecture of the applications and interdependencies between components.\n- The architecture introduces additional complexity and new problems to deal with, such as network latency, message format design, Backup/Availability/Consistency (BAC), load balancing and fault tolerance\nmat design, Backup/Availability/Consistency (BAC), load balancing and fault tolerance\n\nAlso, unless implemented properly, you face the risk of...\n- Lower overall availability, especially when those services are deployed in one of a handful of microservice anti-patterns\n- Longer response times for end users.\n- Complicated fault isolation and troubleshooting that increases average recovery time for failures.\n- Service bloat: Too many services to comprehend\n\n## Examples\n### Amazon Product Page\nConsider that a product page on Amazon gets data from multiple distinct domains:\n- Product Info Service - basic metadata about the product such as title, author, page count etc.\n- Pricing Service - product price\n- Order service - purchase history for product\n- Inventory service - product availability\n- Review service - customer reviews \n\nThe question becomes, how do the clients access each individual service? This is more complex than what we'd have in a monorepo architecture. As such, problems are:\n- each client (web, mobile) might not necessarily get the same sets of data (ex. mobile probably shows less information)\n\n# Resources\n- [List of Microservice Patterns](https://microservices.io/patterns/index.html)\n","n":0.034}}},{"i":1143,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\n### Why Testing Microservices is more complicated\nYou need a different strategy to test microservices as they follow a different architecture, and...\n- have a lot of integrations with other microservices within one’s organisation as well as from the outside world (3rd party integrations). \n- require a high degree of inter-team collaboration \n- services are single purpose and are deployed independently & regularly. \n\n![](/assets/images/2022-01-12-15-45-49.png)\n\n# UE Resources\n[Martin Fowler on testing in a microservice architecture](https://martinfowler.com/articles/microservice-testing/)\n","n":0.119}}},{"i":1144,"$":{"0":{"v":"Micro Frontend","n":0.707},"1":{"v":"\n# E Resources\nhttps://micro-frontends.org/\n","n":0.577}}},{"i":1145,"$":{"0":{"v":"Lambda","n":1},"1":{"v":"\nThe Lambda Architecture is a deployment model for data processing that organizations use to combine a traditional batch pipeline with a fast real-time stream pipeline for data access.\n\nThe goal is to become more data-driven and event-driven in the face of massive volumes of rapidly generated data, often referred to as “big data.”\n\n### Five main components to Lambda Architecture\n*Data Sources* - Data can be obtained from a variety of sources, which can then be included in the Lambda Architecture for analysis. This component is oftentimes a streaming source like [[Apache Kafka|kafka]] , which is not the original data source per se, but is an intermediary store that can hold data in order to serve both the batch layer and the speed layer of the Lambda Architecture. The data is delivered simultaneously to both the batch layer and the speed layer to enable a parallel indexing effort.\n\n*Batch Layer* - This component saves all data coming into the system as batch views in preparation for indexing. The input data is saved in a model that looks like a series of changes/updates that were made to a system of record, similar to the output of a change data capture (CDC) system. Oftentimes this is simply a file in the comma-separated values (CSV) format. The data is treated as immutable and append-only to ensure a trusted historical record of all incoming data. A technology like Apache Hadoop is often used as a system for ingesting the data as well as storing the data in a cost-effective way.\n\n*Serving Layer* - This layer incrementally indexes the latest batch views to make it queryable by end users. This layer can also reindex all data to fix a coding bug or to create different indexes for different use cases. The key requirement in the serving layer is that the processing is done in an extremely parallelized way to minimize the time to index the data set. While an indexing job is run, newly arriving data will be queued up for indexing in the next indexing job.\n\n*Speed Layer* - This layer complements the serving layer by indexing the most recently added data not yet fully indexed by the serving layer. This includes the data that the serving layer is currently indexing as well as new data that arrived after the current indexing job started. Since there is an expected lag between the time the latest data was added to the system and the time the latest data is available for querying (due to the time it takes to perform the batch indexing work), it is up to the speed layer to index the latest data to narrow this gap.\n- This layer typically leverages stream processing software to index the incoming data in near real-time to minimize the latency of getting the data available for querying. When the Lambda Architecture was first introduced, Apache Storm was a leading stream processing engine used in deployments, but other technologies have since gained more popularity as candidates for this component (like Hazelcast Jet, Apache Flink, and Apache Spark Streaming).\n\n*Query* - This component is responsible for submitting end user queries to both the serving layer and the speed layer and consolidating the results. This gives end users a complete query on all data, including the most recently added data, to provide a near real-time analytics system.\n\n![](/assets/images/2021-11-30-13-50-33.png)\n\n# E Resources\n[What is Lambda Architecture?](https://hazelcast.com/glossary/lambda-architecture/)\n","n":0.042}}},{"i":1146,"$":{"0":{"v":"Hexagonal","n":1},"1":{"v":"\nThe goal of a hexagonal architecture is to have a loose coupling between components of a system. To connect these systems, we use thin layers which are manisfested as [[adapters|general.patterns.structural.adapter]] and ports.\n- the port uses a particular protocol, and define an abstract API.\n  - ex. ports for event sources (e.g. UIs), ports for notifications, ports for database etc.\n- the adapters serve as glue between the components (via the ports) and the outside world.\n  - there may be several adapters for a single port. For instance, if data could be provided through various means, such via a GUI, via an automated data source, via a test script... This would necessitate multiple adapters for the single port.\n\nThe idea of Hexagonal Architecture is to put inputs and outputs at the edges of our design. \n- Business logic should not depend on whether we expose a REST or a GraphQL API, and it should not depend on where we get data from — a database, a microservice API exposed via gRPC or REST, or just a simple CSV file.\n- The pattern allows us to isolate the core logic of our application from outside concerns, meaning you can easily change data source details without a significant impact or major code rewrites to the codebase.\n\nWith a traditional layered architecture, we would have all of our dependencies point in one direction, each layer above depending on the layer below. The transport layer would depend on the interactors, the interactors would depend on the persistence layer.\nIn Hexagonal Architecture all dependencies point inward — our core business logic does not know anything about the transport layer or the data sources. Still, the transport layer knows how to use interactors, and the data sources know how to conform to the repository interface.\n\nThe purpose is to have a highly modular system that allows us to swap out components (e.g. the database, the UI etc.), and only be required to make changes to the adapter.\n- an added benefit is that we greatly simplify the testing strategy.\n\nThe hexagonal architecture can be thought of as the precursor to the [[microservice|general.arch.microservice]] architecture.\n\n## Breaking it down\n![](/assets/images/2022-04-26-14-28-26.png)\n\n### Inner components\nThe inner components do not reach into the outside world. They don't know how to talk to the database, nor the web. In other words, they have no technology concerns.\n\nA good practice is to use the principle of Dependency Inversion, with the internal rule that only outer ring components may depend on inner ring components, and never the contrary.\n\n### Outer components\nThe outer components each serve a distinct purpose.\n- ex. we can have a User Component which knows how to pass data to and from Users. The User component is connected to the Inner components via the port, and in this case we might decide we need 2 adapters: an HTTP adapter and a GUI adapter.","n":0.046}}},{"i":1147,"$":{"0":{"v":"Event-driven Architecture","n":0.707},"1":{"v":"\nAn event can be defined as something that results in \"a significant change in state\".\n- ex. when a consumer purchases a car, the car's state changes from \"for sale\" to \"sold\"\n\nEvent streaming is the digital equivalent of the human body's central nervous system.\n\nEvents do not travel, they just occur.\n\nEvent-driven architecture can complement [[SOA|general.arch.SOA]] because services can be activated by triggers fired on incoming events\n\n## Illustration of the problem it solves\nConsider if we had an API for handling an order. First, the customer signals their intent to make a purchase, then an invoice is generated. We have these as two decoupled systems. At least, that's what they appear to be:\n\n![](/assets/images/2021-11-22-15-02-44.png)\n\nBut what happens when you start to create more services downstream from the Order Service?\n\n![](/assets/images/2021-11-22-15-14-23.png)\n\nHere, each downstream service must publish its own API, and the Order Service becomes responsible for talking to each of them. To accomplish this, the Order Service must understand retry semantics for each service, as well as bundle an SDK for each. As more downstream services are introduced, they become dependent on the Order Service, introducing an integration cost to hooking up to the Order Service.\n\nThis architecture also has a tendency to get complicated easily. Imagine the FulfillmentService doesn't have the item in stock, and therefore has to inform the InvoiceService? Then we have to choreograph the flow of all of this business logic.\n\nNow, imagine that someone created a RewardsService to track consumer reward points, but your team wasn't informed. Now the RewardService is in a position where it doesn't know what's happened.\n\n## Overview\nEvents are observable, not directed.\n- In the Payment Domain example, the Service APIs used direct commands, whereas the event-driven approach is observable.\n\n![](/assets/images/2021-11-22-15-21-57.png)\n\nThe benefit of the event-driven approach is that the services emitting the events don't have to have any knowledge about who is listening to those events. That burden is removed from the emitter and placed onto the consuming service.\n- The end result is that service can consume upstream events without requiring upstream changes.\n\nThe event producer sends events to an event bus via an endpoint. A router manages directing and filtering those events to the appropriate downstream consumers.\n\nNow, we have an architecture where the OrderService sends events to the Event bus, which gets configured to recognize which downstream services should be notified of the event.\n![](/assets/images/2021-11-22-15-26-31.png)\n\nAnother benefit of this architecture is that if OrderService throws an error, it can send an Error event to the event bus, and all the subscribed nodes will be notified of it so they can act accordingly.\n\nIn the above example, we were at a disadvantage when one team added a RewardService without telling the OrderService team. In this Event-driven architecture, the RewardService team only needs to add a new rule to the bus to be subscribed to emitted events from OrderService.\n","n":0.047}}},{"i":1148,"$":{"0":{"v":"Domain Event","n":0.707},"1":{"v":"\nDomain events can be used to explicitly implement side effects of changes within your domain\n- in other words, we can use domain events to explicitly implement side effects across multiple aggregates.\n\nAn event is something that has happened in the past.\n\nA domain event is something that happened in the domain that you want other parts of the same domain (in-process) to be aware of.\n- The notified parts usually then react somehow to the events.\n\nex. The system event might be the actual psi reading of the tire pressure, but the domain event would be \"low tire pressure\". In this scenario, the service that is consuming this domain event cares only when the tire pressure is low, they don't care if everything else is fine.\n\nAn important benefit of domain events is that side effects can be expressed explicitly.\n\nDomain events are similar to messaging-style events, with one important difference:\n- with regular messaging, a message is always sent asynchronously and communicated across processes and machines, which is useful for integrating multiple [[Bounded Contexts|general.principles.DDD.bounded-context]], microservices, or even different applications.\n- with domain events, you want to raise an event from the domain operation you are currently running, but you want any side effects to occur within the same domain.\n\nThe domain events and their side effects (the actions triggered afterwards that are managed by event handlers) should occur almost immediately, usually in-process, and within the same domain. Thus, domain events could be synchronous or asynchronous. Integration events, however, should always be asynchronous.\n\nDomain events is a preferred way to trigger side effects across multiple aggregates within the same domain\n\nIt's important to ensure that, just like a database transaction, either all the operations related to a domain event finish successfully or none of them do.\n\n### Rationale\nWithout domain events, we would just code the functionality we need close to what triggers the event. As a result, this business-logic gets coupled, implicitly, to the code. This means we'd have to look into that code itself to realize that the rule is implemented there.\n\nOn the other hand, if we use Domain Events, we make the business logic explicit.\n\nIn short, domain events help you to express, explicitly, the domain rules, based in the ubiquitous language provided by the domain experts.\n\n### Example\nImagine we had a main event bus for our entire system for tracking vehicle events, such as geofence crossing, tire pressure, low battery etc.\n\nWithin our specific domain, it wouldn't make much sense to subscribe directly to these events because _________\n\nInstead, we define domain events, which are events for what we really care about (example here?)\n","n":0.049}}},{"i":1149,"$":{"0":{"v":"Event Aggregator","n":0.707},"1":{"v":"\n## What is it?\nA system with lots of objects can lead to complexities when a client wants to subscribe to events. The client has to find and register for each object individually, if each object has multiple events then each event requires a separate subscription.\n\nTo get around this, we can add a layer in between called an Event Aggregator which does the work of registering to the events of many different objects, combining them, then allowing clients to register with the aggregator itself.\n\nAn Event Aggregator is a simple element of [[indirection|general.terms.indirection]]\n\na Event Aggregator also simplifies the memory management issues in using [[observers|general.patterns.behavioural.observer]].\n","n":0.099}}},{"i":1150,"$":{"0":{"v":"Edge Compute","n":0.707},"1":{"v":"\nEdge compute is an architecture where compute and storage resources are decentralized and distributed\n- for example it could mean compute environments are distributed all over the globe, located on every oil rig owned by an oil company or put into every truck in a fleet of food delivery vehicles.\n\nThe goal of edge compute is to minimize the amount of raw, unprocessed data sent to/from applications operating at edge locations (think CCTV cameras, or digital signage) and to reduce the time it takes for instructions to be sent from a central server to devices located at the edge\n- In an edge environment you can have sensors and devices talking directly to applications running on the network, with the application processing information in real time\n","n":0.09}}},{"i":1151,"$":{"0":{"v":"Decentralized Architecture","n":0.707},"1":{"v":"\nDecentralized computing is the allocation of resources (both hardware and software) to each individual machine.\n\nNowadays, most business computing requirements are trivial operations for a computer to perform. As a result, most desktop computers would remain idle for a significant portion of time, if we were taking a more traditional centralized approach. A decentralized system can use the potential of these systems to maximize efficiency.\n\nDecentralized means that there is no single point where the decision is made. Every node makes a decision for it’s own behaviour and the resulting system behaviour is the aggregate response, which would result from a single server if this was a centralized system.\n\nIn decentralized systems, there are many machines that each store a copy of the same resources that users can access. This allows us key benefits:\n- It allows us to place edge nodes closer to users, thereby improving performance to them.\n- It allows us to be more [[fault tolerant|deploy.distributed.fault-tolerance]]. If several nodes fail, extant ones may pick up the slack and allow the system to remain functional.\n\n# UE Resources\n- [Cell-based Architecture](https://github.com/wso2/reference-architecture/blob/master/reference-architecture-cell-based.md)\n\n# E Resources\n- [decentralized vs centralized vs distributed](https://berty.tech/blog/decentralized-distributed-centralized/)\n","n":0.074}}},{"i":1152,"$":{"0":{"v":"Client-Server Architecture","n":0.707},"1":{"v":"\nThe frontend aims to be stateful (that is, keep track of the state between requests). If the frontend wasn’t stateful, you would have to log in every time you navigated to a new page.\n\nThe backend, however, aims to be stateless. This means that the state must be provided on every new invocation. For instance, the API does not keep track of whether you are logged in or not. It determines your authentication state by reading the token in your API request.\n- If you used a Redux store on your Node.js server, the state would be cleared every time the node process stops\n- It becomes even more involved when you consider scaling. If you were to scale your application horizontally by adding more servers, you’d have multiple Node processes running concurrently, and each would have their own version of the state. This means that two identical requests to your backend at the same moment could easily get two different responses.\n","n":0.079}}},{"i":1153,"$":{"0":{"v":"Cache","n":1},"1":{"v":"\nCaches are used in almost every computing layer: hardware, operating systems, web browsers, web applications, and more. \n- they are often found at the level nearest to the client (term used loosely to mean any component that makes requests), where they are implemented to return data quickly without taxing downstream levels.\n\n## Cache invalidation\nCache Invalidation is the idea that if the data is modified in the data layer, it should be invalidated in the cache; if not, this can cause inconsistent application behavior.\n\nCache invalidation involves an action that has to be carried out by something other than the cache itself. Something (e.g., a client or a pub/sub system) needs to tell the cache that a mutation happened.\n\nTo solve this, there are three main schemes that are used:\n1. *Write-through cache*: Under this scheme, data is written into the cache and the corresponding database simultaneously. The cached data allows for fast retrieval and, since the same data gets written in the permanent storage, we will have complete data consistency between the cache and the storage. Also, this scheme ensures that nothing will get lost in case of a crash, power failure, or other system disruptions.\n    - Although write-through minimizes the risk of data loss, since every write operation must be done twice before returning success to the client, this scheme has the disadvantage of higher latency for write operations.\n\n2. *Write-around cache*: This technique is similar to write-through cache, but data is written directly to permanent storage, bypassing the cache. This can reduce the cache being flooded with write operations that will not subsequently be re-read, but has the disadvantage that a read request for recently written data will create a “cache miss” and must be read from slower back-end storage and experience higher latency.\n\n3. *Write-back (ie. write-behind) cache*: Under this scheme, data is written to cache alone, and completion is immediately confirmed to the client. The write to the permanent storage is done after specified intervals and/or under certain conditions (such as low traffic periods). This results in low-latency and high-throughput for write-intensive applications; \n- this performance benefit comes with the risk of data loss in case of a crash or other adverse event because the only copy of the written data is in the cache.\n    - To mitigate this risk, write-back caches may employ mechanisms like write-ahead logging or transaction logs to recover data in case of failures.\n- this cache pattern is an example of weak [[consistency|db.acid.consistency]]\n\n\n### with ETags\n[[ETags|protocol.http.etag]] identify a certain version of a resource.\n- ex. An API can send back an ETag with a request for a list of 20 items. This ETag identifies this unique batch of items. If an identical request is made to the same endpoint and the ETag is also identical, then we know that the data is the same as well. We can use this ETag to make decisions on whether we want to fetch again or not. If however, our identical request shows a different ETag, then we take that to mean that the data from origin has somehow changed. We can use this information to determine whether or not we should invalidate data in a cache, or whether or not to render a long list of items on the page, etc.\n\n### Cache eviction policies\nCache eviction means to free the memory of the old, unused data in the cache.\n\nThere are many cache eviction strategies, and what makes it difficult is that there is no one-size-fits-all approach. Agreeing to use one and not another can be a difficult endeavor.\n\nFollowing are some of the most common cache eviction policies:\n- Least Recently Used (LRU): Discards the least recently used items first (most commonly used).\n- Most Recently Used (MRU): Discards, in contrast to LRU, the most recently used items first.\n- Least Frequently Used (LFU): Counts how often an item is needed. Those that are used least often are discarded first.\n- First In First Out (FIFO): The cache evicts the first block accessed first without any regard to how often or how many times it was accessed before.\n- Last In First Out (LIFO): The cache evicts the block accessed most recently first without any regard to how often or how many times it was accessed before.\n- Random Replacement (RR): Randomly selects a candidate item and discards it to make space when necessary.\n\n### Why is cache invalidation considered difficult?\nSuccess in a caching implementation involves seeing into the future, or multiple possible futures, which is hard in any non-deterministic situation.\n- Sources of non-determinism include both your own workload as well as other workloads that may affect the cost of things you depend on (cache fill / eviction actions, for example).\n\nThe two main reasons why cache invalidation is difficult:\n1. Space complexity issues\n    - You have to guess when the data is not likely to be needed in memory; It can't be too short that the cache is useless, and too long that you'll get a [[memory leak|memory#memory-leak]].\n    - Suppose we had infinite memory, then cache all the data; but we don't so we have to decide what to cache that is meaningful to have the cache implemented (is a ??K cache size enough for your use case? Should you add more?) - It's the balance with the resources available.\n\n2. Time complexity issues\n    - The underlying data might get changed by another process and then your process that uses the cache will be working with incorrect data\n    - Suppose all your data was immutable, then cache all the data indefinitely. But this isn't always to case so you have to figure out what works for the given scenario (A person's mailing address doesn't change often, but their GPS position does).\n\nEtags identify a certain version of a resource.\n- ex. An API can send back an ETag with a request for a list of 20 items. This ETag identifies this unique batch of items. If an identical request is made to the same endpoint and the ETag is also identical, then we know that the data is the same as well. We can use this ETag to make decisions on whether we want to fetch again or not. If however, our identical request shows a different ETag, then we take that to mean that the data from origin has somehow changed. We can use this information to determine whether or not we should invalidate data in a cache, or whether or not to render a long list of items on the page, etc.\n\n## UE Resources\n- [Solving cache consistency (Facebook engineering article)](https://engineering.fb.com/2022/06/08/core-data/cache-invalidation)\n- [Cache replacement policies (e.g. MRU, LRU)](https://en.wikipedia.org/wiki/Cache_replacement_policies)","n":0.03}}},{"i":1154,"$":{"0":{"v":"Service-Oriented Architecture","n":0.707},"1":{"v":"\n\nSOA is kind of a precursor to microservices. Microservices solved a lot of issues that SOA had, such as their heavy weight.\n\nSOA is well suited in an environment where services are provided between software components using a communication protocol (like HTTP).\n\nService orientation is a way of thinking in terms of services and the outcomes of those services.\n\nThere are 4 properties of a service:\n1. It logically represents a repeatable business activity with a specified outcome\n2. It is self contained.\n3. Those who consume the service view it as a black box.\n4. The service may be composed of other services.\n\n### Service Mesh\nA SOA architecture may employ a Service Mesh, which is a dedicated infrastructure layer for facilitating service-to-service communication.\n\nThe service mesh uses a proxy\n\nThe service mesh can provide some benefits, such as:\n- provide observability into communications (ie. system monitoring)\n- provide secure connections\n- automating retries\n\nA service mesh consists of network proxies paired with each service, along with a set of task management processes.\n- proxies are known as the Data Plane.\n\t- Data plane intercepts calls between different services and processes them\n- management processes known as Control Plane.\n\t- Control plane is the brain of the mesh that coordinates behaviour of proxies and provides APIs for operations personnel to manipulate and observe the entire network.\n\n[Difference between SOA and microservices](https://www.ibm.com/cloud/blog/soa-vs-microservices)\n\n# UE Resources\n[Anti-patterns in SOA](https://www.infoq.com/articles/SOA-anti-patterns/)\n","n":0.068}}},{"i":1155,"$":{"0":{"v":"Rules Engine","n":0.707},"1":{"v":"\nA rules engine can be visualized as a set of if-then-else statements to implement some business logic of our application.\n\nExample: determining car insurance premiums\n```\nif car.owner.hasCellPhone then premium += 100;\nif car.model.theftRating > 4 then premium += 200;\nif car.owner.livesInDodgyArea && car.model.theftRating > 2 then premium += 300;\n```\n\nA rules engine is a tool that makes it easier to program using this computational model\n\nAn important property of rule engines is chaining - where the action part of one rule changes the state of the system in such a way that it alters the value of the condition part of other rules.\n- Chaining sounds appealing, since it supports more complex behaviors, but can easily end up being very hard to reason about and debug.\n\t- Often, it is easy to set up a rules system, but very hard to maintain it because nobody can understand this implicit program flow; a consequence of leaving the imperative computational model\n\n# E Resources\n- [Martin Fowler thoughts on why to avoid a rules engine](https://martinfowler.com/bliki/RulesEngine.html)\n- [JSON rules engine repo](https://github.com/cachecontrol/json-rules-engine)\n","n":0.077}}},{"i":1156,"$":{"0":{"v":"Enterprise Service Bus","n":0.577},"1":{"v":"\n### Enterprise Service Bus (ESB)\nAn ESB implements a communication system between services in a [[SOA (Service Oriented Architecture)|general.arch.SOA]].\n\nAn ESB is a centralized component that performs integrations between applications.\n- It performs transformations of data models, handles connectivity, performs message routing, converts communication protocols and potentially manages the composition of multiple requests. \n\nAn ESB is an essential component of an SOA.\n- It is possible to implement a SOA without an ESB architecture, but this would be equivalent to just having a bunch of services. Each application owner would need to directly connect to any service it needs and perform the necessary data transformations to meet each of the service interfaces. This is a lot of work (even if the interfaces are reusable) and creates a significant maintenance challenges in the future as each connection is point to point.\n- In theory, a centralized ESB offers the potential to standardize—and dramatically simplify—communication, messaging, and integration between services across the enterprise. Hardware and software costs can be shared, provisioning the servers as needed for the combined usage, providing a scalable centralized solution.  A single team of specialists can be tasked (and, if necessary, trained) to develop and maintain the integrations.\n- Software applications simply connect (‘talk’) to the ESB and leave it to the ESB to transform the protocols, route the messages, and transform into the data formats as required providing the interoperability to get the transactions execut\n    - This enables developers to spend dramatically less time integrating their software with other software.\n\nAn ESB is a pattern, rather than any specific technology. We implement it using event-driven and message-oriented middleware (MOMs) in combination with [[message queues|general.patterns.messaging]] as technology frameworks.\n\nThe idea is you have some kind of pipeline with computers connected to it, and whenever one of them sends a message, it’s dispatched to all of the others. Then, they decide if they want to consume the given message or just discard it.\n\nAn ESB represents a software architecture for [[distributed computing|deploy.distributed]]\n- It is a special variant of the [[client-server architecture|general.arch.client-server]], where any service can act as the client or the server.\n\nESB promotes agility and flexibility when it comes to high-level protocol communication between services.\n\nThe idea of a [[hardware bus|hardware.bus]] is not all that different to an event bus.\n- in the analogy, the machines are your hardware components, the message is the event or the data you want to communicate, and the pipeline is your `EventBus` object.\n\nEvent buses help loosen coupling between classes and promotes a [[publish-subscribe|general.patterns.messaging.pubsub]] pattern. It also help components interact without being aware of each other.\n- Applications can integrate each other in a loosely coupled manner through this mediator (ie. the bus), so that they will not be dependent on each other's interfaces.\n\nA bus uses the [[pub-sub|general.patterns.messaging.pubsub]] model.\n","n":0.047}}},{"i":1157,"$":{"0":{"v":"MVC Architecture","n":0.707},"1":{"v":"\nEven with the so-called MVC design pattern, there is some variation between the traditional MVC pattern and its modern interpretation in various programming languages. \n- ex. some MVC–based frameworks will have the view observe the changes in the models, while others will let the controller handle the view update.\n\n### Model\n- Models represent knowledge. A model could be a single object (rather uninteresting), or it could be some structure of objects.\n    - There should be a one-to-one correspondence between the model and its parts on the one hand, and the represented world as perceived by the owner of the model on the other hand.\n- The classes which are used to store and manipulate state, typically in a database of some kind.\n- The model retrieves and populates the data.\n- When a model changes, typically it will notify its observers that a change has occurred.\n\n### View\n- A view is a visual representation of its model. \n    - It highlights certain attributes of the model and suppress others. It is thus acting as a presentation filter.\n\n### Controller\nA controller is the link between a user and the system, or the glue between the view and the model. \n- The controller updates the view when the model changes. It also adds event listeners to the view and updates the model when the user manipulates the view.\n\nIt provides the user with input by arranging for relevant views to present themselves in appropriate places on the screen. It provides means for user output by presenting the user with menus or other means of giving commands and data. The controller receives such user output, translates it into the appropriate messages and pass these messages on to one or more of the views.\n- The brains of the application. The controller decides what the user's input was, how the model needs to change as a result of that input, and which resulting view should be used.\n- job is to provide a bit of orchestration between Models and Views\n\n- define the actions to be taken when given a route\n- Performs the database interactions (using Mongoose or an SQL equivalent)\n- Determines the response that the server will give","n":0.053}}},{"i":1158,"$":{"0":{"v":"Algorithms","n":1},"1":{"v":"\n#### What may cause an algorithm to not be deterministic?\n1. The algorithm uses external state, that is not passed in as an argument to the function\n    - ex. global variable, hardware timer value, random value etc.\n2. Timing is sensitive. If the algorithm is run two times, is there a possibility that different processes of the algorithm finish in different orders (e.g. if there are multiple processes)\n3. hardware error causes its state to change in an unexpected way.\n\nAlthough real programs are rarely purely deterministic, it is easier for humans (as well as other programs) to reason about programs that are.\n\nmost programming languages and especially functional programming languages make an effort to prevent the above events from happening except under controlled conditions\n\n### Making an algorithm that is open-ended\nConsider a function that counts vowel occurrences in an array\n```js\nconst countVowels = (str) => {\n    const vowels = { a: 0, e: 0, i: 0, o: 0, u: 0 }\n\n    for (let i = 0; i < str.length; i++) {\n        const currentChar = str[i].toLowerCase()\n        if (Object.keys(vowels).includes(currentChar)) {\n            vowels[currentChar]++\n        }\n    }\n    return Object.values(vowels).reduce((acc, sum) => acc + sum)\n}\n```\n\nConceptually, this could be made a bit simpler, because we don't actually care about the breakdown of each vowel. In other words, we don't really care if there are 2 `a`s, or 3 `e`s... All we care about is how many there are in total. Therefore, we could have just iterated on the string and counted the vowels and returned that value. However, there is a benefit to having put the results of the vowel count into an object. It may be a reasonable assumption that the breakdown of values is of value (or future value) to us. Building our algorithm this way makes it a simple adjustment to include the breakdown, and doesn't really add much overhead.","n":0.058}}},{"i":1159,"$":{"0":{"v":"Algorithm Practice","n":0.707},"1":{"v":"\n### Sorted indexes\nGiven an array, sort the elements from the largest to the smallest. Create a result array using the indices of the original array. \n- Example: arr = [4,4,4,10,6,6,5]\n- res = [3,4,5,6,0,1,2]\n\n### Missing results from graphql query\nGiven a graphql-like query string and an object, return an array of paths that exist in the graphql-like string and don't exist in the object.\n- Example\ngiven:\n```js\nconst graphqlQuery = `\n{\n    age\n    name {\n        first\n        last\n    }\n    parent {\n        name {\n            first\n            last\n        }\n        age\n    }\n    contact {\n        phone\n        email\n    }\n}`\n```\n\nand \n```js\nconst person = {\n    name: {\n        first: 'joe'\n    },\n    contact: {\n        email: 'joe@joe.joe',\n        phone: 7788713377\n    }\n}\n```\n\nShould return\n```ts\n[\n    'age',\n    'name.last',\n    'parent.name.first',\n    'parent.name.last',\n    'parent.age',\n]\n```\n\n### Find path through data\n```js\n// function(string): string\nfunction shuffle(array) {\n    array.sort(() => Math.random() - 0.5);\n}\n\nconst testData = [\n    { \"source\": \"Home\", \"destination\": \"Home Cleaning\" },\n    { \"source\": \"Home Cleaning\", \"destination\": \"Restaurants\" },\n    { \"source\": \"Restaurants\", \"destination\": \"Delivery\" },\n    { \"source\": \"Delivery\", \"destination\": \"Address Search\" },\n    { \"source\": \"Address Search\", \"destination\": \"Burgers\" },\n    { \"source\": \"Burgers\", \"destination\": \"Order Delivery\" },\n    { \"source\": \"Order Delivery\", \"destination\": \"Start Order\" },\n    { \"source\": \"Start Order\", \"destination\": \"Turkey Burger\" },\n];\n\nshuffle(testData);\n\n// Given the shuffled click data and an origin page, find the final destination page\n// ex: input: 'Home' -> output: 'Turkey Burger'\nfunction getDestinationFromOrigin(origin) {\n    \n    const initialSourceObject = testData.filter(el => el.source === origin)[0]\n    \n    const history = []\n    let currentSource = initialSourceObject.source\n    let currentDestination = initialSourceObject.destination\n    history.push(currentDestination)\n \n    // stay in while loop while currentDestination does not exist as a source in the inputArray\n    while (testData.map(el => el.source).indexOf(currentDestination) !== -1) {\n\n        // find element where source = currentDestination\n        currentSource = currentDestination\n        // find element of array where source = currentSource, then get the destination of that object and set it to currentDestination\n        currentDestination = testData.find(el => el.source === currentSource).destination\n\n        if (history.includes(currentSource)) {\n            console.log('hey, you\\'re in a loop!')\n            return\n        }\n        history.push(currentDestination)\n    }\n    return currentDestination\n}\n// what if we get an input origin that doesn't exist?\n// what if we get a circular path?\n// what if a single source has multiple destinations? (ie. multiple objects with same source but different destination)\n```","n":0.054}}},{"i":1160,"$":{"0":{"v":"Patterns","n":1}}},{"i":1161,"$":{"0":{"v":"Two Pointers","n":0.707},"1":{"v":"\nUseful for problems where we deal with sorted arrays (or LinkedLists) and need to find a set of elements that fulfill certain constraints\n- the set of elements could be a pair, a triplet or even a subarray\n\n### Example: find sum of pairs in array\nGiven an array of sorted numbers and a target sum, find a pair in the array whose sum is equal to the given target.\n\nSince the array is sorted, we can just move inwards from the ends of the array, comparing the sums to the `target` until we find the match. We could alternatively brute-force this by using a nested for-loop and iterating over each element twice, but since the array is sorted, we can leverage this fact to be smart about which comparisons we make\n![](/assets/images/2021-10-10-11-45-01.png)\n\n```js\nfunction findSumPairs(arr, target) {\n    let frontIndex = 0\n    let backIndex = arr.length - 1\n\n    let tempSum\n    while (frontIndex !== backIndex) {\n        tempSum = arr[frontIndex] + arr[backIndex]\n        if (tempSum < target) {\n            frontIndex++\n        } else if (tempSum > target) {\n            backIndex--\n        } else {\n            return true\n        }\n    }\n    return false\n}\n```\n","n":0.075}}},{"i":1162,"$":{"0":{"v":"Sliding Window","n":0.707},"1":{"v":"\nTime Complexity - The time complexity of this pattern is `0(N)`\n\nSpace Complexity - The algorithm runs in constant space `O(1)`\n\n### Example: Calculate averages\nThis function takes in an array, and sums the first `k` elements, computes its average then sticks it in the array. Then it sums the next `k` elements, computes average, adds to an array, and so on until the end of the array. Consider that we could brute-force this by having each iteration take a sum then average of each `k` sized window. However this has a major inefficiency, which is that the same elements are counted multiple times needlessly.\n\nInstead, our approach here lets us leverage previously summed up numbers by simply removing the start of the window (`frontIndex`) and adding the end of the window (`backIndex`) on each iteration.\n![](/assets/images/2021-10-10-11-11-22.png)\n```js\nconst calculateAverages = (arr, size) => {\n    let backIndex = size - 1\n    let frontIndex = 0\n    const averageResults = []\n    let sum = arr\n        .slice(frontIndex, backIndex + 1)\n        .reduce((acc, val) => acc + val)\n\n    while (backIndex < arr.length) {\n        averageResults.push(sum / size)\n        \n        // remove the front of the window, and add the back of the window\n        sum = sum - arr[frontIndex] + arr[backIndex + 1]\n\n        frontIndex++\n        backIndex++\n    }\n    return averageResults\n}\n\nconst arr = [1, 3, 2, 6, -1, 4, 1, 8, 2]\nconst k = 5\n\nconst averages = calculateAverages(arr, k)\n// [ 2.2, 2.8, 2.4, 3.6, 2.8]\n```\n","n":0.066}}},{"i":1163,"$":{"0":{"v":"Space and Time Complexity","n":0.5},"1":{"v":"\n## Space vs Time\n\"space can be reused, while time can't.\"\n\n\"Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform.\"\n\n### Space complexity\na function describing the amount of memory that an algorithm takes in terms of input to the algorithm.\n- Space complexity is sometimes ignored because the space used is minimal and/or obvious, but sometimes it becomes as important an issue as time.\n\nex. we might say \"this algorithm takes constant extra space,\" because the amount of extra memory needed doesn't vary with the number of items processed.\n\n### Time complexity\na function describing the amount of time an algorithm takes in terms of the amount of input to the algorithm.\n- \"Time\" can mean...\n\t- the number of memory accesses performed,\n\t- the number of comparisons between integers,\n\t- the number of times some inner loop is executed,\n\t- or some other natural unit related to the amount of real time the algorithm will take.\n- We try to keep this idea of time separate from \"wall clock\" time, since many factors unrelated to the algorithm itself can affect the real time\n\nex. we might say \"this algorithm takes n2 time,\" where n is the number of items in the input.\n\n## Analogy: Ramen noodles vs. Stew\nConsider that if making one cup of ramen noodles takes 3 min, making 5 cups would take 15 min (3x5). Conversely, if I was going to make a beef stu, if one serving takes 50 min, 5 servings will still be roughly 50 min. This is the concept of time. Now consider, that ramen has a 1:1 relationship with space. In other words, if I am filling up a shopping cart, the amount of ramen that takes up space is directly proportional to the number of serving I have to make. 5 servings = 5 cups of ramen noodles. On the other hand, stu for 1 person or 5 will have a less linear relationship. This is the concept of space.\n- We want to build algorithms that are more like the stu.\n![](/assets/images/2021-08-06-21-40-26.png)\n\nAnd in mathematical terms...\n![](/assets/images/2021-08-06-21-44-09.png)\nThe above shows how the left side of the equation is time/space, and the right side is the O Notation\n\nTime and Space Complexity at this level describe the relationship between Inputs and rate of operations/space taken\n\n## Big O\nBased on the above, ramen noodles have a time complexity of `O(n)`. In other words, their \"growth\" rates are linear, and each additional operator will add an a period of time that is equal to the average durations of each operator.\n\nCases (listed from best to worst):\n\n### Constant; $O(1)$\nIt takes at most the same amount of time or space for one or man\n![](/assets/images/2021-08-06-21-53-20.png)\n\n### Logarithmic; $O(\\log n)$\ntime and space complexity increases at first but then it stabilizes and changes less\n![](/assets/images/2021-08-06-21-52-38.png)\n\n### Linear; $O(n)$\nhere time and space increase at worst at a fixed/liner rate :\n![](/assets/images/2021-08-06-21-53-54.png)\n\n### Quadratic & Polynomial\nIn general as your inputs increase your, time becomes greater at a certain greater rate which is related to the exponent, ^2 for quadratics, ^K for Polynomials and everything in between:\n![](/assets/images/2021-08-06-21-54-20.png)\n\n### Exponential\nrapidly increases until time and space requirements tend towards the infinite.\n![](/assets/images/2021-08-06-21-55-00.png)\n\nAny time you have a nested loop, where each loop is $O(n)$, you are working with an $O(n)^2$ complexity.\n- ex. Nested loops may not be readily apparent. In the following code, we have an `.includes()` method inside a loop. The time complexity can be considered as $O(n) * O(n) = O(n^2)$, because we are potentially iterating through one string for each character in the other string.\n\n```js\nfor (const letter of word1AsArray) {\n  if (!word2AsArray.includes(letter)) {\n    return false;\n  }\n}\n```\n\n# Resources\n- [Find O-notations of common things, like arrays, objects etc](https://www.bigocheatsheet.com/)\n","n":0.041}}},{"i":1164,"$":{"0":{"v":"Approach to Algorithms","n":0.577},"1":{"v":"\n### Approach for algorithmic problems\nExample: implement a method on a LinkedList class that will delete a node from that list, given a position.\n\n#### Have a clear understanding of what the resulting data will be after the algorithm is run.\nFirst, we have to understand what does it mean to delete an item from a linked list. In technical terms, it means to change the pointer of the previous node to point to the node after the one being deleted. If we had a linked list of 4 elements:\n\n> first -> second -> third -> fourth\n\nand wanted to remove the third node\n\n> first -> second -> fourth\n\nWe would need to modify the second node's `next` value to point to the fourth element. This is effectively deleting a node from a linked list. If this is not clear to us, then it is impossible to write an algorithm that solves the problem.\n\n*\"A problem well put is a problem half-solved.\"*\n\nA way to fulfill this step might be to satisfy some test cases. For example...\n- If the list does not have a head, then return early, since there are no elements to remove\n- If we want to delete the node at position 0 (the head), then we need to assign a new head. \n    - Therefore, after the removal does `list.head` properly point to the second node instead of the first?\n- If we want to delete any other node, we need to change the `next` value of the previous node so that it refers to the node that was after the deleted node.\n\n### Tips\n- It's best to start by defining the interface of the function. This answers the question \"how would my code be used by others?\". If you are implementing a Binary Search Tree, then you probably want users to be able to add an element something like this: `tree.insert(32)`. From there, work backwards.\n- Any time you have to map over an array and you have to *remember* something about each iteration, consider using a map/object as a way to remember\n    - ex. The problem \"loop over array, sort it, and return an array of the original indices\" requires us to remember which index the number was at before we sort it. We can solve this by making an object where the key is the original index, and the value is the value at that index. From there, we can sort it and do whatever else we want with it, because we have already \"remembered\" that mapping between the value and its index.\n","n":0.049}}},{"i":1165,"$":{"0":{"v":"Random Number Generator","n":0.577},"1":{"v":"\nIn actuality, RNGs implemented using computers are Pseudo Random Number Generators, since they are deterministic (ie. the output is known ahead of time)\n- this sequence is completely determined by an initial value (the **seed**)\n    - Given the same seed, the algorithm will produce the same sequence of numbers every time.\n    - If the same random seed is deliberately shared, it becomes a secret key, so two or more systems using matching pseudorandom number algorithms and matching seeds can generate matching sequences of non-repeating numbers which can be used to synchronize remote systems, such as GPS satellites and receivers.\n\n### Analogy: Pi\nImagine we created a simple PRNG algorithm that just took each decimal digit of Pi (3.14159...). So the first time, we got 1, then 4, and so on. In this example, we used a seed of `1`, meaning we start at the first decimal place. To introduce some element of randomness, we might then use a seed of `2` on a subsequent execution of the PRNG, giving us a sequence of `4, 1, 5, 9...`\n","n":0.076}}},{"i":1166,"$":{"0":{"v":"Game","n":1}}},{"i":1167,"$":{"0":{"v":"Game Engine","n":0.707},"1":{"v":"\nthe core part of a game engine is a renderer, a physics system, and other more complicated things like AI.\nGames that run on the same engine will have similar physics and Artificial intelligence, though those values can still be tweaked by developers\n\nif a development team desires to make a game using two different engines, but using the same assets and textures, the game will look exactly the same. The engine's job is to load the images created by the artists.\n- consider that Gimp, windows media player, photoshop all load an image and look exactly the same, even though they're not the same program.\n\n### Making your own?\na game engine is just a bunch of convenience classes and structural decisions for common tasks in games.\nIf you want to do it without an engine, you have to write all those classes and make those decisions yourself.\n","n":0.084}}},{"i":1168,"$":{"0":{"v":"Game Development","n":0.707},"1":{"v":"\n## Production process\n### Pre-production\n- studios produce a design document; a single source of truth for its creative direction (for a film, this would be roughly adjacent to a script or storyboard)\n    - includes information about the story, gameplay, art direction, intended target audience and accessibility.\n- purpose is to iterate quickly while costs to do so are low. This is no different than designing a website mocks/designs/user testing before actually building the website.\n### Production\nRoles involved in production (overseen by the Producer):\n![](/assets/images/2021-08-18-14-27-37.png)\n\n### Post-production\nproject is evaluated, edited, polished, and fixed.\n- includes alpha and beta testing\n\n## Development team\n### Designer\n- conceives of gameplay and the structure/rules of the game\n- often the main visionary of the game\n- One of the roles of a designer is being a writer, often employed part-time to conceive game's narrative, dialogue, commentary, cutscene narrative, journals, video game packaging content, hint system, etc.\n- they are responsible for various parts of the game, such as, game mechanics, user interface, characters, dialogue, graphics, etc\n\n### Artist\n- designs video game art\n- An artists work may be 2d oriented or 3D oriented\n- 2D artists may produce concept art, sprites, textures, environmental backdrops or terrain images, and user interface. 3D artists may produce models or meshes, animation, 3D environment, and cinematics. Artists sometimes occupy both roles.\n\n### Programmer\nThe programmer develops video games or related software (such as game development tools).\n\nIndividual programming disciplines roles include:\n- Physics – the programming of the game engine, including simulating physics, collision, object movement, etc.;\n- AI – producing computer agents using game AI techniques, such as scripting, planning, rule-based decisions, etc.\n- Graphics – the managing of graphical content utilization and memory considerations; the production of graphics engine, integration of models, textures to work along the physics engine.\n- Sound – integration of music, speech, effect sounds into the proper locations and times.\n- Gameplay – implementation of various games rules and features (sometimes called a generalist);\n- Scripting – development and maintenance of high-level command system for various in-game tasks, such as AI, level editor triggers, etc.\n- UI – production of user interface elements, like option menus, HUDs, help and feedback systems, etc.\n- Input processing – processing and compatibility correlation of various input devices, such as keyboard, mouse, gamepad, etc.\n- Network communications – the managing of data inputs and outputs for local and internet gameplay.\n- Game tools – the production of tools to accompany the development of the game, especially for designers and scripters.\n\n### Level designer\n- a person who creates levels, challenges or missions for video games using a specific set of programs. These programs may be commonly available commercial 3D or 2D design programs, or specially designed and tailored level editors made for a specific game (like Wowedit)\n\n## Domain\nAs far as the actual development of the game goes, there are 2 main routes: programming and designing.\n\n### Design\n- 3D artist: create 3D objects and backgrounds; optimize graphics performance\n- 2D artist: create 2D objects and backgrounds; optimize graphics performance; create storyboards and concept images\n- Lighting artist: develop optimized lighting effects; create post-processing effects such as bloom, ambient occlusion, depth of field, and color correction\n- Audio designer: create and implement sound effects, voice overs, and music\n- Character artist: sculpt and model characters and their assets such as hair, accessories, armor, and weapons\n- Technical artist: work with artists and programmers to implement their work within the final product\n- Visual Effects artist: create realistic and stylized visual effects, such as lava, smoke, and fire; implement physical properties such as collisions and falling\n- Animator: create and optimize high-quality animation of all moving characters and objects; ensure that lighting and visual effects operate well in motion\n- Game / Level Designer: construct and implement gameplay; conduct playtesting and implement feedback; validate playability with the testers, level artists, and gameplay programmers\n\n### Programming\n- Quality assurance technician: find and report bugs; write test cases and plans\n- Programmer analyst: write code based on documented design; modify applications to maintain functionality\n- C# developer: develop new features and applications; manage the release of products \n- Unity engineer: develop new features and applications using Unity\n- Research engineer: design, implement, analyze, and optimize state-of-the-art algorithms\n- Software developer: design, develop, test, and release new features and tools\n- VR / AR developer: prototype and build next generation VR/AR products; deploy and maintain VR/AR applications; stay current on the latest trends in VR/AR\n- Mobile developer: develop new mobile features and applications; stay current on mobile technologies \n- UI / interface programmer: code user interfaces to meet the intent of the project's design and flow; integrate UI systems into features\n- Gameplay programmer: analyze and understand the game design documents; develop gameplay systems such player’s action, character’s behavior, game elements, and game progression\n\n* * *\n\nOnly top 20 percentile of games make profit\n\n# Resources\n[Project design document](https://connect-prd-cdn.unity.com/20190524/19ad3c2b-506e-46c9-9700-07180536a9d2_Project_Design_Doc__PDF_.pdf)\n[Unity design document tutorials](https://learn.unity.com/tutorial/lab-1-personal-project-plan?uv=2020.3&courseId=5cf96c41edbc2a2ca6e8810f&projectId=5caccdfbedbc2a3cef0efe63)\n","n":0.036}}},{"i":1169,"$":{"0":{"v":"Filesystem","n":1}}},{"i":1170,"$":{"0":{"v":"VFS (Virtual File System)","n":0.5},"1":{"v":"\na component of the kernel\n- handles all system calls related to files and file systems\n- serves as an interface between a user and a particular file system\n\t- In other words, it abstracts away the specific filesystem implementation and let's us access it on a command line.\n\t- it accomplishes this by specifying an interface (a contract) between the kernel and the underlying FS\n- we interact with the underlying FS by using the API provided by the VFS. \n- this abstraction allows us to bridge the differences between Windows filesystems, Mac filesystems, and Unix filesystems\n- when an external device attached to the system (such as a USB stick), Unix can run the `mount` command, which will create a new directory on the VFS.\n","n":0.091}}},{"i":1171,"$":{"0":{"v":"Network Filesystem","n":0.707},"1":{"v":"\na filesystem that is distributed across a network\n- distributed means that the FS does not share block level access to the data, instead opting for a network protocol (likely IP)\n\n- the NFS mounts directly into the file system on mount points, so we can use regular unix commands like `cp`, `ls` etc.\n- ex. a NAS exposes its data via a NFS\n","n":0.128}}},{"i":1172,"$":{"0":{"v":"Disk Filesystem","n":0.707},"1":{"v":"\nDisk file systems are file systems which manage data on permanent storage devices. As magnetic disks are the most common of such devices, most disk file systems are designed to perform well in spite of the seek latencies inherent in such media.\n","n":0.154}}},{"i":1173,"$":{"0":{"v":"NTFS","n":1},"1":{"v":"\nContrast with [[fat|fs.fat]]\n- unlike FAT, NTFS supports permissions, a change journal, and other features\n- Mac can only read NTFS, not write. \n","n":0.213}}},{"i":1174,"$":{"0":{"v":"FAT (File Allocation Table)","n":0.5},"1":{"v":"\nThe FAT filesystem uses an index table stored on the device to identify chains of data storage that are associated with a single file. \n- the table is a linked list of entries for each cluster\n\t- a cluster is an atomic unit of space on a hard disk that can be allocated to hold files.\n\t\t- Storing small files on a filesystem with large clusters will therefore waste disk space (slack space)\n\t- being a linked list, each entry contains either the number of the next cluster in the file, or else a marker indicating the end of the file, unused disk space, or special reserved areas of the disk.\n\t\t- also, the root directory of the disk contains the number of the first cluster of each file in that directory\n\t\t- this enables the operating system to traverse the FAT, looking up the cluster number of each successive part of the disk file as a cluster chain until the end of the file is reached\n\t\t- Sub-directories are implemented as special files containing the directory entries of their respective files.\nFAT is a legacy system, and is supported for backward-compatability reasons\n","n":0.073}}},{"i":1175,"$":{"0":{"v":"Ext4","n":1},"1":{"v":"\next4 is a journaling file system for Linux\n- A journaling file system is a file system that keeps track of changes not yet committed to the file system's main part by recording the goal of such changes in a data structure known as a \"journal\", which is usually a circular log. In the event of a system crash or power failure, such file systems can be brought back online more quickly with a lower likelihood of becoming corrupted\n- For example, deleting a file on a Unix file system involves three steps:\n    1. Removing its directory entry.\n    2. Releasing the inode to the pool of free inodes.\n    3. Returning all disk blocks to the pool of free disk blocks.\n\nThe ext4 filesystem can support volumes with sizes in theory up to 64 zebibyte (ZiB) and single files with sizes up to 16 tebibytes (TiB) with the standard 4 KiB block size, and volumes with sizes up to 1 yobibyte (YiB) with 64 KiB clusters, though a limitation in the extent format makes 1 exbibyte (EiB) the practical limit.\n\next4 uses checksums in the journal to improve reliability, since the journal is one of the most used files of the disk.","n":0.071}}},{"i":1176,"$":{"0":{"v":"ExFAT","n":1},"1":{"v":"\nlightweight file system like FAT32, but without the extra features and overhead of NTFS\n- not as compatible as plain FAT, but much more compatible than NTFS\n- Assuming every device we may want to mount to is compatible with exFAT, this is the filesystem we should probably use.\n","n":0.146}}},{"i":1177,"$":{"0":{"v":"Formik","n":1},"1":{"v":"\nFormik keeps track of your form’s state and then exposes it plus a few reusable methods and event handlers (handleChange​, handleBlur​, and handleSubmit​) to your form via props​. \n- `handleChange​` and `handleBlur​` work exactly as expected — they use a name​ or id​ attribute to figure out which field to update.\n- Formik handles validation and errors for us as well\n\nThe button with `type=\"submit\"` is what submits the Formik form. \n- like HTML `<form>`, the default behaviour is to send a GET and put the form values in the URL.\n\n### `withFormik`\n\n#### `handleSubmit`\nhelps with the form submission in Formik. It is automatically passed the form values and any other props wrapped in the component\n\n#### `mapPropsToValues`\nused to initialize the values of the form state. Formik transfers the results of mapPropsToValues​ into an updatable form state and makes these values available to the new component as props.values.\n- helps to transform the outer props into form values. It returns all the values gotten from the form details.\n\n### Components\n#### `<Field />`\nused to automatically set up React forms with Formik. \n- It’s able to get the value by using the `name` attribute, which it uses to match up the Formik state \n- it is by default set to the `input` element. That can easily be changed by specifying a component prop.\n\nWe can use the `<Field />` component to get values in and out of Formik internal state\n\n```tsx\n// text input\n{touched.fullname && errors.fullname && <p>{errors.fullname}</p>}\n<Field className=\"input\" type=\"text\" name=\"fullname\" placeholder=\"Full Name\" />\n\n// select\n{touched.editor && errors.editor && <p>{errors.editor}</p>}\n<div className=\"control\">\n  <Field component=\"select\" name=\"editor\">\n    <option value=\"atom\">Atom</option>\n    <option value=\"sublime\">Sublime Text</option>\n  </Field>\n</div>\n\n// checkbox\n<label className=\"checkbox\">\n  {touched.newsletter && errors.newsletter && <p>{errors.newsletter}</p>}\n  <Field type=\"checkbox\" name=\"newsletter\" checked={values.newsletter} />\n  Join our newsletter?\n</label>\n```\n\n#### `<Form/>`\nhelper component ​​that extends the native form​ element. ​​It automatically assigns the `onSubmit​` event handler to props.handleSubmit\n\nThe component is used to encompass the whole code needed for the form.","n":0.058}}},{"i":1178,"$":{"0":{"v":"Apache Flink","n":0.707},"1":{"v":"\n## What is it?\nApache Flink is a stream processing framework that is scalable and fault-tolerant.\n- it is based on the idea that it should not be hard to express simple computations (e.g. calculating an average and then grouping by a certain attribute) while still be able to scale indefinitely, and in a fault-tolerant manner.\n- spec: Flink is [[MapReduce|general.patterns.map-reduce]] like [[apache.hadoop]], but with streaming data.\n  - might not be true.\n\nFlink is a dataflow engine.\n- Like MapReduce, they work by repeatedly calling a user-defined function to process one record at a time on a single thread. They parallelize work by partitioning inputs, and they copy the output of one function over the network to become the input to another function.\n- Unlike in MapReduce, these functions need not take the strict roles of alternating map and reduce, but instead can be assembled in more flexible ways (these functions are called operators)\n\nFlink's operations can be stateful\n- therefore, the processing of one event can depend on the accumulated effect of all the events that came before it.\n\nThe dataflows in Flink applications form directed graphs that start with one or more sources (e.g. [[kafka]], [[aws.svc.kinesis]]), and end in one or more sinks (e.g. Cassandra, [[DynamoDB|aws.svc.dynamo]], [[elastic-search]]).\n- the reason these sources are eligible to be data sources to Flink is because they support low-latency, high throughput parallel reads in combination with rewind and replay – the prerequisites for high performance and fault tolerance.\n\nFlink can enrich data in its streams by using REST APIs and/or making databases queries.\n\nFlink can be written in Java, Python, Scala and SQL\n\n### Stream Execution Environment\nEvery Flink application needs an execution environment. Streaming applications need to use a `StreamExecutionEnvironment`.\n\nex.\n```java\nfinal StreamExecutionEnvironment env =\n  StreamExecutionEnvironment.getExecutionEnvironment();\n```\n\nCalls to the DataStream API made in your application will build a job graph that is attached to the `StreamExecutionEnvironment`\n- When `env.execute()` is called, this graph is packaged up and sent to the `JobManager`, which parallelizes the job and distributes slices of it to the `TaskManagers` for execution. Each parallel slice of your job will be executed in a task slot.\n- `JobManager` determines which TaskManager should run the job\n  - therefore `env.execute()` must be called in order for our program to run\n\n![](/assets/images/2023-01-06-21-05-34.png)\n\n### APIs of Flink\nFlink has 2 core APIs (DataStream and Dataset), and other high-level APIs built on top of them:\n1. *DataStream API* for bounded or unbounded streams of data \n2. *DataSet API* for bounded data sets.\n3. *Table API*, which is a SQL-like DSL (ie. expression language) for relational stream and batch processing that can be easily embedded in Flink's DataStream and DataSet APIs.\n4. *Async I/O API*, which allows users to use asynchronous request clients with data streams.\n\n#### DataStream API\nenables transformations (e.g. filters, aggregations, [[window functions|pg.lang.func.window]]) on bounded or unbounded streams of data.\n\navailable in Java and Scala\n\n#### DataSet API\nenables transformations (e.g., filters, mapping, joining, grouping) on bounded datasets.\n\nAs of Flink 1.12 the DataSet API has been soft deprecated. In its place, the Table API and SQL should be used.\n- alternatively, use the DataStream API with BATCH execution mode.\n\navailable in Java, Scala and Python (experimental)\n\n#### Table API\nA SQL-like expression language for relational stream and batch processing that can be embedded in Flink's Java and Scala DataSet and DataStream APIs\n- therefore, a relational Table abstraction is used. Thus, we can do things like [[select|sql.clause.select]], aggregate, and [[join|sql.join]]\n- Tables have a schema attached\n\nTable API uses DataStream and DataSet APIs under the hood.\n\n#### Async I/O API\nAsync I/O API is a high-level API that handles the integration with DataStreams, well as handling order, event time, fault tolerance, retry support, etc.\n\nIf we were to synchronously access a database within our Flink app, that database interaction would result in massive latency costs.\n- therefore, whenever we make database calls in our Flink app, we must use a database client that supports async requests.\n  - in the unlikely event that the database client doesn't support async requests, we can turn a synchronous client into a limited concurrent client by creating multiple clients and handling the synchronous calls with a thread pool. Naturally, this approach is usually less efficient than a proper asynchronous client.\n\n(In addition to the async db client) three parts are needed to implement a stream transformation with asynchronous I/O against the database:\n- An implementation of AsyncFunction that dispatches the requests\n- A callback that takes the result of the operation and hands it to the ResultFuture (similar to a [[Promise|js.lang.promises]])\n- Applying the async I/O operation on a DataStream as a transformation with or without retry\n\n## How does it work?\n### Parallelism\nPrograms in Flink are inherently parallel and distributed across a cluster.\n- Flink will basically partition the incoming stream of data and then perform some computation on that subset of data.\n- Flink allows us to specify how much parallelism you want for each of these subtasks. To scale up is to simply increase the parallelism of the bottleneck subtask.\n- During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks (each of which is independent from one another, and executes in different threads)\n- The number of operator subtasks is the parallelism of that particular operator.\n- The set of parallel instances of a stateful operator is effectively a sharded key-value store.\n  - Each parallel instance is responsible for handling events for a specific group of keys, and the state for those keys is kept locally.\n\nThe diagram below shows a job. Notice:\n- The job runs with a parallelism of two across the first three operators in the job graph, terminating in a sink that has a parallelism of one. \n- The third operator is stateful.\n- A fully-connected network shuffle is occurring between the second and third operators. \n  - This is being done to partition the stream by some key, so that all of the events that need to be processed together, will be.\n\n![](/assets/images/2023-01-06-20-44-57.png)\n\nIn production, your application will run in a remote cluster or set of containers.\n\n### State\nFlink offers some compelling features for the state it manages:\n- local: Flink state is kept local to the machine that processes it, and can be accessed at memory speed, helping the application achieve high throughput and low-latency. \n- durable: Flink state is fault-tolerant (ie. it is automatically checkpointed at regular intervals, and is restored upon failure)\n- vertically scalable: Flink state can be kept in embedded RocksDB instances that scale by adding more local disk\n- horizontally scalable: Flink state is redistributed as your cluster grows and shrinks\n- queryable: Flink state can be queried externally via the Queryable State API.\n\nYou can choose to keep state on the JVM heap, or if it is too large, in efficiently organized on-disk data structures.\n\n![](/assets/images/2023-01-06-20-48-22.png)\n\n#### State Snapshots\nFlink is able to provide fault-tolerance with *state snapshots* and *stream replay*\n\n*State Snapshots* capture the entire state of the distributed pipeline, recording offsets into the input queues as well as the state throughout the job graph that has resulted from having ingested the data up to that point.\n\nWhen a failure occurs, the sources are rewound, the state is restored, and processing is resumed. As depicted above, these state snapshots are captured asynchronously, without impeding the ongoing processing.\n\n### Fault-Tolerance: checkpoints and savepoints\nDistributed *checkpoints* provide Flink with a lightweight fault-tolerance mechanism.\n- A checkpoint is an automatic, asynchronous snapshot of the state of an application and the position in a source stream.\n\nIn the case of a failure, a Flink program with checkpointing enabled will, upon recovery, resume processing from the last completed checkpoint, ensuring that Flink maintains exactly-once state semantics within an application.\n\nThe checkpointing mechanism exposes hooks for application code to include external systems into the checkpointing mechanism as well (like opening and committing transactions with a database system).\n\nFlink also includes a mechanism called *savepoints*, which are manually-triggered checkpoints.\n- A user can generate a savepoint, stop a running Flink program, then resume the program from the same application state and position in the stream.\n\n## Time\nFlink explicitly supports three different notions of time:\n- event time: the time when an event occurred, as recorded by the device producing (or storing) the event\n  - use when needing to compute reproducible results (e.g. calculatemaximum price a stock reached in a day) so that the result won’t depend on when the calculation is performed.\n- ingestion time: a timestamp recorded by Flink at the moment it ingests the event\n- processing time: the time when a specific operator in your pipeline is processing the event\n\n### Event Time\nIf you want to use event time, you will also need to supply a *Timestamp Extractor* and *Watermark Generator* that Flink will use to track the progress of event time.\n\n#### Watermark\nImagine we are building a stream sorter, which simply takes in a stream of events and sorts them by their chronological occurrence (by timestamp). An inherent problem here is that since streams are continuous, we will never really be 100% sure if we have the right order, since events with lower timestamps may technically arrive later on in the stream. At the same time, we can't wait forever. Watermarks solve this problem\n\nWatermarks allow Flink to implement the policy that defines when, for any given timestamped event, to stop waiting for the arrival of earlier events.\n- *watermark generators* insert special timestamped elements into the stream (called watermarks). \n  - A watermark for time `t` is an assertion that the stream is (probably) now complete up through time `t`.\n  - therefore, our stream sorter program will stop waiting for an event with timestamp prior to 2 once a watermark arrives with a timestamp of 2, or greater.\n\nWatermarks give you control over the tradeoff between latency and completeness\n- Unlike in batch processing, where one has the luxury of being able to have complete knowledge of the input before producing any results, with streaming you must eventually stop waiting to see more of the input, and produce some sort of result.\n\n##### Watermarking strategies\n\"Bounded-out-of-orderness watermarking\" - The most simple strategy for watermarking is to assume that there is a maximum delay that we will wait for any \"chronologically prior events\" to show up\n- for most applications a fixed delay works well enough.\n\n\n\n* * *\n\n### Example: Stock market aggregator\nProblem statement: Imagine we have a program that consumes a stream of stock market trades and we want to get data on how many trades happen by industry each minute\n- this sort of problem has scalability problems built-in, since we can expect there to be a large amount of data, and we will have no forewarning of when trade volume unexpectedly increases.\n\nImplementation: Configure Flink to:\n- partition the input stream based on the industry name of the stock\n- apply a moving average on a window of 1 minute.\n\nConsider that reading from the stream and applying an `AVG` calculation are 2 different tasks. Flink allows us to scale up each independently. Therefore, if we determine that trade volume has increased and there are more trades happening per minute, we simply scale up the number of readers. On the other hand, if the number of industries to group by has increased, we simply scale up the number of operators that calculate the `AVG`.\n\n## E Resources\n- https://medium.com/archsaber/a-simple-introduction-to-apache-flink-2a603119041e\n\n2 types of clusters:\n- session cluster\n- application cluster","n":0.023}}},{"i":1179,"$":{"0":{"v":"Datastream","n":1}}},{"i":1180,"$":{"0":{"v":"Windows","n":1},"1":{"v":"\nWindows split the infinite input stream into \"buckets\" of finite size.\n\na window is created as soon as the first element that should belong to this window arrives\n\nthe window is completely removed when the time (event or processing time) passes its end timestamp plus the user-specified allowed lateness (see Allowed Lateness). \n- ex. for an *event-time-based window* that creates non-overlapping (or tumbling) windows every 5 minutes and has an allowed lateness of 1 min, Flink will create a new window for the interval between 12:00 and 12:05 when the first element with a timestamp that falls into this interval arrives, and it will remove it when the watermark passes the 12:06 timestamp.\n- removal only guaranteed for *time-based windows*\n\n### WindowAssigner\nThe window assigner is responsible for determining where each incoming element is assigned to a window.\n- we specify the type of WindowAssigner to use in the call to `window(...)` (for keyed streams) or the `windowAll()` (for non-keyed streams)\n\nFlink has pre-defined window assigners for the most common use cases (*tumbling windows*, *sliding windows*, *session windows* and *global windows*).\n- All pre-defined window assigners (except the global windows) assign elements to windows based on time (which can either be processing time or event time).\n- [[see: explanation on window types|general.patterns.streaming.window]]\n\n```java\n// tumbling event-time windows\ninput\n  .keyBy(<key selector>)\n  .window(TumblingEventTimeWindows.of(Time.seconds(5)))\n  .<windowed transformation>(<window function>);\n```\n\n### Trigger\nA Trigger determines when a window (as formed by the window assigner) is ready to be processed by the window function. Each `WindowAssigner` comes with a default Trigger. \n- a custom trigger can also be specified if necessary using `trigger(...)`.\n\nThe trigger interface has five methods that allow a Trigger to react to different events:\n- The `onElement()` method is called for each element that is added to a window.\n- The `onEventTime()` method is called when a registered event-time timer fires.\n- The `onProcessingTime()` method is called when a registered processing-time timer fires.\n- The `onMerge()` method is relevant for stateful triggers and merges the states of two triggers when their corresponding windows merge, e.g. when using session windows.\n- Finally the `clear()` method performs any action needed upon removal of the corresponding window.\n\n### Evictor\nThe evictor has the ability to remove elements from a window after the trigger fires and before and/or after the window function is applied. The two methods to do this are:\n- `void evictBefore(Iterable<TimestampedValue<T>> elements, int size, W window, EvictorContext evictorContext)`\n- `void evictAfter(Iterable<TimestampedValue<T>> elements, int size, W window, EvictorContext evictorContext)`\n\nFlink comes with three pre-implemented evictors. These are:\n1. `CountEvictor`: keeps up to a user-specified number of elements from the window and discards the remaining ones from the beginning of the window buffer.\n2. `DeltaEvictor`: takes a DeltaFunction and a threshold, computes the delta between the last element in the window buffer and each of the remaining ones, and removes the ones with a delta greater or equal to the threshold.\n3. `TimeEvictor`: takes as argument an interval in milliseconds and for a given window, it finds the maximum timestamp max_ts among its elements and removes all the elements with timestamps smaller than max_ts - interval.\n\n### Window Function\nThe *window function* is responsible for processing the elements (ie. performing some computation) of each window.\n- therefore, the trigger determines when the window function gets called.\n\nThe *window function* can be one of `ReduceFunction`, `AggregateFunction`, or `ProcessWindowFunction`. \n- ReduceFunction specifies how two elements from the input are combined to produce an output element of the same type.\n- AggregateFunction is a generalized version of a ReduceFunction that has three types: an input type (IN), accumulator type (ACC), and an output type (OUT)\n\n```java\n// ReduceFunction\ninput\n  .keyBy(<key selector>)\n  .window(<window assigner>)\n  .reduce(new ReduceFunction<Tuple2<String, Long>>() {\n    public Tuple2<String, Long> reduce(Tuple2<String, Long> v1, Tuple2<String, Long> v2) {\n      return new Tuple2<>(v1.f0, v1.f1 + v2.f1);\n    }\n  });\n```\n\n### Window Join\nA *window join* joins the elements of two streams that share a common key and lie in the same window.\n- these windows are defined by the *window assigner*\n\nThe elements from both streams are passed to a user-defined `JoinFunction` or `FlatJoinFunction` where the user can emit results that meet the join criteria.\n\n```java\nstream.join(otherStream)\n  .where(<KeySelector>)\n  .equalTo(<KeySelector>)\n  .window(<WindowAssigner>)\n  .apply(<JoinFunction>);\n```\n\n#### Types of Window Joins\n- Tumbling Window Join\n- Sliding Window Join\n- Session Window Join\n- Interval Join\n\n[docs](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/joining/)\n","n":0.039}}},{"i":1181,"$":{"0":{"v":"API","n":1},"1":{"v":"\n## Operators\nOperators transform one or more `DataStreams` into a new `DataStream`.\n\n### `map`/`flatMap`\nsuitable only when performing a one-to-one transformation: for each and every stream element coming in, `map()` will emit one transformed element\n- Otherwise, you will want to use `flatmap()`\n\n### `keyBy`\nLogically partitions a stream.\n- All records with the same key are assigned to the same partition.\n\nIt is often very useful to be able to partition a stream around one of its attributes\n- the result is that all events with the same value of that attribute are grouped together.\n- ex. imagine we had a stream of data for taxi rides, and we wanted to find the longest taxi rides starting in each of the grid cells.\n  - if this were a SQL query, this would mean doing some sort of `GROUP BY` with the startCell, while in Flink this is done with keyBy(KeySelector)","n":0.084}}},{"i":1182,"$":{"0":{"v":"Firebase","n":1},"1":{"v":"\nWhen you create a new Firebase project in the Firebase console, you're actually creating a Google Cloud Platform (GCP) project behind the scenes\n- You can think of a GCP project as a virtual container for data, code, configuration, and services. A Firebase project is a GCP project that has additional Firebase-specific configurations and services (put another way, a firebase project is a wrapper around a GCP project). \n- therefore, a Firebase project ***is*** a GCP project. Everything that is possible in GCP is also possible in Firebase.\n\t\n## Terminology\n### Reference \n- a locally existing pointer that points to a location within the cloud where data is stored. The client (ex. mobile app) can interact with the reference in order to interact (CRUD) with the database.\n\n### Task \n- an object that is returned from calling a CRUD operation on a `reference` (ex. `.putFile()`. Holds info similar to `res` object in Express.js\n\n### Cloud functions\n- A service from Google that allows you to run snippets of code within their own infrastructure. If you were to build your own server, you would simply execute these functions in your backend code. Normally they are executed in response to some event. Since Firebase is a Backend as a Service, we don't have this ability. Cloud functions allow us to fulfill this need.\n- The functions can also be fired directly with a simple HTTP request.\n- Normally however, we have an event provider (the origin of the event) such as Firestore that will wait on a certain event to occur \n\t- ex. we can execute functions in response to databse writes, user creation etc.\n- can be used as a webhook - Via a simple HTTP trigger, respond to events originating from 3rd party systems like GitHub, Slack, Stripe, or from anywhere that can send HTTP requests.\n\n#### Features\n- these functions are stateless, meaning that if we were trying to make a counter function that holds the current value, we would get unpredictable results. This is because multiple instances of the same function will be created depending on how many people are using your app and executing that function. Therefore, all storage needs must be delegated to some other Google cloud service (like Firestore) \n\n## Resources\n- [Supabase: open source alternative to Firebase](https://github.com/supabase/supabase)","n":0.052}}},{"i":1183,"$":{"0":{"v":"Fastlane","n":1},"1":{"v":"\nThe Fastfile is the place where we're going to code the lanes. A lane contains a group of actions that will be executed synchronously in order to automate a process. An action, is a function that performs a task.\n","n":0.16}}},{"i":1184,"$":{"0":{"v":"Express","n":1},"1":{"v":"\nExpress is an HTTP server\n- It is built on top of another framework callet Connect\n- express applications are request handler functions that you pass to http or http Server instances\n\n![](/assets/images/2021-03-07-22-10-51.png)\n\n## Alternatives\n- [[nestjs]]\n- Feathers.js\n    - this is for building real-time APIs\n\n\n## E Resources\n- [Using /services to encapsulate business logic](https://www.coreycleary.me/should-one-express-controller-call-another/)\n    - [also](https://www.coreycleary.me/why-should-you-separate-controllers-from-services-in-node-rest-apis/)\n","n":0.141}}},{"i":1185,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\n# UE Resources\nhttps://sammeechward.com/testing-an-express-app-with-supertest-and-jest/\n","n":0.577}}},{"i":1186,"$":{"0":{"v":"Router","n":1},"1":{"v":"\nA router is essentially just a container for a set of middleware, grouped by the fact they all have to do with http methods and routes\n- The router is an isolated (meaning it operates independently of other routers) instance of middleware and routes. Therefore it can only perform middleware and routing functions.\n- The router can be thought of as a mini-application\n- A route is a combination of a path and a callback (called the route-handler)\n","n":0.115}}},{"i":1187,"$":{"0":{"v":"Objects","n":1}}},{"i":1188,"$":{"0":{"v":"App","n":1},"1":{"v":"\nThe app object denotes the Express application.\n\nThe `app` object has methods for:\n- Routing HTTP requests\n    - ex. `app.get()`, `app.param()`\n- Configuring middleware\n    - `app.route()`\n- Rendering HTML views\n    - `app.render()`\n- Registering a template engine\n    - `app.engine()`\n\n### Express variables\n- we can set and get variables that are available throughout express with `app.set` and `app.get`\n- Express has the concept of the \"app settings table\", which is essentially a list of key-value pairs for configging Express that we can manipulate with `app.set`\n\n### Local properties\nThe `app.locals` object has properties that are local variables within the application.\nOnce set, the value of `app.locals` properties persist throughout the life of the application, \n- can be accessed on `req.app.locals`\n\nin contrast with res.locals properties that are valid only for the lifetime of the request.\n","n":0.09}}},{"i":1189,"$":{"0":{"v":"Middleware","n":1},"1":{"v":"\nMiddleware is a stack of functions that gets called in a chain. However, the next function in that chain does not need to get called necessarily. The middleware that is currently handling the request decides if the request will complete right then and there, continue on to the next middleware in the stack with `next()`, or die instantly.\n- middlewares are the key to flexibility and modularity in Express\n- A middleware is a function that occurs in the lifecycle of a http request before the request hits the server, or before the response gets to the client\n- These functions have access to the request and response objects, and can modify/read however they want\n- When we add middleware to express with `app.use()`, we are appending items to `Server.prototype.stack` in Connect. When the server receives a request, it iterates over the stack, calling the `(req, res, next)` method.\n- Recall that each middleware item will either modify the request object, modify the response object, or call `next()` so the next middleware in the stack is called (?)\n- If a middleware does *not* modify the body or the header of the response, it should call `next()`. If it does, it should not call `next()`\n- If the current mw function does not end the request-response cycle, it must call `next()` so the next mw in the stack can take over.\n\nTechnically, Express itself is just middleware that occurs before the node.js server\n\nmiddleware by its nature is dumb. It doesn't know which handler will be executed after calling the `next()` function.\n\nMiddleware functions can perform the following tasks:\n- execute any code.\n- make changes to the request and the response objects.\n- end the request-response cycle.\n- call the next middleware function in the stack.\n- if the current middleware function does not end the request-response cycle, it must call next() to pass control to the next middleware function. Otherwise, the request will be left hanging.\n\n## Types of middleware\n### Application level\nusing `app.use()`, we can bind a piece of application-level middleware to the app object\n- `app.use` will be called each time a request is sent to the server\n\n### Router level\nworks the same as application-level, except the middleware is bound to an instance of `express.Router()` instead of `express()`\n\n### Error handling\n\n### Built-in\nex. `express.static`, `express.json`\n","n":0.052}}},{"i":1190,"$":{"0":{"v":"Body Parser","n":0.707},"1":{"v":"\n## body-parser\n- when we don't use body-parser, we get the raw request in the request body. In this format, the `body` and `headers` keys are not on the root object of the request parameter (ie. they are nested). This means that we must individually manipulate all the fields\n","n":0.144}}},{"i":1191,"$":{"0":{"v":"Eslint","n":1}}},{"i":1192,"$":{"0":{"v":"Custom","n":1},"1":{"v":"\n### Processors\nAny time we want to make a lint rule for a non-`.js` file, we need to include a processor \n\nA processor is composed of a preprocessor and a postprocessor.\n\n#### Preprocessor\nThe preprocess method takes the file contents and filename as arguments, and returns an array of code blocks to lint\nA code block has two properties text and filename; \n- the text property is the content of the block\n- filename property is the name of the block.\n    - Name of the block can be anything, but should include the file extension, that would tell the linter how to process the current block\nThe linter will consult the `--ext` extensions list of the linting command as executed on the CLI.\n\n#### Postprocessor\n\n* * *\n\n# E Resources\n[Eslint docs for custom rules](https://eslint.org/docs/developer-guide/working-with-rules)\n[Eslint docs for custom plugins](https://eslint.org/docs/developer-guide/working-with-plugins)\n[Create custom eslint rules](https://www.webiny.com/blog/create-custom-eslint-rules-in-2-minutes-e3d41cb6a9a0)\n[creating an eslint plugin](https://medium.com/@bjrnt/creating-an-eslint-plugin-87f1cb42767f)\n","n":0.086}}},{"i":1193,"$":{"0":{"v":"Esbuild","n":1},"1":{"v":"\nWebpack and Babel can be replaced with ESBuild\n\nESBuild can be used to build our Typescript into Javascript","n":0.243}}},{"i":1194,"$":{"0":{"v":"Enzyme","n":1}}},{"i":1195,"$":{"0":{"v":"Terms","n":1}}},{"i":1196,"$":{"0":{"v":"Unpack","n":1},"1":{"v":"\n### Unpack\n- render 1+ level deeper (ie. not shallow)\n- `dive()` - shallow render the component child of the wrapper\n","n":0.229}}},{"i":1197,"$":{"0":{"v":"Ember","n":1},"1":{"v":"\n# Overview\n- provides MVC in the client-side\n- opinionated— follows convention over configuration\n- modules import other modules implicitly because they know exactly where to look. Therefore, following the conventional structure is paramount (like Ruby on Rails) \n\t- ex. data that's fetched in the `/routes/index.js` will be available in `/templates/index.hbs` as `@model`\n\n![](/assets/images/2021-03-10-22-39-10.png)\n\n# Decorators\n- they are special functions that make modifications to the following line\n- they can be thought of as wrapper functions in a sense, if they are before a function.\n- ex. this decorator will cause the following getter to cache the result on the first time.\n\n## Types\n- `@tracked` - in a class component, This annotation on a variable tells Ember to monitor this variable for updates. Whenever this variable's value changes, Ember will automatically re-render any templates that depend on its value.\n- `@action` - in a class component, define a function that is available to the component layout (handlebars html)\n```js\n@cache\nget count() {\n  return this._count++;\n}\n```\n- can receive arguments: `@alias('fullName') name`\n\n# Anatomy of Ember app\n![](/assets/images/2021-03-10-22-39-28.png)\n- url determines the current state of the app\n\t- ex. are they looking at a list? a post? are they editing the post?\n- When the url is entered in the address bar, it...\n\t- connects to a route handler which loads a particular model.\n\t- It renders a template, which has access to the model.\n\n## Models\n- Models represent persistent state. \n\t- ex. in an airbnb app, the details of a rental (price, description, photos etc) would be stored in the `rental` model. we'd also have a `user` model to store the state of the currently logged in user\n- models persist information, whether it's to a web server or as local state\n- model layers can be swapped in, so we could use Redux or Apollo\n\n## Templates\n- similar to `ejs`\n- the route handler makes the data of the model available to the template\n\n### application.hbs\n- a wrapper around the whole application. This is where we can specify a footer and header, since they will appear on all pages.\n- use `{{outlet}}` to specify the whole application (think of it as `children`, which contains the rest of the app)\n\n## Components\n- essentially just a template that goes in another template\n- they can take args (just like passing props in React)\n![](/assets/images/2021-03-10-22-39-41.png)\n- You can think of components as Ember's way for letting you create your own HTML tags.\n- use the `{{yield}}` keyword to pass \"the rest of the data\" (similar to `children` in React)\n\n### Namespaced components\n- you can have a component that exists within another component by:\n\ta. creating a folder within `/components` with `ember generate component <parentDir>/<subComponent>`\n- invoked in templates like this `<Parent::Child>`\n- We can pass down HTML attributes just like props in react:\n```html\n<div class=\"image\">\n  <img ...attributes>\n</div>\n```\n\n### Class components\n- enable us to add behavior to components (by using states)\n- run `ember generate component-class <nameOfComponent>\n- class components extend `Glimmer` components, giving it functionality similar to classes in React, such as state and lifecycle methods.\n- whenever a component is invoked, an instance of its related class component will be instantiated, allowing us to store state in it and call any relevant lifecycle methods.\n- initial state is stored in the constructor (in Ember, writing out constructor seems to be optional)\n- in the template part of the component (ie. the html) we get access to the instance variables (the component state) defined in the class component\n- Glimmer components have access to `this.args`, which is just like `this.props`\n\t- All arguments that can be accessed from this.args are implicitly marked as @tracked\n\n#### Block parameters\n- a block is any code that is between the opening and closing tags of a component (in react it would be called `children`)\n- What if there is a variable that we want to pass to the block content?\n\t- In this case we can the `as |results|` syntax, which would make `results` available to everything inside the block\n\t- this is similar to when we do a `items.map(item)`, and we have the current iteration available to us as `map`\n```html\n  <ul class=\"results\">\n    <Rentals::Filter @rentals={{@rentals}} @query={{this.query}} as |results|>\n      {{#each @rentals as |rental|}}\n\t\t<li>\n\t\t  <Rental @rental={{rental}} />\n\t\t</li>\n      {{/each}}\n    </Rentals::Filter>\n  </ul>\n```\n- this also allows us to pass down the resultings data in the corresponding module that pertains to `rentals/filter.hbs` with `{{yield this.results}}` (see next section)\n\n#### Provider component\n- A pattern we use when we want to set up a piece of state for a component, but don't have any html to render for it. Instead, the html is just passed on down to the next level by using `{{yield this.results}}`\n- The child component then passes data up to it's parent\n\t- look at `rentals.hbs` and `rentals/filter.js`. \n\t\t- `@query={{this.query}} as |results|` passes the `query` variable down to the child, giving it access to it. The child (a class component) uses that variable to make computations, then returns a result, which gets put in the variable `results` (due to the `|results|` line above)\n\n## Routes\n### Model hook\n- The model hook is responsible for fetching and preparing any data that you need for your route. Ember will automatically call this hook when entering a route, so that you can have an opportunity to run your own code to get the data you need. The object returned from this hook is known as the model for the route.\n\t- Usually, this is where we'd fetch data from a server. Since fetching data is usually an asynchronous operation, the model hook is marked as async\n\t\n## Services \n- Services are just singleton objects (ie. they get instantiated only once) to hold long-lived data such as user sessions.\n- serve a similar role to global variables, in that they can be easily accessed by any part of your app\n- For example, we can inject any available service into components, as opposed to having them passed in as an argument. This allows deeply nested components to \"skip through\" the layers and access things that are logically global to the entire app, such as routing, authentication, user sessions, user preferences, etc.\n- A major difference between services and global variables is that services are scoped to your app, instead of all the JavaScript code that is running on the same page. This allows you to have multiple scripts running on the same page without interfering with each other.\n\n### Store service\n- can be injected into a route with `@service store`, making the Ember Data store available to use as `this.store`, and giving us `find` and `findAll` methods.\n```ts\nimport { inject as service } from '@ember/service';\n\nexport default class IndexRoute extends Route {\n  @service store;\n  async model() {\n    return this.store.findAll('rental');\n  }\n}\n```\n- store service might be compared to Redux in its role to fetch from the database and cache it\n\n## Controller\n- def - an object that receives the return value of the `model()` method (which is found in the associated route).\n- def - an object that receives one property when its associated route is hit: `model` (the return value of the Route's model method)\n- controller is only needed if we want to customize the properties or provide actions to the Route\n\t- in other words, they are an extension of the model loaded from the Route\n- if we don't make a `controller` file, one is generated for us by default (we just don't see it)\n- the controller name must match the route that renders it\n- controller is a singleton (ie. they get instantiated only once) \n\t- this means we shouldn't keep state in the controller \n\t\t- (unless it comes from either the Model or the Query params; since these would persist in between activations such as when a user leaves the Route and then re-enters it)\n- Controllers can also contain actions, Query Parameters, Tracked Properties, and more\n- Basically, use controllers when: \n\t1. we want to pass down actions or variables to the components found in a route. \n\t2. we want to support query params\n\t3. we want to compute a value (that we will ultimately pass down to the route's components) that depends on the model hook \n\t\t- in other words, the controller takes in the result of `model()` as its sole argument. What if we want to pass a variable down to the components that depend on the return value of `model()`?\n\n# Libraries\n## Ember Data\n- a library that helps manage data and application state in Ember applications.\n- built around the idea of organizing your app's data into model objects (in `/models` directory).\n\t- These objects represent units of information that our application presents to the user\n- The model represents the shape of the data\n\n```js\nimport Model, { attr } from '@ember-data/model';\n\nconst COMMUNITY_CATEGORIES = [\n  'Condo',\n  'Townhouse',\n  'Apartment'\n];\n\nexport default class RentalModel extends Model {\n  @attr title;\n  @attr owner;\n  @attr city;\n  @attr location;\n  @attr category;\n  @attr image;\n  @attr bedrooms;\n  @attr description;\n\n  get type() {\n    if (COMMUNITY_CATEGORIES.includes(this.category)) {\n      return 'Community';\n    } else {\n      return 'Standalone';\n    }\n  }\n}\n```\n- Ember Data uses Adapters and Serializers. The idea is that, provided that your backend exposes a consistent protocol and interchange format to access its data, we can write a single adapter-serializer pair to handle all data fetches for the entire application.\n\t- Adapters deal with how and where Ember Data should fetch data from your servers, such as whether to use HTTP, HTTPS, WebSockets or local storage, as well as the URLs, headers and parameters to use for these requests. \n\t- Serializers are in charge of converting the data returned by the server into a format Ember Data can understand.\n\n# Structure\n- the root of the ember application is `templates/application.hbs`\n\n# Tests\n- integration tests - components that exist in isolation. They don't have to interact with the context in which they are placed. They can exist as a unit. Essentualy these are our unit tests.\n- acceptance tests - components that need to interact with other areas of the app (ex. navbar link functionality)\n\n# Mirage\n## Factories\n- allow you to create blueprints for your data. In other words, seed your development database\n","n":0.025}}},{"i":1198,"$":{"0":{"v":"Email","n":1},"1":{"v":"\nNever send to generic emails, like `info@[businessname].com`. This hurts your sender reputation score\n\nThe inbox of a recipient needs to authenticate the source of the email (usually with SPF or DKIM)\n\nSPF & DKIM are authentication systems that tell Internet Service Providers (ISPs), like Gmail and Yahoo, that incoming mail has been sent from an authorized system, and that it is not spam or email spoofing.\n\nSPF & DKIM authentication cannot be done for free webmail accounts like Google, Yahoo, and Hotmail.\n\nThere is only one SPF record per domain. (If you have more than one SPF DNS record, ISPs will not know which one to use which may cause authentication issues.)\n- multiple DKIM records is fine\n\nWith SMTP you are sending, relaying, or forwarding messages from a mail client (like Microsoft Outlook) to a receiving email server. A sender will use an SMTP server to carry out the process of transmitting an email message.\n\n### IMAP (Internet Access Message Protocol) \nIMAP is an email protocol that deals with managing and retrieving email messages from the receiving server.\n1. After creating an email and pressing ‘send’, your email client (e.g. Gmail, Thunderbird, Outlook, etc.) will use SMTP to send your message from your email client to an email server.\n2. Next, the email server will use SMTP to transmit the message to the recipient’s receiving email server.\n3. Upon a successful receipt of the SMTP transmission (indicated by a 250 OK response code), the recipient’s email client will fetch the message using IMAP and place it in the inbox for the recipient to access.\n\n### Pop3\nPOP3 downloads the email from a server to a single computer, then deletes the email from the server.\n\nOn the other hand, IMAP stores the message on a server and synchronizes the message across multiple devices.\n\n# UE Resources\n[Email Sender Reputation](https://www.sparkpost.com/resources/email-explained/email-sender-reputation/)\n","n":0.058}}},{"i":1199,"$":{"0":{"v":"Email DNS Records","n":0.577},"1":{"v":"\n### SPF Records\n![[dns.records.SPF]]\n","n":0.577}}},{"i":1200,"$":{"0":{"v":"DKIM","n":1},"1":{"v":"\nDKIM (Domain Keys Identified Mail) is an email authentication technique that allows the receiver to check that an email was indeed sent and authorized by the owner of that domain. \n- Therefore DKIM is supposed to reduce spoofing.\n- This is done by giving the email a digital signature. This DKIM signature is a header that is added to the message and is secured with encryption.\n\nOnce the email receiver (or receiving system) determines that an email is signed with a valid DKIM signature, it is confident that nothing in the email (body, subject, attachments etc) has been modified.\n\nDKIM signatures are validated on the server, and thus aren't visible to end users.\n","n":0.095}}},{"i":1201,"$":{"0":{"v":"Elastic Search","n":0.707},"1":{"v":"\n### What is it?\nElasticSearch is an open-source, RESTful, distributed search and analytics engine built on Apache Lucene\n- You can send data in the form of JSON documents to Elasticsearch using the API. \n  - Elasticsearch automatically stores the original document and adds a searchable reference to the document in the cluster’s index. You can then search and retrieve the document using the Elasticsearch API\n- due to its distributed nature, documents are available on all nodes of the cluster.\n  - Each document in an index belongs to one primary shard, but is replicated amongst the other shards.\n    - ES selects the shards that the query should go to in a round-robin fashion\n- ES is NoSQL and is more powerful, flexible, and faster than SQL's LIKE\n- ES Documents are heavily [[denormalized|db.design.normalization]], resulting in documents that do not reference one another.\n\n#### Example Reddit post as ElasticSearch document:\n```json\n{\n  \"id\": \"abcdefg\",\n  \"title\": \"Amazing subreddit for nature lovers!\",\n  \"content\": \"Hey everyone!\\n\\nI just stumbled upon this incredible subreddit called NatureIsBeautiful and I can't stop scrolling through the posts.\",\n  \"author\": \"nature_enthusiast23\",\n  \"created_at\": \"2023-06-05T14:30:00Z\",\n  \"subreddit\": \"NatureIsBeautiful\",\n  \"upvotes\": 1500,\n  \"comments\": 87,\n  \"tags\": [\"nature\", \"photography\", \"community\"],\n  \"url\": \"https://www.reddit.com/r/NatureIsBeautiful/comments/abcdefg/amazing_subreddit_for_nature_lovers/\"\n}\n```\n\n### Why use it?\nES is typically used when you have:\n- high data volumes, and are likely to need multiple nodes to process the data\n- unstructured or semi-structured data (log files, text, ...). You ingest the raw data in its original form.\n- the data is treated as a blob, and thus never updated. It’s ingested once, queried, and then purged according to some bulk retention policy (e.g. older than 30 days)\n- you need to access aggregate data more than individual records\n- you need to index in real time, allowing you ingest high-throughput data streams and query that data quickly, making it well-suited for applications that require constant updates and querying of rapidly changing data\n\nWhen to use ElasticSearch?\n- If your use case requires a full-text search, including features like fuzzy matching, stemming (e.g. having the word \"run\" also match \"runs\", \"running\" etc), and relevance scoring.\n- If your use case involves chatbots where these bots resolve most of the queries, such as when a person types something there are high chances of spelling mistakes. You can make use of the in-built fuzzy matching practices of the ElasticSearch\n- Also, ElasticSearch is useful in storing logs data and analyzing it\n\nOther use cases:\n- Add a search box to an app or website\n- Store and analyze logs, metrics, and security event data\n- Use machine learning to automatically model the behavior of your data in real time\n- Automate business workflows using Elasticsearch as a storage engine\n- Manage, integrate, and analyze spatial information using Elasticsearch as a geographic information system (GIS)\n- Store and process genetic data using Elasticsearch as a bioinformatics research tool\n\nElastic search scales horizontally with your requirements.\n\nForms part of the ELK stack (along with Logstash and Kibana), giving us log analysis, monitoring, and visualization in the context of application and server logs.\n\n#### As part of the ElasticStack (ELK)\nELK consists of ElasticSearch, Kibana, Beats and Logstash\n- *Logstash* and *Beats* facilitate collecting, aggregating, and enriching your data and storing it in Elasticsearch\n- *Kibana* enables you to interactively explore, visualize, and share insights into your data and manage and monitor the stack.\n- *Elasticsearch* is where the indexing, search, and analysis magic happens.\n\n### How does it work?\nWhen you're searching for text. ES ranks search results based on how close the phrase or words are. SQL doesn't do this nearly as well.\n- ES starts to shine when you start to do a lot of filtering\n\nElasticsearch chooses the best underlying data structure to use for a particular field type. \n- Text is tokenized and stored in an inverted index, which supports very fast full-text searches.\n  - an inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in.\n  - ex. if we search for the string `London`, it is the inverted index that allows us to quickly know that the string occurs in 6 different documents in the index.\n- Numeric and geolocational data is stored in BKD trees\n  - this allows for fast-range searches and nearest-neighbor queries in large data sets\n\nSecondary [[indexes|db.strategies.index]] are the raison d’être of search servers such as Elasticsearch.\n\nMapping is the process by which ES determines how a document is stored and indexed.\n\n#### How data is retrieved\nBased on the query terms passed, each document retrieved will be assigned a score. The documents are then returned to the client sorted by that score.\n- this is [the BM25 algorithm](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables)\n- note: if I pass \"prescription refill\", then ES recognizes that there are 2 terms: `prescription` and `refill`\n\nSome factors that determine the document's score:\n- *rarity* - queries that contain rarer terms (amongst *all* documents) have a higher multiplier, meaning they contribute more to the final score\n  - ex. the word \"the\" is likely to be very common amongst all matching documents, while the word \"elephant\" likely to be rare. As a result, ES recognizes that the word \"elephant\" is more important, and makes its contribution to the final document's score higher.\n  - this is known as *Inverse Document Frequency (IDF)*\n- *density* - documents that are longer than average will have the score penalized. \n  - That is, the more terms in the document (ones that don't match the query), the lower the score for the document.\n  - expl: this makes intuitive sense: if a document is 300 pages long and mentions the word elephant once, the document is more likely to have said something like \"elephant in the room\", rather than it actually being a document about elephants. On the other hand, if the document is a tweet of 140 characters, then the word Elephant is much more likely to have actually been about Elephants.\n  - this is known as *Term Frequency (TF)*\n\nIn the absense of replicas, a given query and set of documents will result in a more-or-less deterministic result\n- this non-determinism resulting from replicas happens because ES determines which shard the query should go to in a round-robin fashion, so the same query run twice in a row will likely go to different copies of the same shard.\n\n### How to use it?\n#### Searching data\nThe Elasticsearch REST APIs support structured queries, full text queries, and complex queries that combine the two.\n- *Structured queries* are similar to the types of queries you can construct in SQL. \n  - ex. you could search the gender and age fields in your employee index and sort the matches by the hire_date field. \n  - [Query SDL](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html), [ElasticSearch SQL](https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-overview.html)\n- *Full-text queries* find all documents that match the query string and return them sorted by relevance—how good a match they are for your search terms.\n\n#### Performing aggregations\nAggregations enable you to build complex summaries of your data and gain insight into key metrics, patterns, and trends.\n\nInstead of just finding the proverbial “needle in a haystack”, aggregations enable you to answer questions like:\n- How many needles are in the haystack?\n- What is the average length of the needles?\n- What is the median length of the needles, broken down by manufacturer?\n- How many needles were added to the haystack in each of the last six months?\n- What are your most popular needle manufacturers?\n- Are there any unusual or anomalous clumps of needles?\n\nBecause aggregations leverage the same data-structures used for search, they are also very fast.\n\n* * *\n\n## ElasticSearch Primitives\nComparison to RDBMS\n- RDBMS => Databases => Tables => Columns/Rows\n- Elasticsearch => Clusters => Indices => Shards => Documents\n\n### Index\nAn Elasticsearch index is a logical namespace that holds a collection of documents\n- That is, an ES Index has nothing to do with [[database indexes|db.strategies.index]], and are more comparable to tables in SQL\n- \"indexing a document\" means \"inserting a document into the index\"\n\n### Index Mapping\nessentially a schema for how data will be structured in the index\n\nEach field in a mapping has an analyzer associated with it\n\n### Analyzer\nEach analyzer contains:\n- a tokenizer\n- a normalizer\n- filters\n\nES has built-in analyzers, but we can define custom ones, where we define our own tokenizer\n\n### Tokenizer\n- converts text into tokens\n  - ex. converts \"a quick brown fox jumps over the lazy dog\" into terms `[\"a\", \"quick\", \"brown\"]` etc.\n\nTokenizer types:\n- word-oriented\n- partial-word\n- structured text\n\n#### N-gram tokenizer\ncan break a word up into a sliding window of continuous letters\n- ex. \"quick\" -> [\"qu\", \"ui\", \"ic\", \"ck\"]\n\n#### Edge N-gram tokenizer\n- ex. \"quick\" -> [\"q\", \"qu\", \"qui\", \"quic\", \"quick\"]\n\n### Filter\nmight do things like removing articles from the terms (e.g. `a`, `the`), or do things like include derivate words in the search (e.g. cleaner -> `[\"cleaning\", \"cleaned\", \"cleans\"]`), or a synonym filter, which adds matches for synonyms that may appear.\n\n### Normalizer\nA special type of analyzer\n- emits a single token for a given input, instead of an array of tokens\n\n* * *\n\n## Queries\nCompound vs Leaf queries\n- leaf query matches against a specific field\n- compound query combine leaf queries in various ways \n\nType of compound queries\n- bool (ex. `should`, `must_and` etc.)\n- boosting\n- constant_score\n- dis_max - only the highest score of any leaf query within a compound query will be considered\n- Function_score - allow us to use more complex functions to determine the score\n\nLeaf queries can have their scores boosted with multipliers\n\n### Full-text Query\nA type of leaf query\n\nMatches against text in a specific field\n\n`match` is the most common type of full-text query\n\n\n# Tools\n- [Kibana: a data visualization platform for Elasticsearch](https://www.elastic.co/kibana)\n","n":0.026}}},{"i":1202,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Query\n```sh\ncurl -X GET \"https://localhost:9200/index_name/_search?q=<query_string>\" \\\n    -H \"Authorization: ApiKey \"${API_KEY}\"\" \\\n    -H \"Content-Type: application/json\"\n```","n":0.267}}},{"i":1203,"$":{"0":{"v":".NET","n":1},"1":{"v":"\n.NET is a framework that contains a large set of programs to be called from the program you are building. These functions/programs can be as simple as *join 2 arrays*, or can be as complex as *translate voice to text*/*recognize red object in an image*.\n\n.NET is made up of an SDK and a runtime environment\n\nthe languages that are used in the framework can all be compiled down to a common language\n- ex. [[c#]], VB, IronPython (a .NET implementation of Python)\n\nThe computers that run a .NET application need to have the corresponding .NET framework installed prior to being able to run it.\n\n### Major components\nCommon Language Runtime (CLR) – This allows the executions of programs written in the .NET framework using C#, VB, Visual C++\n- also used to provide services such as memory management, security, exception handling, loading and executing of the programs.\n\nFramework Class Library (FCL) – This is integrated with the CLR of the .NET framework and allows writing programs using .NET supporting programming languages such as C#, Visual C++, VB, etc.\n\nUser and Program Interfaces – This provides tools to develop desktop and windows applications. Windows forms, web services, Console applications and web forms are some examples of the user and program interfaces.\n\n### ASP.NET vs .NET\n.NET is a (software) development platform that is used to develop, run and execute the applications\n- unified environment is the key value adder of .NET\n\nASP.NET is a web framework that is used to build dynamic web applications\n- ASP.NET is part of the .NET framework\n    - because of this, developers have access to all of the .NET classes and features\n- applications developed by ASP.NET are largely component-based and built on the top of the common language runtime (CLR) and can be written in any of the .NET languages.\n- ASP stands for Active Server Pages\n\n### .NET Framework vs .NET Core\nBoth are are .NET implementations for building server-side applications.\n- the biggest advantage of .NET Core is that it is cross-platform, unlike the .NET Framework which is tied to Windows.\n\n.NET 5 is the successor to both .NET Core and .NET Framework\n\n\n## Example projects\n- https://github.com/dotnet-architecture/eShopOnWeb","n":0.054}}},{"i":1204,"$":{"0":{"v":"Documentation","n":1},"1":{"v":"\n- [Documentation as code](https://plantuml.com/)\n    - [blog post guide](https://blog.dornea.nu/2023/07/30/documentation-as-code-for-cloud-plantuml/)","n":0.354}}},{"i":1205,"$":{"0":{"v":"Docker","n":1},"1":{"v":"\nUsing Docker, we can abstract away the software, OS, and hardware configuration from our application, turning each service into a building block that we can run anywhere\n- when using containers you have to always think of dynamic vs. static parts of your application. You can use your host's file system to store your data and files. A more scalable and efficient way of thinking about this is to store your data to amazon rds and your files to amazon s3. This way you can spin up as many containers of your app/site as you want and have them all point to a single place where they store your dynamic stuff; namely, your data and files.\n\n## .dockerignore\n- Because being lean is a design principle of Docker, it is important to cut out the stuff from the image that is not necessary to running the code.\n\t- This includes git files, travis.yml, .vscode etc. To ensure these files do not become a part of the image, we put these in a .dockerignore file.\n\n```txt\n# Ignore everything\n**\n\n# Allow some\n!/src/**\n!/jest.config.js\n!/.jest/\n!/.eslintrc.json\n!/tsconfig.json\n!/package*.json\n!/.prettier*\n!/.npmrc\n```","n":0.076}}},{"i":1206,"$":{"0":{"v":"Docker Tools","n":0.707},"1":{"v":"\n## Watchtower\n- allows us to update an already running docker container by simply pushing to Docker Hub. Watchtower sees that a new image has been pushed, automatically runs `docker pull`, then starts the container back up again.\n- note: a message from calibre:\n\t- We do not endorse the use of Watchtower as a solution to automated updates of existing Docker containers. In fact we generally discourage automated updates. However, this is a useful tool for one-time manual updates of containers where you have forgotten the original parameters. In the long term, we highly recommend using Docker Compose.\n\n## Portainer\nan open source, platform agnostic tool for managing containerized applications. It works with Kubernetes, Docker, Docker Swarm, Azure ACI in both data centres and at the edge.\n```\ndocker container run -d \\\n  -p 9000:9000 \\\n  -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer\n```\n[Website](https://www.portainer.io/)\n","n":0.087}}},{"i":1207,"$":{"0":{"v":"Tag","n":1},"1":{"v":"\ntagging your image with the fully-qualified name of your registry allows us to tell Docker where to upload the image\n```sh\ndocker tag neverforget-server:0.0.2 registry.digitalocean.com/neverforget/neverforget-server:0.0.2\n```\n\nAfter that, you can push it\n```\ndocker push registry.digitalocean.com/neverforget/neverforget-server:0.0.2\n```","n":0.183}}},{"i":1208,"$":{"0":{"v":"Docker Registry","n":0.707},"1":{"v":"\nA docker registry is a stateless, scalable server-side application that stores and allows us to distribute docker images\n- In other words, it allows us storage for our images\n\n- Think of it like github for docker images\n    - So we have commands like `docker pull`, `docker push` etc.\n- ex. *Docker Hub*\n- Docker registry stores images as 2 parts, with a pointer between them:\n    - image:tag\n    - digest (i.e. the SHA id)\n- Docker Hub is an available registry that is hosted for us \n","n":0.11}}},{"i":1209,"$":{"0":{"v":"Docker Orchestrators","n":0.707},"1":{"v":"\nTools to manage, scale, and maintain containerized applications\n- ie. [[Kubernetes|k8s]], Docker Swarm\n- Orchestration tools are designed to handle Docker containers running stateless applications\n\t- as such, you should not run stateful applications in orchestration tools which are built for stateless apps.\n\t- this makes sense, seeing as containers can be killed and restarted without any issue, multiple can run at once etc.\n\t\t- The same cannot be said for a database, which is why we typically don't dockerize DBs\n\nThe container scheduler is a framework that determines where a service should be deployed and make sure that it maintains the desired run-time specification. The scheduler manages clusters (which are composed of services)\n\nA cluster scheduler has quite a few goals.\n- It makes sure that resources are used efficiently and within constraints.\n- It makes sure that services are (almost) always running.\n- It provides fault tolerance and high availability.\n- It makes sure that the specified number of replicas are deployed.\n- It makes sure that the desired state requirement of a service or a node is (almost) always fulfilled. Instead of using imperative methods to achieve our goals, with schedulers, we can be declarative.\n- We can tell a scheduler what the desired state is, and it will do its best to ensure that our desire is (almost) always fulfilled.\n\t- ex. instead of executing a deployment process five times hoping that we’ll have five replicas of a service, we can tell a scheduler that our desired state is to have the service running with five replicas.\n\nConsider the impact that is had by having declarative methods, as is offered from a container scheduler. With a declarative expression of the desired state, a scheduler can monitor a cluster and perform actions whenever the actual state does not match the desired. Compare that to an execution of a deployment script. Both will deploy a service and produce the same initial result. However, the script will not make sure that the result is maintained over time. If an hour later, one of the replicas fail, our system will be compromised.\n- traditionally, these problems were solved by having alerts, and having DevOps guys manually intervene to get everything back up and running.\n- Think of schedulers as operators who are continually monitoring the system and fixing discrepancies between the desired and the actual state\n\nIn general, the development workflow looks like this:\n1. Create and test individual containers for each component of your application by first creating Docker images.\n2. Assemble your containers and supporting infrastructure into a complete application, expressed either as a Docker stack file or in Kubernetes YAML.\n3. Test, share and deploy your complete containerized application.\n\n* * *\n\nYou should not run stateful applications (like a db) in orchestration tools which are built for stateless apps.\n- Orchestration tools are designed to handle Docker containers running stateless applications. Such applications don’t mind being terminated at any time, any number can run at the same time without communicating with each other and nobody will really notice if a new container will take over on a different machine.\n","n":0.045}}},{"i":1210,"$":{"0":{"v":"Docker Machine","n":0.707},"1":{"v":"\nDocker engine is a tool that lets us install Docker Engine on virtual hosts and manage those hosts\n\t- Therefore, it is something we install on our own computer.\n\t- with the **driver** concept, we can deploy to 3rd party cloud services, like AWS, DigitalOcean, or even VirtualBox on your local machine\n- *docker-machine* commands let us start, inspect, stop, and restart hosts, as well as configure a docker client to talk to the hosts.\n- it allows us to control the docker engine of a VM created using docker-machine\n- The main reason you would use docker-machine is when you want to create a deployment environment for your application and manage all the micro-services running on it\n- To setup, all we need to do is point our `docker-machine` CLI at our managed host, which enables us to run docker commands directly on that host.\n\n### Connecting to Docker Machine\nThe connection to a docker machine is made available through env variables. By default, they are unset, giving us our default connection\n`docker-machine env -u` will show us how to unset all variables to return to our default connection\nIf we wanted to connect to minikube, we could run `eval $(minikube docker-env)\n","n":0.072}}},{"i":1211,"$":{"0":{"v":"Docker Image","n":0.707},"1":{"v":"\n## Overview\nan image is an immutable snapshot of a system\n- The fact that it is immutable gives it predictability. In other words, it will always work as-is\n\n## Under the hood\nan image has a tree hierarchy. There is a base image (aka Parent Image), which is initiated with the `FROM` command in the Dockerfile. It sets the base for the rest of the images generated in the Dockerfile.\n- Therefore, every Dockerfile must have a `FROM` directive\n\n## Layers\nDocker image is made up of a series of read-only `layers`\n- Each line in the `dockerfile` creates a `layer`\n\t- Therefore, a layer contains only the differences between the preceding layer and the current layer.\n- On top of the layers, there is a writable layer (the current one) which is called the container layer\n- layers may be shared between images\n\t- this means if the layer `COPY . /app` is used in multiple places, each iteration of it (from other Dockerfiles) will not contribute to Docker's overall footprint.\n\t- When the Dockerfile is run with `docker build`, each layer gets executed and its result cached\n- The following Dockerfile instructions create a layer and influence the size of the image:\n\t- `RUN`\n\t- `COPY`\n\t- `ADD`\n- The other Dockerfile instructions create intermediate layers, and do not increase the size of the image\n- When we build an image from Dockerfile, we'll notice that it says `removing intermediate container`, rather than what we might expect: `removing intermediate layer`. This is because a build step (ie. a line in the Dockerfile) is executed in an intermediate container, which is no longer needed once the build step is done\n- if we run `docker history <image-id>`, we can recognize the intermediate containers as the ones having 0B size\n\t- There are also a lot of containers labelled `missing`, meaning that those layers are built on a different system and are not available locally.\n\n```dockerfile\n# create a layer from the ubuntu:15.04 image. \nFROM ubuntu:15.04\n# add some files from your Docker client’s current directory. \nCOPY . /app\n# build your application using the make command. \nRUN make /app\n# specifies what command to run within the container.\nCMD python /app/app.py\n```\nIn the previous example, we spun up a container whose basis is a Ubuntu server. We can just as easily spin up a continer whose basis is nodejs.\n\n## Image Hierarchy\nThe image you inspect may have one or more base images. This means the author of the image used other images as starting points when building the image.\n- Often these base images are either OS images such as Debian or Ubuntu, or programming language images such as Python, Node or Java.\n\n## Tags\n- an alias to the ID of an image. ie. they are just a a way to refer to a specific image\n- anal: git tags can be used to refer to a specific commit (ex. map tag SHA 3fhd883nnf9 to v1.4)\n\n## Architecture\nImages are built to run on particular platforms. Therefore, their architectures must be specified while being built (e.g. `arm64`, `amd64`)\n`docker buildx build --platform linux/amd64 -t neverforget-server:0.0.4 .`","n":0.045}}},{"i":1212,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n### `docker images`\n- show all images\n\n### `docker build`\n- purpose is to build an image from a Dockerfile. The command will find the `Dockerfile` and will build an image based on it.\n\n### Inject env variables into the Dockerfile build process\n`--build-arg NPM_TOKEN=$NPM_TOKEN`\n\n## Build image\n- from your application folder:\n`docker build -t <APP NAME> .`\n  - this causes each line of Dockerfile to be executed, building up the image as it goes\n- Images are static once they are created\n","n":0.115}}},{"i":1213,"$":{"0":{"v":"Dockerfile","n":1},"1":{"v":"\nThink of Dockerfile commands as a step-by-step recipe on how to build up our image.\n- it is the first step to containerizing an application\n\nDockerfiles describe how to assemble a private filesystem for a container, and can also contain some metadata describing how to run a container based on this image\n- A Dockerfile specifies the operating system that will underlie the container, along with the languages, environment variables, file locations, network ports, and other components it needs— and, of course, what the container will actually be doing once we run it.\n\nExample Dockerfile:\n```dockerfile\nFROM node:6.11.5\nWORKDIR /usr/src/app\nCOPY package.json .\nRUN npm install\nCOPY . .\nCMD [ \"npm\", \"start\" ]\n```\n\nBuilding up our images takes the following steps:\n\n1. Start FROM the pre-existing `node:6.11.5` image. This is an official image, built by the node.js vendors and validated by Docker to be a high-quality image containing the `node 6.11.5` interpreter and basic dependencies.\n2. Use WORKDIR to specify that all subsequent actions (e.g. COPY, RUN) should be taken from the directory /usr/src/app in your image filesystem (in other words, not the FS on your machine).\n3. COPY the file package.json from your host to the working directory in your image (so in this case, to /usr/src/app/package.json)\n  - The fact that we run `npm install` before copying over \"everything else\" is significant here. If we had copied everything and then run `npm install`, each file change would cause Docker to run `npm install`. Instead, by copying only `package.json`+`package-lock.json`, `npm install` will only get run when there are changes in those 2 files.\n4. RUN the command npm install inside your image filesystem (which will read package.json to determine your app’s node dependencies, and install them)\n5. COPY in the rest of your app’s source code from your host to your image filesystem.\n\nThese above commands effectively set up the filesystem of our image\n\n`CMD` specifies how to run a container based off *this* particular image\n- In this case, it’s saying that the containerized process that this image is meant to support is npm start.\n- i.e. it is a metadata specification\n- there can only be one `CMD` instruction per Dockerfile\n\n`ENTRYPOINT` allows us to configure the container to run as an executable\n- the commands specified in `ENTRYPOINT` will always be run.\n- we also have `CMD`, whose commands will only run if we are spinning up a container and not explicitly setting any CLI arguments\n\t- if we specify arguments when spinning up a container, `CMD` is ignored, but `ENTRYPOINT` commands will still be executed\n\t- ex. `docker run -it <image> <arg1>`\n\t- `CMD` therefore are default arguments\n","n":0.049}}},{"i":1214,"$":{"0":{"v":"Docker Engine","n":0.707},"1":{"v":"\nDocker engine is a client-server application made up of the Docker daemon, a REST API allowing us to interact with the daemon, and a CLI for talking to the daemon. \n- Docker Engine accepts docker commands from the CLI (`docker run` etc)\n- When people say “Docker” they typically mean Docker Engine\n","n":0.14}}},{"i":1215,"$":{"0":{"v":"Docker Containers","n":0.707},"1":{"v":"\nA Container is an instance of an [[Image|docker.image]]\n\nContainers decouple the machine's OS from the application dependencies and the code.\n- Each container had its own filesystem (provided by a Docker image)\n- Image includes everything needed to run an application\n    - inc. the application code, runtimes, dependencies etc\n- Containers are designed to be transient and temporary, but they can be stopped and restarted, which launches the container into the same state as when it was stopped.\n- Containers can communicate with each other by attaching (`docker attach`). They do this by attaching stdin, stdout and stderr steams to one another so that one container's output can be piped into another container as their input \n\t- When we run `docker attach` with a specified container, we are piping our stdin/stdout/stderr to the container, so we are effectively able to write commands in the container's terminal\n\ncontainers can be created from any point in an image’s history, much like source control.\n\n## From the container's perspective \nThe container by default has no access to the outside world. When the docker host specifies the `-p` option when spinning up a new container, the container opens up the specified ports and maps them to the docker host's port.\n- For all the container knows, it is a regular computer. It has a network interface, including an IP address, a gateway, a routing table etc. \n\n![9ede10f99d18b464b0087150a5679308.png](:/26150f69cbf24c1583bf667d69c6ac9b)\n\n- Containers guarantee our applications will run the same anywhere, whether it's our own machine, a data centre, or anything else\n\n## Run container\n`docker run --name *<NAME OF CONTAINER>* -d -v /tmp/mongodb:/data/db -p 27017:27017 *<NAME OF IMAGE:IMAGE TAG>*`\n- `--name`: Name of the container.\n- `-v`: Attach the /tmp/mongodb volume of the host system to /data/db volume of the container.\n- `-p`: Map the host port to the container port.\nLast argument is the name/id of the image.\n\n## Running commands in the container\n- we can use `docker exec` to run commands.\n- by default, the command is executed in the WORKDIR variable that is declared in the Dockerfile\n- often, we see `docker exec -it <containerID> /bin/bash`\n\t- this executes the command `/bin/bash` within the specified container, opening a new bash session for us.\n\t- `-t` tells docker to open a terminal session\n\t- `-i` for interactive, ensures that our stdin input stream remains open \n\n## Stopping container\n- `docker stop`: Send SIGTERM(termination signal), and if needed SIGKILL(kill signal)\n\t- use when you wish to clear up memory or delete all of the processes' -cached- data. Simply put, you no longer care about the processes in the container and are comfortable with killing them.\n- `docker pause`: Send SIGSTOP(pause signal)\n\t- use when you only want to suspend the processes in the container; you do not want them to lose data or state.\n\n## Actions\n- get IP of container - `docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <my-container-name>`\n","n":0.047}}},{"i":1216,"$":{"0":{"v":"Volumes","n":1},"1":{"v":"\nVolumes are the primary way to persist data that exists in a Container\n- in a node app, we specify the following `volume` field attributes\n```yml\nvolumes:\n- '.:/app'\n- '/app/node_modules'\n```\n- This first line is saying \"map the host's PWD (.) to the `/app` directory in the container\". The next line says \"persist the node-modules directory in the container so that when we mount the host directory (the first line) at runtime, the node-modules won't get overwritten\"\n\t- In other words, this would happen:\n\t\t- Build - The node_modules directory is created in the image.\n\t\t- Run - The current directory is mounted into the container, overwriting the node_modules that were installed during the build.\n\n## Bind Mount\nWhen using a bind mount, a file/directory on the host machine is mounted into a container\n- This is in contrast to using volumes, which involves Docker creating a directory on the host that a container can have full access to (volume)\n- A bind mount lets us create something similar to a symbolic linked directory, whereby a directory or file on the host machine gets mounted into the container.\n\t- This means that we can make changes to the directory/file on the host machine, and those changes will reflected in the docker container's version of the directory/file. \n\t- On the other hand, when you use a volume, a new directory is created on the host machine, which is created within Docker's storage directory (which is managed by Docker)\n- bind mounts are created in `docker-compose.yml` with the `volumes` directive\n- Bind mounts have limited functionality compared to volumes.\n\n# UE Resources\n[Primer](https://docs.docker.com/storage/volumes/)\n","n":0.063}}},{"i":1217,"$":{"0":{"v":"Networks","n":1},"1":{"v":"\nNetworks enable containers to be able to communicate with each other and with non-Docker processes (such as a host) \n- Networks are natural ways to isolate containers from other containers or other networks. As such, they provide complete isolation for containers\n- Docker’s networking subsystem is pluggable, made possible by having drivers\n\t- depending on which driver you are using, you will have different core networking functionality \n- `docker network ls` - show all networks\n- all containers within a network can communicate with each other. This can be shown by the fact that you can ping the IP address of one container from another (within the same network) \n\t- you can also simply `ping <container-name>`\n- Docker networking allows you to attach a container to as many networks as you like.\n- to see if 2 containers are properly on the same network, try pinging one container's IP from another.\n\nThere are 2 main types of network: bridge and overlay\n\n## Bridge\n- this is the default\n\t- Docker creates a bridge named `docker0`, and both the docker host and the docker containers have an IP address on that bridge.\n- Bridge networks are usually used when your applications run in standalone containers that need to communicate over the same docker host (ex. a pod?)\n- Limited to a single host running Docker Engine.\n- default type\n- if our `docker-compose.yml` does not explicitly specify a network to use, a special network called *bridge* will be the network that our containers are launched in \n\t- visible with `docker network ls`\n\n## Overlay\n- Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other.\n- can include multiple hosts and is a more advanced configuration\n\n## Host network\n- if we have a standalone container, network isolation between the container and the Docker host will be removed, and the container will use the host's network directly\n- host is only available for swarm services\n","n":0.057}}},{"i":1218,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n##### Remove all containers built from Image 7401323eb08e\n```\ndocker ps -a | awk '{ print $1,$2 }' | grep 7401323eb08e | awk '{print $1 }' | xargs -I {} docker rm {}\n```\n\n* * *\n\n- `docker logs <container name>` - see what happened during container initialization\n- `docker inspect <container-name>` - get low-level info about a container, such as its IP, port mappings,  \n- remove all containers based on one image (ex. monica) - `docker ps -a | awk '{ print $1,$2 }' | grep monica | awk '{print $1 }' | xargs -I {} docker rm {}`","n":0.103}}},{"i":1219,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n## Commands\n### run a command in a new container\n`docker run`\n- spec: Each time we execute `docker run`, the Dockerfile is read. This is why calling `docker run` on an existing container wouldn't make any sense.\n\n### show all running containers in docker engine\n`docker ps`\n- pass `-a` to see all containers\n- `docker-compose ps` will list containers in the docker engine that are related to the images specified in `docker-compose.yml` (therefore `dc ps` is a subset of `docker ps`)\n\n## Start a container\n`docker container run --publish 8000:8080 --detach --name bb <APP NAME>`\n- `--publish` asks Docker to forward traffic incoming on the host’s port 8000, to the container’s port 8080\n  - containers have their own set of ports\n- notice, we didn’t specify what process we wanted our container to run. We didn’t have to, since we used the CMD directive when building our Dockerfile; thanks to this, Docker knows to automatically run the process npm start inside our container when it starts up\n- visit application at localhost:8000\n\n### Show all containers\n`docker container ls`\n\n#### Filter containers by those created by *mongo* image\n`docker container ls -f ancestor=mongo`\n","n":0.075}}},{"i":1220,"$":{"0":{"v":"Docker Compose","n":0.707},"1":{"v":"\nDocker Compose is a tool for defining and running multi-container applications\n- use `docker-compose.yml` to configure the app's services, and start them with a single command\n\t- this file will be parsed everytime you run `docker-compose up`. Therefore, if you make changes to `docker-compose.yml`, all you need to do is `docker-compose down`, then `dc up` again. This is unlike the Dockerfile, which will only be run once when you are building an image\n\nusing compose is a 3 step process:\n1. Define your app’s environment with a Dockerfile so it can be reproduced anywhere.\n2. Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.\n3. Run docker-compose up and Compose starts and runs your entire app.\n\nCompose has commands for managing the whole lifecycle of your application:\n- Start, stop, and rebuild services\n- View the status of running services\n- Stream the log output of running services\n- Run a one-off command on a service\n\n## Anatomy of docker-compose.yml\nThis file will be run with `docker-compose up`, building up the services (and with it, the containers) of the whole application\n- if the image has already been built, then by default the build command will be ignored\n\nThe docker-compose.yml file is organized by service. The `build` field designates the location of the service's dockerfile (this location is called the \"build context\") so that it can be run and the image can be built.\n\nthe docker-compose.yml file can specify environment variables to be executed during process of building the images with the `args` field under `build`\n- these variables are accessed in the dockerfiles by specifying `arg`:\n\ndockerfile:\n```yml\narg buildno\narg gitcommithash\n\nrun echo \"build number: $buildno\"\nrun echo \"based on commit: $gitcommithash\"\n```\n\ndocker-compose.yml\n```yml\nbuild:\n  context: .\n  args:\n    buildno: 1\n    gitcommithash: cdc3b19\n```\n\n### Publish vs. EXPOSE\n- these commands make the services inside the containers available to outside the containers\n- expose makes the processes in the container accessible to other containers within Docker (but not from *outside* Docker)\n\t- note: this is not entirely correct, but useful for understanding. [see](https://stackoverflow.com/questions/22111060/what-is-the-difference-between-expose-and-publish-in-docker/47594352#47594352)\n- publish (`-p`) makes the services available to outside Docker\n\t- Therefore, publish implicitly EXPOSES\n\n## Service\n- there is one image per service, but there can be multiple containers for a service. In other words, a docker \"service\" is one or more containers from one image\n- In our development set up, we are likely to have a single container per image. When we start to scale and need for additional containers per service arises, we can use `docker service` to create multiple containers from the same image\n\t- consider how this relates to the `docker run` command, which starts up 1+ containers from an equal amount of image (ie. one container per image)\n\t- what arises from running this command is Docker's swarm mode\n- Swarm mode is a container orchestration tool, giving the user the ability to manage multiple containers deployed across many different host machines\n\t- ex. imagine there are 5 containers deplayed across 5 different machines across USA. Swarm mode allows you to interact with all 5 containers at once, instead of having to manage one container at a time\n- This above distinction between a container and a service is precicely what differentiates `docker exec` and `docker-compose run`\n\t- naturally, `docker-compose` runs `docker` commands under the hood.\n\t- `docker exec` exectutes a command within a docker container\n\t- `docker-compose run` executes a command on a service (with the help of a swarm manager(?)\n\n# UE Resources\n[Definitive Guide to Docker Compose](https://gabrieltanner.org/blog/docker-compose)\n","n":0.042}}},{"i":1221,"$":{"0":{"v":"DNS","n":1},"1":{"v":"\n## What is it?\nDNS can be thought of as a directory service for the internet.\n- Consider that a real-life directory service works like this: You have the name of someone, and you consult an agent to get \"directions\" (or a phone number) on how to reach them. A DNS is a similar entity which when given a domain name, can give us the IP address that it represents.\n- DNS is used by virtually all inter-networking software, including [[email|email]], [[ssh|unix.cli.ssh]], [[FTP programs|protocol.FTP]], and [[browsers|browser]]\n- DNS is essentially made up of geo-replicated caches\n\nThe DNS is a [[distributed database|db.distributed]]. The management of host information is split among many sites and organizations\n- You don’t need to submit your data to some central site or periodically retrieve copies of the \"master\" database. You simply make sure your section (called a [[zone|dns.db.zone]]) is up to date on your [[nameservers|dns.nameserver]].\n- This structure allows local control of the segments of the overall database, yet data in each segment is available across the entire network through a client/server scheme.\n- [[replication|db.distributed.replication]] and [[caching|deploy.distributed.cache]] is used to achieve robustness and high performance.\n\nThe DNS can be thought of the administrative assistant of the internet. It carries all the administrative responsibility of making things work, albeit behind the scenes. \n\nThe DNS is made up of Internet nameservers and a communication protocol\n\nnote: There is no way to specify port numbers in DNS. If you are running a website, your server must respond to HTTP requests on port 80\n\n## why use it?\nIf it were just us, we could keep a simple key-value storage on our computer of IP addresses and domain names. \n- The DNS system takes this simple concept and makes it remotely accessibly to anyone on the internet.\n- in reality, more than just the IP is stored in the [[dns.db]]. Also included is information about mail routing, etc.\n\nDNS illustrates how a piece of core network functionality (network-name to network-address translation) can be implemented at the application layer of the internet.\n\n## History\nDNS is based on a distributed database that takes time to update globally. In the early internet days, the database (a simple `HOSTS.txt` file) was small and could be updated by hand. As the internet grew, this became unfeasible, so a new management structure was introduced: the concept of *domain name registrars*. The idea was that the updates to the database would be handled by the registrars\n- nowadays, when you make updates to domain name management settings, the registrar will push out the updated information to other DNS servers. \n\n# Resources\nhttps://shapeshed.com/unix-traceroute/\nhttps://activedirectorypro.com/dns-best-practices/#dns-aging-scavenging\n\n# UE Resources\n- [DNS sinkhole](https://en.wikipedia.org/wiki/DNS_sinkhole)\n- [How DNS works guide](https://howdns.works/)\n","n":0.048}}},{"i":1222,"$":{"0":{"v":"DNS Resolver","n":0.707},"1":{"v":"\nThe DNS resolvers constitute the client half of the system's [[client-server architecture|general.arch.client-server]]\n- [[dns.nameserver]] are the server\n\nResolvers are often just functions that create name resolution queries and send them across a network to a [[nameserver|dns.nameserver]]. When the response is received, it hands it back to the program that requested it in the first place (e.g. spec: a browser)\n- in other words, our computer initiates the resolver to query a nameserver at port 53, with the intention of resolving an IP address  \n\nThe DNS resolver (ie. recursive resolver) is at the beginning of the DNS query (with the authoritative nameserver at the end).\n- the resolver receives requests from web browsers, and in turn makes additional requests to the rest of the servers in a final effort to resolve the query. \n- a resolver will tell us what the IP is, given a domain name. By default, if a resolver cannot tell us, then that is it. However, the fact that it is recursive means that not only can we query a resolver, but a resolver can make queries itself. Put another way, if it doesn't have the answer we are looking for, it has the capacity to make queries itself. \n- After receiving a request for a webpage, the resolver will either respond with cached data (given to it by the nameservers), or will send a request to the *root nameserver*, followed by a request to a *TLD nameserver*, and then one last request to an *authoritative nameserver* (This is called [[DNS Lookup|dns.lookup]])\n\t- At each step, there is a possibility that a cached version of the website will be found, and it can immediately be returned to the client, rather than continuing to recurse down the DNS tree.\n- After receiving the requested IP address (from the authoritative nameserver), the recursive resolver sends a response to the client\n\n## Configuration file\nThe resolver is configured in `/etc/resolv.conf`\n- run `scutil --dns` on Mac to see DNS configuration\n\n### Directives\n#### `search`\nFor example, the directive:\n```\nsearch corp.hp.com paloalto.hp.com hp.com\n```\ninstructs the resolver to search the corp.hp.com domain first, then paloalto.hp.com, and then the parent of both domains, hp.com.\n\n#### `nameserver`\ntells the resolver the IP address of a nameserver to query. For example, the line:\n```\nnameserver 15.32.17.2\n```\ninstructs the resolver to send queries to the nameserver running at the IP address 15.32.17.2 instead of to the local host. This means that on hosts not running nameservers, you can use the nameserver directive to point them at a remote nameserver. Typically, you configure the resolvers on your hosts to query your own nameservers.","n":0.049}}},{"i":1223,"$":{"0":{"v":"Domain Name Registry","n":0.577},"1":{"v":"\nA registry is an organization responsible for maintaining a top-level zone’s datafiles, which contain the delegation to each subdomain of that top-level domain.\n\nThe DNS registry is a large database of all domain names and the associated registrant information in the TLDs of the DNS\n\t- With a database, third party entities (companies) can request administrative control of a domain name.\n- Most registries operate on the top-level and second-level of the DNS.\n- while registries manage domain names, they delegate the *reservation* of domain names to registrars\n\nany given top-level domain can have no more than one registry\n\n### Registrar\nA registrar acts as an interface between customers and registries\n\nOnce a customer has chosen a subdomain of the top-level zone, the registrar submits to the appropriate registry the zone data necessary to delegate that subdomain to the nameservers the customer specified.\n\nThe registries act, more or less, as wholesalers of delegation in their top-level zone. The registrars then act as retailers, usually reselling delegation in more than one registry.\n\n### Registration\nRegistration is the process by which a customer tells a registrar which nameservers to delegate a subdomain to and provides the registrar with contact and billing information.","n":0.073}}},{"i":1224,"$":{"0":{"v":"DNS Records","n":0.707},"1":{"v":"\nA resource record is where the data associated with the [[domain|dns.domain]] name is stored.\n\nResource records exist together in a single file known as a *zone datafile*\n\nThe resource records describe all the hosts in the [[zone|dns.db.zone]] and mark any delegation of subdomains\n\nRecords are divided into classes, but the most common class is *internet*.\n- every class of records defines its own *address* record.\n\nResource records are associated with a single [[zone|dns.db.zone]] (as a result, they are also known as *zone files*)\n\nDNS Resource Records are instructions that live in [[authoritative DNS servers|dns.nameserver.authoritative]] and provide information about a domain\n- ex. You can think of a set of DNS records like a business listing on Yelp. That listing will give you a bunch of useful information about a business such as their location, hours, services offered, etc\n- a DNS record is a mapping between domain name and IP address. \n\nwhen we enter `facebook.com` in an address bar, we request an A record from the DNS. When we send an email, we request an MX record from the DNS. \n- A record is therefore the thing that we (the client) query for when engaging with the DNS\n\n## RR Field Values\nValues\n- `@` - indicates that this is a record for the root domain\n- `*` - indicates a wildcard, which will match all subdomains.\n\n### TTL (Time to Live)\nHow long the website will live in the caching nameserver before a new one is requested. If the TTL is 1 hour, then the server will only request a new version of the site every hour. All requests made to that URL will be made to the cached version, until the hour is up, and a new version is used.\n- ex. a value of 14400 for TTL means that if a record gets updated, it takes 240 minutes (14400 seconds) to take effect.\n\nAs we know, there are many layers to DNS. It could be the case that we are able to resolve the domain name in the ISP nameserver, but we also may need to recurve further. The TTL will give us the max amount of time that particular resolution can exist on the ISP nameserver\n- In other words, how long can a given mapping of domain to IP address live for in the cache, before we need to make a more recursive query? \n\n\n### Record Data\n- Think of it like environment variables. We are passing data to the record. Naturally, whatever data we pass will vary in result depending on what type of record we have designated. \n\n### Record Class\n- this is the namespace of the record information.\n\t- The most commonly used namespace is that of the Internet (`IN`)\n\n# UE Resources\n- [DNS records - an introduction](https://www.linode.com/docs/networking/dns/dns-records-an-introduction/)\n","n":0.047}}},{"i":1225,"$":{"0":{"v":"TXT Record","n":0.707},"1":{"v":"\n- originally meant to hold human-readable notes, nowadays they are able to be used to pass in machine-readable code.\n- two of the most important uses for TXT records are email spam prevention and domain ownership verification\n","n":0.167}}},{"i":1226,"$":{"0":{"v":"SRV Record","n":0.707},"1":{"v":"\n- SRV records are how a port can be specified within the DNS\n- must point to an A record.\n","n":0.229}}},{"i":1227,"$":{"0":{"v":"SOA Record","n":0.707},"1":{"v":"\nStands for *Start of Authority*\n\nThe SoA record indicates that \"this nameserver is the best source of information for the data within this zone\".\n- therefore, the nameserver that this record exists on is authoritative *because* of the SoA.\n\nThere can only be one SoA per zone.\n\nThis is the most important address record, and must be specified.\n- Contains the authoritative master nameserver for the zone or domain, as well as an admin email that we specify.\n- Also contains administrative information about the zone\n- *MNAME* - the primary nameserver for the zone\n","n":0.107}}},{"i":1228,"$":{"0":{"v":"NS Record","n":0.707},"1":{"v":"\nA Nameserver Record indicates which [[authoritative nameserver|dns.nameserver.authoritative]] contains the actual DNS records\n- Basically, NS records tell the Internet where to go to find out a domain's IP address\n- Each authoritative nameserver in our [[dns.db.zone]] should have an NS record.\n\t- ex. if we have 2 nameservers, we should have 2 NS records.\n\nA domain often has multiple NS records which can indicate primary and backup nameservers for that domain. \n- if one nameserver goes down or is unavailable, DNS queries can go to another one\n- Typically there is one primary nameserver and several secondary nameservers, which store exact copies of the DNS records in the primary server\n\t- Updating the primary nameserver will trigger an update of the secondary nameservers as well.\n\nWithout properly configured NS records, users will be unable to load a website or application.\n\nWhen multiple nameservers are used (as in most cases), NS records should list more than one server\n\nNS records must point to an A record\n- ex. the resolver may have the NS records, but no A record, and it will still be able to query those nameservers directly, rather than having to go through the TLD server\n\nUpdating NS records\n- Domain administrators should update their NS records when they need to change their domain's nameservers\n- update NS records if you want a subdomain to use different nameservers than the domain (ex. example.com and blog.example.com have 2 different nameservers)\n","n":0.066}}},{"i":1229,"$":{"0":{"v":"MX Record","n":0.707},"1":{"v":"\nMail exchange (SMTP)\n- MX Records point to the incoming smtp server for a domain\n- the record indicates how email messages should be routed (in accordance with SMTP)\n\t- ex. When a user sends an email to john.smith@gmail.com, the *Message Transfer Agent* (MTA) sends a DNS query to identify the mail servers for that email address. The MTA establishes an SMTP connection with those mail servers, starting with the prioritized domains\n- must point to an A record\n- `priority` - lower number indicates preference. In the result of a send failure, the next priority domain will be attempted \n\t- `mailhost1.tycholiz.com` might have `priority` of 10, and `mailhost2.tycholiz.com` might have `priority` of 20, which would mean `mailhost2` only gets used when the first message fails to send. \n\t- if we use the same priority, then both servers will receive equal amount of mail (effectively a load balancer) \n\t- priority exists to prevent mail-routing loops\n\nMX records specify a mail exchanger for a domain name: a host that will either process or forward mail for the domain name (through a firewall, for example)\n- Processing the mail means either delivering it to the individual to whom it’s addressed or gatewaying it to another mail transport, such as X.400\n- Forwarding means sending it to its final destination or to another mail exchanger closer to the destination via [[SMTP|protocol.SMTP]]","n":0.067}}},{"i":1230,"$":{"0":{"v":"CNAME","n":1},"1":{"v":"\n- Canonical name record\nA CNAME record maps an alias to its canonical (ie. proper) name.\n- When a nameserver looks up a name and finds a CNAME record, it replaces the name with the canonical name and looks up the new name. \n- ex. when the nameserver looks up wh.movie.edu, it finds a CNAME record pointing to wormhole.movie.edu. It then continues the lookup, and tries wormhole.movie.edu and returns both addresses.\n\n- points to a domain, not an IP address\n- CNAME records allow a machine to be known by more than one hostname\n- map an alias name to a true (canonical) domain name\n- typically used to map a subdomain, like `www.stuff.com` to `stuff.com`, or `stuff.com` to `blog.stuff.com`)\n\t- This is good, because if the host IP address changes, then we only need to change the A record. The CNAME record depends on the domain, not the IP address it's associated with.\n- anal: Imagine a scavenger hunt where each clue points to another clue, and the final clue points to the treasure. A domain with a CNAME record is like a clue that can point you to another clue (another domain with a CNAME record) or to the treasure (a domain with an A record).\n- ex. imagine we give `blog.tycholiz.com` a CNAME with value `tycholiz.com`. This means that any time a DNS server hits the DNS records for `blog.tycholiz.com`, it actually triggers another DNS lookup to `tycholiz.com`, since we specified that as the CNAME\n\t- in this example, the canonical name (true name) is `tycholiz.com`\n\t- from this example, you can see how CNAMEs are kind of like relays, since they don't map to an IP at all, but point to a domain name, which maps to an IP. In other words, CNAME records cause A records to resolve domain names. \n- The CNAME record only points the client to the same IP address as the root domain\n\t- Therefore, the CNAME record does not have to resolve to the same website as the domain it points to.\n\t- ex. in the case where we hit `blog.example.com`, the DNS will return us the same IP as if we hit `example.com`.  \n\t\t- when the client actually connects to that IP address, the web server will look at the URL, see that it is blog.example.com, and deliver the blog page rather than the home page.\n- Pointing a CNAME to another CNAME is possible, but there is no point\n\n* * *\n\n- [A little background on CName](https://blog.cloudflare.com/introducing-cname-flattening-rfc-compliant-cnames-at-a-domains-root/)","n":0.05}}},{"i":1231,"$":{"0":{"v":"ANAME Record","n":0.707},"1":{"v":"\n### ANAME \n- Like a *CNAME record*, but at the root of the domain\n- Allows us to point the \"naked\" version of domain (eg. example.com) to another host\n- Common use case is CDN\n","n":0.174}}},{"i":1232,"$":{"0":{"v":"A Record","n":0.707},"1":{"v":"\nThe `A` (address) record maps the IP address to a given domain\n- Therefore, the `A record` serves as a lookup for the IP address of a given domain\n- it is normal to have just one A record.\n- the most fundamental record\n- `AAAA` record for IPv6\n\nMost websites have a single A record, but with multiple A records, you can implement round robin load balancing\n","n":0.126}}},{"i":1233,"$":{"0":{"v":"SPF","n":1},"1":{"v":"\nSPF defines which IPs are allowed to send email for your domain.\n- This allows you to instruct recipient servers on how to treat email that claims to be from your domain, but wasn’t sent by a server you approved \n    - If someone is spoofing your domain and sending mail that claims to be from you but isn’t, this is how you define the methods used to handle that.\n\nIt also improves your inbox delivery, since by including it, you are signalling that security is important.\n\nAn SPF record may or may not be required by a mail server\n\nAn SPF record is a TXT DNS record\n- there is an SPF record, but it is deprecated.\n\nThe SPF record starts with `v=spf1`\n\nThe end of the record allows us to decide what you want to request of recipient servers if they receive email from your domain that doesn’t come from one of the places you specified in your SPF record.\n- We have 3 options:\n    - `?` - “I don’t care what you do if you get an email from a server that I haven’t approved to send from my domain. Do what you want.”\n    - `~` - “If you get an email from a server I didn’t approve, it probably wasn’t me. You should do what you think is best.”\n    - `-` - “If you get an email from a server that I didn’t approve, please reject it. It wasn’t from me.”\n        - we should be confident in our SPF record to pick this one.\n","n":0.063}}},{"i":1234,"$":{"0":{"v":"Nslookup","n":1},"1":{"v":"\nwe can use *nslookup* to send queries in much the same way that [[resolvers|dns.resolver]] do.\n- we can also use it to query other nameservers as a nameserver would\n\nThe biggest difference between *nslookup*’s behavior and the resolver’s behavior is that *nslookup* talks to only one nameserver at a time.\n- in `/etc/resolv.conf`, if there are 2 nameservers listed, then the resolver will try the first, then the second. On the other hand, `nslookup` will only ever try the first.\n\nBy default, nslookup looks up [[A records|dns.records.a]]\n- if you type in an IP address (and the nslookup query type is A or PTR), nslookup inverts the address, appends in-addr.arpa, and looks up PTR records instead\n\nWe can also use `https://dnslookup.online`","n":0.093}}},{"i":1235,"$":{"0":{"v":"DNS Nameservers","n":0.707},"1":{"v":"\nThe DNS nameservers constitute the server half of the system's [[client-server architecture|general.arch.client-server]]\n- [[Resolvers|dns.resolver]] are the client\n\nNameservers contain information about some segments of the [[DNS database|dns.db.zone]] and make that information available to [[resolvers|dns.resolver]].\n- nameservers have complete information about some part of the domain namespace. This information is either loaded directly from a file (if the nameserver is authoritative about that particular zone), or from another (upstream) nameserver. This is called *name resolution*.\n- if the nameserver has the information on file, then it is known as [[authoritative|dns.nameserver.authoritative]]\n\nA nameserver needs only one piece of information to find its way to any point in the tree: the domain names and addresses of the root nameservers. \n- A nameserver can issue a query to a [[root nameserver|dns.nameserver.root]] for any domain name in the domain namespace, and the root nameserver will start the nameserver on its way.\n\nEach nameserver queried either gives the querier information about how to get closer to the answer it’s seeking or provides the answer itself.\n\nNameservers listen on port 53\n\n- A DNS nameserver is a server that stores the DNS records for a domain\n\t- a DNS nameserver responds with answers to queries against its database.\n- Nameservers point to DNS providers\n- A nameserver is a computer designated to translate domain names into IP addresses\n- Nameservers can be \"authoritative\", meaning that they give answers to queries about domains under their control. Otherwise, they may point to other servers, or serve cached copies of other name servers’ data.\n- nameservers make your zone’s data available to all the other nameservers on the network.\n- due to the distributed nature of the DNS database, nameservers are able to navigate through the database and find data in any zone.\n- more than one nameserver can store data about a zone, which sometimes leads to inconsistencies between copies of the zone data.\n\n## Master and Slave Nameservers\nThe DNS specification defines two types of nameserver: Master and Slave (a.k.a primary master and secondary master)\n- A master nameserver for a zone reads the data for the zone from a file on its host\n- A slave nameserver for a zone gets the zone data from another nameserver authoritative for the zone, called its master server\n\t- often, the master server is the zone's master\n\nA slave can load zone data from another slave.\n\nWhen a slave starts up, it contacts its master nameserver and, if necessary, pulls the zone data over (*zone transfer*).\n\nBoth the master and slave nameservers are [[authoritative|dns.nameserver.authoritative]] for that zone.\n\nOnce you’ve created the data for your zone and set up a master nameserver, you don’t need to copy that data from host to host to create new nameservers for the zone. All you need to do is set up slave nameservers that load their data from the primary master for the zone.\n- when new zone data becomes available, the slaves will handle its transfer.\n\na nameserver can be a master for one zone and a slave for another.\n\nmaster nameservers load their zone data from *zone datafiles*\n- these datafiles contain [[resource records|dns.records]] that describe the zone.\n\n## DNS Server\nThere are only 4 types of DNS Server:\n1. DNS Recursor\n\t- the server that responds to a DNS query and asks another DNS server for the address, or already has the IP address for the site saved.\n2. Root nameserver\n\t- A root name server is the name server for the root zone. It responds to direct requests and can return a list of authoritative name servers for the corresponding top-level domain.\n3. TLD nameserver\n\t- The top-level domain server (TLD) is one of the high-level DNS servers on the internet. When you search for www.varonis.com, a TLD server for the ‘.com’ will respond first, then DNS will search for ‘varonis.’\n4. Authoritative nameserver\n\t- The authoritative name server is the final stop for a DNS query. The authoritative name server has the DNS record for the request.\n\n- In instances where the query is for a subdomain such as `foo.example.com`, an additional nameserver will be added to the sequence after the authoritative nameserver, which is responsible for storing the subdomain’s `CNAME` record.\n- the ISP typically supplies the nameserver, but you can use public servers, like those offered by Google (which have IP `8.8.8.8` and `8.8.4.4`) or Cloudflare (`1.1.1.1` and `1.0.0.1`)\n\t- You could consider each IP address here to be a resolver.\n- The DNS server has expanded its role beyond only resolving domain names, and has other anciliary functionality\n\t- ex. a real-time blackhole list for spam\n- The DNS database is traditionally stored in a structured text file (the *Zone File*)\n\t- The Zone File describes a DNS Zone, and contains all RR for every domain in the zone.\n\nYour home network typically relies on a DNS Server supplied by your ISP\n- therefore your ISP's DNS servers see every domain you request.\n- some ISPs have found a way to monetize their DNS service. When you hit an erroneous domain, one that has no actual IP address, they divert your browser to a search and advertising page preloaded with a search phrase derived from the domain name\n\n### Library Analogy\n- *Resolver* - a librarian who, given a title, is asked to go fetch a book\n- *Root NS* - the blocks of bookshelves in the library\n- *TLD NS* - the specific rack within the bookshelf block\n- *Authoritative NS* - the specific book you asked for.\n\n## Misc\nthe same domain name may have multiple IP addresses associated with it.\n\nAnything that can be done with a DNS address can also be done with an IP address, since all a DNS does is translate from hostname (www.____.com) to IP.\n","n":0.033}}},{"i":1236,"$":{"0":{"v":"TLD Nameserver","n":0.707},"1":{"v":"\nHolds information about all domains sharing the common TLD\n- ex. a `com` TLD nameserver contains information for all the `.com` sites.\n\nWhen queried, the top-level nameserver returns the names and addresses of the nameservers that are authoritative for the second-level zone that the domain name ends in (e.g. `facebook.com`)\n\nbroken into 2 groups:\n\t1. Generic TLD, like .com, .org, .net, .edu, and .gov.\n\t2. Country code TLD, like .ca, .uk\n\n- overseen by IANA (a branch of ICANN)\n","n":0.117}}},{"i":1237,"$":{"0":{"v":"Self-Hosted DNS Server","n":0.577},"1":{"v":"\nA Synology NAS can be configured to run a DNS server. Even once the server is set up, of course by default nothing will happen, since no one will be configured to ask the NAS for DNS records\n- There are 2 ways we can achieve this: by host, or by the router\n    - by host means we configure it on every single client on our network\n\n### Host-based method\nIn Mac Network Preferences you can find a place to add DNS server IP addresses. You might have your ISP's DNS server IP, you may have Cloudflare's (`1.1.1.1` and `1.0.0.1`), you may have Google's (`8.8.8.8` and `8.4.4.8`)\n\nYou can replace one of these IP addresses with the IP of your NAS. It's important to retain one of the Cloudflare/Google IP addresses, so that if the NAS is down, the whole network isn't down.\n- note: we want to configure our NAS DNS server so that it forwards requests on to the Cloudflare/Google DNS servers if the NAS doesn't have the IP address in its cache (rule: enable forwarders)\n\n* * *\n\n## Router DNS \nWhen our Mac has its DNS server configured as the router IP address, your router is running a caching DNS server, and setting itself as the DNS server via DHCP.\n- Your router is acting as a DNS forwarder, you ask your router and your router asks a DNS server for you (the DNS server it forwards it to on your behalf can be configured in the Router settings)\n","n":0.064}}},{"i":1238,"$":{"0":{"v":"Root Nameserver","n":0.707},"1":{"v":"\nThe root nameservers know where the authoritative nameservers for each of the top-level [[zones|dns.db.zone]] are. \n\nWhen queried, the root nameserver accepts the domain name and returns the names and addresses of the nameservers that are authoritative for the top-level zone the domain name ends in (e.g. `com`)\n\nThere are 13 types of root nameservers (therefore 13 different addresses). They are overseen by ICANN\n- ex. **F-Root Server**\n","n":0.124}}},{"i":1239,"$":{"0":{"v":"Authoritative Nameserver","n":0.707},"1":{"v":"\nThe authoritative nameserver is the server that actually holds and is responsible for [[resource records|dns.records]]\n- This could be considered the [[nameserver|dns.nameserver]] of the [[domain|dns.domain]].\n\nWhen a DNS server queries other DNS servers, it’s making an \"upstream\" query. Queries for a domain can go \"upstream\" until they lead back to domain’s authority, or \"authoritative name server.\"\n- An authoritative name server is where administrators manage server names and IP addresses for their domains. Whenever a DNS administrator wants to add, change or delete a server name or an IP address, they make a change on their authoritative DNS server (sometimes called a \"master DNS server\").\n\n- Holds information specific to the actual domain name it serves (eg. google.com), and will send the IP address back to the recursive resolver.\n\t- The IP address it sends is found in the DNS A record.\n- Each domain has its own authoritative NS\n- spec: this nameserver holds the records that we see on our domain registrar dashboard for our domain. \n\na nameserver can be authoritative for more than one zone","n":0.076}}},{"i":1240,"$":{"0":{"v":"DNS Lookup","n":0.707},"1":{"v":"\n## Process\n1. When `www.facebook.com` is searched, the browser DNS cache is checked to see if the domain and IP address key-value pair is stored, after that, the OS's cache, then the DNS server configured on the system (which might be the DNS server in the home router, the ISP (Internet Service Provider), or the public DNS server.)\n2. after the browser cache is checked, the resolver intercepts the request and checks its own cache. \n3. If it doesn't have anything, it will make a request to the root server to see if it has it cached. \n\t- the part of the URL corresponding to root server is the final `.` of `www.facebook.com.`\n4. If it does not, then it proceeds down the chain until the authoritative nameserver. \n\t- the next parts of the chain are `com`, then `facebook`, then `www`. each of these is essentially a zone\n\nAt any point, if the nameserver does indeed have the website cached, then it will return it to the resolver, who will proceed to return it to the client in domain form. \n\n## DNS Request\nA DNS request is executed by the browser on a device. The first thing the OS checks is the hosts file. If the hosts file has an entry for the DNS, then this entry is always used, regardless of what comes next.\n- If the hosts file turns up no result, then the network card settings will be queried. This can come from one of 2 places:\n\t1. IP addresses of DNS servers configured in router\n\t2. IP addresses of DNS servers configured on device itself (in which the DNS settings on router would be set to *manual*)\nWhen the DNS resolves, the browser is enabled to connect to a web server or a CDN edge server \n\nDNS queries and responses are sent in plaintext (via UDP), which means they can be read by networks, ISPs, or anybody able to monitor transmissions (even when using HTTPS)\n\n<!-- During a new DNS lookup, the lookup passes through the resolver, root server, and TLD server. -->\n\nAt each step of the DNS lookup, information is gathered and cached for later use\n- Therefore, in a DNS lookup, the resolution process runs until either it reaches the DNS server (?) and gets the IP, or one of the stages returns a cached version of the website. \n\nDNS Lookup:\n![DNS lookup](/assets/images/2021-08-01-21-31-10.png)\n\n*Below may be roughly the same info as above, but it's a good resource. combine the 2 snippets when we have more time*:\n1. A DNS request starts when you try to access a computer on the internet. For example, you type www.varonis.com in your browser address bar.\n2. The first stop for the DNS request is the local DNS cache. As you access different computers, those IP addresses get stored in a local repository.  If you visited www.varonis.com before, you have the IP address in your cache.\n3. If you don’t have the IP address in your local DNS cache, DNS will check with a recursive DNS server. Your IT team or Internet Service Provider (ISP) usually provides a recursive DNS server for this purpose.\n4. The recursive DNS server has its own cache, and if it has the IP address, it will return it to you. If not, it will go ask another DNS server.\n5. The next stop is the TLD name servers, in this case, the TLD name server for the .com addresses. These servers don’t have the IP address we need, but it can send the DNS request in the right direction.\n6. What the TLD name servers do have is the location of the authoritative name server for the requested site. The authoritative name server responds with the IP address for www.varonis.com and the recursive DNS server stores it in the local DNS cache and returns the address to your computer.\n7. Your local DNS service  gets the IP address and connects to www.varonis.com to download all the glorious content. DNS then records the IP address in local cache with a time-to-live (TTL) value. The TTL is the amount of time the local DNS record is valid, and after that time, DNS will go through the process again when you request Varonis.com the next time.\n\n## Reverse DNS Lookup\n- *Forward DNS lookup* is using an Internet domain name to find an IP address. *Reverse DNS lookup* is using an Internet IP address to find a domain name\n\t- when you put a URL in the address bar, the address is transmitted to a nearby router which does a forward DNS lookup in a routing table to locate the IP address\n\t- `PTR Record` is a [[RR|dns.records]] for enabling reverse DNS lookups, which is the exact opposite of an A record.\n- Reverse lookups are commonly used by email servers, who check and see if an email message came from a valid server before bringing it onto their network.\n\t- Many email servers will reject messages from any server that does not support reverse lookups (the absense of a `PTR record` means reverse lookups aren't supported)\n\n### The `in-addr.arpa` domain\nWhen an IP address is resolved from a domain name, a subdomain of the `in-addr.arpa` domain is created.\n- each subdomain value of this domain corresponds to the IP address of the resolved domain we searched for, though in reverse order\n- ex. if the IP address of `blog.example.com` is `15.16.192.152`, then the corresponding node in the `in-addr.arpa` domain is `152.192.16.15.in-addr.arpa`, which maps back to our domain name `blog.example.com`\n\t- the `15.in-addr.arpa` zone contains the reverse-mapping information for all hosts whose IP addresses start with `15`.","n":0.033}}},{"i":1241,"$":{"0":{"v":"Domain","n":1},"1":{"v":"\nA *domain* is simply a subtree of the domain namespace.\n\nA domain can be spread across multiple [[zones|dns.db.zone]]\n- consider that those who manage the `com` domain don't manage the `facebook.com` domain; it has delegated responsibility to facebook.\n    - `facebook.com` is within the `com` domain (and `facebook.com` domain), but is only in the `facebook.com` zone. It is not part of the `com` zone.\n\nIn the absense of subdomains (and thus delegation), the zone and the domain contain the same information\n\n### Subdomain\nThe term *subdomain* is relative. The `facebook` domain is a subdomain of the `com` domain, and the `com` domain is a subdomain of the root domain.\n\nThe most common subdomain is `www`\n- The www subdomain is so widely used that most domain registrars include it with domain name purchases.\n\nThe concept of subdomain can be achieved in 2 different ways:\n1. The DNS zone file pertaining to the parent domain can be edited\n    - in other words, subdomains are contained within parent domains (this is not the case for the second)\n2. a record can be made to map a name to the [[A record|dns.records.a]].\n    - network operations teams consider the second to not really be a subdomain, and instead restrict the definition to only including those subdomains which are provided by the zone NS records, and any server-destination other than that.\n\n### Domain names\na domain name can be in many domains\n- ex. `pa.ca.us` is part of the `ca.us` domain, but also part of the `us` domain.\n\n![](/assets/images/2022-03-12-19-59-47.png)\n\nDomain names are used as [[indexes|db.strategies.index]] into the [[DNS database|dns.db]].\n\nInterior domain names (ie. not leaf nodes) can name a host and point to informa- tion about the domain; it doesn't have to be one or the other.\n- ex. `hp.com` is both to Hewlett Packard's domain, as well as a domain name that refers to the host that runs HPs main webserver.\n\nWhen you use a domain name, the information that is retrieved depends on the protocol we used to access that information\n- ex. if we are using HTTP or [[unix.cli.ssh]], then we get the IP address. If we are using mail protocols, then we get mail-routing information.\n\n### Apex domain\nThe apex domain is the root domain, without any `www` subdomain.\n\n### Fully Qualified Domain Name (FQDN)\n- The full domain name, rather than its parts separated. The FQDM is notable because it is fully unambiguous. It therefore points to a very specific place, and cannot be interpreted in any other way.\n- It always ends in the TLD (therefore, paths don't count)\n- FQDN in DNS records should always be appended with a `.`\n\t- This is to differentiate them from relative paths. Therefore, the trailing `.` serves the same purpose as the leading `/` when we're talking about a path.\n\t- Names without trailing dots are sometimes interpreted as relative to some domain name other than the root.\n","n":0.047}}},{"i":1242,"$":{"0":{"v":"Dynamic DNS","n":0.707},"1":{"v":"\nDDNS is a method of automatically updating a nameserver in the Domain Name System with the active DDNS configuration of its configured hostnames, addresses or other information.\n- each time your IP address changes, a program redirects the new address to the domain name and makes it permanently available on the internet.\nDDNS solves the problem of having your public IP address change. Since getting a static public IP address from an ISP can be expensive, this is another solution to that problem.\n\nThe term is used to describe two different concepts\n1. **dynamic DNS updating** - systems that are used to update traditional DNS records without manual editing\n2. ?\n\nDNS is only suitable for devices that don't change their IP often.\n- DDNS is a system that addresses the problem of rapid updates.\n","n":0.088}}},{"i":1243,"$":{"0":{"v":"DNS Database","n":0.707},"1":{"v":"\nNotice that the structure of the DNS database is similar to the structure of a UNIX filesystem.\n- In the DNS database, the root node is written as a dot (`.`). In the Unix filesystem, the root is written as a slash (`/`).\n\nEach partition of the database is known as a [[zone|dns.db.zone]] (a.k.a *namespace*)\n\nEach subtree represents a partition of the overall database— a directory in the Unix filesystem, or a *domain* in the Domain Name System.\n- each domain can be further partitioned into *subdomains*\n\nIn a filesystem, directories contain files and subdirectories. Likewise, domains can contain both hosts and subdomains.\n\nA full domain name identifies its position in the database, similar to how an absolute path specifies its place in the filesystem.\n\n\n![](/assets/images/2022-03-12-15-46-04.png)\n","n":0.092}}},{"i":1244,"$":{"0":{"v":"DNS Zone","n":0.707},"1":{"v":"\nA DNS Zone is an autonomously administered piece of the [[namespace|dns.db]]\n\nThe zone contains pointers to the [[authoritative nameservers|dns.nameserver.authoritative]] (ie. their IP addresses).\n\nA new zone is created when a TLD (e.g. `.com`) delegates responsibility of a domain to the new owner (e.g. `facebook.com`)\n- a new zone may or may not be created when subdomains are made (e.g. `developer.facebook.com`), depending on if `facebook.com` administrators delegate responsibility for it to other organizations. \n\nConsider that there is a hierarchy in who manages each domain. The root zone is managed by ICANN. The `edu` domain is managed by EDUCAUSE, but delegates responsibility of the `berkley.edu` domain to U.C. Berkeley. \n- when the authority for `berkley.edu` is delegated, a new zone is created. The new `berkley.edu` zone is now independent from `edu` and contains all domain names that end in `berkeley.edu`. Also, the `edu` zone no longer contains `berkley.edu`, since that domain now exists in a delegated zone.\n- anal: consider that when we [[remotely mount a filesystem|fs.network]] our own, that directory structure looks like it is hosted on our machine. If I mount a [[NAS|nas]] to my local filesystem, the NAS is still responsible for the filesystem at `/Volumes/Synology-NAS`.\n  - therefore, each zone can be thought of as a mounted volume to the higher level zone (ie. the `berkley.edu` zone is mounted on the `edu` zone, the `edu` zone is mounted on the root zone).\n\nA zone contains all the domain names of the domain, unless it has been delegated.\n- ex. the `ca` domain has subdomains `bc.ca`, `ab.ca`, `sk.ca`. The authority for each one *may* be delegated to [[nameservers|dns.nameserver]] in each province.\n\t- once authority has been delegated, the `ca` domain is only left with pointers to the subdomains.\n\nThis diagram shows how it may be the case that the `ca` domain has delegated authority of `ab`, `on` and `qc` to the respective provinces, but continues to manage the `bc` and `sk` domains (perhaps because provincial authorities in those provinces aren't yet ready to manage their own subdomains.)\n- this shows that the [[zone|dns.db.zone]] is what the nameserver loads; not the [[domain|dns.domain]]\n![](/assets/images/2022-03-12-20-49-28.png)\n\nDomains can be very large, so they are further organized into smaller books, called, “zones.”\n- The DNS is broken up into different zones. A DNS zone is a portion of the DNS namespace that is managed by a specific organization or administrator\n- A DNS zone is a subset, often a single domain, of the hierarchical domain name structure of the DNS\n\t- if DNS was a filesystem, a DNS zone would be each directory.\n\t- ex. if we are talking TLD DNS zone, then examples are `.com`, `.net`. If we are talking Domain-level DNS, then examples are `facebook.com`, `google.com`\n- A portion of the domain name space where administrative responsibility has been delegated to a single manager\n- The DNS Zone is described by the Zone File (aka. DNS record), which serves as the database for each nameserver.\n\t- The zone file contains mappings between domain names and IP addresses, along with other resources. This file is organized around resource records (A, CNAME etc.). In other words, resource records form the basis of the database. \n\t- We recognize the zone file when we go to a domain registrar, click onto one of our domains, and see all of the RRs that we have made.\n- a 'zone' is an area of control over namespace. A zone can include a single domain name, one domain and many subdomains, or many domain names. \n\t- In some cases, 'zone' is essentially equivalent with 'domain,' but this is not always true.\n\t- each zone has a *zone serial number*, which is a unique identifier\n\t\t- A DNS server can quickly look up a zone's records in its database via the serial number, which will bring up the SOA record.\n- A common mistake is to associate a DNS zone with a domain name or a single DNS server\n\t- In fact, a DNS zone can contain multiple subdomains and multiple zones can exist on the same server\n- We can decide which URLs should be their own zone, and which should be combined into a single zone.\n\t- ex. Below, as far as Cloudflare subdomains go, we have `blog`, `support`, and `community`. Support and community are small, so we put them in the same zone as the main `cloudflare.com`. However, the `blog` subdomain is a robust independent site that needs separate administration, so we give it its own zone.\n\nEach below is an example of a zone:\n![](/assets/images/2021-03-07-15-16-30.png)\n![](/assets/images/2021-03-07-15-16-44.png)\n\n### Zone Apex\n- Where `SOA` and `NS` records live. They are records whose names are the same as the zone itself.\n","n":0.036}}},{"i":1245,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Check which DNS server is configured for the device\nMac\n`scutil --dns | grep 'nameserver\\[[0-9]*\\]'`\n\nLinux\n`cat /etc/resolv.conf`\n","n":0.258}}},{"i":1246,"$":{"0":{"v":"DNS Cache","n":0.707},"1":{"v":"\nThere are multiple caches involved in the entire DNS. They are checked in sequential order. \n1. Browser\n2. OS - the process that handles this query is called a **stub resolver**, or **DNS Client** \n\t- the stub resolver will first check to see if it has the cached data, and if not, will call the DNS query that gets handled by the resolver (which is hosted by the ISP).\n\nA residential router internally runs a DNS cache and DNS proxy server\n- it also advertises itself as the DNS server in all [[DHCP|network.lan#Dynamic-Host-Configuration-Protocol-(DHCP)]] responses.\n\nThere are actually DNS caches at every hierarchy of the lookup process\n- The computer reaches your router, which contacts your ISP, which might hit another ISP before ending up at what's called the \"root DNS servers.\" Each of those points in the process has a DNS cache for the same reason, which is to speed up the name resolution process.\n- spec:the DNS cache caches records.\n","n":0.08}}},{"i":1247,"$":{"0":{"v":"BIND","n":1},"1":{"v":"\nBIND (Berkeley Internet Name Domain) is software that is the most popular implementation of the DNS specification.\n","n":0.243}}},{"i":1248,"$":{"0":{"v":"DevOps","n":1},"1":{"v":"\n### Before DevOps\nSystems administration as a craft has a sloppy history. System administrators previously would manage hardware manually by either connecting to and provisioning machines in a physical server rack or over a cloud provisioning API. In addition to the manual provisioning process, large amounts of manual configuration work was a regular routine. Administrators would keep custom collections of imperative scripts and configurations, cobble them together, and place them in various places. These scripts could break at any time or get lost. Collaboration was challenging as the custom tool chains were not regularly documented or shared.\n\nThe DevOps movement arose from this primordial swamp of systems administration. DevOps borrowed the best ideas from software engineering and applied them to systems administration, where the cobbled-together tools became version-controlled code. \n\n[[IaC|devops.IaC]] is one of the biggest revelations of DevOps. Previously system administrators favored custom imperative scripts to configure systems. Imperative software follows a sequence of steps to achieve a desired state.\n","n":0.08}}},{"i":1249,"$":{"0":{"v":"IaC (Infrastructure-as-Code)","n":0.707},"1":{"v":"\nInfrastructure as code (IaC) is the practice of keeping all infrastructure configuration stored as code.\n- The actual desired state may or may not be not stored as code (e.g., number of replicas or pods).\n\nIaC encourages and promotes declarative system administration tools over custom imperative solutions, over the primitive way of doing it using loose imperative scripts that were hard to keep track of and difficult to understand.\n- This led to the emergence of technologies like Docker Containers, Ansible, Terraform, and Kubernetes, which utilize static declarative configuration files.\n\nIaC is the opposite of click ops.\n\nIaC allows developers to test infra changes very early on in the development cycle, instead of waiting until the last step to deploy to prod, hoping everything works, we just deploy to staging, which should be as close as possible to production (staging should essentially be prod without the users)\n\n### Examples\n- AWS Cloudformation\n- Azure Resource Manager\n\nCloud Agnostic:\n- Chef\n- Puppet\n- [[Terraform|terraform]]\n- Serverless\n- Pulumi\n","n":0.081}}},{"i":1250,"$":{"0":{"v":"GitOps","n":1},"1":{"v":"\n### What is it?\nGitOps is an evolution of IaC that leverages Git as the single source of truth, and control mechanism for creating, updating, and deleting system architecture.\n- we do this by using Git pull requests to verify and automatically deploy system infrastructure modifications.\n- teams that adopt GitOps use configuration files stored as code ([[IaC|devops.IaC]])\n- GitOps adds some magic to the pull request workflow that syncs the state of the live system to that of the static configuration repository.\n\nGitOps is implemented by using the Git distributed version control system (DVCS) as a single source of truth for declarative infrastructure and applications. Every developer within a team can issue pull requests against a Git repository, and when merged, a \"diff and sync\" tool detects a difference between the intended and actual state of the system. Tooling can then be triggered to update and synchronise the infrastructure to the intended state.\n\nOver and above simply implementing [[IaC|devops.IaC]], GitOps provides a way to plan, review and approve moves, adds and changes to the infrastructure.\n- It also requires well documented procedures, or runbooks, to be successful.\n\n### Why do it?\nDone right, GitOps will let engineers constantly apply updates to infrastructure meet all the needs of the applications in a continuous delivery format.\n\nGitOps ensures that a system’s cloud infrastructure is immediately reproducible based on the state of a Git repository.\n- Once approved and merged, the pull requests will automatically reconfigure and sync the live infrastructure to the state of the repository. This live syncing pull request workflow is the core essence of GitOps.\n- Git becomes the single source of truth for what is running in production (or staging, or development).\n\nWhile IaC itself does use Git for version control, the declarative configuration of that infrastructure (ie. the YAML files) still feels disconnected from the live system, since the live system needs to be manually updated to match the state of the static repo. This is the exact problem GitOps solves.\n\nPracticing GitOps allows a team to track all modifications to the configuration of a system. This gives a “source of truth” and valuable audit trail to review if something breaks or behaves unexpectedly. Teams can review the GitOps history and see when a regression was introduced.\n\nGitOps principles can be applied to all types of infrastructure automation including VMs and containers, and can be very effective for teams looking to manage Kubernetes-based infrastructure.\n\nWhile many tools and methodologies promise faster deployment and seamless management between code and infrastructure, GitOps differs by focusing on a developer-centric experience.\n\n### Implementing GitOps\nGitOps requires three core components:\n- [[IaC|devops.IaC]]\n    - To implement GitOps, we need the ability to create a declarative cloud infrastructure, using something such as Terraform and Kubernetes, with the help of Helm or Flux.\n- Pull Requests \n    - The change mechanism of our infra config.\n- [[CI/CD|deploy.CI-CD]] \n    - When new code is merged, the CI/CD pipeline enacts the change in the environment. Any configuration drift, such as manual changes or errors, is overwritten by GitOps automation so the environment converges on the desired state defined in Git\n\n\nGitOps procedures are performed by an underlying orchestration system, most likely [[Kubernetes|k8s]].\n- Some alternative GitOps tool sets are coming to market that support direct [[Terraform|terraform]] manipulation. \n\na pipeline platform is required, such as CircleCI, Jenkins, Gitlab CI, Github Actions\n- Pipelines automate and bridge the gap between Git pull requests and the orchestration system. Once pipeline hooks are established and triggered from pull requests, commands are executed to the orchestration piece. \n\nA new pattern or component that is specifically introduced with GitOps is the GitOps “operator,” which is a mechanism that sits between the pipeline and the orchestration system. A pull request starts the pipeline that then triggers the operator. The operator examines the state of the repository and the start of the orchestration and syncs them. The operator is the magic component of GitOps.\n- ex. we have a non-optimal config for our load balancer, so we make a pull request that adjusts the values. Merging in the code kicks off a GitOps pipeline, which triggers the GitOps operator. The operator sees the load balancer configuration was changed. It confirms with the systems orchestration tool that this does not match what is live on the teams cluster. The operator signals the orchestration system to update the load balancer configuration. The orchestrator handles the rest and automatically deploys the newly configured load balancer. The team then monitors the newly updated live system to see it return to a healthy state.\n    - if we then realize that we made a mistake, reverting back to the last config is as simple as reverting the commit.\n\n### Considerations\nGitOps allows for greater collaboration, but that is not necessarily something that comes naturally for some individuals or organizations. A GitOps approval process means that developers make changes to the code, create a merge request, an approver merges these changes, and the change is deployed. This sequence introduces a “change by committee” element to infrastructure, which can seem tedious and time-consuming to engineers used to making quick, manual changes.\n\nIt is important for everyone on the team to record what’s going on in merge requests and issues.\n\n### Benefits \nFull oversight, architectural review, project management and all the other pieces of the process are brought together in GitOps in an effort to end ops versus everyone else and get all the stakeholders involved in the process. The basics don’t change–an ops engineer will still create an infrastructure–but that will just be one part of a much larger process that everyone can contribute to and collaborate on. No more worrying about an engineer creating a cloud infrastructure that can’t be replicated; with GitOps everything is tracked making repeatability easy. Suddenly it will also be simple to answer the “what changed?” question because no one will have to hunt for an answer. The bottom line is a successful GitOps program will benefit the actual bottom line because operational processes will be dramatically streamlined.\n\n## UE Resources\nhttps://www.weave.works/blog/gitops-operations-by-pull-request","n":0.032}}},{"i":1251,"$":{"0":{"v":"Development Process","n":0.707},"1":{"v":"\n- [Trunk-based development](https://trunkbaseddevelopment.com/)\n    - ie. using short-lived branches, merging often into main and feature toggles\n\n### The problem with long-lived feature branches\nWe need to merge main into the feature branch often. The problem with that is that you can have multiple long-lived feature branches that are pulling in main, but they're not syncing with each other.\n- ex. if featureA is doing some refactoring of a module, and featureB is editing that same module, then it doesn't matter how frequently they pull in main, there's still going to be massive conflicts when one of the branches (whichever isn't first) tries to merge.","n":0.1}}},{"i":1252,"$":{"0":{"v":"Agile","n":1},"1":{"v":"\n**Epic** - a collection of user stories\n- epics are delivered over a series of sprints \n- An epic should solve some business case\n- epics can be created around OKRs\n- ***ex.*** - the *Export to PDF* feature would be the epic, while each task associated with accomplishing the feature would be a *user story*\n\n## Before Agile\nBefore agile appeared on the scene most of the thinking about software process was about understanding requirements early, signing off on these requirements, using the requirements as a basis for design, signing off on that, and then proceeding with construction. This is a plan-driven cycle, often referred to (usually with derision) as the waterfall approach\n\nIn order to make agile work, you need a different attitude to design. Instead of thinking of design as a phase, which is mostly completed before you begin construction, you look at design as an on-going process that is interleaved with construction, testing, and even delivery. This is the contrast between planned and evolutionary design.\n\nOne of the vital contributions of agile methods is that they have come up with practices that allow evolutionary design to work in a controlled manner. So instead of the common chaos that often happens when design isn't planned up-front, these methods provide techniques to control evolutionary design and make them practical.\n\nOne of the tenets of agile methods is that people with different skills and backgrounds need to collaborate very closely together.\n\n### Story points\nStory points are not a measure of productivity. They are a measure of velocity. We point tickets *as a team* based on our shared knowledge. If we have a ticket that uses a framework that the person to implement has never used before, we don’t point the ticket any differently. Yes, it will take longer than if it was done by someone who was familiar with the framework, but the idea is that over the course of a year these inconsistencies get evened out.\n\n# UE Resources\n[Scaling Agile: Spotify model](https://www.atlassian.com/agile/agile-at-scale/spotify)\n","n":0.056}}},{"i":1253,"$":{"0":{"v":"BDD (Behaviour-Driven Development)","n":0.577},"1":{"v":"\n## Overview\n*\"BDD is an outside-in, pull-based, multiple-stakeholder, multiple-scale, high-automation, agile methodology. It describes a cycle of interactions with well-defined outputs, resulting in the delivery of working, tested software that matters.\"*\n\n*\"It's using examples to talk through how an application behaves... And having conversations about those examples.\"*\n\n### Approach\nThe BDD process encourages collaboration among: \n- developers\n- quality assurance testers\n- customer representatives in a software project\n\nThis is done through inter-team conversation and using concrete examples to formalize a shared understanding of how the application should behave\n\nBDD suggests a specific way that unit tests and acceptance tests should be written:\n- unit test names be whole sentences starting with a conditional verb (\"should\" in English for example) and should be written in order of business value.\n    - ex. all other music players should pause when pressing play on a music player.\n- acceptance tests should be written in the form of a user story (\"Being a `[role/actor/stakeholder]` I want to `[feature/capability]` so I can `[benefit]`\")\n\n### BDD Focuses on:\n- Where to start in the process\n- What to test and what not to test\n- How much to test in one go\n- What to call the tests\n- How to understand why a test fails\n\nBDD emerged from [[TDD|testing.TDD]]\n- The general techniques and principles of TDD are preserved in BDD,\n- further incorporated are ideas from [[DDD|general.principles.DDD]] and [[OOP|paradigm.oop]]\n\n### Implementing BDD Results in:\nThe result is that software development and other stakeholders (management teams, designers, PMs etc.) now have shared tools and a shared process to collaborate on software development.\n\nTo properly implement BDD, special software tools have to be used to ease this collaboration amongst teams.\n","n":0.062}}},{"i":1254,"$":{"0":{"v":"Deploy","n":1},"1":{"v":"\nA deployment is a running instance of the app.\n\nA deployment happens in 3 stages:\n1. *build stage* - transform source code into an executable bundle known as a **build**.\n2. *release stage* - takes the build and combines it with the deployment's current config (e.g. the Kubernetes config), producing something that is ready for immediate execution in an environment.\n3. *run stage* (aka runtime) - runs the app in the execution environment by launching some of the app's processes against a selected release.\n    - should be kept to as few moving parts as possible, since these moving parts will be the things that cause breakages in the middle of the night.\n\n### Config\nAn app’s *config* is everything that is likely to vary between deploys (staging, production, developer environments, etc), including...\n- backing services, such as databases and caches\n- credentials to 3rd party services\n- per deploy values like hostname (URL) of where the app will be deployed\n\nConfig varies substantially across deploys, while code does not.\n\n* * *\n\n### Blue-green deployments\nA blue-green deployment is one without any downtime. In contrast to rolling updates, a blue-green deployment works by starting a cluster of replicas running the new version while all the old replicas are still serving all the live requests. Only when the new set of replicas is completely up and running is the load-balancer configuration changed to switch the load to the new version. A benefit of this approach is that there’s always only one version of the application running, reducing the complexity of handling multiple concurrent versions.\n\n# UE Resources\n[Kent C. Dodds Deployment pipeline breakdown](https://kentcdodds.com/blog/how-i-built-a-modern-website-in-2021)\n","n":0.062}}},{"i":1255,"$":{"0":{"v":"Serverless","n":1},"1":{"v":"\nServerless is \"server-less\" in the same way that Javascript is \"memory-less\"; it's still there, but it's abstracted away from us.\n\nServerless sometimes refers to the concept of \"functions as a service\", basically taking the microservices pattern and realizing \"hey, because we're doing things this way, we basically can forget about the server\"\n- Somebody is still doing the heavy lifting (AWS, Google Cloud, etc.) but it's now an abstraction and you have limited ability to configure it, and this is fine if you're using it for the right use cases.\n\nIn the case of AWS, you use Lambda as the compute service to run the code and API Gateway to create API endpoints to point at the Lambda functions. Under the hood, once a Lambda is invoked through the event trigger which is the API Gateway endpoint, a Docker container is spun up to run the code you deployed into the cloud.\n","n":0.082}}},{"i":1256,"$":{"0":{"v":"Serverless Functions","n":0.707},"1":{"v":"\nA cold start for serverless functions is directly related to the size of the function itself. The reason why cold starts exist is because the function is stored as a zip file, and it needs to be unzipped before it is mounted. Therefore, to have shorter cold starts we must write smaller functions.\n- Therefore, each serverless function in Node.js should have its own `package.json`\n\n# Tools\nbegin.com\n","n":0.124}}},{"i":1257,"$":{"0":{"v":"Scaling","n":1},"1":{"v":"\nIt is meaningless to say “X is scalable” or “Y doesn’t scale.” Rather, discussing scalability means considering questions like \"If the system grows in a particular way, what are our options for coping with the growth?\" and \"How can we add computing resources to handle the additional load?\"\n\nA system that is designed to handle 100,000 requests per second, each 1 kB in size, looks very different from a system that is designed for 3 requests per minute, each 2 GB in size—even though the two systems have the same data throughput.\n- the 100,000 requests per second system must optimize for low [[latency|deploy.performance#latency]] and high throughput. This means designing the system to minimize the time it takes to process each request and maximizing the number of requests that can be processed simultaneously. In this case, sacrificing storage efficiency may be tolerable.\n    - You might achieve this system by using techniques like [[load balancing|deploy.distributed.load-balancer]], horizontal scaling, and [[caching|general.arch.cache]].\n- the 3 requests per minute system must optimize for storage and retrieval. This means designing the system to efficiently store and retrieve large data objects, and to handle the performance requirements of large data transfers. In this case, sacrificing latency and throughput may be tolerable.\n    - You might achieve this system by using techniques like sharding, compression, and content delivery networks.\n\n## Scaling Types\n### Vertical Scaling (a.k.a. *scaling up*)\ngetting bigger machines to handle the increasing workload.\n- ie. make each node more powerful (adding CPUs etc)\n\nThe cost of vertical scaling is not linear: twice as much RAM, CPU and disk space costs significantly more than 2x\n- also, due to bottlenecks, a machine with 2x the resources cannot necessarily handle twice the load.\n\nA concern with vertical scaling might be the issue of [[locking|deploy.distributed.locks]], in cases where multiple clients are trying to modify the data simultaneously.\n\n### Horizontal Scaling (a.k.a. *scaling out*)\ngetting many more smaller machines to handle the increase.\n- ie. add more nodes\n\nA system that has taken a horizontally scaled approach is said to be a *shared-nothing system*\n\n- ex. Kubernetes does this by spinning up new containers\n- ex. Serverless functions do this by spinning up new function handler instances\n- ex. Database can do this by [[sharding|db.distributed.partitioning.sharding]]\n\n### Z-Axis Scaling\n[source](https://microservices.io/articles/scalecube.html)","n":0.053}}},{"i":1258,"$":{"0":{"v":"Deployment Process","n":0.707},"1":{"v":"\n## Deployment Strategies\n### Branching Model\nThe idea here is to have a 1:1 mapping between branches and environments (we might have branches `main`, `staging` and `dev`).\n\nMerging code into any of these environment branches will automatically trigger the [[CD|deploy.CI-CD]] and deploy the code to the corresponding environment\n\n### Manual Deployment Workflow\nThe idea with this approach is to merge code into the `main` branch once it has been validated. From there, our CD pipeline is set up to automatically deploy to our development environment. However, we will also have additional manual jobs that will allow us to deploy to each additional environment. Typically, we manually deploy to QA first so that testing can be done, followed by staging and then finially production.\n- in order to isolate each CD job into its own environment, we typically would create multiple stages of our CD (e.g. `dev`, `staging-us-1`, `staging-us-1`, `prod-us-1` etc.), and make the latter stages dependent on the earlier stages. That way, we cannot deploy to production unless we've already deployed to staging.\n\nIn addition to the manual deploy jobs, we can also have optional `rollback` jobs for each environment stage that allow us to revert to the most recent deployment.","n":0.072}}},{"i":1259,"$":{"0":{"v":"Pricing","n":1},"1":{"v":"\n### per Second\nThis is normally for something like [[serverless functions|deploy.serverless.functions]], where we pay for the time that the server is executing our functions. \n- ex. if we are charged $0.01 per second and the function takes 333ms to run, then the function can be executed 3 times for $0.01\n\n### per Uptime\nThis is likely for container-based deployments (ex. Heroku), where we can pay per hour of uptime. This usually comes with the idea of \"cold starts\", where if no one is using our website, then the server will go offline (and therefore we won't get charged)\n","n":0.103}}},{"i":1260,"$":{"0":{"v":"Performance","n":1},"1":{"v":"\n### Load\nLoad on a system can be described with *load parameters*, which depend on the architecture of our system. For example:\n- requests per second to a web server \n- ratio of reads to writes in a database\n- number of simultaneously active users in a chat room\n- hit rate on a cache\n\nThen we can analyze load in one of 2 ways:\n1. increase a load parameter and see how it impacts performance\n2. increase a load parameter and see how much system resources need to be increased to maintain the same performance\n\nLoad can be tested with [[testing.method.load]]\n\n### Response Time\nAverage response time is a poor metric if we want to know \"typical response time\", since it doesn't tell us how many users experienced any given outlier of a delay time.\n- using percentiles is a lot better approach, because then we can make statements such as \"90% (ie. p90) of our users experience a response time of 200ms or less\"\n    - to figure out how bad the outliers really are, we can look at high *p* values, like p99 or even p999 (99.9th percentile)\n- using high *p* values to determine our benchmarks is a good strategy, since users experiencing the longest response times probably have the most data to process, and thus tells us the latest \"ceiling\" value for how much data we can expect to process. \n    - also consider that those with the most data might therefore be the most important users, so it's important to keep them happy.\n\n## Metrics\n![](/assets/images/2023-04-04-09-43-59.png)\n\n### Latency\n*Latency* is the duration that a request is waiting to be handled—during which it is latent, awaiting service.\n\nLow latency can be achieved by:\n- Minimizing network latency: This can be done by reducing the distance and number of network hops between the client and server, using CDNs, and using the right network protocols.\n- Using fast and efficient algorithms: This involves selecting algorithms that are optimized for speed and efficiency, such as hash tables and binary search.\n- Using caching: This involves storing frequently accessed data in memory to reduce the time it takes to retrieve data from disk.\n- Optimizing hardware and infrastructure: This involves using high-performance hardware, such as solid-state drives (SSDs) and GPUs, and using infrastructure that is designed for low latency, such as data centers with low-latency networking.\n- Using asynchronous programming: This involves designing the system to handle multiple requests simultaneously, using techniques like event-driven programming and non-blocking I/O.\n- Choosing the right (low level) programming language\n\n### Response Time\n*Response time* is what the client sees: besides the actual time to process the request (the service time), it includes network delays and queueing delays","n":0.048}}},{"i":1261,"$":{"0":{"v":"Jamstack","n":1},"1":{"v":"\nJAM: JavaScript, APIs, and markup\n- JavaScript: any dynamic programming during the request/response cycle is handled by JS, running entirely on the client. This could be any frontend framework or library, or even vanilla JavaScript.\n- APIs: all server-side processes or database actions are abstracted into reusable APIs, accessed over HTTPS with JavaScript. These can be custom-built or leverage third-party services.\n- Markup: templated markup should be prebuilt at deploy time, usually using a site generator for content sites, or a build tool for web apps.\n\n![](/assets/images/2021-03-20-18-45-27.png)\n\n## Examples\n### Site Generators\nNext.js, Gatsby, Jekyll\n\n### Headless CMS\nNetflify\n\n## Architecture\nPurpose is to make the web faster, more secure, and easier to scale\n- core principles are pre-rendering, and decoupling\n    - pre-render - to generate the markup which represents a view (front-end) in advance of when it is required. This happens during a build rather than on-demand so that web servers do not need to perform this activity for each request recieved.\n- entire front end is prebuilt into highly optimized static pages and assets during a build process. This process of pre-rendering results in sites which can be served directly from a CDN. Since the CDN only has to serve already-rendered markup, it can be delivered very quickly and securely\n    - On this foundation, Jamstack sites can use JavaScript and APIs to talk to backend services, allowing experiences to be enhanced and personalized.\n- The Jamstack philosophy is to be modular and have a strong capacity to be able to hook into various 3rd party services/APIs\n- Jamstack distinguishes between front-end and back-end and allows for efficient front-end development that is independent of any back-end APIs.\n\n## Benefits\n### Security\n- Jamstack naturally results in less moving parts, meaning there are naturally less surfaces to have to protect against.\n- Since CDNs only serve pre-generated files, meaning we can use read-only hosting, further reducing the damage that a third party can do.\n\n### Scaling\nsince everything is pre-generated, the CDN can cache the whole site.\n\n### Performance\nJamstack sites remove the need to generate page views on a server at request time by instead generating pages ahead of time during a build.\n\n### Maintenance\nJamstack apps are easier to maintain, as they only need to be served directly from a simple host (or CDN)\n- The work was done during the build, so now the generated site is stable and can be hosted without servers which might require patching, updating and maintain.\n\n## Approaches\n#### Thinking from CDN perspective\nBecause Jamstack projects don’t rely on server-side code, they can be distributed instead of living on a single server. Serving directly from a CDN unlocks speeds and performance that can’t be beat. The more of your app you can push to the edge, the better the user experience.\n\n#### Automating builds\nBecause Jamstack markup is prebuilt, content changes won’t go live until you run another build. Automating this process will save you lots of frustration. You can do this yourself with webhooks, or use a publishing platform that includes the service automatically.\n\n#### Atomic Deploys\nAs Jamstack projects grow really large, new changes might require re-deploying hundreds of files. Uploading these one at a time can cause inconsistent state while the process completes. You can avoid this with a system that lets you do “atomic deploys,” where no changes go live until all changed files have been uploaded.\n\n#### Instant Cache Invalidation\nWhen the build-to-deploy cycle becomes a regular occurrence, you need to know that when a deploy goes live, it really goes live. Eliminate any doubt by making sure your CDN can handle instant cache purges.\n\n#### Everything Lives in Git\nWith a Jamstack project, anyone should be able to do a git clone, install any needed dependencies with a standard procedure (like npm install), and be ready to run the full project locally. No databases to clone, no complex installs. This reduces contributor friction, and also simplifies staging and testing workflows.\n","n":0.04}}},{"i":1262,"$":{"0":{"v":"Hosting","n":1}}},{"i":1263,"$":{"0":{"v":"VPS (Virtual Private Server)","n":0.5},"1":{"v":"\n## VPS Hosting vs Shared Hosting\n### Shared Hosting\n- share space on a server along with other websites, for the purpose of cost benefits\n- best utilized for static sites.\n\n### VPS Hosting\n- have superuser access to system\n- for practical purposes, they are functionally equivalent to a dedicated physical server.\n- underlying hardware is shared with other VPSes, which may result in lower performance \n- the virtualization aspect provides a high level of security.\n- Because each virtual server is isolated from the others, it can run its own OS\n- the physical server runs a hypervisor, which creates, manages, and allocates resources to the guest OSes (VMs)\n","n":0.099}}},{"i":1264,"$":{"0":{"v":"Distributed Computing","n":0.707},"1":{"v":"\n## What is it?\nA distributed system is a system whose components are spread across many machines on a single [[network|network]].\n- this network can be anything from a local network to the internet, in cases where we want geographic distribution. \n\nIn distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via [[message passing|general.patterns.messaging]].\n\nData partition and replication strategies lie at the core of any distributed system.\n- [[Replication|db.distributed.replication]] - Keeping a copy of the same data on several different nodes, potentially in different locations. \n  - provides redundancy and improves performance (giving us more nodes to read from).\n- [[Partitioning|db.distributed.partitioning]] - Splitting a big database into smaller subsets (partitions) so each can be assigned to different nodes \n  - ex. [[sharding|db.distributed.partitioning.sharding]].\n\nA [[client-server architecture|general.arch.client-server]] might be thought of as the simplest implementation of a distributed system.\n\nA distributed system is not to be confused with [[decentralized architecture|general.arch.decentralized]].\n- Distributed means that the processing is shared across multiple nodes, but the decisions may still be centralized and use complete system knowledge.\n\nDistributed systems typically have the following characteristics:\n- [[concurrency|general.concurrency]] of components\n- lack of a global clock\n- failure of components\n\n### From a Single computer to a distributed network\nWith a single computer, when the hardware is working correctly, the same operation always produces the same result (it is deterministic). If there is a hardware problem, the result is normally a total system failure. Alas, an individual computer with properly functional software is either fully functional or entirely broken (this is deliberate, so as to simplify troubleshooting).\n\nIn a distributed system, some computers may be broken in some unpredictable way while others are working fine. This is known as *partial failure*, and they add considerable difficulty because they are a class of failure that is *nondeterministic*.\n- sometimes we may not even know if something failed or succeeded, since the time it takes for a message to travel across a network is also nondeterministic. If you send a request and don’t get a response, it’s not possible to distinguish whether (a) the request was lost, (b) the remote node is down, or (c) the response was lost.\n  - this can be summed up as *\"problems in the network cannot reliably be distinguished from problems at a node\"*.\n- this nondeterminism and possibility of partial failure is what adds difficulty to distributed systems.\n\nBuilding a distributed system is about building a reliable system from unreliable components.\n- anal: [[protocol.TCP]] is a relible protocol built on the unreliable [[protocol.IP]].\n\n#### Consensus\nThe difficulty involved in reaching consensus amongst all areas of a distributed system is one of the most important and fundamental problems of architecting a distributed system. \n\nThe consensus problem is normally formalized as follows: one or more nodes may propose values, and the consensus algorithm decides on one of those values. Everyone decides on the same outcome, and once you have decided, you cannot change your mind. \n- In an example of seat-booking of an airplane, when several customers are concurrently trying to buy the last seat, each node handling a customer request may propose the ID of the customer it is serving, and the decision indicates which one of those customers got the seat.\n\na consensus algorithm must satisfy the following properties \n- Uniform agreement - No two nodes decide differently.\n- Integrity - No node decides twice.\n- Validity - If a node decides value v, then v was proposed by some node.\n- Termination - Every node that does not crash eventually decides some value.\n\nConsensus much be achieved for...\n- *leader election*. In a database with single-leader replication, all nodes must agree on which node is the leader.\n- *atomic commits*. In a database that supports transactions spanning several nodes/partitions, we have the possibility of a transaction failing on some nodes but succeeding on others. If we want to maintain transaction [[atomicity|db.acid.atomicity]], we must have all nodes in agreement about the outcome of a transaction (either they all abort+rollback or commit).\n\nZookeeper and [[k8s.node.master.components.etcd]] provide consensus algorithms.\n\n## Why do it?\nKey characteristics of a distributed system include Scalability, Reliability, Availability, Efficiency, and Manageability.\n\n## How does it work?\nMessages may be passed between nodes of a distributed system by using technologies like HTTP, [[RPCs|deploy.distributed.RPC]], or [[message queues|general.patterns.messaging]]\n\nHistorically, *distributed* referred to the fact that there were multiple computers all working in unison to solve some problem. Nowadays, the term is much broader, and can be used to refer to a system of multiple autonomous processes existing on a single machine. In this case, a node of the \"network\" would be a single process, rather than an entire computer.\n- In the case of distributed computing, a node is simply something that passes messages and has its own local memory.\n\nThere is no clear distinction between distributed systems and parallel systems, and there is a lot of overlap, thought one distinction is that the nodes of a parallel system all share the same memory, while nodes of a distributed system manage their own:\n![Distributed system vs Parallel system](/assets/images/2021-07-16-13-10-56.png)\n- Examples of distributed systems: MMOs, the internet itself\n\n## Considerations for distributed systems\nAccording to [[CAP theorem|deploy.distributed.CAP-theorem]], out of consistency, availability and partition tolerance, you can only really achieve 2/3.\n\n### Problems specific to distributed systems\nThe typical problems in a distributed system are the following:\n- maintaining the system state (liveness of nodes)\n- communication between nodes\n\nThe potential solutions to these problems are as follows:\n- centralized state management service (eg [[zookeeper]], [[k8s.node.master.components.etcd]])\n  - A centralized state management service such as Apache Zookeeper can be configured as the service discovery to keep track of the state of every node in the system\n  - provides a strong consistency guarantee, the primary drawbacks are the state management service becomes a single point of failure and runs into scalability problems for a large distributed system\n- peer-to-peer state management service\n  - The peer-to-peer state management approach is inclined towards high availability and eventual consistency. The gossip protocol algorithms can be used to implement peer-to-peer state management services with high scalability and improved resilience 1.\n\n# UE Resources\n- [Distributed Systems in One Lesson (Kafka Guy)](https://www.youtube.com/watch?v=Y6Ev8GIlbxc)\n- [Considerations for distributed systems](https://www.aosabook.org/en/distsys.html)\n- [Distributed Transactions Without Atomic Clocks](https://vimeo.com/545130381)\n","n":0.032}}},{"i":1265,"$":{"0":{"v":"Strategies","n":1},"1":{"v":"\n# Approaches to common problems arising because of a distributed system\n## Key Generation System\nImagine we were making a URL shortener like TinyURL. As a database solution, we opt for a NoSQL approach key-value store, where each key is the shortform URL, and the value is the longform URL.\n\n### Naive approach\nEach time a write request is made (ie. user creates a new shortform URL), the application server generates a new random 6 character string, and attempts to insert it into the DB. If that key exists already, then it tries again, until there is no failure. This is naive because it involves a lot of back and forth between application server and database, and results in an unpredictable time complexity.\n\n### Smarter approach\nHave a standalone Key Generation Service (KGS), which generates the random 6 character strings in advance and stores them in a separate database (the Key-DB). Whenever a user wants to generate a new TinyURL, we take a key from the Key-DB and use it. This takes out the worry of collision and duplications of keys.\n\nIn our Key-DB, we can have 2 tables to store the keys, for:\n1. `unused_keys`\n2. `used_keys`\n\nThe purpose of the 2 tables is to handle read concurrency issues. The problem we are trying to solve is to not give the same key to two servers (as we don’t want two URLs to have the same short key). Since there is only 1 KGS in the system at a time, there are inherently no write concurrency issues.\n\nThe KGS can always keep some unused keys in memory, so that whenever a server needs them, it can provide them quickly. As soon as the KGS loads some keys in memory, it moves them from `unused_keys` to `used_keys`, so that we can ensure each server gets unique keys. It's true that if the KGS goes offline, all those keys will be lost (and will still exist in the `used_keys` table). This is ok, since there are such a large amount of keys anyway.\n\nSince the KGS would be a single point of failure, we can have a standby replica, and whenever the primary server dies it can take over to generate and provide keys.\n\n![](/assets/images/2021-10-13-11-24-19.png)\n","n":0.053}}},{"i":1266,"$":{"0":{"v":"Storage","n":1},"1":{"v":"\nWhen it comes to high-availability (HA) clusters, the two main replication topologies you’ll encounter in data storage are:\n1. Active/active clusters\n2. Active/passive clusters\n\n|                          | Active/Active                                                                                                                                                           | Active/Passive                                                                                                                                                                     |\n|--------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Benefits**             | - Better resource utilization since both nodes can handle traffic.<br>- No failover needed.<br>- Higher availability, with no single point of failure<br/>- Better scalability since load is spread across multiple nodes.<br/>- Easy to scale out by adding more database shards | - Only one node is active at a time, so there is no contention for resources.<br/>- Simplified data consistency as there is only one active node handling write operations.<br>- Simpler configuration and management.<br>- Easier to test failover since it only happens when the active node fails. |\n| **Drawbacks**            | - Can be more complex to configure and manage.<br/>- Scaling limitations on a single host.<br/>- Inflexible data model<br/>- hot spots and underutilization of shards due to the difficulty in re-sharding<br>- Synchronization and consistency issues need to be carefully addressed.<br>- Increased latency due to the need to synchronize data between nodes. | - Higher resource utilization since only one node is active.<br>- Failover can take longer since the passive node needs to start up and take over.<br>- Potential for data loss if the active node fails before changes have been replicated to the passive node.|\n\n\n## Active/Active\nIn Active/active, client machines connect to a [[load balancer|deploy.distributed.load-balancer]] that distributes their workloads across multiple active servers.\n- with active/active, the database can be written to from any region. Therefore if there is a degradation in one of the regions, there won't be any perceived downtime for end users.\n\n### When to use\n- Suitable for applications that require high availability and demand continuous operation even during planned or unplanned maintenance or node failures.\n- Use active-active when you need to distribute the workload across multiple nodes to handle high traffic and achieve better performance, responsiveness, and throughput.\n- Beneficial for mission-critical applications, real-time systems, or cloud services that require seamless scalability and minimal downtime during maintenance.\n\n## Active/Passive\nIn Active/passive, client machines connect to the main server, which handles the full workload, while a backup server remains on standby, only activating in the event of a failure.\n- with active/passive, we’d have some short period of time where nobody could login in (even if they could still keep querying) while we stand up a new primary\n\n### When to use\n- Suitable for applications where high availability is critical, but the primary concern is minimizing downtime in the event of a failure or maintenance.\n\n## Analogy: Building Power Generators\nAn active/active cluster is like having two separate generators that are both running and providing power to the building simultaneously. If one generator were to fail, the other would continue to provide power without interruption. This is similar to a building with redundant power sources that are both actively providing power, and if one source were to fail, the other would seamlessly take over.\n\nOn the other hand, an active/passive cluster is like having a primary generator that is providing power to the building, with a secondary backup generator that is on standby in case the primary one fails. In this setup, the secondary generator is not actively providing power to the building until it is needed, at which point it would be switched on and take over from the primary generator. This is similar to a building with a backup generator that is not running until the primary power source fails, at which point the backup generator would start up and provide power.","n":0.042}}},{"i":1267,"$":{"0":{"v":"Locks","n":1},"1":{"v":"\nA *distributed lock manager* (DLM) runs in every machine in a cluster\n\n## Purpose\n- [source](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)\n\nThe purpose of a lock is to ensure that 2+ nodes in a distributed system are not doing the same work. That work might be to write some data to a shared storage system, to perform some computation, to call some external API etc.\n\nAt a high level, there are 2 reasons you might want a lock: efficiency, or ensure correctness.\n- the efficiency problem is less worrysome, and usually results in nothing more than a nuisance. For instance, a user might get 2 password reset emails, or we might make 2 database reads for the same data.\n- the correctness problem is the one to be concerned about. If 2 nodes concurrently work on the same piece of data, we may very well result in a corrupted file, data loss, or permanent inconsistency.\n","n":0.083}}},{"i":1268,"$":{"0":{"v":"Load Balancer","n":0.707},"1":{"v":"\nA Load Balancer helps to spread the traffic across a cluster of servers\n- Purpose is to improve responsiveness and availability of applications, websites or databases.\n\nLoad balancers can be placed in the network to direct server requests according to server performance and the method of traffic distribution chosen, such as round robin for example. In certain cases, network managers prefer to place load balancers outside the cluster to provide for increased horizontal scalability.\n\nLoad balancing helps you scale horizontally across an ever-increasing number of servers\n\nThe LB sits between some client and some server. Broadly speaking, then can be added at three places:\n1. Between the user and the web server\n2. Between web servers and an internal platform layer (e.g. application servers or cache servers)\n3. Between internal platform layer (e.g. application server) and database/cache.\n\n![](/assets/images/2021-10-12-09-10-37.png)\n\nLB also keeps track of the status of all the resources while distributing requests\n- If a server is not available to take new requests or is not responding or has elevated error rate, LB will stop sending traffic to such a server.\n- Load balancers should only forward traffic to \"healthy\" backend servers. To monitor the health of a backend server, \"health checks\" regularly attempt to connect to backend servers to ensure that servers are listening. If a server fails a health check, it is automatically removed from the pool, and traffic will not be forwarded to it until it responds to the health checks again.\n\nSince the LB can itself be a single point of failure, it's considered a good practice to have a second redundant LB to form a cluster.\n- Each LB monitors the health of the other and, since both of them are equally capable of serving traffic and failure detection, in the event the main load balancer fails, the second load balancer takes over.\n\n### Purpose of LB\n- Users experience faster, uninterrupted service.\n    - Users won’t have to wait for a single struggling server to finish its previous tasks. Instead, their requests are immediately passed on to a more readily available resource.\n- Service providers experience less downtime and higher throughput. \n    - Even a full server failure won’t affect the end user experience as the load balancer will simply route around it to a healthy server.\n- Smart load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen.\n\n### LB Algorithms\n- *Round Robin Method* — This method cycles through a list of servers and sends each new request to the next server. When it reaches the end of the list, it starts over at the beginning. \n    - This strategy is most useful when the servers are of equal specification and there are not many persistent connections.\n    - A problem with Round Robin LB is that we do not consider the server load. As a result, if a server is overloaded or slow, the LB will not stop sending new requests to that server.\n    - Also, Round Robin LB doesn't consider that different requests might be more expensive than others.\n- *Least Connection Method* — This method directs traffic to the server with the fewest active connections.\n    - This approach is useful when there are a large number of persistent client connections which are unevenly distributed between the servers.\n    - It is also useful when all requests are not equal and some may take longer time to process than others.\n    - use this strategy in a web application where users can perform a variety of actions – from quick data retrieval to running long, complex reports.\n- *Least Response Time Method* — This algorithm directs traffic to the server with the fewest active connections and the lowest average response time.\n    - useful for applications where you have a mix of static and dynamic content. \n    - ex. a media streaming application where you have static images, text, and also video content.\n- *Least Bandwidth Method* - This method selects the server that is currently serving the least amount of traffic measured in megabits per second (Mbps).\n- *Weighted Round Robin Method* — The weighted round-robin scheduling is designed to better handle servers with different processing capacities. Each server is assigned a weight (an integer value that indicates the processing capacity). Servers with higher weights receive new connections before those with less weights and servers with higher weights get more connections than those with less weights.\n- *IP Hash* — Under this method, a hash of the IP address of the client is calculated to redirect the request to a server.\n    - This approach provides session persistence and is useful when you want to maintain the user's session on the same server. \n    - ex. e-commerce sites where users have shopping carts that need to persist would benefit from this approach.\n- *Geographic Load Balancing* - This method routes traffic based on the geographic location of the user, often to the nearest or best-performing datacenter. \n    - Useful for multinational applications where you want to provide the best user experience in terms of latency. \n    - ex. global streaming services like Netflix or YouTube use this strategy.\n","n":0.035}}},{"i":1269,"$":{"0":{"v":"Fault Tolerance","n":0.707},"1":{"v":"\nFault tolerance is the property that enables a system to continue operating properly in the event of the failure of one or more faults within some of its components.\n\n### Ways to achieve fault tolerance\n- [[atomic broadcast|network.broadcast#atomic-broadcast]]","n":0.167}}},{"i":1270,"$":{"0":{"v":"Distributed Cache","n":0.707},"1":{"v":"\nDistributed [[caches|general.arch.cache]] take advantage of the *locality of reference principle*: recently requested data is likely to be requested again.\n\n#### Shopping cart example\nLet's say User1 adds 5 different things in their shopping cart. The inventory count of those items are cached in Redis or Memcached.\n\nWhile the user is checking out, a Manual Inventory has been completed and gets loaded into the system. A manual inventory is the result of someone walking through a warehouse and counting items by hand. They discover that there are actually zero of a certain item that happens to be in User1's digital cart. The inventory person obviously doesn't know that.\n\nUser1 pays for the items that the system says they still have inventory for. Ideally, an error occurs as the purchasing system attempts to move a number of items from \"in stock\" to \"sold\", as there are actually none \"in stock\". This happens a lot when one item is next to another item and they have very similar packaging. Warehouse employees will accidentally ship the wrong item. If the cart checkout code was less defensive, it may just set a negative \"in stock\" value and the company would have a more serious problem of selling something they didn't own.\n\nHopefully, the person who wrote the Manual Inventory Entry code knows about the caching that the shopping cart does and sends them some sort of message that \"inventory has been updated. Please invalidate these cache entries.\" But that requires that they know exactly what is being cached and how it's being cached; otherwise they may not give the right information to ensure that data is being invalidated. ie- \"Please invalidate the following SKU's - \" vs \"Please invalidate all SKU's associated with Vendor XYZ\".\n\n### When to cache\n- We don't want to cache many keys that change continuously.\n- We don't want to cache many keys that are requested very rarely.\n- We want to cache keys that are requested often and change at a reasonable rate. For an example of key not changing at a reasonable rate, think at a global counter that is continuously incremented.\n\n## Parameter cache requirements\n### Cache memory\nHow much cache memory should we have? We can start with 20% of daily traffic and, based on clients’ usage patterns, we can adjust how many cache servers we need.\n- Alternatively, we can use a couple of smaller servers to store all the cached data.\n- Since a modern-day server can have 256GB of memory, we can easily fit all the cache into one machine.\n\n## Examples\n- [[redis]]\n- Memcached\n- Riak","n":0.049}}},{"i":1271,"$":{"0":{"v":"RPC (Remote Procedure Call)","n":0.5},"1":{"v":"\nAn RPC is when we call a function that lives on a different machine, without the programmer explicitly coding the details for the remote interaction\n- That is, the programmer writes essentially the same code whether the subroutine is local to the executing program, or remote\n- caller is client, executor is server\n\nThe RPC model implies a level of location transparency, namely that calling procedures are largely the same whether they are local or remote, but usually they are not identical, so local calls can be distinguished from remote calls.\n\nAlthough RPC seems convenient at first, the approach is fundamentally flawed, stemming from the fact that a network request is very different from a local function call:\n- A local function call is predictable and either succeeds or fails, depending only on parameters that are under your control. A network request is unpredictable, and is mostly out of our control.\n- A local function call either returns a result, or throws an exception, or never returns, while a network call may return without a result due to a *timeout*\n- When you call a local function, you can efficiently pass it references (pointers) to objects in local memory. When you make a network request, all those parameters need to be encoded into a sequence of bytes that can be sent over the network. That’s okay if the parameters are primitives like numbers or strings, but quickly becomes problematic with larger objects.\n\nAll of these factors mean that there’s no point trying to make a remote service look too much like a local object in your programming language, because it’s a fundamentally different thing. \n- Part of the appeal of REST is that it doesn’t try to hide the fact that it’s a network protocol\n\nRemote calls are usually orders of magnitude slower and less reliable than local calls, so distinguishing them is important.\n\nUsing RPCs involve tight coupling and difficulty to debug\n\nRPCs are a form of inter-process communication\n- ie. different processes have different address spaces\n\n# UE Resources\n[node and Go tutorial](https://blog.logrocket.com/introduction-to-rpc-using-go-and-node/)\n\n","n":0.055}}},{"i":1272,"$":{"0":{"v":"CDN","n":1},"1":{"v":"\nCDNs are a kind of [[cache|deploy.distributed.cache]] that comes into play for sites serving large amounts of static media. In a typical CDN setup, a request will first ask the CDN for a piece of static media; the CDN will serve that content if it has it locally available. If it isn’t available, the CDN will query the back-end servers for the file, cache it locally, and serve it to the requesting user.\n- If the system you are building is not large enough to have its own CDN, you can ease a future transition by serving the static media off a separate subdomain (e.g., `static.yourservice.com`) using a lightweight HTTP server like Nginx, and cut-over the DNS from your servers to a CDN later.\n\n- A website server normally has a caching server along with it, which will cache the webpage. All requests to the URL will retrieve the content from the cache, until the expiry time is hit. At that point, the user will make a fetch, which will see that the cached content is expired, and will then hit the server, which will retrieve the content, pass it to the caching server, which passes it to the user. All subsequent calls (from other users) will be intercepted by the caching server until it is expired (warm caching)\n![](/assets/images/2021-03-11-15-50-18.png)\n- A CDN provider will place servers in many locations, but some of the most important are the connection points at the edge between different networks\n- Without a CDN, transit may take a slower and/or more convoluted route between source and destination\n- Originally, a CDN was like a cache for static assets that don't change, like images, logos, stylesheets. Nowadays, that benefit only make up ~10% of everything a CDN can do. \n- while CDNs perform caching, not everything that performs caching is a CDN\n- CDNs reduce the importance of where your web server is located, since this would theoretically only impact the timecost of the first fetch of data of each expiry cycle.\n\t- In practice, this is only true for data that doesn't change often. If we are talking about a Facebook news feed that updates frequently, then you are still going to be interfacing a lot with the server, making the location of your server ultimately still important. \n- The DNS plays a central role in the CDN \n- a CDN is known as a distributed internet service\n\n![](/assets/images/2021-03-11-15-50-32.png)\n\nIf any of the following is true for your website, you should definitely opt for a CDN:\n- Large amounts of traffic\n- A scattered, international user-base\n- Expected growth\n- Lots of media content, such as images and videos\n\nIf the answer to all 4 is no, you won’t notice a difference in speed, but you will still get the benefit of [[DDoS|security.DDOS]] protection.\n","n":0.047}}},{"i":1273,"$":{"0":{"v":"CAP Theorem","n":0.707},"1":{"v":"\n## What is it?\nAt times when the network is working correctly, a system can provide both consistency (ie. [[linearizability|db.acid.consistency.strong#linearizability]]) and total availability. When a network fault occurs, you have to choose between either linearizability or total availability. \n- Thus, a better way of phrasing CAP would be \"when a network partition occurs, a distributed system must choose between either consistency or availability\".\n- A more reliable network needs to make this choice less often, but at some point the choice is inevitable.\n- note: this seems to be at least somewhat of a controversial tone, given that SQL databases occupy CA. This view above would state that CA systems are incoherent, though this opinion might not appear to be reputable, given the prevalence of relational databases.\n- regarding CA systems then, what CAP theorem would argue is that these systems have an inherent weakness, which is that in the case of a network partition, they will be forced to give up either consistency or availability.\n\nCAP is sometimes presented as Consistency, Availability, Partition tolerance: pick 2 out of 3. Unfortunately, putting it this way is misleading because network partitions are a kind of fault (due to the failure of network devices), so they aren’t something about which you have a choice: they will happen whether you like it or not.\n\n### Why must it choose between availability and consistency?\nThe reason the system must choose is because if there is a network partition, different parts of the system may result in having different views of the data.\n- If the system chooses to prioritize consistency, it will ensure that all nodes have the same view of the data, but it may sacrifice availability to all clients, since some nodes may be unavailable due to the partition.\n- If the system chooses to prioritize availability, it will continue to provide service to all clients, but it may not be able to ensure that all nodes have the same view of the data, which can lead to inconsistencies.\n\n### Consistency (C) \nAll nodes see the same data at the same time. This means users can read or write from/to any node in the system and will receive the same data. It is equivalent to having a single up-to-date copy of the data.\n\nDepending on the business needs of the application, we may wish to make the trade-off of sacrificing consistency for availability. Say we are implementing a facebook newsfeed. It is acceptable for the application to miss some data points here and there. For instance, if a friend of yours uploads a new post, it's not of critical importance that you get that data right away. That is, if one client is getting its data from a data store that is not up-to-date, then it's not the end of the world (assuming everything can be made up-to-date in a timely manner).\n\nThe \"C\" refers to only one type of consistency: *linearizability*\n\nThe basic idea behind linearizability is simple: to make a system appear as if there is only a single copy of the data.\n\nlinearizability is a recency guarantee on reads and writes of a single item in a database.\n- In a linearizable system, as soon as one client successfully completes a write, all clients reading from the database must be able to see the value just written. Maintaining the illusion of a single copy of the data means guaranteeing that the value read is the most recent, up-to-date value, and doesn’t come from a stale cache or replica.\n\nConsider that a unique constraint in a relational database requires linearizability\n\nDepending on our use-case, we must decide if our system requires linearizability. \n- ex. we may determine that for our flight booking application, the likelihood of two users trying to book the same seat on a flight to be remote, and thus something that we don't feel compelled to account for. In the event of this situation, we might just consider it a business expense to compensate the affected party in another way.\n\n### Availability (A) \nAvailability means every request received by a non-failing node in the system must result in a response. Even when severe network failures occur, every request must terminate. In simple terms, availability refers to a system’s ability to remain accessible even if one or more nodes in the system go down.\n- in other words, high availability results from running in a redundant configuration on multiple machines\n\nIn an AP (Availability/Partition-tolerant) system, the system is essentially saying “I will get you to a node, but I do not know how good the data you find there will be”; or “I can be available and the data I show will be good, but not complete.”\n\n### Partition tolerance (P) \na.k.a robustness\n\nPartition tolerance is the ability of a data processing system to continue processing data even if a network partition causes communication errors between subsystems\n- A single node failure should not cause the entire system to collapse.\n- A partition is a communication break (or a network failure) between any two nodes in the system, i.e., both nodes are up but cannot communicate with each other. \n\nA partition-tolerant system continues to operate even if there are partitions (ie. communication breakdowns) in the system. Such a system can sustain any network failure that does not result in the failure of the entire network. Data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages.\n\n![](/assets/images/2021-10-12-10-38-14.png)\n\n## When to prioritize Availability or Consistency?\nGo with eventual consistency if favouring availability, and strong consistency if favouring consistency\n\n### Availability\n- When eventual consistency can be tolerated.\n- When the system needs to remain operational and responsive even in the presence of network partitions or node failures. \n- ex. real-time messaging, online gaming, social networks, CDNs, IoT systems\n\n### Consistency\n- When the application demands strict data consistency and requires all nodes to have a consistent view of the data at all times.\n- When data conflicts and inconsistencies can lead to significant negative consequences.\n- ex. financial transactions (e.g. banking app), healthcare systems, inventory management, critical infrastructure (e.g. power grids, transportation systems), blockchain\n\n## PACELC Theorem\nOne place where the CAP theorem is silent is what happens when there is no network partition? What choices does a distributed system have when there is no partition?\n\nThe PACELC theorem states that in a system that replicates data:\n- if there is a partition (‘P’), a distributed system can tradeoff between availability and consistency (i.e., ‘A’ and ‘C’);\n- else (‘E’), when the system is running normally in the absence of partitions, the system can tradeoff between latency (‘L’) and consistency (‘C’).\n![](/assets/images/2021-10-12-10-43-34.png)\n\nThe first part of the theorem (PAC) is the same as the CAP theorem, and the ELC is the extension. The whole thesis is assuming we maintain high availability by [[replication|db.distributed.replication]]. So, when there is a failure, CAP theorem prevails. But if not, we still have to consider the tradeoff between consistency and latency of a replicated system.\n\n### Examples\n- Relational databases are CP\n- [[couchdb]] is AP\n    - it's *Partition-Tolerant*, every node is *Available* at all times, but it's only *eventually Consistent*.\n- [[aws.svc.dynamo]] and Cassandra are PA/EL systems: They choose availability over consistency when a partition occurs; otherwise, they choose lower latency.\n- BigTable and HBase are PC/EC systems: They will always choose consistency, giving up availability and lower latency.\n- MongoDB can be considered PA/EC (default configuration): MongoDB works in a primary/secondaries configuration. In the default configuration, all writes and reads are performed on the primary. As all replication is done asynchronously (from primary to secondaries), when there is a network partition in which primary is lost or becomes isolated on the minority side, there is a chance of losing data that is unreplicated to secondaries, hence there is a loss of consistency during partitions. Therefore it can be concluded that in the case of a network partition, MongoDB chooses availability, but otherwise guarantees consistency. Alternately, when MongoDB is configured to write on majority replicas and read from the primary, it could be categorized as PC/EC.\n","n":0.028}}},{"i":1274,"$":{"0":{"v":"Deployment Artifact","n":0.707},"1":{"v":"\nA deployment artifact is synonymous with a *build*: it is the application code as it runs in production (compiled, built, bundled, minified, and optimized)\n- most often it's a single binary, or bunch of files compressed in an archive.\n\t- due to this nature, artifacts can be stored and versioned.\n\nUltimate goal of an artifact is for it to be downloaded as fast as possible on a server, and run immediately.\n\nAn artifact should be configurable so it can be deployed on any environment. Only the configuration (ie. env variables) changes.\n- ie. we need to be able to deploy the same artifact to both staging and production servers.\n\t- if you have to build the artifact 2 times for staging and production, you are doing it wrong.\n\nTypically, an artifact is either stored in an S3 bucket or it is deployed to an external server.\n\nThe artifact-based workflow gives us advantages:\n- quick to deploy\n- instant to rollback\n- backups are readily available\n- if a build works on staging, the only reason it wouldn't work on production is because of the configuration (or the environments aren't the same, which shouldn't be the case).\n- feature flagging is easier, since we just need to check the env variables.\n","n":0.071}}},{"i":1275,"$":{"0":{"v":"CI/CD","n":1},"1":{"v":"\n## Continuous Integration (CI)\na CI system will automatically build code, run linters, run tests etc., and provide immediate feedback on the result of those actions.\n- a CI can also automate other parts of your development process by managing deployments and notifications.\n- \"continuous\" means that it happens with each \"checkin\" of the code (updating code on the host)\n\nWhen you instruct a build to be made, the CI runner clones the github repo into a brand-new virtual environment, then runs the tests and creates the build.\n- if any of those tasks fail, the build is considered broken\n- if all tasks succeed, the build passes and Travis CI can deploy the code\n\n### Examples\n- [[Github Actions|github.actions]]\n- Travis CI\n- Jenkins\n- Circle CI\n\n## Continuous Deployment\nThis process is about taking all of the commits we make during the development process, and deploying them to the live (production) application automatically.\n- As part of this process, we subject our code to a multitude of tests (unit, e2e etc.), which is CI.\n\n## Continuous Delivery\nOne step past, whereby we end up in a state where a deployment can be triggered, albeit manually.\n- more realistically might be called \"continuous deliverables\".\n- the idea here is that it empowers the decision makers to be able to control deployments. The code then becomes something like a deliverable to a boss' desk, rather than you just going ahead and deploying it whenever you merge into main\n\nContinuous Delivery is about making sure that every change a developer makes can make it through the pipeline that includes subjecting our code to a series of tests \n- does it compile? does it pass unit tests? does it pass E2E? etc\n\nIt is slightly broader than Continuous Deployment, in the sense that not all changes that go through the pipeline need to necessarily make it into production.\n- In this way, you could think of Continuous Delivery as baking in the business-level ability to trigger deployments. Putting code into production is a business-decision, not a technical one. The executives of a company may decide \"we only want to deploy new releases every 2 weeks, so that the amount of change that user's experience isn't so jarring\". The point is that from a technical standpoint, we can do this— we just decide not to for business reasons.\n    - This above explanation shows that in order to do Continuous Delivery, we have to already be doing Continuous Deployment\n\n### Why use CD (both forms)?\n- reduced deployment risk\n    - with smaller deployments, it's easier to spot when/where something goes wrong\n        - anal: same reason why smaller commits are preferable\n- user feedback\n    - with incremental and automated change, it's easier to get more reliable feedback from users, rather than having to depend on what they tell you they want.\n","n":0.047}}},{"i":1276,"$":{"0":{"v":"Dendron","n":1},"1":{"v":"\n\n## References\n[Embed notes in current note (transclusion)](https://wiki.dendron.so/notes/f1af56bb-db27-47ae-8406-61a98de6c78c/)","n":0.378}}},{"i":1277,"$":{"0":{"v":"Vault","n":1},"1":{"v":"\n### Local Vault\nA local vault is a folder in your file system.\n\n### Remote Vault\nA remote vault is a git repository. If you choose a remote vault, you can choose from a vault from the registry or enter a custom git url.\n- Note that when you add a remote vault, the url can also point to a remote workspace. In that case, dendron will inspect the dendron.yml to get a list of all vaults within the workspace and add all vaults from inside the workspace.\n","n":0.109}}},{"i":1278,"$":{"0":{"v":"Commands","n":1},"1":{"v":"\nGo up one level\n`cmd+shift+up`\n\nOpen Lookup with current path pre-populated\n`cmd+shift+down`\n\nCycle between note siblings\n`cmd+shift+[`/`]`\n\nCopy to clipboard a reference link of the highlighted header\n`cmd+shift+r`\n\n### Backlinks\n\nLink to a note\n- `[[code.git.cli]]` will link to a note of the same hierarchy\n- we can generate a new note by entering a new hierarchy within the braces and hitting `F12`\n- we can alias a backlink like this `[[git cli|code.git.cli]]`\n- all backlinks of current note can be seen on sidebar\n\nEmbed a note/section of note in another note\n- You can create a note reference by using CMD+SHIFT+R while inside a note, then paste that reference into another note. This creates an embedded note in the current note.\n\t- Instead of embedding a whole note, we can embed just what's within a header, by highlighting the head before using CMD+SHIFT+R\n\t\t- If we want to retain our original header, remove the `,1` in the reference\n\nLink to particular section of a note\n- You can use `CMD+SHIFT+C`\n\t- can do it for the whole note, or just a highlighted section\n\nExtracting out text to put into a lower-level note\n1. select text to extract\n2. cmd+shift+s\n3. turn \"scratch note\" off, and \"selection extract\" on\n4. rename file\n5. cmd+shift+r from the extracted file\n6. paste into the original file to create the ref to the lower-level file\n\n[Link ranges](https://wiki.dendron.so/notes/f1af56bb-db27-47ae-8406-61a98de6c78c/)\n\n\n## Navigating\ngo up one level\n- cmd+shift+up\n\n## Searching\n### Scoping search to a sub-tree\n1. Open vscode advanced search with `<Cmd+shift+f>`\n2. Input search term\n2. In \"files to include\", include as many hierarchies (separated by `.`), followed by `**` at the end\n    - ex. Search within `postgres` domain - `postgres**`\n    - ex. `graphql.operators**`","n":0.063}}},{"i":1279,"$":{"0":{"v":"Debugging","n":1},"1":{"v":"\nThere are 3 categories of bug origin in our code (LSD)\n1. Logic - Are the steps you used to carry out the operation correct?\n2. Syntax - Is what you are trying to portay in the code the right way to write it given the programming language being used?\n3. Data - Is the data passing through in a way that you'd expected? Does the programmatic state of the program align with what you'd expect from your code?\n\n### Stack Trace\nA stack trace is a treasure map of what happened up until things went wrong.","n":0.104}}},{"i":1280,"$":{"0":{"v":"DBeaver","n":1}}},{"i":1281,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Change Keyboard Shortcuts\n`Preferences` -> `User interface` -> `Keys`\n","n":0.333}}},{"i":1282,"$":{"0":{"v":"DB","n":1},"1":{"v":"\n### A simple database\nThe simplest database in the world consists of:\n- an insertion method (`db_set`)\n    - `db_set 123456 '{\"name\":\"London\",\"attractions\":[\"Big Ben\",\"London Eye\"]}'`\n- a retrieval method (`db_get`)\n    - `db_get 42`\n- a text file, as a log (meaning append-only) of key-value pairs\n\nThe write speed of this database is very fast because data is simply appended, but the read speed is very slow, at `0(n)`\n\nIf we wanted to speed this up, we could use an [[index|db.strategies.index]]\n\nmany databases internally use a log, which is an append-only data file.\n\n### Foward and Backward Compatability Requirements of Databases\nin an environment where the application is changing, it is likely that some processes accessing the database will be running newer code and some will be running older code—for example because a new version is cur‐ rently being deployed in a rolling upgrade, so some instances have been updated while others haven’t yet.\n- This means that a value in the database may be written by a newer version of the code, and subsequently read by an older version of the code that is still running. Thus, forward compatibility is also often required for databases.\n\n### The living nature of a database\nWhen you are designing a database with relations, you have to consider that the database is a living object. At any moment in time, you have definitions and purposes of each specific model. However, as the needs of the database change and grow with your business, you need to be flexible to change the definition of models that no longer makes sense, given that business logic\n- For instance, we previously had the concept of a payment profile in the database, which was basically a credit card. Therefore, you might find a payment profile as \"a way to pay\". However, within corporations have coupons into the payment system, it became apparent that a coupon was really not to do similar from a credit card in the way that it would function. Therefore, it made sense to change the way that we thought of a payment profile. Incorporating coupons into this model meant reconsidering the name. Instead of payment profile, they would be called payment methods. Now the definition became something more like \"some type of exchange good that you can pay with to receive a good in return\". By allowing this definition to change, we were able to leverage existing functionality to incorporate new and unexpected designs into the database model.\n\n## Access Pattern\nDatabase should be designed around the way that you plan to access and use the data\n\n### Analogy: Lego\nImagine having a big box of lego with 1000s of pieces mixed altogether. This would naturally be difficult to go through and pull out the pieces we need. Now, consider how we are using the lego. Maybe we are following the schematic of building a car, or maybe we are building something a little more ad hoc. In the first case, the color and shape of the piece matters, but maybe in the second case we only care about the shape. In the first case, we might benefit from first separating the pieces into separate piles based on their color. In the second case, maybe it makes more sense to separate the pieces into piles based upon their shape.\n\nThis analogy helps us think about the importance of the data access pattern. Whether we are building from a schematic or in an ad hoc way, our priorities change and we think differently about how we want to organize the lego pieces so that we can find them easier.","n":0.041}}},{"i":1283,"$":{"0":{"v":"Type","n":1}}},{"i":1284,"$":{"0":{"v":"Key Value Store","n":0.577},"1":{"v":"\nKey–value databases work in a very different fashion from the better known relational databases (RDB)\n- RDBs predefine the data structure in the database as a series of tables containing fields with well defined data types. Exposing the data types to the database program allows it to apply a number of optimizations.\n- In contrast, key–value systems treat the data as a single opaque collection, with no unified structure to the value of each record.\n  - this more closely follow the principles of [[OOP|paradigm.oop]]\n\nkey–value databases often use far less memory than RDBs to store the same data\n- This is because optional values are not represented by placeholders or input parameters, as they are in most RDBs","n":0.094}}},{"i":1285,"$":{"0":{"v":"Graph Database","n":0.707},"1":{"v":"\nGraph databases are similar to [[document databases|db.type.document]], but add another layer (the relationship), which allows them to link documents for rapid traversal.\n\nGraph databases excel at representing hierarchical relationships\n\nGraph databases are very useful in \n- supply chain management\n    - entities: suppliers, manufacturers, distributors, products, customers\n    - relationships: supplier-to-manufacturer, manufacturer-to-distributor, distributor-to-customer\n- pharmaceutical research\n- log analysis\n\n## Implementatioms\n- https://dgraph.io/","n":0.135}}},{"i":1286,"$":{"0":{"v":"Document Database","n":0.707},"1":{"v":"\nDocument-oriented databases are inherently a subclass of the key-value store\n- The difference lies in the way the data is processed: \n    - in a key-value store, the data is considered to be inherently opaque to the database, \n        - this means that while a value *can* be a complex data type (like an object), we cannot do intelligent querying based on its fields. The database only sees the object as a string and it doesn’t really care about the structure of the content it represents. \n    - a document-oriented system relies on internal structure in the document in order to extract metadata that the database engine uses for further optimization.\n\nDocument databases store all information for a given object in a single instance in the database, and every stored object can be different from every other. This eliminates the need for object-relational mapping while loading data into the database.\n\nDocument databases typically provide for additional metadata to be associated with and stored along with the document content. That metadata may be related to facilities the datastore provides for organizing documents, providing security, or other implementation specific features.\n\n","n":0.074}}},{"i":1287,"$":{"0":{"v":"Distributed","n":1}}},{"i":1288,"$":{"0":{"v":"Synchronization","n":1},"1":{"v":"\n## Syncing the same document\nThe three most common approaches to synchronization of a single document are *locking*, *event passing* and *three-way merges*.\n\n### Locking\nThe simplest technique, whereby a shared document may only be edited by one user at a time.\n- ex. when opening Microsoft Word on a networked drive, the first user to open it has global access, but subsequent users have read-only access.\n\nA refinement on locking would be Subsection Locking, whereby we only lock the parts of the document that are being edited, but this still limits collaboration.\n\nLocking is not a suitable approach when connectivity is unreliable, since the lock or unlock signals can get lost, leaving no owner.\n- ex. imagine being in the middle of editing a document, and then passing through a tunnel. With luck, the user will still be editing when they come out of the other side of that tunnel, and will be in connectivity when they finally save and close the document. But what if they save and close while in the tunnel and out of connectivity? Whoever manages the lock will not be aware that the client as finished accessing the document, and it will assume that it should still be locked, thereby preventing access to any other clients.\n\n### Event passing\nEvent passing involves capturing all user actions and replaying them on other nodes of the network.\n\nPopular for implementing edit-based collaborative systems.\n\nAny failure in event passing results in a fork\n- due to the best-effort nature of the web, this is potentially commonplace. \n- ex. think of a SaaS like Jira, where you have many different users interacting with the same resources often (e.g. all members of engineering team editing the same sets of documentation). When there is a conflict, Jira just chooses one of the writes as the winner, and sends a \"sorry, please try again\" message to the losing client.\n    - edit: not sure if this point belongs here. Not sure if this example actually demonstrates a fork. This point might belong elsewhere\n\n### Three-way merge\n1. The client sends the contents of the document to the server.\n2. The server performs a three-way merge to extract the user's changes and merge them with changes from other users.\n3. The server sends a new copy of the document to the client.\n\nThree-way merges are not a good solution for real-time collaboration across a network with latency.\n\n### Differential Synchronization\nDifferential synchronization is a symmetrical algorithm employing an unending cycle of background diffs and patch operations.\n- https://neil.fraser.name/writing/sync/\n- [Google whitepaper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35605.pdf)\n\n* * *\n\n## Synchronization Strategies\n### Create some means to \"diff\" the the most recent \nMimic the schemas of the client-side database and server-side database as much as possible\n\nThe key is for both the local database and remote database to track when each value was each of its fields were last changed. \n- To implement this, we can consider the fields we are interested in for a particular model (e.g. `title`, `media_items`). That is, we want to know when these values have changed. We can store this value by adding N more columns, which are timestamps of when the row was last updated (e.g. `title__last_updated`, `media_items__last_updated`). The client and remote databases must each update their own version of this column when the value changes, allowing the other databases to know who has fresher data.\n    - ex. when remote database updates its value in response to a change added by the client\n    - ex. when the client database receives a new value from the remote server.\n    - ex. when the the user updates a document via the app's UI\n\n### Log of Changes\nAnother approach is to collect a log of user interactions with the client database. We determine that there has been a change in state by the presence of a log indicating so. When this happens, we initiate a 2-way sync with the remote database.\n\nWhen the remote database receives the changes with the 2-way sync pulse, it compares the values to the ones it has stored. With each version's timestamp, we award legitimacy to the value using *last-write wins* and make that the new canonical value for the field. If the remote database has changes that don't yet exist on the client database, then it will return those back to the client. Finally, the client proceeds to update those values.\n\nEach client-side database should initiate a 2-way sync on 2 occasions:\n- when there has been a change in state (e.g. creating a new document, deleting one, updating etc.)\n- after 5 seconds of inactivity\n\n### Push and Pull\nThe push function basically receives a set of data changes as a JSON object and writes them to the database, while the pull function receives the timestamp of the last sync and compiles a JSON object containing all changes the database has accumulated since this timestamps and returns it to the client.\n\n","n":0.036}}},{"i":1289,"$":{"0":{"v":"Replication","n":1},"1":{"v":"\n## What is it?\nReplication means keeping a copy of the same data on multiple machines that are connected via a network.\n\nEach node that stores a copy of the database is called a replica. \n\nIf you look at two database nodes at the same moment in time, you’re likely to see different data on the two nodes, because write requests arrive on different nodes at different times (replication lag).\n- These inconsistencies occur no matter what [[replication strategy|db.distributed.replication.strategies]] the database uses (single-leader, multi-leader, or leaderless replication).\n- Most replicated databases provide at least eventual consistency\n\t- in other words, this means that the inconsistency among replicas is temporary and eventually resolves itself.\n\n## Why do we do it?\n- Latency: To keep data geographically close to your users (and thus reduce latency)\n- Availabrlity: to allow the system to continue working even if some nodes have failed\n- Scalability: to [[scale out|deploy.scaling#horizontal-scaling-aka-scaling-out]] the number of machines that can serve read queries (and thus increase read throughput)\n- Network fault-tolerance: to keep the application working when there is a network interruption\n\nIn a world where our data didn't change over time, replication would be simple: we would just copy the data to each node and be done.\n- what makes replication hard is figuring out how to handle changes to replicated data.\n\nNormally, replication is quite fast: most database systems apply changes to followers in less than a second.\n- However, there are circumstances when followers might fall behind the leader by several minutes or more; for example:\n\t- if a follower is recovering from a failure\n\t- if the system is operating near maximum capacity\n\t- if there are network problems between the nodes\n\nThere are tradeoffs to consider when implementing replication (both are often configuration options in databases):\n- synchronous or asynchronous replication\n- how to handle failed replicas\n\n## Replication strategies\n[[db.distributed.replication.strategies]]","n":0.058}}},{"i":1290,"$":{"0":{"v":"Strategies","n":1},"1":{"v":"\nWith multiple replicas, a question inevitably arises: how do we ensure that all the data gets copied to all the replicas? \n- Naturally, every write to the database needs to be processed by every replica. Every replication strategy must ensure that the data is eventually the same in all replicas.\n\n# Replication Strategies\nThere are three main algorithms for replicating changes between nodes: \n- single-leader \n- multi-leader\n- leaderless\n\n## Single-Leader based replication\n- a.k.a *active/passive* or *master–slave replication*\n- this mode of replication is widespread, and is built into [[postgres|pg]], [[kafka]], [[mongo]] etc.\n\nThere are no conflict resolution issues to deal with in single-leader replication.\n\n### Approach\n1. A client wants to write to the database. It sends its request to the leader, which writes to its own local storage.\n2. When the leader writes to its local storage, it also sends the data change to all of its followers as part of a replication log (or change stream)\n3. Each follower applies all of the writes from the replication log\n4. Now, the leader and any of the followers can fulfill read requests.\n\nSince we can only write to the leader, but can read from any follower, this method works well in the web, where there are many more reads than writes.\n\n### Adding new followers\n1. Take a consistent snapshot of the leader’s database at some point in time.\n2. Copy the snapshot to the new follower node.\n3. The follower connects to the leader and requests all the data changes that have happened since the snapshot was taken. \n\t- This requires that the snapshot is associated with an exact position in the leader’s replication log. \n\t\t- That position is called the *log sequence number* in Postgres.\n4. When the follower has processed the backlog of data changes since the snapshot, we say it has caught up. It can now continue to process data changes from the leader as they happen.\n\n### Failover\nOn its local disk, each follower keeps a log of the data changes it has received from the leader. If a follower crashes and is restarted, or if the network between the leader and the follower is temporarily interrupted, the follower can recover quite easily. It simply needs to connect to the leader and request all the data changes that occurred during the time when the follower was disconnected\n\nHowever, things are much trickier if the leader fails. Three things must happen (this process is called *failover*): \n1. one of the followers needs to be promoted to be the new leader. \n2. clients need to update who they send their write requests to. \n3. the remaining followers must be aware of who the new leader is.\n\n#### Failover comes with some wrinkles:\n- If asynchronous replication is used, the new leader may not have received all the writes from the old leader before it failed. If the former leader rejoins the cluster after a new leader has been chosen, what should happen to those writes? The new leader may have received conflicting writes in the meantime. The most common solution is for the old leader’s unreplicated writes to simply be discarded, which may violate clients’ durability expectations.\n- what if our leader database has an incrementing strategy for assigning IDs? Now, when the leader fails, unless the follower-to-be-leader is perfectly up to date, it will start assigning IDs that have already been assigned by the original leader.\n- we could wind up in a situation where two former followers both think they are the new leader (a situation called *split brain*)\n\n### Implementing a Replication Log\na replication log is a stream of database write events, produced by the leader as it processes transactions. The followers apply that stream of writes to their own copy of the database and thus end up with an accurate copy of the same data. The events in the replication log describe the data changes that occurred.\n\nThere are several different replication methods\n- statement-based replication\n- WAL shipping\n\n#### Statement-based replication\nThe leader logs every write request (e.g. INSERT, UPDATE, DELETE) that it executes and sends that statement log to its followers. Then each follower executes those commands as if they had come from the client directly.\n\nDrawbacks:\n- what happens with non-deterministic functions like `now()` and `rand()`?\n- if IDs are incremented, or if a statement depends on existing data, they must be executed in the exact same order on each replica\n- statements that have side effects (e.g. [[triggers|pg.lang.triggers]], [[functions|pg.lang.func.custom]]) may result in different side-effects on each replica, unless they are deterministic.\n\n#### Write-ahead log (WAL) shipping\nIn the normal order of business, every write is appended to a [[WAL|db.acid.atomicity#write-ahead-logging-wal,1]].\n- instead of the leader simply writing the WAL to disk, it also sends it to all followers. This allows each follower to build a copy of the exact same data structures as found on the leader.\n- used in [[pg]]\n\nDrawback:\n- the log describes the data on a very low level: a WAL contains details of which bytes were changed in which disk blocks, meaning replication is closely coupled to the storage engine (A different strategy using a *logical log* exists to solve this issue of coupling. This is [supported by Postgres](https://www.postgresql.org/docs/10/logical-replication.html)).\n\t- If the database changes its storage format from one version to another, it is typically not possible to run different versions of the database software on the leader and the followers.\n\t- the only way to solve this (without downtime) is to upgrade all the followers, then perform a *failover*, resulting in one of the followers becoming the new leader. For this to work, the replication protocol must support version mismatching like this. Typically, WAL shipping doesn't allow this.\n\n## Multi-Leader based replication\nIn this setup, replication still happens in the same way as *single-leader replication*: each node that processes a write must forward that data change to all the other nodes.\n- The key difference is that each leader simultaneously acts as a follower to the other leaders.\n\nDue to the added complexity of setup, we use this configuration mainly when we have multiple datacenters. In this case, each datacenter would have its own leader.\n- each datacenter implements single-leader replication, but between datacenters, each leader replicates its changes to the leaders in other datacenters.\n\nWith this replication strategy, each datacenter can continue operating independently of the others\n\nBDR is a Postgres tool for implementing this replication strategy.\n\nanal: consider an [[offline-first|general.arch.offline-first]] application like a calendar. We can have a calendar on our laptop, tablet and mobile, and each acts as a leader (ie. it accepts write requests). But we also need some way to sync data between devices (ie. an asynchronous multi-leader replication process)\n- From an architectural point of view, each device is a datacenter with extremely unreliable connection between them (because of the offline-first nature)\n\n### Write Conflicts\nThis strategy has a major downside: the same data may be concurrently modified in two different datacenters, requiring us to resolve those write conflicts.\n- because of this, the multi-leader replication strategy is dangerous and should be avoided if possible.\n- write conflicts don't occur in single-leader replication, since writes are applied sequentially.\n\t- either the database would lock and wait for the first data modification to be written, or the second transaction would simply fail and require the user to retry the write.\n\nIn multi-leader, if each follower simply applied writes in the order that it saw them, the database would end up in an inconsistent state. Conflicts must be solved in a convergent way, rather than sequential.\n\nconflict resolution usually applies at the level of an individual row or document, not for an entire transaction\n- Thus, if you have a transaction that atomically makes several different writes, each write is still considered separately for the purposes of conflict resolution.\n\nConsider that conflicts don't necessarily have to be about writing different data to the same row. What if we had a meeting room booking system, and 2 different people booked the same room (which occurred because they each wrote to their own respective leaders)\n\nIn the event of write conflicts, there are a few strategies to resolve them:\n- out of the contenders, designate some field value to be the determination factor. This could be a `createdAt` timestamp (we simply take the latest one), or we might assign each write a unique UUID. In the case of conflict, we simply accept the one with the highest ID.\n\t- this approach is popular, but it is naturally prone to data loss.\n- give each replica a unique ID, and pre-designate replicas with the higher ID as the winner.\n\t- also implies data loss.\n- merge the values together.\n\t- if we're talking about an object, we might be able to do this cleanly. But if we're talking a string, we might have to concatenate them.\n- record the conflict in a separate data structure that preserves all information, and write application code that resolves the conflict at some later time (perhaps by prompting the user to handle the conflict).\n- custom logic (most multi-leader replication tools allow us to write application code to implement custom conflict-resolution logic). This code gets executed on either read or write.\n\t- *on write* - As soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler.\n\t-\t*on read* - When a conflict is detected, all the conflicting writes are stored. The next time the data is read, these multiple versions of the data are returned to the applica‐ tion. The application may prompt the user or automatically resolve the conflict, and write the result back to the database.\n\t\t- this is how CouchDB works.\n\n## Leaderless Replication\nIn a leaderless replication scheme, any replica can accept writes from clients directly.\n\nGeneral replication then happens in one of two ways:\n1. the client directly sends its writes to several replicas\n2. a coordinator node does this on behalf of the client\n\t- Unlike a leader database, that coordinator does not enforce a particular ordering of writes.\n\nLeaderless replication is also suitable for multi-datacenter operation, since it is designed to tolerate conflicting concurrent writes, network interruptions, and latency spikes.\n\nLeaderless replication could be summarized as *\"the database will do as much as it can, and if it runs into an error, it won’t undo something it has already done\"*— so it’s the application’s responsibility to recover from errors\n\nused by [[DynamoDB|aws.svc.dynamo]], Cassandra, Riak, [[CouchDB|couchdb]]\n\n* * *\n\n# Synchronous / Asynchronous Replication\nIn relational databases, this is often a configurable option; other systems are often hardcoded to be either one or the other.\n\nWhether a replication scheme is synchronous or asynchronous has a profound effect on the system behavior when there is a fault.\n\n## Synchronous\nIn a synchronous flow, the leader will wait until the follower has confirmed that it received the write before reporting success to the user, and before making the write visible to other clients.\n\nThe advantage of synchronous replication is that the follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. If the leader sud‐ denly fails, we can be sure that the data is still available on the follower. The disad‐ vantage is that if the synchronous follower doesn’t respond (because it has crashed, or there is a network fault, or for any other reason), the write cannot be processed. The leader must block all writes and wait until the synchronous replica is available again.\n- therefore, it is impractical for all followers to be synchronous: any one node outage would cause the whole system to grind to a halt.\n- In practice, if you enable synchronous replication on a database, it usually means that one of the followers is synchronous, and the others are asynchronous. If the synchronous follower becomes unavailable or slow, one of the asynchronous followers is made synchronous. This guarantees that you have an up-to-date copy of the data on at least two nodes: the leader and one synchronous follower.\n\n## Asynchronous\nIn an asynchronous flow, the leader sends the message, but doesn’t wait for a response from the follower.\n\nAsynchronous replication can be fast when the system is running smoothly, but there are complications to consider when replication lag increases and servers fail.\n- If a leader fails and you promote an asynchronously updated follower to be the new leader, recently committed data may be lost.\n\nOften, leader-based replication is configured to be completely asynchronous. \n- In this case, if the leader fails and is not recoverable, any writes that have not yet been replicated to followers are lost. This means that a write is not guaranteed to be durable, even if it has been confirmed to the client. \n- However, a fully asynchronous configuration has the advantage that the leader can continue processing writes, even if all of its followers have fallen behind.\n\nif an application reads from an asynchronous follower, it may see outdated information if the follower has fallen behind\n- if you run the same query on the leader and a follower at the same time, you may get different results, because not all writes have been reflected in the follower (due to *replication lag*). This is just a temporary state, since the followers will eventually catch up. This effect is known as *eventual consistency*\n\nThe value of asynchronous replication is gets higher:\n- as the number of followers increases\n- as our followers get more geographically distributed.\n\n### Consistency models for dealing with replication lag\nWhen implementing an asynchronous strategy, there are some things to be aware of. Depending on our application, we might not necessarily care too much. \n- ex. in Facebook's newsfeed, it doesn't really matter if all the latest posts are actually there. As long as it's eventual, it's fine.\n\n#### read-after-write consistency\nWhat happens if a user performs some action that writes some data to the database (ie. to the leader node), but by the time the user goes to view that data in the UI, the replication of data hasn't taken place yet between the leader and the follower that is handing the GET request? We need a way of implementing read-after-write consistency.\n\nThere are different strategies to implement read-after-write consistency, depending on what we're doing:\n- when user is reading data that they recently modified, allow them to read directly from the leader. The issue is that we need to know if data has been modified without first querying for it. Therefore, we can make a simple rule: if the data is modifiable by the user, read from the leader; otherwise, read from a follower.\n\t- ex. in a social media website, you can only edit your own information. Therefore, if you are querying for your own data, always read from the leader.\n- track the time of the last update and, for one minute after the last update, make all reads from the leader. You could also monitor the replication lag on followers and pre‐ vent queries on any follower that is more than one minute behind the leader.\n- The client can remember the timestamp of its most recent write, then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp. If a replica is not sufficiently up to date, either the read can be handled by another replica or the query can wait until the replica has caught up.\n\n#### Monotonic reads\nImagine user1 writes a comment on a post. The leader node replicates it instantaneously to follower1, but experiences lag in replicating it to follower2. User2 then logs in and reads the data from follower1, so it sees the comment from user1. Then, User2 refreshes the page, but this time the data is read from follower2. Due to the lag, User2 no longer sees the comment from user1.\n- *Monotonic reads* is a guarantee that this anomaly doesn't happen.\n- we achieve this by making sure that each user always makes their reads from the same replica\n\n#### Consistent Prefix Reads\nImagine user1 comments on a post saying \"how is the weather in Chicago?\", then user2 writes a comment in response \"pretty good\". Now, imagine that user3 sees this post with the comments (via the data provided from followers). The comment from user2 gets replicated with little lag, but the comment from user1 experiences a lot of lag. The result may be that user3 sees these comments out of order.\n- *Consistent prefix reads* is a guarantee that this anomaly doesn't happen.\n\t- This guarantee says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order.\n\nThis is a particular problem in partitioned ([[sharded|db.distributed.partitioning.sharding]]) databases\n- If the database always applies writes in the same order, this anomaly doesn't happen\n- But, in many distributed databases, different partitions operate independently, so there is no global ordering of writes: when a user reads from the database, they may see some parts of the database in an older state and some in a newer state.\n\n* * *\n\n# Conflict Resolution Strategies\n### Last-Write Wins (LWW)\nWidely used in both multi-leader replication and leaderless databases \n\nProblems with LWW:\n- Because of [[clock|hardware.cpu.clock]] drift between nodes, a node with a lagging clock is unable to overwrite values previously written by a node with a fast clock until the clock skew between the nodes has elapsed. This can cause arbitrary amounts of data to be silently dropped without ever alerting the application.\n- LWW cannot distinguish between writes that occurred sequentially in quick succession (e.g. client B’s increment definitely occurs after client A’s write) and writes that were truly concurrent (neither writer was aware of the other).","n":0.019}}},{"i":1291,"$":{"0":{"v":"Partitioning","n":1},"1":{"v":"\n## What is it?\nTo partition is to divide data across tables or databases stored on multiple nodes.\n\n## Why do it?\nThe justification for partitioning a database is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers.\n\nThere are two challenges when we try to distribute data:\n1. How do we know on which node a particular piece of data will be stored?\n2. When we add or remove nodes, how do we know what data will be moved from existing nodes to the new nodes? Additionally, how can we minimize data movement when nodes join or leave?\n\nPartitioning is usually combined with [[replication|db.distributed.replication]] so that copies of each partition are stored on multiple nodes\n\nBy design, every partition operates mostly independently, which is what allows a partitioned database to scale to multiple machines. \n\nThe amount of partitions you have should be at least double the number of instances.\n- This allows for growth; if you need to add instances then you can move one partition to another instance.\n\n## Types of Partitioning\nThere are many different schemes one could use to decide how to break an application database into multiple smaller DBs. Three of the most popular schemes used by various large-scale applications are horizontal partitioning, vertical partitioning, and directory-based partitioning.\n\n### Horizontal Partitioning (a.k.a. Range-based partitioning)\nIn this scheme, we put different rows into different tables. \n- ex. if we store different places in a table, we can decide that locations with ZIP codes less than 10000 are stored in one table and places with ZIP codes greater than 10000 are stored in a separate table.\n    - The key problem with this approach is that if the value whose range is used for Partitioning isn’t chosen carefully, then the partitioning scheme will lead to unbalanced servers. In the previous example, splitting locations based on their zip codes assume that places will be evenly distributed across the different zip codes. This assumption is not valid as there will be a lot of places in a thickly populated area like Manhattan as compared to its suburb cities.\n\nConsider the flexibility afforded when you take a horizontal partitioning strategy. We can have a large number of logical partitions to accommodate future data growth, such that in the beginning, multiple logical partitions reside on a single physical database server. Since each database server can have multiple database instances running on it, we can have separate databases for each logical partition on any server. So whenever we feel that a particular database server has a lot of data, we can migrate some logical partitions from it to another server. We can maintain a config file (or a separate database) that can map our logical partitions to database servers; this will enable us to move partitions around easily. Whenever we want to move a partition, we only have to update the config file to announce the change.\n\nSee [[sharding|db.distributed.partitioning.sharding]], a specific type of horizontal partitioning\n\n### Vertical Partitioning\nIn this scheme, we divide our data to exist on different servers based on their domain.\n- ex, if we are building an Instagram-like application where we need to store data related to users, photos they upload, and people they follow, we can decide to place user profile information on one DB server, friend lists on another, and photos on a third server etc.\n\nVertical Partitioning is straightforward to implement and has a low impact on the application. The main problem with this approach is that if our application experiences additional growth, then it may be necessary to further partition a domain specific DB across various servers (e.g. it would not be possible for a single server to handle all the metadata queries for 10 billion photos by 140 million users).\n\n### Directory-Based Partitioning\nA loosely coupled approach to work around issues mentioned in the above schemes is to create a lookup service that knows your current partitioning scheme and abstracts it away from the DB access code. So, to find out where a particular data entity resides, we query the directory server that holds the mapping between each tuple key to its DB server. This loosely coupled approach means we can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application.\n\n## Problems that arise from partitioning a database\nMost of these constraints are due to the fact that operations across multiple tables or multiple rows in the same table will no longer run on the same server.\n\n1. *Joins and Denormalization*: Performing joins on a database that is running on one server is straightforward, but once a database is partitioned and spread across multiple machines it is often not feasible to perform joins that span database partitions. Such joins will not be performance efficient since data has to be compiled from multiple servers. \n- A common workaround for this problem is to denormalize the database so that queries that previously required joins can be performed from a single table. Of course, the service now has to deal with denormalization’s perils, such as data inconsistency.\n\n2. *Referential integrity*: just like how performing a cross-partition query on a partitioned database is not feasible, trying to enforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult.\n- Most RDBMS do not support foreign keys constraints across databases on different database servers. This means, applications that require referential integrity on partitioned databases often have to enforce it in application code. Often in such cases, applications have to run regular SQL jobs to clean up dangling references.\n\n3. *Rebalancing*: There could be many reasons we have to change our partitioning scheme:\n    - The data distribution is not uniform, e.g., there are a lot of places for a particular ZIP code that cannot fit into one database partition.\n    - There is a lot of load on a partition, e.g., there are too many requests being handled by the DB partition dedicated to user photos.\n\nIn such cases, either we have to create more DB partitions or have to rebalance existing partitions, which means the partitioning scheme changed and all existing data moved to new locations. Doing this without incurring downtime is extremely difficult. Using a scheme like directory-based Partitioning does make rebalancing a more palatable experience at the cost of increasing the complexity of the system and creating a new single point of failure (i.e. the lookup service/database).\n\n## Partitioning Strategies\n- *Hot spot* - a partition disproportionately high load.\n- *skew* - the phenomenon of some partitions having more queries made to them, or some partitions holding more data.\n\n### by Key Range\nImagine we had a database of the English dictionary. Our partitioning strategy could be \"have 2 letters per partition\" (A+B in partition1, C+D in partition2, and so on). This gives us the benefit of knowing exactly which partition to search.\n- naturally, this 2 letter per partition approach would lead to some partitions being much bigger than others, so the boundaries would need to adapt to the data.\n\nWithin each partition, we can alphabetize the keys making range scans easy.\n\nThe downside of Key-range partitioning is that certain access patterns can lead to hot spots.\n- ex. imagine we have an IoT device that writes many logs of information, keyed by a timestamp. Therefore, our strategy is to partition by date. If we know have a month, we know exactly which partition the data is in. The downside is that we have all writes going in a single partition, while the rest sit idle. To remedy this problem, we could prepend each timestamp with the name of the sensor, in the hopes that writes get more evenly spread across all partitions.\n\n### by Hash of Key\nWith this method, we assign a range of hashes to each partition. When writing to the database, we simply hash the key, and insert it into whichever partition would be responsible for it.\n- Because a [[hash|crypt.hashing]] is generated from the key, we get a uniform distribution of partitions with this method.\n\nThe downside of this strategy is that we can no longer efficiently query ranges. While before we could potentially limit our range query to a single partition, now we need to involve all partitions.\n- Cassandra uses a hybrid approach, whereby a table is declared with a compound primary key consisting of several columns. Only the first part of that key is hashed, which determines which partition it will belong to. The other columns are used as a concatenated index for sorting the data in Cassandra’s SSTables. As a result, while we cannot perform a range query over the first column, the rest can be range queried.\n    - ex. consider a social media site where users can make posts. In a one-to-many relationship such as this, where one user can make multiple posts, we can define the compound primary key as (`user_id`, `update_timestamp`), and make an efficient range query on the timestamp.\n\nWhile this strategy heavily reduces hotspots, it does not eliminate them. \n- ex. imagine user profiles of Twitter being stored across many partitions. Because some users are more popular than others, it's reasonable to expect a skew in querying for certain data. Imagine some global event occurs which causes everyone to interact with Donald Trump's user. People retweeting or commenting on Donald Trump's tweets will result in a large volume of writes to the same key (where the key is perhaps Donald's userId, or the ID of his tweet)\n    - In cases such as this, if one key is known to be very hot, a simple technique is to add a random number to the beginning or end of the key. Just a two-digit decimal random number would split the writes to the key evenly across 100 different keys, allowing those keys to be distributed to different partitions.\n\n#### CRC-32 Hashing\nCRC-32 is a hashing function. Since the output is in hexadecimal, it can be represented as a decimal number, and can have calculations done on it.\n\nWhen we pass an id to CRC-32, we get back a hexadecimal value:\n$$\nCRC32(\"id-123\") = F9FDB2C9 (hex) \n\\\\\nor\n\\\\\n4194153161 (decimal)\n$$\nNow we can use that number to determine which of 6 partitions to live on by using modulus:\n$$\n4194153161 \\% 6 = 5\n$$\n\n[[MongoDB|mongo]] and Cassandra use this\n\n### Secondary Index Partitioning\nPartitioning with secondary indexes adds considerable complexity, sithey don’t map neatly to partitions. There are two main approaches to partitioning a database with secondary indexes: document-based partitioning and term-based partitioning.\n\n#### Document-based\nConsider a website that lists cars. Each car is stored in its own document. We have fields like `year` and `color`. We want to give users the ability to query by color, so we add a secondary index to this field. Because of this index, whenever a red car is added to the database, the database partition automatically adds it to the list of document IDs for the index entry `color:red`.\n\nBecause the documents are partitioned based on their primary key, we now end up in a situation where we can have red cars existing across all partitions. However, because of the secondary index, we can still query by all of these cars in each partition. In effect, each partition only worries about its own secondary indexes. Naturally, partition1 doesn't care about the red cars existing in partition2. Because of this, a document-partitioned index is also known as a *local index* (as opposed to a *global index*).\n- as mentioned, not all red cars exist on a single partition, so we need to query all partitions and combine the results (known as *scatter/gather*). Even if we query all the partitions in parallel, it is prone to tail latency amplification.\n- used by [[mongo]], Riak, Cassandra, [[elastic-search]].\n\n#### Term-based\nInstead of each partition having its own secondary index (ie. local index), we can construct a *global secondary index* that covers all partitions.\n- That global index must itself be partitioned, since it cannot live on a single node (which would defeat the purpose of partitioning)\n\nIn our car example above, imagine we had all cars with colors starting with *a* to *r* in partition 1, and *s* to *z* in partition 2.\n\nThis type of index is called *term-partitioned*, where the term would be the field:value that we are indexing (here, `color:red`)\n\nThe upside of term-based over document-based is that reads are made more efficient, since we don't have to *scatter/gather* over all partitions, and only need to query the partition(s) that have the particular term we are interested in.\n- the downside is that writes are more complex and slower, since a single write may affect multiple partitions of the index (every term in the document might be on a different partition).\n\nUpdates to global secondary indexes are asynchronous.\n- if you read the index shortly after a write, the change you just made may not yet be reflected in the index.\n\n## Rebalancing\nRebalancing is the process of moving load (ie. data storage, read and write requests) from one node in the cluster to another. \n- The goal is to even out load across all nodes in the cluster.\n\nTo accomplish this smoothly, we should have many more partitions than we have nodes. If we have 10 nodes in the cluster, we may decide to have 1000 partitions, meaning each node holds 100 partitions. In the event that we add a node to the cluster, all we have to do is steal some partitions from each existing node and put them onto the new node until we reach a smooth distribution again. Removing a node is the same process in reverse.\n- for this reason it's a good idea to set the number of partitions at the outset of setting up the database, and treat that number as unchanging. The idea is to choose a number that's high enough to accommodate growth (the number of partitions we have is the max amount of nodes we can have), but not so high that we are overwhelmed with the management overhead that comes with partitions.\n\nRebalancing is an expensive operation, because it requires rerouting requests and moving a large amount of data from one node to another. If it is not done carefully, this process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress.\n- automating rebalancing can be dangerous in combination with automatic failure detection. \n    - ex. say one node is overloaded and is temporarily slow to respond to requests. The other nodes conclude that the overloaded node is dead, and automatically rebalance the cluster to move load away from it. This puts additional load on the overloaded node, other nodes, and the network— making the situation worse and potentially causing a cascading failure.\n        - this is why it's generally not a good idea to do a fully-automatic rebalancing.\n\nFollowing the rebalancing process, data may no longer exist on the same partition as before, so the clients need to update the IP address that they will request data from. This general problem is called *service discovery*. To handle, we can...\n- allow clients to contact any node (via a round-robin [[load balancer|deploy.distributed.load-balancer]]). If that node doesn't have the partition with the data requested, then it can forward the request on to the correct node, then receive the response and forward it back on to the client (like a proxy). \n- all requests will go first to a *routing tier* (basically a partition-aware load balancer), which determines which node the request should be sent to.\n- require that clients know which node has the partition that holds the data it needs.\n\nNo matter which approach we take, the question then becomes: how do we propogate this new information about which nodes hold which partitions?\n- For this, we can use a separate coordination system called ZooKeeper to keep track of this cluster metadata.\n    - here, each node registers itself with ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. Now, to know which node holds which partition, we only have to consult ZooKeeper.\n- An alternative approach is to use a *gossip protocol* among the nodes to disseminate any changes in cluster state. Requests can be sent to any node,\nand that node forwards them to the appropriate node.\n- When using a routing tier or when sending requests to a random node, clients still need to find the IP addresses to connect to. These are not as fast-changing as the assignment of partitions to nodes, so it is often sufficient to use DNS for this purpose.\n\n* * *\n\nA partition is known as a *vnode* in Cassandra and Riak","n":0.019}}},{"i":1292,"$":{"0":{"v":"Sharding","n":1},"1":{"v":"\nSharding is a special type of [[horizontal partitioning|db.distributed.partitioning#horizontal-partitioning-aka-range-based-partitioning]] where partitions spans across multiple [[database replicas|db.distributed.replication]].\n\nSharding is a method of splitting and storing a single logical dataset in multiple databases (running on different servers). By distributing the data among multiple machines, a cluster of database systems can store larger dataset and handle additional requests. Sharding is necessary if a dataset is too large to be stored in a single database\n- instead of vertically scaling a database with progressively heftier instances, horizontally scale by partitioning data across multiple databases, enabling us to easily spin up additional hosts to accommodate growth\n\nEach shard is responsible for storing and managing one or more chunks of data.\n- a chunk is a contiguous range or subset of data that is stored within a specific shard.\n- purpose is to allow for more efficient data distribution and management across the shards in a sharded database.\n- As the data changes or the sharding configuration is modified (e.g., adding new shards), chunk migration may occur to rebalance data distribution.\n\nWith sharding, you would know in which shard the data lies based on the ID.\n- illustration: imagine if there were 10 different shards, and the ID of the person was prefixed with a number from 0-9, signifying which shard the data could be found in. This is obviously not how it works, but a database object would have metadata about which shard it could be found in.\n\nEvery sharded cluster needs to have some logic in how it will place each piece of data. For instance, you might implement a simple round-robin using modulus:\n- `photoId % 10` - This will spread the photos evenly around all 10 of the databases, so that where to retrieve them is predictable.\n\n### Shard Key\nThe shard key is a column (in SQL) or field (in NoSQL) value that determines which shard a given row of data will exist in.\n\nWhat makes a good candidate for shard key?\n- The resulting shards should hold about the same amount of data so we don't get hotspots\n- Choose a shard key with high cardinality. A shard key with low cardinality reduces the effectiveness of horizontal scaling in the cluster.\n    - imagine the column value being an enum. How many different values could it be?\n    - ex. imagine our data table had a field `continent`, and our strategy was to shard based on which continent the user lived on. This would mean our shard key has a cardinality of `7`, meaning there can be no more than `7` chunks within the sharded cluster\n- Consider the most common types of queries performed in your application and how the shard key impacts query performance. The shard key should ideally align with the most frequent query patterns to minimize cross-shard queries.\n    - ex. if your application frequently performs range queries on a specific attribute, using that attribute as the shard key can improve performance.\n- Analyze the write and update patterns in your application. Consider how the shard key affects write and update operations.\n    - ex. if you expect a high volume of writes for a specific set of data, distributing that data across multiple shards based on the shard key can improve write throughput.\n- A good shard key should be chosen based on the application's query patterns to isolate queries to specific shards. Queries that involve the shard key in the filter can be efficiently executed on a single shard, minimizing the need to search across all shards.\n\n\n### Use-cases\nAn obvious use-case for sharding is how to store locale information in MMORPGs. Consider that the quest text for a quest in French does not have any benefit to being stored together with the English version of it. \n- This is the nature of your sharding strategy: \"let's increase lookup speed by removing the amount of rows in our database table. We can create a different table, and call it a shard of the first table. As long as the requester of the data knows which shard the data can be found in, then the result can be substantially faster queries.\"\n\nAnother possibility is that you have a large user table which is accessed heavily. You could create 12 shards for the user data, and give each database server the responsibility over one of the twelve months.\n\n## E Resources\n- [Sharding Postgres at Notion](https://www.notion.so/blog/sharding-postgres-at-notion)\n","n":0.037}}},{"i":1293,"$":{"0":{"v":"Design","n":1}}},{"i":1294,"$":{"0":{"v":"Normalization","n":1},"1":{"v":"\nDenormalization is the process whereby you put data often used together in a single table to increase performance, at the sake of some database purity. Many find it to be an acceptable trade, even going so far as to design the schema intentionally denormalized to skip the intermediary step of having to join tables.\n- degree of normalization is defined as ~6 levels called Normal Forms (NF)\n\t- getting to level 6 (6NF) is not necessarily better than just getting to level 3 (3NF). \"more normalization\" does come at a cost.\n\t- ex. 6NF requires tables to have \"1 PK, and at most 1 attribute\"\n- purpose is to reduce data redundancy and improve data integrity\n- Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design).\n\t- more normalization means more tables\n\t- more \"normal\" schemas have less attributes when designing a schema, the goal is to find the right balance of normalization: too much: you have a proliferation of tables; too little: you have coupled, repeated, and difficult to query data\n- A hypothetical example of a failure to properly normalize SQL tables would be a hospital database having a table of patients which included a column for the telephone number of their doctor. The phone number is dependent on the doctor, rather than the patient, thus would be better stored in a table of doctors. The negative outcome of such a design is that a doctor's number will be duplicated in the database if they have multiple patients, thus increasing both the chance of input error and the cost and risk of updating that number should it change (compared to a third normal form-compliant data model that only stores a doctor's number once on a doctor table)\n\n## How much normalization?\n- A good rule of thumb for whether or not to normalize is to think about if the attribute might be updated in the row's lifetime. If the attribute probably won't be, then it's fine to denormalize.\n\t- ex. imagine Medium.com, which has the concept of organizations (eg. FreeCodeCamp) that can post blogs. Since we can be reasonably assured that a post will not change the organization it is attached to, it's probably ok to denormalize `organization_id` into `posts` (ie. organization_id becomes a column of the posts table)\n- Normalization can be embraced more when a table has a lower row count.\n- Denormalization can be embraced when a table has many rows and/or has expensively calculated properties that are accessed often.\n\t- we should make use of triggers to automatically update attributes in other tables where data has been duplicated as a result of denormalization. In this case, we can prefix the columns with `gen_` (generated), to be explicit about the fact that this column should not be updated by hand, but is a column that is only updated as a result of the trigger.\n- If we have a write heavy table that is not read often, the overhead of adding denormalized data at write time might decrease performance more than it improves read performance.\n\n### Example: Twitter Home Timeline\nConsider Twitter’s home timeline, which is a cache of recently written tweets by the people a particular user is following (like a mailbox). This is another example of read-optimized state: home timelines are highly denormalized, since your tweets are duplicated in all of the timelines of the people following you.\n- However, the fan-out service keeps this duplicated state in sync with new tweets and new following relationships, which keeps the duplication manageable.","n":0.04}}},{"i":1295,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\n# UE Resources\nhttps://sammeechward.com/mocking-a-database-with-jest-in-javascript/\n","n":0.577}}},{"i":1296,"$":{"0":{"v":"Strategies","n":1},"1":{"v":"\n## Approaches\n### Compaction\nCompaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.\n- This is where we'd keep an append-only log of our database, and remove every key-value pair that is not the latest one (that is, removing the history of each key), thereby reducing the size of the dataset.\n\n![](/assets/images/2022-03-07-20-36-56.png)\n\n### Segment merging\n![](/assets/images/2022-03-07-20-44-25.png)","n":0.129}}},{"i":1297,"$":{"0":{"v":"Transaction","n":1},"1":{"v":"\nTransactions are an abstraction layer that allows an application to pretend that certain concurrency problems and certain kinds of hardware and software faults don’t exist.\n- A large class of errors is reduced down to a simple transaction abort, and the application just needs to try again.\n\nAn application with very simple access patterns, such as reading and writing only a single record, can probably manage without transactions.\n\nTransactions have a status to them. They might be in a \"running\" state, or they might be in a \"failed\" state.\n\nAlmost all relational databases (and some nonrelational) support transactions.\n\nTransactions come with a computational load. For example, if we are populating a database for the first time, we should disable AUTOCOMMIT, and instead only commit all inserts one time. This has the added benefit of being able to rollback the inserts if there is some sort of error.\n\nNot every application needs transactions, and sometimes there are advantages to weakening transactional guarantees or abandoning them entirely \n- ex. without transactions, we achieve higher performance or higher availability. \n- Some safety properties can be achieved without transactions.\n\n### Transactions to ensure write+read from same replica\nTransactions can be used to ensure that multiple operations are performed on the same replica.\n- ex. if we have a bulk write operation and a read operation where we read those documents we just created, we can wrap the write and the read in a transaction, which ensures that the read happens on the same replica that we wrote to, and doesn't read from a replica which may not yet have received those updates.\n\n### The distinction between single-object and multi-object transactions\nUsed properly, transactions refer to multi-object transactions. That is, we can update objects across different tables in a single transaction, and we get an all-or-nothing result. However, there is also the concept of single-object transactions, which is the guarantee for instance that a JSON value being updated will either update the whole thing or nothing at all; we won't end up in a state where only half of the JSON has been updated by the system. This single-object transaction guarantee is implemented by almost every database.\n- this idea of single-object transaction covers the whole document when updating in a [[document database|db.type.document]].\n\nWhen a transaction is started, it is given a unique, always-increasing transaction ID (txid). Whenever a transaction writes anything to the database, the data it writes is tagged with the transaction ID of the writer.\n\n### Transactions in Distributed Databases\nTransactions (multi-object) have mostly been abandoned by distrubuted databases, since they are difficult to implement across partitions. They also get in the way of high-availability and performance, which is normally a staple of [[nosql]] databases.\n\nThere is nothing that fundamentally prevents transactions in a distributed database. However, the costs outweigh the benefits, and the thought is that we can achieve all the transactional integrity we need with single-object operations.\n\nEven if we need to keep different objects in sync, we can still achieve this without transactions. However, error handling becomes much more complicated without [[atomicity|db.acid.atomicity]], and the lack of [[isolation|db.acid.isolation]] can cause concurrency problems.\n","n":0.044}}},{"i":1298,"$":{"0":{"v":"Connection Pools","n":0.707},"1":{"v":"\n## What is it?\nA cache of database connections maintained so that the connections can be reused when future requests to the database are required\n- after a connection is created, it is placed in the [[pool|general.patterns.creational.pool]] and it is used again so that a new connection does not have to be established\n\n## Why do it?\nConnecting a new client to the PostgreSQL server requires a handshake which can take 20-30 milliseconds. During this time passwords are negotiated, SSL may be established, and configuration information is shared with the client & server. Incurring this cost every time we want to execute a query would substantially slow down our application.\n\nThe client pool allows you to have a reusable pool of clients you can check out, use, and return\n- used to enhance the performance of executing commands on a database\n- If you're working on a web application or other software which makes frequent queries you'll want to use a connection pool.\n\n## How does it work?\n1. When the connection pool is created by the client (most often at the same time as when the client server starts), it pre-establishes a set number of database connections. These connections sit ready to be used (ie. with status `available`) by clients to the database server.\n2. When `.connect()` is called on the database client, one of the connections is borrowed from the pool. The connection that is assigned to the client gets marked as `inUse`\n3. After the client finishes the query, it calls `'.close()`, which returns the connection back to the pool (instead of actually closing it), and gets marked as `available`\n4. If a client request comes in and there are no available connections, the Pool Manager will either:\n    - create a new connection (up to the configured maximum limit)\n    - or wait until a connection becomes available.\n\nWe can configure the pool to have a minimum number of idle connections, and a maximum number of connections that can possibly be created\n\nTypically, each database client would have its own pool\n- ex. If there are 4 Express servers running, then there will be 4 pools\n- between those 4 servers and the users sending requests to them sits a *load balancer*\n\nThe pool sits between the application server(s) and the database:\n> front end client → load balancer → application server → pool → database\n","n":0.051}}},{"i":1299,"$":{"0":{"v":"Migrations","n":1},"1":{"v":"\nA migration is just a script that performs a set of synchronous database interactions.\n- it is predictable, and can be combined with other migrations to be run in the exact same order every time.\n\nAn important consideration for computationally expensive migrations (ie. migrations that take a long time) is that the database will be locked for the duration, and thus unusable by other entities.\n\nPreservation of data during migrations in general is not guaranteed because schema changes such as the deletion of a database column can destroy data.\n- Therefore, you should never `drop table` in a migration if there is still data in that table.\n\nMigrations should always be backwards compatible. This gives us the freedom to rollback the migration, and have a database that is still valid.\n- The proper approach to migrations is to expand the schema to work with the old version as well as the new version, then contract it to work only with the new version\n- Imagine 2 different users have different versions of an application. Upon signup, the older version asks for first and last name, while the newer asks for middle name as well.\n\n## UE Resources\n- [Migrations without downtime](https://thorben-janssen.com/update-database-schema-without-downtime/)\n","n":0.072}}},{"i":1300,"$":{"0":{"v":"Index","n":1},"1":{"v":"\n## What is it?\nThe general idea behind indexes is to keep some additional metadata on the side, which acts as a signpost and helps you to locate the data you want. \n- If you want to search the same data in several different ways, you may need several different indexes on different parts of the data.\n\nAn index is an additional structure that is derived from the primary data.\n\nIndexes are hidden objects that can exist against a table. You can create an index against a set of columns and the database engine can jump to the specific record based on the index.\n- One way of optimizing queries is to create an index on a column or columns you might commonly filter/join against.\n\na condition in a query is said to be *sargable* if the DBMS engine can take advantage of an index to speed up the execution of the query.\n- Sargable operators: =, >, <, >=, <=, BETWEEN, LIKE, IS [NOT] NULL\n- sargable -> Search ARGument ABLE\n\nWhen deciding what indexes to create, you need to consider a number of factors, including your query shapes, query volume, read-to-write ratio, and the size of your database\n\nThink about what happens when you run `CREATE INDEX` to create a new index in a relational database. The database has to scan over a consistent snapshot of a table, pick out all of the field values being indexed, sort them, and write out the index. Then it must process the backlog of writes that have been made since the consistent snapshot was taken (assuming the table was not locked while creating the index, so writes could continue). Once that is done, the database must continue to keep the index up to date whenever a transaction writes to the table.\n\n### Primary Index\nOther rows in the database can refer to a row via its primary key. The primary index is used to resolve these references.\n\n### Secondary Index\nImagine you had a bunch of data related to a single user: their `address`, their `favorite_books` etc., all connected by the `user_id` on each of these tables. If we added an index to the `user_id` row, then we could more quickly find all the rows belonging to this single user.\n\nA key-value index maps perfectly to a primary key, since each primary key is unique, but for secondary keys, this isn't the case. To solve this, we can either:\n- make each value in the index a list of matching row identifiers (like a postings list in a full-text index) \n- make each key unique by appending a row identifier to it\n\nSecondary indexes are the bread and butter of relational databases, and they are common in document databases too.\n\nA secondary index usually doesn’t identify a record uniquely, but rather is a way of searching for occurrences of a particular value: \n- ex. find all actions by user 123, find all articles containing the word hogwash, find all cars whose color is red etc.\n\nSecondary indexes add considerable complexity when it comes to [[partitioning|db.distributed.partitioning]] our database.\n- The problem with secondary indexes is that they don’t map neatly to partitions. With a primary index, we just need to determine which partition the data will belong on based on the primary key.\n\n### Analogy: Umbrella\nThe metal tips on the edge of an umbrella represent the items that are available for retrieval in a database. If you were not using an index, you'd have to look at each tip to see if it's the item you want or not. With an index, you look from the top of the umbrella, and each arm leading to the tip would light up, indicating which items should be retrieved.\n- therefore, indexes basically tell us how to get a certain piece of data.\n\n### Analogy: Textbook\nImagine studying for a test and you stumbled upon a topic you don't understand or forgot. You might consider looking at the INDEX at the back of the textbook for the topic. The index will tell you which page to go find the information. You can then jump straight to the correct page. Now imagine not having an index at the back of the textbook. You'd have to skim through the ENTIRE book until you found what you were looking for.\n\n## Why use it?\nIndexing makes querying by a column faster, at the slight expense of create/update speed and database size.\n- For example, you will often want to query all comments belonging to a post (that is, query comments by its `post_id` column), and so you should mark the `post_id` column as indexed.\n- However, if you rarely query all comments by its author, indexing `author_id` is probably not worth it.\n- In general, most `_id` fields are indexed. Sometimes, boolean fields are worth indexing if you often use it for queries. However, you should almost never index date (`_at`) columns or string columns.\n\nConsider the following SQL statement: \n```sql\nSELECT first_name FROM people WHERE last_name = 'Smith';\n```\nTo process this statement without an index the database software must look at the `last_name` column on every row in the table (ie. it must perform a full table scan). \n- With an index, the database simply follows the index data structure (typically a [[B-tree|general.lang.data-structs.tree.B]]) until the Smith entry has been found; this is much less computationally expensive than a full table scan.\n\nWe should index the things that we are likely to query on. If we frequently want to retrieve books from the db filtered by a certain year, we'd want to add an index onto the \"year\" column, making that data easier to retrieve.\n- otherwise the DBMS would have to search through every single book to see if it satisfies the filter requirements for the year we are seeking.\n\nQuerying by an index changes the performance of your query from $O(n)$ to $O(log(n))$\n- With 80 million documents, this would mean around 26 operations, rather than 80 million operations\n\nIn the case of data sets that are many terabytes in size, but have very small payloads (e.g., 1 KB), indexes are a necessity for optimizing data access. Finding a small payload in such a large dataset can be a real challenge, since we can’t possibly iterate over that much data in any reasonable time.\n- Furthermore, it is very likely that such a large data set is spread over several physical devices—this means we need some way to find the correct physical location of the desired data. Indexes are the best way to do this.\n\nAn index allows the system to have new options to find the data that your queries need\n- In the absence of an index, the only option available to your database is a sequential scan of your tables. The index access methods are meant to be faster than a sequential scan, by fetching the data directly where it is.\n\nIndexes add overhead, especially on writes.\n- The index also needs to be updated every time data is written.\n- consider that it's going to be difficult to out-perform simply appending to a file (as opposed to random writes)\n- anal: Using a knowledge-management system like Dendron comes at a cost of write speed, since we need to consider where we want to store the note. If we wanted to optimize \"write speed\" and nothing else, we would just put all of our notes in a single massive text file. However, since we care about retrieval, we sacrifice write speed for the benefit of retrieving it faster. This is the concept of an Index.\n\nDepending on whether our data store is [[OLAP|db.olap-oltp]] or OLTP, our indexing strategies will be very different.\n\n## How does it work?\nThe key in an index is the thing that queries search for, but the value can be one of two things: \n1. it could be the actual row (document, vertex) in question\n2. it could be a reference to the row stored elsewhere\n  - this \"elsewhere\" is known as a *heap file*\n\n## Types\n### Hash Index\nThis is a common index employed with key-value data.\n\nThe index itself is a key-value index\n\nThis indexing strategy works best when there are lots of writes, but not too many distinct keys (so that keeping it all in-memory is feasible)\n\nImagine our database was a simple append-only file of key-value pairs. To implement indexing, we could have a [[hash map|general.lang.data-structs.hash-table]].\n\nEvery time we append a new entry to the database file, \n\nImagine our database file was:\n```json\n123456:{\n  \"name\":\"London\",\n  \"attractions\": [\"Big Ben\",\"London Eye\"]\n} \n42:{ \n  \"name\":\"San Francisco\",\n  \"attractions\": [\"Golden Gate Bridge\"]\n}\n```\n\nAll we need to do is minify it (remove all spaces and newlines), and then make our index hash map as a series of key-value pairs, where each key maps to a byte offset of characters from the start.\n- ex. key `123456` maps to `0`, since it is at the start. Key `42` maps to `64`, since it is 64 bytes (characters) from the start:\n```json\n{\n  123456: 0,\n  42: 64\n}\n```\n\n### B-Tree\n[[general.lang.data-structs.tree.B]]\n\nB-tree is a key-value index.\n\n### Clustered Index\nA clustered index determines the physical order of data in a table. \n- Therefore, a table has only one clustered index (primary key/composite key).\n\nClustered indexes are indexes whose order of the rows in the data pages corresponds to the order of the rows in the index\n\nA clustered index defines the order in which data is physically stored in a table\n\nHaving a clustered index involves storing all row data within the index\n\n### Non-clustered Index\nHaving a non-clustered index involves storing only references to the data (the heap file) within the index\n\nA non clustered index is analogous to an index in a Book. The data is stored in one place. The index is stored in another place and the index has pointers to the storage location. this help in the fast search of data. For this reason, a table has more than 1 Nonclustered index.\n\n### Multi-column Index\nThe previous indexes only map a single key to a value, but what if we need to query multiple columns of a table simultaneously?\n\n#### Concatenated Index\nMost common type of Multi-column index is called a *concatenated index*\n- works by simply combining several fields into one key by appending one column to another\n- anal: a phone book uses the format `last name, first name` as an index. This provides benefits to quick searching by last name. However, the drawback is that order matters. Therefore, it is trivial to find out how many Smiths (ie. last name) there are in total, but it is very difficult to find out how many Johns (ie. first name) there are. \n\n### Multi-dimensional Index\nAllow us to more easily query several columns at once. In other words, it allows us to narrow down our query based on multiple columns at once, rather than having to search by a single column, then filter out the ones that don't find our query.\n\n#### Example: location queries\nimagine we had an application using Google maps, and depending on the user's current zoom radius, we need to query the database for locations within that area.\n- this is an example of a two-dimensional range query\n```sql\nSELECT * FROM restaurants \n  WHERE latitude > 51.4946 \n    AND latitude < 51.5079 \n    AND longitude > -0.1162 \n    AND longitude < -0.1004;\n```\n\nA standard B-tree or LSM-tree index cannot perform this query efficiently. By its nature, it would make us choose between:\n- all the restaurants in a range of latitudes (but at any longitude)\n- all the restaurants in a range of longitudes (but anywhere between the North and South poles)\n\n#### Other examples\n- on an e-commerce website, we could allow the user to query for products in a certain range of color (e.g. only red, green or blue)\n- in a weather database, we could allow the user to query on both `date` AND `temperature`, giving us the ability to search for all observations of 40+ degree weather within 2020.\n  - to do this with single-dimensional indexes, we'd have to scan over *all* the records from 2020 and then filter them by temperature (or vice versa)\n\n### Full-text search and Fuzzy Indexes\nThese type of indexes are good if we don't know exactly what we are searching for (ie. we are searching for similar keys, such as misspelled words)\n\nConsider the allowances that a search engine gives for a full-text search query:\n- synonyms are included\n- regional spelling differences are ignored (e.g. organization vs organisation)\n- vicinity of the searched word to other searched words in the same document","n":0.022}}},{"i":1301,"$":{"0":{"v":"ETL Pipeline","n":0.707},"1":{"v":"\nStands for **Extract**, **Transform**, **Load**\n\nThe ETL Pipeline can be thought as a series of processes whose goal is to take data from some external source, transform it to fit our needs, then loading that transformed data into our own database. \n- With this capability, we are able to enhance reporting, analysis and data synchronization.\n\n### Extract\n- data might be extracted from business systems, APIs, data from physical sensors, marketing tools, transaction databases (eg. Stripe)\n\n### Transform\ndata is temporarily stored in at least one set of staging tables as part of the ETL process\n\n### Load\nthe load phase doesn't have to be the end of the pipeline. Once the data has been successfully inserted into our database, it can trigger webhooks in other systems to perform more actions.\n\n## Implementations\n- [[Apache Flink|flink]]\n","n":0.089}}},{"i":1302,"$":{"0":{"v":"Cursors","n":1},"1":{"v":"\nA cursor is a piece of data (likely just an ID) that points to a location in a paginated list (ie. a single page).\n- the cursor is the thing that allows a user to traverse over records in a database\n\nA cursor conceptually has a `next()` method (which gets the next page of the larger dataset) and `forEach()` (which allows some code to be exectuted for each page of the whole dataset).\n- cursors may also implement helper methods like `toArray()`, which will allow us extract the data in an array format. \n\nA cursor can be viewed as a pointer to one (or many) row(s) within a larger dataset.\n\nCursors also facilitate retrieval of the records, as well as creating and removing records\n\nA cursor is conceptually and behaviourally very similar to an [[general.patterns.behavioural.iterator]]\n\nCursors are used to process individual rows returned by database system queries\n\na cursor makes it possible to define a result set and perform complex logic on it, on a row by row basis\n","n":0.079}}},{"i":1303,"$":{"0":{"v":"Concurrency","n":1},"1":{"v":"\nConcurrency control is about the database ensuring that correct results for concurrent operations are generated, while getting those results as quickly as possible\n\nWithout concurrency control, if someone is reading from a database at the same time as someone else is writing to it, it is possible that the reader will see a half-written or inconsistent piece of data.\n- ex. when making a wire transfer between two bank accounts if a reader reads the balance at the bank when the money has been withdrawn from the original account and before it was deposited in the destination account, it would seem that money has disappeared from the bank.\n\n[[Isolation|db.acid.isolation]] is the property that provides guarantees in the concurrent access to data\n- implemented by means of a concurrency control protocol. \n\nThe simplest way to achieve concurrency is to make all readers wait until the writer is done (*read-write lock*).","n":0.083}}},{"i":1304,"$":{"0":{"v":"Multi-version Concurrency Control (MVCC)","n":0.5},"1":{"v":"\nMVCC aims to solve the problem of concurrency by keeping multiple copies of each data item.\n- therefore, each user connected to the database sees a snapshot of the database at a particular instant in time, and any changes made by a single writer will not be seen by anyone else until the transaction has been committed.\n\nAn MVCC database doesn't actually overwrite database records when it updates them. Instead, it creates a newer version of the item.\n- therefore, it is inherent in MVCC that multiple versions of a single item are stored.\n- this introduces a new problem: when do we delete objects that are *definitely* obsolete?\n\nAn MVCC database also allows each transaction's read operations to read objects of a previous revision\n\nWhen a [[transaction|db.strategies.transaction]] reads from a consistent [[snapshot|db.acid.isolation#snapshot-isolation,1:#*]] in an MVCC database, it ignores writes that were made by any other transactions that hadn’t yet committed at the time when the snapshot was taken.","n":0.081}}},{"i":1305,"$":{"0":{"v":"Cleaning","n":1},"1":{"v":"\n### TinyURL example\nImagine a database where the main data entry is a shortened URL. In this structure, the shortform of the URL is matched with the longform. At some point, it will make sense to delete entries. For instance, the user may choose to have the URL only last for 24 hours, after which point it will no longer be a valid URL. In this scenario, all we need to do is introduce an `expired` property/column to the object/table, and check it any time a user tries to access the shortform URL. Now this leads to the principle issue of cleaning: when do we go about deleting items from the database?\n- If we chose to continuously search for expired links to remove them, it would put a lot of pressure on our database. Instead, we can slowly remove expired links and do a lazy cleanup.\n- additionally, \n    - Whenever a user tries to access an expired link, we can delete the link and return an error to the user.\n    - A separate Cleanup service can run periodically to remove expired links from our storage and cache. This service should be very lightweight and scheduled to run only when the user traffic is expected to be low.\n    - We can have a default expiration time for each link (e.g., two years).\n    - After removing an expired link, we can put the key back in the key-DB to be reused.\n    - Should we remove links that haven’t been visited in some length of time, say six months? This could be tricky. Since storage is getting cheap, we can decide to keep links forever.\n","n":0.061}}},{"i":1306,"$":{"0":{"v":"Backward Compatability","n":0.707},"1":{"v":"\n# UE Resources\n[Recommendation from Juan](https://spring.io/blog/2016/05/31/zero-downtime-deployment-with-a-database)\n[Recommendation from Juan](https://thorben-janssen.com/update-database-schema-without-downtime)\n","n":0.378}}},{"i":1307,"$":{"0":{"v":"Race Conditions","n":0.707},"1":{"v":"\nDepending on the level of [[isolation|db.acid.isolation]] that the database uses, some types of race conditions can be eliminated.\n- *serializable isolation* will protect against all.\n\n## Examples of Race conditions\n### Dirty reads\nOne client reads another client’s writes before they have been committed. The read committed isolation level and stronger levels prevent dirty reads.\n\n### Dirty writes\nOne client overwrites data that another client has written, but not yet committed.  Almost all transaction implementations prevent dirty writes.\n\n### Read skew (nonrepeatable reads)\nA client sees different parts of the database at different points in time. This issue is most commonly prevented with snapshot isolation, which allows a transaction to read from a consistent snapshot at one point in time. It is usually implemented with [[db.strategies.concurrency-control.MVCC]]\n\n### Lost updates\nTwo clients concurrently perform a read-modify-write cycle. One overwrites the other’s write without incorporating its changes, so data is lost. Some implemen‐ tations of snapshot isolation prevent this anomaly automatically, while others require a manual lock (`SELECT FOR UPDATE`).\n\n### Write skew\nA transaction reads something, makes a decision based on the value it saw, and writes the decision to the database. However, by the time the write is made, the premise of the decision is no longer true. Only serializable isolation prevents this anomaly.\n\n### Phantom reads\nA transaction reads objects that match some search condition. Another client makes a write that affects the results of that search. Snapshot isolation prevents straightforward phantom reads, but phantoms in the context of write skew require special treatment, such as index-range locks","n":0.064}}},{"i":1308,"$":{"0":{"v":"OLAP/OLTP","n":1},"1":{"v":"\nDatabases fall into 2 main categories: those fit for handling business logic, and those fit for handling business analytics.\n\nOLAP are used for business analysis, and feed into reports that help the management of a company make better decisions (ie. business intelligence)\n\nOLTP are used to handle business logic in the application.\n\n| Property             | Transaction processing systems (OLTP)             | Analytic systems (OLAP)                   |\n|----------------------|---------------------------------------------------|-------------------------------------------|\n| Main read pattern    | Small number of records per query, fetched by key | Aggregate over large number of records    |\n| Main write pattern   | Random-access, low-latency writes from user input | Bulk import (ETL) or event stream         |\n| Primarily used by    | End user/customer, via web application            | Internal analyst, for decision support    |\n| What data represents | Latest state of data (current point in time)      | History of events that happened over time |\n| Dataset size         | Gigabytes to terabytes                            | Terabytes to petabytes                    |\n\nIn the past, both OLTP and OLAP operations were done on the same SQL databases, but soon after it was realized that analytics data should be stored in its own database. This is what are known as [[data warehouses|data.warehouse]] today.\n","n":0.073}}},{"i":1309,"$":{"0":{"v":"Local Storage","n":0.707},"1":{"v":"\nanything inside local storage is accessible by any script inside your page. This is why storing JWTs in local storage is a bad idea, but storing app state would be fine.\n\n# Resources\n### real-time data\n[RethinkDB](https://github.com/rethinkdb/rethinkdb)\n[realtime](https://github.com/supabase/realtime)\n","n":0.171}}},{"i":1310,"$":{"0":{"v":"DBMS (Database Management System)","n":0.5},"1":{"v":"\n*\"SQL tells the database what information you want, and the DBMS (Postgres, MySQL) determines the best way to provide it.\"*\n- its role in your software architecture is to handle concurrent access to live data that is manipulated by several applications, or several parts of an application.\n\nWhen a select query is made, The database translates the query into a \"query plan\" which may vary between executions, database versions and database software. This functionality is called the \"query optimizer\" as it is responsible for finding the best possible execution plan for the query, within applicable constraints.\n\nGoal of a DBMS is to handle concurrent access to live data that is manipulated by several applications, or several parts of an application\n- at the core of this is the concept of a transaction (RDBMS)\n\n### Object–relational Impedance Mismatch\nThe object–relational impedance mismatch is a set of conceptual and technical difficulties that are often encountered when a relational database management system (RDBMS) is being served by an application program (or multiple application programs) written in an object-oriented programming language or style, particularly because objects or class definitions must be mapped to database tables defined by a relational schema.\n\n\n## UE Resources\n- [Evolutionary database design](https://martinfowler.com/articles/evodb.html)\n","n":0.072}}},{"i":1311,"$":{"0":{"v":"DB Cluster","n":0.707},"1":{"v":"\n- a collection of databases servers (ie. nodes, instances) that are connected to a single database file. It is made up of one master node, and 1+ slave nodes.\n- The benefits of clustering are: data redundancy, load balancing, high availability, and monitoring and automation.\n- a collection of databases managed by a single PostgreSQL server instance constitutes a database cluster.\n- a cluster is created with the `initdb` command\n- in file system terms, a database cluser is a single directory under which all data is stored.\n\t- normally `/usr/local/pgsql/data` or `/var/lib/pgsql/data` (determined by the package)\n- the unix user postgres should own this directory\n\n## Node\n- **node** - an instance of a database server\n- the master node is typically the only one that gets written to. \n\t- The master can operate without slaves, and if there is an emergency, the slave node can be promoted to master, since it has the most up to date data.\n- the slave node typically exists as a backup, or as a read replica\n\t- a read replica means that the read and write traffic can be split between the two\n","n":0.075}}},{"i":1312,"$":{"0":{"v":"ACID","n":1},"1":{"v":"\nin practice, one database’s implementation of ACID does not equal another’s implementation. \n- when a system claims to be “ACID compliant,” it’s unclear what guarantees you can actually expect.\n- systems that don't meet the ACID criteria are called BASE (Basically Available, Soft state, and Eventual consistency)\n\n\"As a developer, we should think of a database as something that is ACID-compliant.\" —Dmitri Fontaine\n\n[[Transactions|db.strategies.transaction]] are one way to achieve full ACID-compliance.\n- except for [[durability|db.acid.durability]], which is sort of its own thing.\n\n* * *\n\nFrom the Postgres-XL docs, here is an implication of it being fully ACID:\n*When you start a transaction or query in Postgres-XL, you’ll see a consistent version of your data across the entire cluster. While you are reading your data on one connection, you could be updating the same table or even row in another connection without any locking. Both connections are working with their own versions of the rows, thanks to global transaction identifiers and snapshots.  Readers do not block writers and writers do not block readers.*\n\n## UE Resources\nhttp://ithare.com/databases-101-acid-mvcc-vs-locks-transaction-isolation-levels-and-concurrency/\n","n":0.077}}},{"i":1313,"$":{"0":{"v":"Isolation","n":1},"1":{"v":"\nIsolation determines the extent to which items involved in a single [[transaction|db.strategies.transaction]] are visible to everything outside the transaction.\n- a lower isolation level increases the ability of many users to access the same data at the same time, but increases the number of concurrency effects (such as dirty reads or lost updates) users might encounter. \n- a higher isolation level reduces the types of concurrency effects that users may encounter, but requires more system resources and increases the chances that one transaction will block another.\n\nIsolation is the idea that if we have multiple things to do in a single transaction, we can roll it back as a chunk.\n- Imagine a transaction consists of 2 inserts and 1 update. The fact that we have atomicity means that we can rollback that whole transaction. This distinction is even more important when you consider multi-table transactions, which MongoDB does not offer.\n- ex. In old versions of MongoDB if you needed to remove an item from inventory and add it to someone's order at the same time, you could not.\n- ex. in multi-threaded programming, if one thread executes an atomic operation, that means there is no way that another thread could see the half-finished result of the operation. The system can only be in the state it was before the operation or after the operation, not something in between.\nA transaction in process and not yet committed must remain isolated from any other transaction.\n\nIsolation is about having multiple [[concurrently|general.concurrency]] executing transactions that are isolated from each other.\n- The database ensures that when the transactions have committed, the result is the same as if they had run *serially* (one after another), even though in reality they may have run concurrently\n\nIsolation is the opposite side of atomicity.\n- while we are doing our queries, are we allowed to see what is happening concurrently in the rest of the system?\n\t- ex. what if we want to make a backup with `pg_dump` that needs to run for several hours? That backup needs to be a consistent snapshot of the production database. If during the backup someone is doing inserts, we don't want these to be in the backup, since we want a snapshot that doesn't move.\t\t- to do this, postgres uses an isolation mode that prevents this from happening.\t\n\nisolation can be implemented using a lock on each object (allowing only one thread to access an object at any one time)\n\nIn theory, isolation should make your life easier by letting you pretend that no concurrency is happening.\n\n## Levels of isolation\nBy increasing levels of isolation, we are able to prevent some race conditions from happening.\n\nHigher levels of isolation come with a performance cost that in most cases aren't worth it. It’s therefore common for systems to use weaker levels of isolation, which protect against some concurrency issues, but not all. \n\n### Read committed\nThis is the most basic level of transaction isolation.\n\nIt makes two guarantees:\n1. When reading from the database, you will only see data that has been committed (no dirty reads).\n\t- if a transaction has written some data to the database, but the transaction has not yet committed or aborted, and another transaction can see that uncommitted data, that is called a *dirty read*.\n2. When writing to the database, you will only overwrite data that has been committed (no dirty writes).\n\t- if 2 transactions concurrently try to update the same object in a database, and second write overwrites an uncommitted value from the first write, that is called a *dirty write*. To prevent this, the second write must be delayed until the first transaction has completed.\n\t- databases prevent dirty writes by using row-level locks: when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object, and must only release it when the transaction has completed. \n\nRead committed isolation does not prevent race conditions occurring when 2 different users are trying to increment the same counter simultaneously.\n\nRead committed is a popular isolation level, and is the default setting in Postgres.\n\n### Snapshot isolation\nSnapshot isolation is a guarantee that all reads made in a transaction will see a consistent snapshot of the database, and the transaction itself will successfully commit only if no updates it has made conflict with any concurrent updates made since that snapshot.\n- in practice it reads the values resulting from the last committed transaction that existed at the time it started\n\nSnapshot isolation has the mantra \"readers never block writers, and writers never block readers\".\n\nThe idea is that each transaction reads from a consistent snapshot of the database— that is, the transaction sees all the data that was committed in the database at the start of the transaction. Even if the data is subsequently changed by another transaction, each transaction sees only the old data from that particular point in time.\n- snapshot isolation is therefore considered necessary for long-running, read-only queries such as backups and analytics. \n\nAllows better performance than serializability, yet still avoids most of the concurrency anomalies that serializability avoids.\n\nimplementations of snapshot isolation typically use write locks to prevent dirty writes\n\nFrom a performance point of view, a key principle of snapshot isolation is readers never block writers, and writers never block readers. \n- therefore, reads do not require any locks.\n\nBecause it maintains several versions of an object side by side, this technique is known as multiversion concurrency control (MVCC).\n\nSnapshot isolation is usually implemented by [[MVCC|db.strategies.concurrency-control]].\n\nused by Postgres (called *repeatable read*), Mongo, MySQL\n\n### Serializability\nSerializable isolation means that the database guarantees that transactions have the same effect as if they ran serially (i.e., one at a time, without any concurrency)\n- by doing this, all possible race conditions are avoided.\n\nSerializability means each transaction can pretend that it is the only transaction running on the entire database. \n\nSerializable isolation is rarely used because it carries a performance penalty.\n\nThere are 3 principal techniques to implementing serializability:\n1. Literally execute transactions in a serial order\n\t- possible when the entire active dataset can reside in [[memory|hardware.ram]]\n\t- used in [[Redis]]\n2. Two-phase locking\n3. Optimistic concurrency control techniques such as serializable snapshot isolation\n\nIn Postgres, Serializability is implemented as *serializable snapshot isolation* (SSI), which provides full serializability, but has only a small performance penalty compared to snapshot isolation. \n- optimistic concurrency control technique. Optimistic in this context means that instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right. When a transaction wants to commit, the database checks whether anything bad happened (i.e., whether isolation was violated); if so, the transaction is aborted and has to be retried. Only transactions that executed serializably are allowed to commit.\n\n## Strategies\n### Record Locking\n","n":0.03}}},{"i":1314,"$":{"0":{"v":"Durability","n":1},"1":{"v":"\nDurability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes.\n- in a single-node system, durability means that the data has been written to disk, \n- in a multi-node system, durability means that the data has been copied to some number of nodes.\n- to provide a durability guarantee, a database must wait until these writes or replications are complete before reporting a transaction as successfully committed\n\nA second layer of durability is usually provided through WALs write-ahead logs, which allows recovery in the event that the data structures on disk are corrupted.\n","n":0.095}}},{"i":1315,"$":{"0":{"v":"Consistency","n":1},"1":{"v":"\nConsistency refers to an application-specific notion of the database being in a \"good state\".\n\nConsistency is the idea that we have business-logic that we can share with the database, and have it implement those guarantees for us. \n- ex. each employee must have a non-negative integer for their salary\n- ex. in an accounting system, credits and debits must be balanced.\n- Includes things like what data types we use, the shape of our schema, what constraints we have (not null, fkey etc.), relations...\n\t- ex. The schema and the data types. When we define these structures in our database, all data that is entered must comply with the rules set out. For instance, if we have an id column of type int, then the data entering *must* be an int.\n\t- ex. the fact that MongoDB is schemaless means that we do not have Consistency. Nor does it have transactions (the A and I)\n\nThe application may rely on the database’s [[atomicity|db.acid.atomicity]] and [[isolation|db.acid.isolation]] in order to achieve consistency, but it’s not up to the database alone.\n\nA [[transaction|db.strategies.transaction]] either creates a new and valid state of data, or, if any failure occurs, returns all data to its state before the transaction was started.\n\n## UE Resources\n- [Consistency](https://systemdesign.one/consistency-patterns/)\n\n\nDon Ross ","n":0.07}}},{"i":1316,"$":{"0":{"v":"Weak Consistency","n":0.707},"1":{"v":"\nWith weak consistency, the data will be lost if the cache crashes before propagating the data to the database\n\n### How does it work?\n1. the client executes a write operation against the cache server\n2. the cache writes the received data to the message queue\n3. the cache sends an acknowledgment signal to the client\n4. the event processor asynchronously writes the data to the database\n","n":0.127}}},{"i":1317,"$":{"0":{"v":"Strong","n":1},"1":{"v":"\nStrong consistency means replication happens synchronously across data nodes\n- This means subsequent read operations must all return the same latest data, no matter which replica was queried\n\n### How does it happen?\n1. the server (client) executes a write operation against the primary database instance\n2. the primary instance propagates the written data to the replica instance\n3. the replica instance sends an acknowledgment signal to the primary instance\n4. the primary instance sends an acknowledgment signal to the client\n\nThe popular use cases of the strong consistency model are the following:\n- File systems\n- Relational databases\n- Financial services such as banking\n- Semi-distributed consensus protocols such as two-phase commit (2PC)\n- Fully distributed consensus protocols such as Paxos\n\nGoogle Spanner and Google Bigtable both use strong consistency \n\n* * *\n\n### Linearizability\nLinearizability is a variant of strong consistency and is also known as atomic consistency\n\nThe following techniques can be used to implement linearizability:\n- single leader to handle both read and write operations\n- distributed consensus algorithm such as Paxos\n- distributed quorum\n\nThe advantages of linearizability are as follows:\n- makes a distributed system behave as if the system were non-distributed\n- simple for application to use\n\nThe disadvantages of linearizability are the following:\n- degraded performance\n- limited scalability\n- reduced availability","n":0.072}}},{"i":1318,"$":{"0":{"v":"Causal Consistency","n":0.707},"1":{"v":"\nCausal consistency is a consistency model whereby all participants of a system (say database [[replications|db.distributed.replication]]) observe causally-related operations in a common order. That is, they all agree on the order of the causally-related operations.\n- if 2 operations are causally unrelated (that is, one doesn't cause the other), then the participants don't care about preserving their order. It only makes an effort to agree on the related operations.\n  - if all participants *do* have to agree on every single operation, then that is known as *sequential consistency*.\n\nCausal means \"the possibility of being causally related\". That is, if Process A performs a write operation, followed by Process B performing a write operation, we say that A \"potentially causes\" or \"causally precedes\" B. This means every participant in the system must agree on that order.\n\nCausal consistency is a variant of eventual consistency and emerges as a middle ground between eventual consistency and strong consistency\n\nThe write operations that are causally unrelated or occur in parallel in real-time are known as concurrent events. The causal consistency pattern does not guarantee ordering for concurrent events\n\nThe opposite of causal is concurrent (or, causally independent).\n\nApache Cassandra provides lightweight transactions with causal consistency\n\n### Use-cases\nIn Reddit, replies to the same comment thread must be causally ordered. However, unrelated comment threads can be shown in any order. \n\nThe causal consistency pattern is also used in real-time chat services such as Slack.\n\nA comment thread on social media platforms such as Reddit. \n- Replies to the same comment thread on Reddit must be causally ordered. However, unrelated comment threads can be shown in any order","n":0.062}}},{"i":1319,"$":{"0":{"v":"Atomicity","n":1},"1":{"v":"\nAtomicity describes what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed\n- ex. a process crashes, a network connection is interrupted, a disk becomes full, or some integrity constraint is violated.\n- If the writes are grouped together into an atomic transaction, and the transaction cannot be completed (committed) due to a fault, then the transaction is aborted and the database must discard or undo any writes it has made so far in that transaction.\n\nThe ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity.\n- therefore we could think of this concept as *abortability*\n\nImagine a situation we want to update the schema of our database (eg introduce a new table or modify an existing one). If our system crashed in the middle of the update and our database didn't have Atomicity, then the result would be a mix between v1 and v2.\n\nIn Postgres, Atomicity is provided through write-ahead logs. Shadow Paging is another technique to provide atomicity.\n\n### Transaction\n- logically clumping together multiple database interactions (CRUD) as if they were one action\n\t- ex. in a balance sheet, we want to treat the asset change that corresponds to the liability+shareholer's equity change as atomic. Without transactions, if we were to make one change and for some reason the second change fell over, we would have an imbalanced balance sheet. At least with Transactions, we can be guaranteed that this would never occur\n- When an execution of transaction is interrupted, the transaction is not executed at all.\n\n### Write-Ahead Logging (WAL)\n- Before the results of a transaction are written to the database, the changes are first recorded in a log. This log is written to stable storage to ensure that the whole complete file is saved. Only once this log is known to be securely stored is the change made to the database. \n\t- ex. imagine the machine hosting the database lost power midway through performing some operation. When the machine boots back up, we can use WAL to determine what atomic changes were *supposed* to have been made, and compare that to what actually changed. With this information, we can determine whether or not we should rollback. This guarantees both atomicity and durability.\n- the log describes the data on a very low level: a WAL contains details of which bytes were changed in which disk blocks.\n","n":0.05}}},{"i":1320,"$":{"0":{"v":"Datadog","n":1},"1":{"v":"\n## Statsd\nStatsd is a protocol for capturing metrics. We communicate with the Statsd via a client library that exists in our application code. the daemon will then generate aggregate metrics and relay them to virtually any graphing or monitoring backend.\n\nThe purpopse is to instrument code.\n- this can be as simple as adding a [[decorator|general.patterns.structural.decorators]] to a function that we want to time, or it might be a one-liner within the function to track a value.\n\nThe Statsd receives these calls from client libraries ([[UDP|protocol.UDP]] traffic), aggregate it over some period of time, then \"flush\" it to our backend (e.g. Datadog)\n\n## Datadog Agent\nThe Datadog Agent is software that runs on your hosts. It collects events and metrics from hosts and sends them to Datadog\n\nThe Datadog Agents has embedded its own extended version of Statsd (DogStatsD)","n":0.087}}},{"i":1321,"$":{"0":{"v":"Application Performance Monitoring (APM)","n":0.5},"1":{"v":"\nAPM offers:\n- end to end application monitoring\n- end to end tracing (which tracks the entire life of a request)\n\n## Setting up Datadog APM\nWe need to carry out 2 steps:\n- Configuring the Datadog Agent for APM.\n- Adding the Datadog Tracing Library to your code.","n":0.152}}},{"i":1322,"$":{"0":{"v":"Data","n":1},"1":{"v":"\n# Push vs Pull data creation\n- Pull and Push are two different protocols that describe how a data Producer can communicate with a data Consumer.\n\n|      | Producer                              | Consumer                               |\n|------|---------------------------------------|----------------------------------------|\n| Pull | Passive: produces data when requested | Active: decides when data is requested |\n| Push | Active: produces data at its own pace | Passive: reacts to received data       |\n\nNote:\n- A regular function is a lazily evaluated computation that synchronously returns a single value on invocation.\n- A generator function is a lazily evaluated computation that synchronously returns zero to (potentially) infinite values on iteration.\n- A Promise is a computation that may (or may not) eventually return a single value.\n- An Observable is a lazily evaluated computation that can synchronously or asynchronously return zero to (potentially) infinite values from the time it's invoked onwards.\n\n**Pull**\n- Consumer determines when it receives data from the data Producer\n- The Producer itself is unaware of when the data will be delivered to the Consumer.\n- ex. every javascript function is a pull system\n\t- The function itself is the producer, and the calling code is the consumer. The reason the calling code is called the consumer is because the calling code \"pulls\" out a *single* return value\n- ex. React is a pull system. When React needs to re-render, it will call the render function of every affected component. This will return a new representation of the UI, which React can reconcile with the previous one. Any changes are then propagated to the DOM.\n\n**Push**\n- Producer determines when to send data to the consumer, and the consumer is unaware of when it will receive that data.\n\t- called push because now the producer of the state is responsible for handing the new value over to those that depend on it.\n\t- This has a positive effect: only those entities that depend on the value that has changed will update, and it can be done without having to make comparisons or detect changes.\n- ex. Promises and Observables are a push system, since the promise (a producer) delivers a resolved value to the registered callbacks (the consumers).\n\t- it is the Promise which is in charge of determining precisely when that value is \"pushed\" to the callbacks.\n\t- ex. An Observable is a Producer of multiple values, \"pushing\" them to Observers (Consumers).\n- ex. [[RxJS|rxjs]] uses a push-based approach, where you declaratively define streams and their relationships, and RxJS will propagate every change from one stream to the next one.\n","n":0.05}}},{"i":1323,"$":{"0":{"v":"Data Warehouse","n":0.707},"1":{"v":"\nA data warehouse is a database optimized to analyze relational data coming from transactional systems and line of business applications.\n\nData is transferred into the warehouse via an [[ETL pipeline|db.strategies.etl]] using either a periodic data dump (ie. a [[batch|general.patterns.batching]]) or a continuous stream of updates.\n\nThe data model of a data warehouse is most commonly relational, because SQL is generally a good fit for analytic queries.\n\nThe data structure, and schema are defined in advance to optimize for fast SQL queries, where the results are typically used for operational reporting and analysis.\n- Data is cleaned, enriched, and transformed so it can act as the “single source of truth” that users can trust.\n\nIt differs from a [[Lake|data.lake]] because it stores the data in a single format, whereas a data lake stores the data as-is (Postgres, Mongo etc.)\n\n### Star schema\nWhen it comes to data analytics, there is much less diversity of data models. Many data warehouses are used in a fairly formulaic style, known as a *star schema* (a.k.a *dimensional modeling*)\n\nUsually, facts are captured as individual events, because this allows maximum flexi‐ bility of analysis later.\n- as a result, fact tables are massive. A company like Walmart would have tens of petabytes of data in a fact table.\n\nOther tables that \"feed into\" the fact table are known as dimension tables.\n- If each row in the fact table represents an event, the dimensions represent the who, what, where, when, how, and why of the event.\n\n![](/assets/images/2022-03-08-20-48-41.png)","n":0.065}}},{"i":1324,"$":{"0":{"v":"Data Mesh","n":0.707},"1":{"v":"\n# Overview\n\nData mesh at core is founded in decentralization and distribution of responsibility to people who are closest to the data in order to support continuous change and scalability.\n\nWith a data mesh, the number of places and teams who provide data increases.\n\nData meshes invert the model of responsibility of past models ([[data lakes|data.lake]], [[data warehouses|data.warehouse]]), with the accountability of data quality shifting upstream as close to the source of the data as possible.\n\nData mesh lets you scale your data architecture in 2 ways:\n- horizontal scalability\n- across an organization as the business evolves and changes\n\nData mesh solves the problem of data pipelines, which often become brittle over time, due to things like having to rely on availability of services, incompatible changes to APIs etc.\n\nData mesh reduces the likelihood of there being inconsistencies between the same sets of data in different parts of the organization.\n\nData mesh can be thought of as a network to exchange data (between nodes) about the business throughout the organization.\n- the nodes are the *data products*\n\nData mesh takes us from the ad hoc way of building microservices (many apps, dbs and SaaS's that have various pipelines to each other), into something that is conceptually a little more centralized (with something like Kafka in the middle connecting everything)\n- the idea is to centralize an immutable stream of facts, but decentralize the freedom to act, adapt and change.\n\nWhile [[DDD|general.principles.DDD]] has been implemented widely to application code, it has not been applied to domain concepts in a data platform.\n- Data mesh takes plenty of concepts from DDD.\n- data meshes are to data and analytics what microservices are to application architecture.\n\nA technology like [[kafka]] is a good fit for Data Mesh\n\nData mesh aims to alleviate the issue of management and access to analytical data at scale.\n\n# Principles\nThe objective of a Data Mesh is to create a foundation for getting value from analytical data and historical facts at scale.\n- To achieve this, there are 4 underpinning principles that a data mesh implementation provides:\n    1. domain-oriented data ownership and architecture\n    2. data as a product\n    3. self-serve data infrastructure as a platform\n    4. [[federated|general.terms.federated]] computational governance\n\nThese 4 principles are all necessary.\n\n## Domain ownership\nA domain usually follows the same lines as the [[bounded context|general.principles.DDD.bounded-context]].\n- ex. If we had a music streaming application, the podcast domain would need to be in charge of both APIs to create a podcast episode, but also an analytical data endpoint for retrieving all podcast episode's data over the last 12 months.\n    - This implies that the architecture must remove any friction or coupling to let domains serve their analytical data and release the code that computes the data, independently of other domains. \n\nAs a result, there is no data team or data analytics team.\n\nNaturally, each domain can have dependencies to other domains' operational and analytical data endpoints.\n\n## Data as a product\nThe Data as a Product principle is designed to address the data quality and age-old data silos problem, which is that data is collected from all areas of the business, but is never used to its potential.\n- Analytical data provided by the domains must be treated as a product, and the consumers of that data should be treated as customers \n\nFor a domain data to be considered a product, a data mesh implementation should support discoverability, security, explorability, understandability and trustworthines\n\n*Data Product* is the architectural quantum (ie. an independently deployable component) of a data mesh.\n\nData product is the node on the mesh that encapsulates three structural components required for its function, providing access to domain's analytical data as a product:\n- Code for...\n    - consuming, transforming and serving upstream data \n    - APIs that provide access to data, semantic and syntax schema, observability metrics and other metadata\n    - enforcing traits such as access control policies, compliance, provenance, etc.\n- Data & Metadata. Data can be served as events, batch files, relational tables, graphs, etc.\n- Infrastructure, which enables building, deploying and running the data product's code, as well as storage and access to big data and metadata.\n\nIn a data mesh, pipelines (code) are managed as independent components from the data they produce\n- Data product is a composition of all components - code, data and infrastructure - at the granularity of a domain's bounded context.\n\n## Availability and self-serve nature of data\nData is available everywhere and it is self-serve from anywhere in the organization.\n\nIf you wanted to be able to source data from multiple domains to aggregate in some kind of reporting system, data mesh should allow you to do that very quickly.\n\nThis principle is necessary, due to the extra infrastructure that needs to be provisioned and run, as well as the skills needed to replicate it everwhere.\n- the only practical way that each team could realistically own their data products would be to have access to a high-level abstraction of infrastructure, so that they don't have to manually manage the lifecycle of data products.\n\n### Planes\nThe self-serve platform is composed of multiple planes, each serving a different profile of users.\n- A plane is representative of a level of existence - integrated yet separate.\n    - this is a similar concept to *control plane* in networking— which is the part of a network that controls how data packets are forwarded from one place to another.\n- a hierarchy of planes is desirable\n\n#### Example planes\n- *Data infrastructure provisioning plane*, which supports provisioning of the underlying infrastructure which is required to run the components of a data product and mesh of products. This would include the provisioning of distributed file storage, storage accounts, access control management system, the orchestration to run data products internal code, provisioning of a distributed query engine on a graph of data products, etc. This type of plane would likely only be used by higher-level planes, since it is such a low-level data infrastructure lifecycle management plane.\n- *Data product developer experience plane*, which is the interface that a data product developer is more likely to use. The purpose of the plane is to use simple declarative interfaces to manage the lifecycle of a data product.\n- *Data mesh supervision plane*, which operates at the mesh level, enabling it to work with a graph of connected data products. For instance, this would grant the ability to discover data products for a particular use case.\n\n## Data is governed where it is\nData is governed where it is created and stored.\n\nBecause the scope of data is always changing, this principle allows us to more quickly navigate data in the mesh and have confidence in that data.\n\nTo get value in forms of higher order datasets, insights, or machine intelligence, there is a need for these independent data products to interoperate. \n- We need to be able to correlate them, create unions, find intersections, or perform other graphs or set operations on them at scale.\n- To achieve this, a data mesh implementation requires a governance model that embraces decentralization and domain self-sovereignty, interoperability through global standardization, a dynamic topology and most importantly automated execution of decisions by the platform.\n\nThis raises an important point, which is \"what should be standardized globally across the organization, and what should be left up to each domain?\"\n- ex. in the case of an audio streaming app, the *podcast audienceship* data model should be left up to the Podcast domain team, whereas the decision of how to identify a *podcast listener* should be a global concern, since a podcast listener is a member of the more organization-wide concept of a *user*, which is an upstream [[bounded context|general.principles.DDD.bounded-context]], and thus present in other domains. Because we'd realistically want to correlate a single user across multiple domains (e.g. a user who is both a podcast listener and a music listener), this is a global concern.\n\n* * *\n\n## Data Lake/Warehouse vs Data Mesh\n| Pre-data mesh governance aspect (data lake/warehouse)               | Data mesh governance aspect                                                                                                           |\n|---------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n| Centralized team                                                    | [[general.terms.federated]] team                                                                                                                        |\n| Responsible for data quality                                        | Responsible for defining how to model what constitutes quality                                                                        |\n| Responsible for data security                                       | Responsible for defining aspects of data security i.e. data sensitivity levels for the platform to build in and monitor automatically |\n| Responsible for complying with regulation                           | Responsible for defining the regulation requirements for the platform to build in and monitor automatically                           |\n| Centralized custodianship of data                                   | Federated custodianship of data by domains                                                                                            |\n| Responsible for global canonical data modeling                      | Responsible for modeling polysemes - data elements that cross the boundaries of multiple domains                                      |\n| Team is independent from domains                                    | Team is made of domains representatives                                                                                               |\n| Aiming for a well defined static structure of data                  | Aiming for enabling effective mesh operation embracing a continuously changing and a dynamic topology of the mesh                     |\n| Centralized technology used by monolithic lake/warehouse            | Self-serve platform technologies used by each domain                                                                                  |\n| Measure success based on number or volume of governed data (tables) | Measure success based on the network effect - the connections representing the consumption of data on the mesh                        |\n| Manual process with human intervention                              | Automated processes implemented by the platform                                                                                       |\n| Prevent error                                                       | Detect error and recover through platform’s automated processing                                                                      |\n\n# Resources\n[Original article](https://martinfowler.com/articles/data-mesh-principles.html)","n":0.026}}},{"i":1325,"$":{"0":{"v":"Data Lake","n":0.707},"1":{"v":"\nA Data Lake is a single centralized repository that can accept all of our data in whatever format it comes in.\n\nAWS implementation would be [[S3|aws.svc.S3]]\n\nThe purpose is to have all of our data in a single place so that we can perform ultra-informed analytics on the data. the alternative would be that we pull data from each data layer, aggregating it and transforming it somehow into a single format so that we could feet it into an analytics engine.\n- spec: The purpose is not to have lightning-fast query speeds.\n\nWe can run different types of analytics on the data to guide better decision-making.\n- ex. dashboards and visualizations, big data processing, real-time analytics, machine learning\n\nit is often very technically involved to build and maintain a data lake.\n","n":0.089}}},{"i":1326,"$":{"0":{"v":"CSS (Cascading Style Sheets)","n":0.5},"1":{"v":"\n## Mobile-first\n`@media` queries should favor `min-width`\n- Max-width queries come into play when you want styles to be constrained below a certain viewport size. A combination of both min-width and max-width media queries will help to constrain the styles between two different viewport sizes.\n\n`min-width: 800px` => \"these styles will apply once you hit 800px\"\n`max-width: 1200px` => \"these styles no longer apply once you hit 1200px\"\n\n# E Resources\n[Writing mobile-first CSS: good examples](https://zellwk.com/blog/how-to-write-mobile-first-css/)\n","n":0.12}}},{"i":1327,"$":{"0":{"v":"Selectors","n":1}}},{"i":1328,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Select all except first and last children\n```css\n&:nth-child(n+2):nth-last-child(n+2) {\n    padding-left: 1em;\n}\n```\n","n":0.302}}},{"i":1329,"$":{"0":{"v":"CSS Rendering","n":0.707},"1":{"v":"\n## Block Formatting Context (BFC)\nA BFC is a part of a visual CSS rendering of a web page. It's the region in which the layout of block boxes occurs\n\nA BFC is basically a mini-layout within the main layout.\n- This means, for instance, that floated elements will be contained within (achieved with `overflow: auto` on container).\n\nBFCs prevent margin collapsing\n\nCreated by (non-exhaustive):\n- `<html>` tag\n- elements with `position` equal to `absolute`/`fixed`\n- elements with `display` equal to `inline-block`\n- Block elements where `overflow` has a value other than `visible` and `clip`.\n- Flex items, provided they are not flex/grid containers themselves\n- Grid items, provided they are not flex/grid containers themselves\n","n":0.098}}},{"i":1330,"$":{"0":{"v":"Prop","n":1}}},{"i":1331,"$":{"0":{"v":"Transform","n":1},"1":{"v":"\n`3dtransform()` will use GPU, `transform(x)` will use the CPU\n- `3dtransform` will be a smoother experience, and is therefore the best practice\n","n":0.218}}},{"i":1332,"$":{"0":{"v":"Position","n":1},"1":{"v":"\n### Static\nThe element goes with the normal document flow. `z-index` has no meaning here, and neither do `top`/`left`/`bottom`/`right`\nThis is default\n\n### Relative \nThe element goes with the normal document flow, and then is offset relative to itself (using `top`/`left`/`bottom`/`right`)\nThe element does not impact any other elements in the document\nThe space given to the component in the page layout is the same as if position were `static`\n\n### Absolute\nThe element is removed from the normal document flow, and no space is created for the element in the page layout.\nInstead, the element is positioned relative to it's closest positioned ancestor (ie. an ancestor with a position value that is not `static`)\n- If there are no positioned ancestors, then the element is positioned relative to the initial containing block (ie. the parent of the `<html>` tag)\n\nCreates a new stacking context as long as `z-index` is not set to `auto`\n\n### Fixed\nThe element is removed from the normal document flow, and no space is created for the element in the page layout.\nThe element is positioned relative to the initial containing block (ie. the parent of the `<html>` tag)\nIf one of the element's ancestors has any of: `transform`, `perspective`, or `filter`, then that ancestor is used as the containing block.\nCreates a new stacking context\n\n### Sticky\nThe element goes with the normal document flow, and then is offset relative to 2 containing blocks: the nearest scrolling ancestor, and the nearest block-level ancestor.\n","n":0.066}}},{"i":1333,"$":{"0":{"v":"Layout","n":1},"1":{"v":"\n### Layout-isolated component\nA layout-isolated component is a component that is unaffected by the parent it is placed within, and does not itself affect the size and position of its siblings.\n    - this only applies to the root element of a reusable component\n![](/assets/images/2021-03-28-19-49-24.png)\n- avoid any properties on the root element of a component that affect, or are affected by elements outside of the bounds of that component.\n    - ex. margin, because it acts on elements outside of the component's scope\n    - ex. align-self, as it will stretch the width or height of the component depending on the flex-direction of its parent\n    - by contrast, padding is fine, as it is confined to the scope of the component\n    - Basically, if a property depends on, or impacts other components outside of its scope, discourage its use.\n```js\n// Does NOT conform to layout isolation principals\nfunction MyComponent() {\n  return (\n    <div style={{alignSelf: 'center'}}>\n      <div />\n    </div>\n  \n}\n\n// This component is layout isolated, since the potentially dangerous property is not on the root element\nfunction MyComponent() {\n  return (\n    <div>\n      <div style={{alignSelf: 'center'}}/>\n    </div>\n  )\n}\n```\n","n":0.075}}},{"i":1334,"$":{"0":{"v":"Flexbox","n":1},"1":{"v":"\nThe 2 main components of a flexbox system are the `flex container` and the `flex items`\n- therefore, some of the CSS properties are to be set on the container, while others are to be set on the items\n\nWhen aligning things with flexbox, we must consider the **main axis** and the **cross axis**.\n- when using `flex-direction: row`, the main axis is horizontal, while the cross axis is vertical. This is swapped when we have `flex-direction: column`.\n\n![](/assets/images/2023-08-28-10-19-25.png)\n\n- **main axis** – The main axis of a flex container is the primary axis along which flex items are laid out.\n- **main-start | main-end** – The flex items are placed within the container starting from main-start and going to main-end.\n- **main size** – A flex item's width or height, whichever is in the main dimension, is the item’s main size. The flex item’s main size property is either `width` (for `fd: row`) or `height` (for `fd: column`).\n- **cross axis** – The axis perpendicular to the main axis is called the cross axis.\n- **cross-start | cross-end** – Flex lines are filled with items and placed into the container starting on the cross-start side of the flex container and going toward the cross-end side.\n- **cross size** – The width or height of a flex item, whichever is in the cross dimension is the item's cross size. The cross size property is either `height` (for `fd: row`) or `width` (for `fd: column`).\n\nnote: all examples below assume `flex-direction: row`. If `column`, then substitute width for height.\n\n## Flex Container Properties\nUsing explicit margins breaks flexbox's ability to position things nicely, since it doesn't play nicely with the math that it's relying on\n\nWhen doing flexbox row, don't use margin left/right, and when doing column, don't use margin top bottom, because it interferes with the math that flex is doing\n- try and use justify-content: space-between instead, and set flex: 0 49% for example, which will almost serve as our margin\n\n### `gap`\n`gap` should be used when determining spacing between flex items, since it only affects spacing between the items themselves. Therefore, `gap` will not add spacing to the side of a flex item that has no other item on that side.\n- could be thought of as a minimum gutter, as if the gutter is bigger somehow (because of something like justify-content: space-between;) then the gap will only take effect if that space would end up smaller.\n\n## Flex Item Properties\n\n### `flex-grow`\n- when there is available space after all boxes have taken up their room, `flex-grow` will allow the item to grow proportionally to the other item's `flex-grow` property.\n    - therefore, all items having `flex-grow: 1` is the same as all items having `flex-grow: 100`. Think of it as a proportional growth rate. \n        - ex.an item with `flex-grow: 3` will grow 3 times faster than an item with `flex-grow: 1` \n- a value of `0` means the item won't be resized during the size calculation to\n    accommodate the flex container's full main axis size\n- Imagine we use different flex-basis for each flex item. Even if we have `flex-grow: 1` on each item, they will still grow at a different rate, because of the basis at which they started.\n\n#### UE Resources\n[flex grow article](https://css-tricks.com/flex-grow-is-weird/)\n\n### `flex-shrink`\n- The rate at which a flex item will shrink when the page is resized to a size smaller than the flex-containers most \"comfortable\" size.\n\n### `flex-basis`\n`flex-basis` defines the default size of a flex item before any available space is distributed among the flex items (which is determined according to flex-shrink and flex-grow). \n- the value can either be a length (e.g. `20%`, `10px`) or a keyword (`auto` or `content`)\n- `flex-basis` will override any `width` property, unless left as `auto`\n- `flex-basis` still obeys `min/max-width` settings\n- In a sense, it is similar to `min-width` (although `flex-shrink` determines how the flex item will behave when the size of the whole flexbox shrinks below initial size)\n    - If we were to use `min-width` and shrink the browser size, the browser would make us scroll horizontally to see all of the content.\n\n#### Keywords\n- `auto` (default): \"look at my `width` or `height` property. If I have none, use the content of the item to determine size\"\n- `content`: \"size it based on the item's content\"\n    - may have some browser [support issues](https://caniuse.com/?search=flex-basis%3A%20content)\n\n* * *\n\n## Flex Shorthand\n- by default, it is `flex: 0 1 auto`\n- `flex: 1 1 0` - ==flex-grow== ==flex-shrink== ==flex-basis==\n","n":0.037}}},{"i":1335,"$":{"0":{"v":"Elements","n":1},"1":{"v":"\n### The problem with building isolated components using CSS+HTML\nA fundamental problem of building isolated components with HTML+CSS is that the CSS `display` property sets both an element's inner layout (how it lays out its children) and its outer layout (how it is laid inside it parent).\n- ex. you have to make an element `display: inline-flex` to make it an inline flexbox. You can't just make it `flex` and have the parent decide whether it is inline, a block or whatever.\n\nThe consequence of this is that a parent component cannot properly layout a child component without knowing its internal layout structure. And if it doesn't don't know the child's layout (if it's dynamically supplied for example) then it can't safely do anything with it. So long as this limitation exists, there will never be true layout encapsulation on the web platform.\n\n* * *\n\n## Element Types: the `display` property\n### Block-level elements\nOccupies the entire horizontal space of its parent element, and vertical space equal to the height of its contents, thereby creating a \"block\".\nBrowsers display the block-level element with a newline both before and after the element. \n- You can visualize them as a stack of boxes.\n\nThe block-level element\n\n#### Examples\n`<div>`, `<p>`, `<h1>`, `<ul>`\n\n### Inline elements\nOnly occupy the space bounded by the tags defining the element.\nDoes not start on a new line, and only takes up the width it needs.\nDoes not break the flow of the content.\nYou can't put block elements inside inline elements.\nThe block-direction margin is ignored entirely\nThe padding on inline elements doesn’t affect the height of the line of text\n- ex. `padding-top` on an `<a>` tag won't cause the height of the line of text to change\n\nAll inline elements (not just text) can be laid out by a parent with the `text-align` property\n- also with `vertical-align`, which would be the inverse property, manipulating upon the vertical axis.\n\n#### Inline-block elements\nAble to have width/height values set.\nWhile `transform` property can't be used on inline elements, it can be used on inline-block.\n\nBe careful with inline-block elements. The components you build with them may look fine, but the quirks may cause them not to place nicely with their parents:\n![](/assets/images/2021-11-02-10-53-16.png) \n\n#### Inline-flex\nVirtually the same as `display: flex`, except the flex container itself is inline, instead of block.\n","n":0.052}}},{"i":1336,"$":{"0":{"v":"Pseudo","n":1},"1":{"v":"\n`::after`, `::before`\n\ndefine an element in html using `content`, which cant be anything from an entity, to text to whatever, and then we can style it up.\n","n":0.196}}},{"i":1337,"$":{"0":{"v":"CSS Best Practices","n":0.577},"1":{"v":"\n## General Readability Principles\n### Line-height\npick a minimum line-height: 1.4 for body copy. Headings should be tighter.\n\n### Line-width\nKeep it to 60-80 characters wide\n\n## Size\n- use `rem` for `font-size`\n- use `px` for `border-width`\n- use `em` for everything else\n\nDifferent font sizes on a webpage should be related by a common ratio ([source](https://www.type-scale.com))\n- ex. if we choose that ratio to be 1.25, then our ems will be: 0.64, 0.8, 1, 1.25, 1.563, etc.\n\nWe can set the base-size font on a container using `rem`, and then we set size of the children content using `em`. This allows the children using `em` to leverage the `font-size` set on the parent (the `rem` value). Now, when we adjust the `rem` value, all the `em` values depending on it will adjust as well, while keeping their proportions.\n![](/assets/images/2021-11-02-16-11-32.png)\n- This method works well for images as well\n\nThis method can be synergized with:\n- `@media` queries\n- `vw` unit with `font-size`. This will cause the font-size to grow/shrink smoothly with no abrupt breakpoints. We need to do some math to define min size (0.5em):\n    - `font-size: calc(0.5em + 1vw)`\n\n* * *\n\n### Magic numbers\nWhen using magic numbers, add a comment to explain how it was derived:\n```css\n.gallery__item {\n    @media (min-width: 800px) and (max-width: 1200px) {\n        width: 21.25%; /* (100% - 15%) / 4 */\n        &:nth-child (4n) {\n            margin-right: 0;\n        }\n    }\n}\n```\n","n":0.068}}},{"i":1338,"$":{"0":{"v":"Crypt","n":1}}},{"i":1339,"$":{"0":{"v":"Public Key Cryptography","n":0.577},"1":{"v":"\nA public key certificate is a file whose purpose is to verify the validity of a public key.\n- a certificate *contains* a public key.\n\nThink about the public key as a lock-only mechanism, so you need to know the private key to unwrap, unlock or decode data again.\n\nThe certificate contains information about:\n- the public key\n- the identity of the public key's owner (called the *subject*)\n- the digital signature of an authority that has verified the certificate's contents (the *issuer*)\n\nTypically, a certificate is itself signed by a certificate authority (CA) using CA's private key. This verifies the authenticity of the certificate.\n\nWhen the issuer can confirm that the digial signature is valid (ie. when the issuer *signs* the message), it will have effectively authenticated the other party (the subject). That is, it is highly confident that the other party is who they say they are.\n- also, the issuer knows that the message was never altered in transit (*integrity*).\n\nspec: the certificate is used with the public key to sign the message. Therefore, the issuer can be assured that the message that was sent to them originates from the correct identity.\n\n### Types of certificate\n- [[https.TLS.certificates]]\n- Email certificate\n- Self-signed certificate - a certificate with a `Subject` that matches its issuer, and a signature that can be verified by its own public key.\n\t- a self-signed certificate is mostly useless, but a the digital certificate chain of trust must start with a self-signed certificate (ie. a *root certificate*). A [[certificate authority|https.TLS.certificates.certificate-authority]] self-signs a root certificate to be able to sign other certificates.\n- Intermediate certificate - has a similar purpose to the root certificate; its only use is to sign other certificate, but the intermediate certificate is not self-signed; it must be signed by either a root certificate or another intermediate certificate.\n- Code signing certificate - sign binaries so the identity of the developer is verified as the one who created the binary.\n\n### Public Key Infrastructure (PKI)\nThe PKI is the underlying system needed to create, manage, distribute, use, store and revoke digital certificates and manage public-key encryption.\n- at its core, it is a set of roles, policies, hardware, software and procedures that enable us to create/manage/distribute the keys.\n\nThe PKI performs trust services, meaning it serves to trust the actions/outputs of entities, whether they are computers or people.\n- the objective of a trust service is 1+ of the following:\n\t- Confidentiality - Assurance that no entity can maliciously or unwittingly view a payload in clear text\n\t- Integrity - Assurance that it would be obvious if an entity tampered with transmitted data\n\t- Authenticity - Assurance that you have certainty of what you are connecting to (server-side authentication; typically used when authenticating to a web server using a password), or evidencing your legitimacy when connecting to a protected service (client-side authentication; sometimes used when authenticating using a smart card, ie. hosting a digital certificate and private key).\n\n### Difference between Symmetric encryption and Asymmetric encryption\nIn a simple cryptographic system, I have some message that I can encrypt using a key. I can then give you that key along with the information, and you can decrypt it and read the message.\n- This is known as Symmetric Encryption\n- The problem with this, is that I need to send you the key. Since we have not yet established a secure connection, we cannot be sure that no one is listening in.\n\nIn Public Key Cryptography, we generate a key pair: a private key and public key.\n- This is known as Asymmetric Encryption\n- It is Asymmetric because one key is used to encrypt the message, while the other is used to decrypt it.\n\t- ie. Anything we encrypt with key A can only be decrypted with key B, and anything encrypted with key B can only be decrypted with key A\n- Now we can have a situation where both you and I have our own key-pairs, with each person's public key being widely distributable. \n\t- Now if I want to send something to you, I just need your public key. I can encrypt the message like this, and you are the only person that can decrypt it, since you have the private key. \n- This system also allows the receiver of a message know exactly who sent it. Consider that if I use my private key to encrypt a message and send it out, anyone with my public key can decrypt it. \n\t- This has an important implication, which is that if you were able to successfully decrypt my message, then you know for a fact that I was the one who encrypted it. \n- See: Diffie-Helman\n\n### Mail Poker Example\n- *note:* is this an actual analogy of public key, or something else?\n- [source](https://www.youtube.com/watch?v=mthPiiCS24A&list=PLt5AfwLFPxWLXe-ZqZyu0kSsaWd4FjXbj)\n- Imagine we wanted to play a game over poker over the mail. Of course, each player needs to be assured that the same rules apply as if the game was in person (neither side knows the other's cards, each side only takes 5 cards, etc). \n- Player 1 puts each card in an envelope, and attaches a lock. This lock is only openable by player 1, but only one key is needed to open all the locks.\n- Player 1 mails all 52 cards to Player 2. Player 2 then puts his own lock on all envelopes. Again, his key will open all envelopes. Then Player 2 will send the double-locked envelopes back to Player 1. (the randomized nature of the mail process acts as a secure and trusted method of shuffling) \n\t- note: we have created a situation where neither player can reveal any card on their own. \n- Player 1 now has 52 double-locked envelopes, so now must choose 5 cards for Player 2. Once they are chosen, Player 1 takes his lock off those cards and mails just those 5 cards. Once Player 2 receives those cards, he can unlock them and now knows his hand. \n- Player 1 now has 47 double-locked envelopes, so now he must choose 5 for himself. Once he chooses, he sends them to Player 2. Player 2 removes his lock from those 5 cards, and sends them back to Player 1. Player 1 removes his lock, and now knows his own hand.\n\n- This system of double-locking creates what is called *commutativity*. A commutative crypographic algorithm is one that is order-independent (ie. it doesn't matter which order the locks are taken off)  \n\n#### Bridging the Analogy Gap\n- If this game of poker were to take place over the internet, each of the 52 cards would have a numeric value (1-52). We need a way to \"lock\" each card. Player 1 creates his key (`k`) by choosing a random number. He then takes that number and raises each card to that value.\n\t- ex. if `k = 3`, then each card value is raised to the 3rd power. This is equivalent of locking the envelopes, as long as Player 2 doesn't know what `k` is.\n- Player 2 then can create his own key (`j`), and raise each (already encrypted) card value to that value. Now we have double-locked cards.\n\t- It's important to note that `(2^5)^3` is mathematiacally equivalent to `(2^3)^5`. In other words, the locks can be applied or removed in any order. \n- Now, to remove the locks, each player simply needs to multiple the exponent by the inverse of their key (if Player 1 wants to remove the locks of 5 cards, then he just needs to multiply the exponent by `-3`)\n\n* * *\n\nGenerating the keys\n```sh\n# generate RSA private key\n$ openssl genrsa > private.pem\n\n# derive public key\n$ openssl rsa -in private.pem -pubout -out public.pem\n```\n\n* * *\n\n*Sharing identical keys works fine among 2 people. What if Alice want to exchange stuff with another guy named Carl, and Alice doesn’t want anybody to see their stuff too? Alice can’t use the same lock and key that she shared with Bob, else Bob can unlock the box easily!  Of course Alice can share a completely new and different lock and key with Carl, but what if Alice wants to exchange stuff with 10 different people? She will need to keep and manage 10 different keys!  So Alice come out with a brilliant solution. Now, she only maintains one key (private key). She distribute the same padlocks (public key) to her friends. Anyone can close the padlocks (encrypt), but only she has the key to open (decrypt) them. Now, anyone can send stuff to Alice using the padlock she distributed, and Alice no longer have to manage different keys for different people.  If Alice wants to send something to Carl, she will ask for Carl’s padlock (public key) so that she can use it to lock (encrypt) her stuff and send it to Carl.  The basic principle is: everyone has their own private key to decrypt message, and they will provide senders their own public key for message encryption.*\n\n## UE Resources\n- [Keychain Access in Macos](https://www.youtube.com/watch?v=fdJKk89hTkw)\n- [stackoverflow](https://superuser.com/questions/620121/what-is-the-difference-between-a-certificate-and-a-key-with-respect-to-ssl)","n":0.026}}},{"i":1340,"$":{"0":{"v":"Hashing Function","n":0.707},"1":{"v":"\nLet's say we have a function that takes a number from zero through nine, adds three and, if the result is greater than ten, subtracts ten. So f(2) = 5, f(8) = 1, etc. Now, we can make another function, call it f', that goes backwards, by adding seven instead of three. f'(5) = 2, f'(1) = 8, etc.\n- Theoretically, any mathematical functions that maps one thing to another can be reversed. In practice, though, you can make a function that scrambles its input so well that it's incredibly difficult to reverse.\n- taking an input and applying a one-way function is called \"hashing\" the input\n- SHA1 is an example of this kind of \"one-way\" function\n\n### Properties of a hashing function\n- Irreversible - like a meat grinder, we can't turn mince meat back into steaks\n- Reproducible - with the same input, we will always get the same output\n- No collisions - no 2 different inputs will ever produce the same output\n- Unpredictability - given a known output, we can never guess what the input was","n":0.076}}},{"i":1341,"$":{"0":{"v":"K Anon","n":0.707},"1":{"v":"\nWhen entering a password, the client hashes it into a 130 bit hash, then sends only the first several characters to the server. The server checks its database of passwords hashes, and returns all of the passwords that begin in the same way. Once the client receives this, it selects the hash that matches with its password\n- anal: this is similar to when you are authenticating in a government website, and it asks you questions like: \"which of the following 4 addresses did you live at in 2016?\"\n\nThe benefit of this is that a password hash is never actually sent from the client to the server. MITM attacks are less threatening. The server also doesn't gain any valuable information, since it won't know if any of the hashes it is sending actually matches the one owned by the client.\n- ex. run `curl https://api.pwnedpasswords.com/range/f42b7e\n`. We will get in return a list of hashes for passwords that have been cracked (ie. leaked in plain white text)\n","n":0.078}}},{"i":1342,"$":{"0":{"v":"Hashing Algorithm","n":0.707},"1":{"v":"\n### RSA\nRSA security is based on 2 matching keys. There is a public key for each user, and everybody can (should) know it. There is also a private key that only the user should know. A message encrypted by the public key can only be decrypted by the private key, and visa versa.\n- Thus, if I want to send you a message that only you can read, I get (from the network) your public key, encrypt the message with that key and you are the only person who can decrypt it.\n- Or, if I want to prove to you that I sent a message, I can encrypt the message with my private key, tell you (in open text or in another message) how it was encrypted. Then you could decrypt the message with my public key, and if it becomes readable, you know it came from me.\n- RSA-129 is a publicly available 129 digit number that is derived from multiplying 2 very large prime numbers (those numbers are unknowable)\n","n":0.077}}},{"i":1343,"$":{"0":{"v":"Cracking","n":1},"1":{"v":"\nWith `hashcat`, dedicated servers for cracking passwords can hash a word and check for the presence of the hash in a big list of hashes 40 billion times per second (if the hashes were hased with md5 algo). If a more secure algo is used like SHA512 or Bcrypt, it would be in the thousands instead. \n- In a brute force attack, a 7 letter word all lower-case has 26^7 possible combinations, which is just over 8 billion combinations. This makes an md5 stored 7 letter word breakable in less than a second.\n","n":0.104}}},{"i":1344,"$":{"0":{"v":"CouchDB","n":1},"1":{"v":"\n## What is it?\nCouchDB is a [[document database|db.type.document]], with each document represented as JSON. \n\nThe documents are organised via [[views|couchdb.design-doc.view]]. \n- Views are defined with aggregate functions and filters are computed in parallel, much like MapReduce.\n\nEach database is a collection of independent documents. Each document maintains its own data and self-contained schema. An application may access multiple databases, such as one stored on a user's mobile phone and another on a server. Document metadata contains revision information, making it possible to merge any differences that may have occurred while the databases were disconnected.\n\nUnlike other solutions, `databases` in CouchDB are cheap to make. They are more comparable to namespaces in other database systems.\n\nCouchDB was designed with sync in mind, and this is exactly what it excels at. Many of the rough edges of the API serve this larger purpose\n- unlike other common [[replication|db.distributed.replication]] systems, CouchDB doesn't use leader-follower replication— it uses multi-master, meaning any node can be read from and written to.\n- By default, CouchDB (and [[PouchDB|couchdb.pouch]]) are designed to store all document revisions forever, just like [[git]]\n\n### From SQL to CouchDB\n| SQL                    | CouchDB                      |\n|------------------------|------------------------------|\n| Queries                | HTTP requests with JSON      |\n| Each table has rows    | Each database has documents  |\n| Each table has columns | Each document has fields     |\n\nCouchDB config files are found in `ini`, and there are multiple [options](https://docs.couchdb.org/en/3.2.0/config/intro.html#configuration-files) for location.\n\n## Why use it?\nCouchDB is well suited for applications with accumulating, occasionally changing data, on which pre-defined queries are to be run and where versioning is important (e.g. CRMs, [[CMSs|CMS]]). \n- Master-master replication is an especially interesting feature, allowing easy multi-site deployments.\n\nCouchDB is really good at [[replication|couchdb.replication]] and [[scalability|deploy.scaling]]\n- Couchdb implements [[multi-master replication|db.distributed.replication.strategies#multi-leader-based-replication,1:#*]]\n- Couchdb is designed with bi-directional replication (or synchronization) and off-line operation in mind. \n    - That means multiple replicas can have their own copies of the same data, modify it, and then sync those changes at a later time.\n\nCouchdb offers document-level ACID semantics by implementing a form of Multi-Version Concurrency Control, meaning that CouchDB can handle a high volume of concurrent readers and writers without conflict.\n\nCouchDB guarantees eventual consistency to be able to provide both availability and partition tolerance.\n\n### Conflict resolution\ninstead of locks, Couchdb uses [[MVCC|db.strategies.concurrency-control#multiversion-concurrency-control-mvcc,1]]\n\nConflict resolutions are automatic and deterministic (since the contents of the document are hashed with MD5 and sorted alphanumerically)\n- anal: this is like a CPU-cheap version of blockchain, but instead of solving some complex mathematical formula to prove your conflict is the correct one, a simple MD5 formula is used to make that determination.\n- Custom (and even manual) resolution can be built into the app if we don't want to do this by sorting alphanumeric MD5 sums.\n- In either case, both revisions are preserved.\n    - actually \"deleting\" is only soft deleting in CouchDB, with a field of `_deleted`.\n\nEach document in CouchDB is versioned (via its `_rev` field)\n- the `_rev` is prefixed with an integer that increments with each new revision.\n- if 2 people update the same document at the same time, then we will end up with two revisions prefixed with the next incremented number:\n    - `2-1583746` and `2-2583368`. In this case, the first would win, because of its alphanumerical position.\n\nResolving a conflict generally involves first merging data into one of the documents, then deleting the stale one.\n\nCouch uses MVVC instead of locks.\n\n### Resources\n`http://<host>:<port>/<database>/<document>/<attachments>`\n\nSystem endpoints/keys are prefixed with `_`\n\nFauxton dashboard: `http://localhost:5984/_utils`\n\n### Querying data\nThere are 3 ways to query data: MapReduce, Mango queries, and Full-text search (with Lucene).\n\nMango queries are meant to resemble the MongoDB query language.\n- allows us to do arbitrary searches with `=`, `>`, [[regex]] etc.\n- if we use Mango queries, the view is created for us automatically.\n\n### Security\nCouchDB's level of security is not as sophistocated as other database solutions.\n- security is per database (via the `_security` endpoint), and we can specify read-only access, read/write access, or admin access.\n    - admins can update indexes, views etc.\n\nIf finer grained control is needed, we can write custom `update` functions to implement our own business logic for determining when data should be accessible or not.\n\n* * *\n\nCouchDB and PouchDB do not support transactions. A document is the smallest unit of operations.\n\n## Hosting\n- A2 Hosting\n- Cloudant (IBM managed CouchDB)","n":0.038}}},{"i":1345,"$":{"0":{"v":"Replication","n":1},"1":{"v":"\nReplication in CouchDB works by comparing the source and the destination database to determine which documents differ between the source and the destination database. \n- it follows the [Changes Feeds](https://docs.couchdb.org/en/3.2.0/api/database/changes.html#changes) on the source and compares the documents to the destination. Once it reaches the end of the feed, the replication process has finished.\n- we can set a replication to be `continuous`, meaning it will wait for new changes to appear until the task is canceled.\n- replications can also be paused with checkpoints, and continued. This is helpful is case of a fault.\n\nChanges are then sent in batches to the destination, where they are compared and inform CouchDB if there are conflicts.\n\nIf you know Git, then you know how Couch replication works. Replicating is very similar to pushing or pulling with distributed source managers like Git.\n- Like Git, Couch documents have a linked-list revision history. If a revision exists in the other's history, then it can be fast-forwarded.\n- In Git, if 2 documents have a common ancestor, it's called a fork; in Couch, it's a conflict.\n    - therefore neither Git or CouchDB have linear lists.\n    - conflicts in CouchDB do not correspond to conflicts in Git\n        - a conflict in CouchDB is a divergent revision history \n- When a child has multiple parents, Git allows us to merge. CouchDB can be configured to do this via the application code.\n    - In Git, this is like copying and pasting the changes from branch A into branch B, then committing to branch B and deleting branch A. \n\nWhen a document is updated, it is given an incremented `_rev` value\n- when an item is deleted (in CouchDB, soft deleted), its `_rev` value will be updated. Since it is considered as a new revision, when a document is deleted on the source, that deletion will be replicated to the target.\n\nReplication tasks can be initiated by either the sending node or the receiving node.\n- when the sending node initiates the replication task, it is called *push replication*\n- when the receiving node initiates the replication task, it is called *pull replication*\n- Replication tasks only transfer changes in one direction.\n    - the easiest way to perform a full sync is to do a \"push\" followed by a \"pull\" (or vice versa).\n\nThere are three options for controlling which documents are replicated, and which are skipped:\n1. Defining documents as being local.\n2. Using [Selector Objects](https://docs.couchdb.org/en/3.2.0/replication/replicator.html#selectorobj).\n  - allows us to apply business logic through query expressions to determine whether or not a document should be included in the replication. The selector specifies fields in the document, and provides an expression to evaluate with the field content or other data. If the expression resolves to true, the document is replicated.\n  - selector objects exist on a replication document.\n  - note: selector objects are more performant than filter functions and should be preferred where possible (ie. when filtering on document attributes only).\n3. Using [Filter Functions](https://docs.couchdb.org/en/3.2.0/ddocs/ddocs.html#filterfun).\n  - filter functions exist on a design document. They are special functions that are called whenever there is an entry in the Changes Feed that matches some criteria.\n    - in principle they are like [[lambdas|aws.svc.lambda]] being triggered by some event.\n\nReplications are created by either Writing documents to the `_replicator` database (preferred) or POSTing to the `_replicate` HTTP endpoint (legacy) \n\nThe first database of CouchDB used *transient replication*. Transient means that there are no documents backing up the replication— after a restart of the CouchDB server, the replication will disappear.\n- this still exists for backward compatability purposes\n- nowadays, *persistent replication* is used.\n  - Persistent means the `_replicator` database is used, which keeps documents containing your replication parameters.\n\n### Replication database\nThe `_replicator` database is like any other database in CouchDB, except for one feature: documents added to it will trigger replications\n- there is a default `_replicator` database, but we can create our own.\n\nthrough the `_replicator` database, we can control the persistent replication\n- each document describes one replication process\n\nExample document of `_replicator` database:\n```json\n{\n    \"_id\": \"rep_from_A\",\n    \"source\":  \"http://aserver.com:5984/foo\",\n    \"target\": {\n        \"url\": \"http://localhost:5984/foo_a\",\n        \"auth\": {\n            \"basic\": {\n                \"username\": \"user\",\n                \"password\": \"pass\"\n            }\n        }\n    },\n    \"continuous\":  true\n}\n```\n\n[config options](https://docs.couchdb.org/en/3.2.0/json-structure.html#replication-settings)\n\n#### Replication scheduler\nThe scheduler manages the replication jobs as soon as they are created.\n\nThe scheduler is the replication component which periodically stops some jobs and starts others\n\n### Replication Lifecycle (state machine)\nReplication jobs during their life-cycle pass through various states. This is a diagram of all the states and transitions between them:\n![](/assets/images/2022-05-11-22-13-28.png)\n\n* * *\n\nCouchDB replication does not have its own protocol. A replicator simply connects to two DBs as a client, then reads from one and writes to the other. Push replication is reading the local data and updating the remote DB; pull replication is vice versa.\n\n- Fun fact 1: The replicator is actually an independent Erlang application, in its own process. It connects to both couches, then reads records from one and writes them to the other.\n- Fun fact 2: CouchDB has no way of knowing who is a normal client and who is a replicator (let alone whether the replication is push or pull). It all looks like client connections. Some of them read records. Some of them write records.\n\nA record's revision ID is the checksum of its own data. Subsequent revision IDs are checksums of: the new data, plus the revision ID of the previous.\n\n* * *\n\n### Conflicts\nWhen putting a new version of a document, the source must provide the previous `_rev` value that it has. If the target does not have the same `_rev` value, then it will throw a 409 Conflict error.\n\nWhen the update conflict occurs, the client who errored out must fetch again, and then either\n1. apply the same changes as were applied to the earlier revision, and submit a new PUT\n2. redisplay the document so the user has to edit it again\n3. just overwrite it with the document being saved before (which is not advisable, as user1’s changes will be silently lost)","n":0.032}}},{"i":1346,"$":{"0":{"v":"PouchDB","n":1},"1":{"v":"\nPouchDB is a Javascript implementation of CouchDB which is API compatible with it. \n- So you can use [[CouchDB|couchdb]] on the server side and Pouch in the application itself and once the application comes online you can sync both.\n- the replication algorithm in PouchDB is identical to the one in CouchDB.\n\nSince Couchbase and Cloudant (both descendents of Couchdb) share the same sync protocol, they can be used with PouchDB.\n\n### The nature of PouchDB\nPouchDB itself is not a database\n\nPouchDB databases can be either remote or local\n- When creating a local database, whatever datastore that is available is used:\n  - IndexedDB for most browsers\n  - LevelDB for Node.js\n- When creating a remote PouchDB database, it communicates directly with the remote database (CouchDB, Couchbase, Cloudant)\n\nThe goal of PouchDB is that we can communicate with either the local or remote database and we shouldn't notice a difference in how we interact with each one.\n\nPouchDB has two types of data: documents and attachments.\n- attachments are the most efficient way to story binary data (e.g. mp3, jpg)\n\n### Plugins\nPouchDB is built in a modular way. We can pick and choose which modules we need from PouchDB, and in turn reduce the bundle size.\n- PouchDB packages come in three flavors: presets, plugins, and utilities.\n  - Presets are a collection of plugins, which expose a PouchDB object that is ready to be used.\n    - ex. pouchdb-browser exists to provide a certain combination of modules that can be expected for use of PouchDB in the browser. Specifically, this means it ships with the IndexedDB adapter as its default adapter, and also contains the replication, HTTP, and map/reduce plugins.\n  - Plugins are features that can be added to a PouchDB instance using the PouchDB.plugin() API.\n  - Utilities are grab-bags of helper functions, and are only recommended for advanced use cases.\n\n\n### Adapters\nSince PouchDB is just an abstraction layer that implements the same interface as CouchDB, we need an underlying \"sourcing layer\" (e.g. database, REST resource via HTTP) to handle retrieval for us. To connect the PouchDB interface to the underlying data layer of our choice, we must use an [[adapter|general.patterns.structural.adapter]]\n- by default, PouchDB ships with the IndexedDB adapter for the browser and LevelDB for Node.js, but we can use different underlying data layers such as SQLite\n- we can even use an HTTP adapter if we want to use a remote CouchDB instance as our data layer. This would effectively be like saying \"I want to just use the PouchDB abstraction layer instead of me having to interact with my CouchDB instance directly\".\n\n* * *\n\n`seq` can be thought of as a version number for the entire database. Basically it answers the question of \"How many total changes have been made to all documents in this database?\" This sets it apart from the revision hash `_rev`, which marks the changes made to a single document.\n- the `seq` between two databases is not guaranteed to be kept in sync. CouchDB and PouchDB have slightly different ways of increasing their `seq` values, so really `seq` is only meaningful within a single database.\n\n\n## Plugins \n- [Relational Pouch](https://github.com/pouchdb-community/relational-pouch)\n    - allows you to interact with PouchDB/CouchDB like a relational data store, with types and relations.\n    - includes an API for many-to-many relationships\n\n## E Resources\n- [Roving engineer problem](https://medium.com/@glynn_bird/replicating-from-a-query-with-couchdb-dd3ffc5c4b31#:~:text=The%20one%20database%20per%20user,mobile%20clients%20replicate%20to%2Ffrom.)","n":0.043}}},{"i":1347,"$":{"0":{"v":"Replication","n":1},"1":{"v":"\n[[see also CouchDB Replication|couchdb.replication]]\n\n### Unidirectional Replication\nWith unidirectional replication, one database will mirror its changes to a second one, but writes to the second database will not propagate back to the master database.\n\n```js\n// replicate all changes from localDB to remoteDB\nlocalDB.replicate.to(remoteDB).on('complete', function () {\n  // yay, we're done!\n}).on('error', function (err) {\n  // boo, something went wrong!\n});\n```\n\n### Bidirectional Replication\n```js\nlocalDB.replicate.to(remoteDB);\nlocalDB.replicate.from(remoteDB);\n\n// or, equivalently:\nlocalDB.sync(remoteDB);\n```\n\n### Live Replication (a.k.a continuous replication)\nWith this mode, changes are propagated between the two databases as the changes occur. \n- In other words, normal replication happens once, whereas live replication happens in real time.\n- however if the user goes offline, an error will be thrown and replication will stop.\n\n```js\nlocalDB.sync(remoteDB, {\n  live: true\n}).on('change', function (change) {\n  // yo, something changed!\n}).on('error', function (err) {\n  // yo, we got an error! (maybe the user went offline?)\n});\n```","n":0.088}}},{"i":1348,"$":{"0":{"v":"API","n":1},"1":{"v":"\n### `put`\n`put` is used to create and update documents\n\nTo update a document, we must provide the entire document along with its current revision marker (`_rev`).\n- if we fail to do this, we will get HTTP 409\n- therefore, to update a document we first need to fetch it so we can provide its `_rev` value to the `put` method.\n\n### `allDocs`\n`allDocs` reads many docs for us at once.\n- it also allows you to reverse the order, filter by _id, slice and dice using \"greater than\" and \"less than\" operations on the _id, and much more.\n- most of the time we should be using `allDocs`, not `query` \n\nFor 99% of your applications, you should be able to use `allDocs()` for all the pagination/sorting/searching functionality that you need.\n\n### `query`\nThe `query` API corresponds to the `_view` API of CouchDB.\n- therefore `query` uses map/reduce functions\n\nTo use this API, we make a [[design doc|couchdb.design-doc]] and query it\n\nPrefer `allDocs` or `changes` APIs when possible.","n":0.08}}},{"i":1349,"$":{"0":{"v":"Document","n":1},"1":{"v":"\n### Local document\nLocal documents are documents that won't be replicated. They can be used for things like config for the particular CouchDB instance\n","n":0.209}}},{"i":1350,"$":{"0":{"v":"Design Document","n":0.707},"1":{"v":"\nDesign documents are used to build [[indexes|db.strategies.index]], validate document updates, format query results, and filter replications\n\nDesign documents contain functions such as view and update functions. These functions are executed when requested.\n\nDesign documents are denoted with an `_id` with the format `_design/my-design-document`\n\n```json\n{\n  \"_id\": \"_design/example\",\n  \"views\": {\n    \"view-number-one\": {\n      \"map\": \"function (doc) {/* function code here - see below */}\"\n    },\n    \"view-number-two\": {\n      \"map\": \"function (doc) {/* function code here - see below */}\",\n      \"reduce\": \"function (keys, values, rereduce) {/* function code here - see below */}\"\n    }\n  },\n  \"updates\": {\n    \"updatefun1\": \"function(doc,req) {/* function code here - see below */}\",\n    \"updatefun2\": \"function(doc,req) {/* function code here - see below */}\"\n  },\n  \"filters\": {\n    \"filterfunction1\": \"function(doc, req){ /* function code here - see below */ }\"\n  },\n  \"validate_doc_update\": \"function(newDoc, oldDoc, userCtx, secObj) { /* function code here - see below */ }\",\n  \"language\": \"javascript\"\n}\n```\n\neach separate design document will spawn another (set of) couchjs processes to generate the view, one per shard. \n- Depending on the number of cores on your server(s), this may be efficient (using all of the idle cores you have) or inefficient (overloading the CPU on your servers).","n":0.073}}},{"i":1351,"$":{"0":{"v":"View","n":1},"1":{"v":"\nViews are the primary tool used for querying and reporting on CouchDB databases.\n- we write some Javascript functions which combine to create a view. This code implements the business logic for retrieving the documents. \n- The view gets created as an object in the database, which is exposed as a resource at an HTTP endpoint.\n\nViews also allow us to build [[indexes|db.strategies.index]] on any document value or structure.\n\nviews are basically highly efficient on-disk dictionaries that map keys to values\n- the key is automatically indexed and can be used to filter and/or sort the results you get back from your views. \n  - in fact the results are always sorted by the key. This fact should be leveraged to get the most out of queries. \n  - This should be the approach taken whenever we would use `SORT BY` in SQL. It can be achieved relatively easily by using an array as a key, \n- When you \"invoke\" a view, you can say that you’re only interested in a subset of the view rows by specifying a `?key=foo` query string parameter.\n\nStored data is structured using views. In CouchDB, each view is constructed by a JavaScript function that acts as the Map half of a map/reduce operation. The function takes a document and transforms it into a single value that it returns. CouchDB can index views and keep those indexes updated as documents are added, removed, or updated.\n\nViews are generally stored in the database and their indexes updated continuously.\n- external servers can also be used to store views. Implementations exist in JavaScript, Python, Ruby etc\n\nEach view you create corresponds to one [[B-tree|general.lang.data-structs.tree.B]]\n- The B-trees used are an *append-only*/*copy-on-write* variant that does not overwrite pages of the tree when they are updated, but instead creates a new copy of each modified page.\n\nAll views in a single design document will live in the same set of index files on disk\n\nA view is just like any other document with a key difference: the `_id` is prefixed with `_design/`\n\nThe most practical consideration for separating views into separate documents is how often you change those views. \n- Views that change often, and are in the same design document as other views, will invalidate those other views’ indexes when the design document is written, forcing them all to rebuild from scratch. Naturally we want to avoid this in production.\n\nWhen you have multiple views with the same map function in the same design document, CouchDB will optimize and only calculate that map function once.\n- This lets you have two views with different reduce functions (say, one with _sum and one with _stats) but build only a single copy of the mapped index.\n\nGenerating a view is an `0(N)` operation (where `N` is the total number of rows in the view).\n\n## View functions\nView functions specify a key and a value to be returned for each row.\n\nBasically, the idea with map/reduce functions is that you divide your query into a map function and a reduce function, each of which may be executed in parallel in a multi-node cluster.\n- this allows us to perform efficient query operations in parallel\n\n### Map function\nTake in a single document and `emit()` key-value pairs that are stored in a View.\n\nA map function is free from side-effects.\n\nThe following `map` function will take in a single `nugget` and will emit a key-value pair for each item in the `buckets` array.\n- `emit()` may be called many times for a single document, so the same document may be available by several different keys.\n```js\nfunction (doc) {\n  if (doc.type === 'nugget' && doc.buckets && Array.isArray(doc.buckets)) {\n    doc.buckets.forEach(function (bucket) {\n      emit(bucket.toLowerCase(), 1);\n    });\n  }\n}\n```\n\nIf you don’t use the key field in the map function, you are probably doing it wrong.\n\nCouchDB is smart enough to run a map function only once for every document, even on subsequent queries on a view. Only changes to documents or new documents need to be processed anew.\n\nMap functions can't modify the document and they can't have side-effects.\n- this is why CouchDB can guarantee correct results without having to recalculate a complete result when only one document gets changed.\n\n### Reduce function\nReduce functions operate on the sorted rows emitted by a map function\n\nA reduce function takes in 2 args: a key, and a list (which is the result of the related map function)\n- it can also take an optional 3rd arg, which is to indicate whether *rereduce* mode is active or not.\n\nWhile the map function will `emit` values, `reduce` will simply `return` the reduced value.\n\nA reduce function should return a scalar value (ie. not an array or object)\n- reduce is not for generating complex aggregate values\n- it is ok to return an array if it is small and fixed size (ie. more of a tuple)\n\nCouchDB has a number of built-in reduce functions which run inside the DB and are much faster than the equivalent implementation in a custom reduce function (in Javascript)\n- `_sum` - returns total number of rows between startkey and endkey\n- `_count`\n- `_stats`\n\nBecause of the way B-trees are structured, we can cache the intermediate reduce results in the non-leaf nodes of the tree, so reduce queries can be computed along arbitrary key ranges in logarithmic time.\n\nCouchDB’s reduce functionality takes advantage of one of the fundamental properties of B-tree indexes: for every leaf node (a sorted row), there is a chain of internal nodes reaching back to the root. Each leaf node in the B-tree carries a few rows (on the order of tens, depending on row size), and each internal node may link to a few leaf nodes or other internal nodes.\n\nThe reduce function is run on every node in the tree in order to calculate the final reduce value. The end result is a reduce function that can be incrementally updated upon changes to the map function, while recalculating the reduction values for a minimum number of nodes. The initial reduction is calculated once per each node (inner and leaf) in the tree.\n\nYou can access a view without enabling the reduce function by disabling reduction (`reduce=false`) when the view is accessed.\n\n#### Rereduce\nThe existence and use of the rereduce parameter is tightly coupled to how the B-tree index works.\n\nWhen run on leaf nodes (which contain actual map rows), `rereduce` is false.\n\nWhen the reduce function is run on inner nodes, `rereduce` is true. This allows the function to account for the fact that it will be receiving its own prior output. When rereduce is true, the values passed to the function are intermediate reduction values as cached from previous calculations. When the tree is more than two levels deep, the rereduce phase is repeated, consuming chunks of the previous level’s output until the final reduce value is calculated at the root node.\n\n## View result\nWhenever you query a view, this is how CouchDB operates:\n1. Starts reading at the top (of the B-tree), or at the position that `startkey` specifies, if present.\n2. Returns one row at a time until the end or until it hits `endkey`, if present.\n  - If you specify `descending=true`, the reading direction is reversed; the sort order of rows in the view doesn't change.\n\nThere is no unique constraint on the keys of a view result.\n- ex. we can have a view result of K-V pairs `author`-`book` where the same author is listed multiple times.\n\nYou always need to give a range of keys, because filtering is done on map's results, not on reduce.\n\nEach view result is stored in a B-tree (index structure) in its own file\n- since view results are in their own file, they can be stored on a separate disk from other database objects to increase performance.\n- The B-tree provides very fast lookups of rows by key, as well as efficient streaming of rows in a key range\n- The B-tree is created only once when the view is first queried. \n  - All subsequent queries will just read the B-tree instead of executing the map function for all documents again.\n  - When we create, update or delete a document, the database engine finds the corresponding rows in the view result and marks them *invalid* so they no longer show up in the view result.\n    - if the document got updated, it gets run through the map function again and the result is inserted into the B-tree at the correct spot.\n\nThe results of a view are generated via the `emit()` function.\n\nIt is not recommended to emit the document itself in the view. \n- Instead, to include the bodies of the documents when requesting the view, request the view with `?include_docs=true`.\n\nView results are sorted by *key*\n- the key can be any data type, including arrays and objects.\n  - You can use JSON arrays as keys for fine-grained control over sorting and grouping.\n\nFor instance, here we get back customers, then orders in the same view result (due to the `0` and `1`, serving as secondary sorts)\n```js\nfunction(doc) {\n  if (doc.Type == \"customer\") {\n    emit([doc._id, 0], null);\n  } else if (doc.Type == \"order\") {\n    emit([doc.customer_id, 1], null);\n  }\n}\n```\n\nAside from the data itself, the view result also contains metadata like `total_rows` and `offset`.\n\n## Collating Views\nImagine our querying client needs 2 different related objects in a single query. In SQL, we'd do a [[JOIN|sql.join]], but there are a few different approaches in CouchDB:\n\n### Embed related data in one document\nWe could embed the related data like so:\n```json\n{\n  \"_id\": \"ABCDEF\",\n  \"title\": \"my_nugget\",\n  \"media_items\": \"…\",\n  \"buckets\": [\n    {\"title\": \"psychology\"},\n    {\"title\": \"humor\"}\n  ]\n}\n```\n\nHowever, a drawback is that to add a bucket, we need to\n1. fetch the document (the nugget)\n2. add the bucket to the `buckets` field\n3. send the updated document to the server\n\nTo boot, if we get multiple clients adding buckets to the same nugget, we are bound to get *HTTP 409 Conflict* (optimistic concurrency in action)\n\n### Keep nuggets and buckets separate\nWe could add a `type` field to separate nuggets and buckets:\n```json\n{\n  \"_id\": \"ABCDEF\",\n  \"type\": \"nugget\",\n  \"title\": \"my_nugget\",\n  \"media_items\": \"…\",\n  \"buckets\": [\"123456\"]\n}\n```\n```json\n{\n  \"_id\": \"123456\",\n  \"type\": \"bucket\",\n  \"title\": \"my_bucket\",\n}\n```\n\nThe problem with this approach is that if we want to get both the nugget and the associated buckets, we have to make 2 GET requests.\n\n### Using view collation\nThankfully, we just have to write a view that leverages the fact that keys are automatically sorted, and can be complex:\n- note: the `0` and `1` are needed so that the nugget comes first, then the buckets.\n- note: this view is not totally tested\n```js\nfunction(doc) {\n  if (type === \"nugget\") {\n    emit([doc.bucket, 0], null)\n  } else if (type === \"bucket\") {\n    emit([doc._id, 1], null)\n  }\n}\n```\n\nWith this view, we get back rows where the first one (with `key=[id, 0]`) is the nugget, and the rest are associated buckets. \n\nNow, to get back a specific nugget with all the associated nuggets, we add the query param to the query:\n- `?startkey=[\"my_nugget\"]&endkey=[\"my_nugget\", 2]&include_docs=true`\n\nTo query by some specific value(s) in CouchDB, we just need to figure out how to get that searchterm in the key so we can use a query param on it.\n- ex. imagine our document had a `$doctype` field and we wanted to return documents where that type is `nugget`:\n```js\nfunction(doc) {\n  emit(doc.$doctype, null)\n}\n```\nand then we just add a query param `?key=\"nugget\"`\n\n* * *\n\n## Performance\nThe database engine only runs through all documents once, when you first query your view. \n- If a document is changed, the map function is only run once, to recompute the keys and values for that single document.\n\n## Resources\n- [Cookbook for SQL developers](https://docs.couchdb.org/en/3.2.0/ddocs/views/nosql.html)","n":0.023}}},{"i":1352,"$":{"0":{"v":"API","n":1},"1":{"v":"\nThe main way to query data from CouchDB is through the REST API.\n\n### URI\nThe URI to query to get a view’s result is `/<database>/_design/<designdocname>/_view/viewname`\n\n### Query Params\nwe can add query params to narrrow the result, which is how we'd achieve similar things to using the [[WHERE|pg.clause.where]] clause in SQL:\n- `?key=\"2009/01/30 18:04:11\"`\n- `?startkey=\"2010/01/01 00:00:00\"&endkey=\"2010/02/00 00:00:00\"` (inclusive range)\n\nWe can sort the results by multiple fields by using an array as a key, since results are always sorted by key.\n- `?startkey=[\"myslug\"]&endkey=[\"myslug\", 2]`\n\nWe can sort by descending order\n- `?descending=true`\n\nWe can include the full body of each document in the results\n- `?include_docs=true`","n":0.102}}},{"i":1353,"$":{"0":{"v":"Authorization","n":1},"1":{"v":"\nThe simple way to handle authentication is by passing a basic authorization header:\n```\nAuthorization: Basic am9lc2htb2U6MTIzNDU=\n```\n\nWhere the value is `username:password` [[base64|binary.encoding.base64]] encoded.\n\nThis puts an unnecessary strain however on the database server, since it needs to compute the hash with each request.\n\nOne database per user is the general recommended approach for storage in CouchDB. Therefore, each user would store their own couchdb login credentials.\n- https://docs.couchdb.org/en/3.2.0/config/couch-peruser.html\n\n## Security\nSecurity in a single CouchDB can only be set up to do either:\n- Everyone can read/write everything (admin party)\n- Everyone can read, some can write\n- Some can read everything, and those same people can write everything","n":0.101}}},{"i":1354,"$":{"0":{"v":"Colors","n":1},"1":{"v":"\nUse warmer colors for foreground elements, cooler colors for background elements.\n\nUse saturated colors (pure hues) when attracting attention is the priority, and desaturated colors when performance and efficiency are the priority.\n\nGenerally, desaturated, bright colors are perceived as friendly and professional; desaturated, dark colors are perceived as serious and professional; and saturated colors are perceived as more exciting and dynamic.\n\n### High-dynamic-range (HDR)\nOutside this range, no features are visible because in the brighter areas everything appears pure white, and pure black in the darker areas. The ratio between the maximum and the minimum of the tonal value in an image is known as the dynamic range\n\n# Resources\n[UI palettes]( https://visme.co/blog/website-color-schemes/ )\n","n":0.096}}},{"i":1355,"$":{"0":{"v":"Cloudflare","n":1},"1":{"v":"\nSince cloudflare can intercept requests, you can use it to change anything you don’t like about a service that your app uses\n- ex. You can change the css offered by a provider \n","n":0.174}}},{"i":1356,"$":{"0":{"v":"Cloudflare Workers","n":0.707},"1":{"v":"\nCloudflare workers are serverless functions that allow us to intercept HTTP requests (and run code before the request hits the server, and before the response reaches the client).\n\nThere are notable differences with serverless functions:\n- Workers run on the edge. This means that they are available on a number of Cloudflare servers around the world so your user will get the lowest possible latency. This is not how cloud functions usually work. For example in AWS you have Lambda and Lambda at Edge.\n- Since workers are running in Cloudflare's infrastructure they have access to some unique API to interact with the CDN and caching which is the main Cloudflare product. So you can receive a request to your worker and then decide \"hey, put this image in the cache for next time\".\n- Workers have access to a Key Value (KV) store which is storing data on the edge.\n\nWorkers runs code on Cloudflare's edge. And each Worker is assigned to at least one HTTP endpoint. So these are scripts that live ~10ms away from most users and can be updated globally easily.\n\nCloudflare workers run their own version of Javascript, not [[Node|js.node]]\n- The API is almost the same as the web workers API\n\nex. Workers can be used to apply http headers if you use a serverless setup for hosting.\nex. You can host every static website / react SPA on cloudfare workers\nex. You can use workers to inject headers into responses without having to change Nginx configs\n","n":0.064}}},{"i":1357,"$":{"0":{"v":"Cloudflare Workers Key-Value Storage","n":0.5},"1":{"v":"\nCloudflare Workers KV provides access to a secure low latency key-value store at all of the data centers in Cloudflare's global network\n- usage ex. save the cart and checkout in Workers KV and when our webhook is notified by Stripe of a successful payment_intent, we create the order in our backend.\n\t- ex. this decouples your server having to be up and running from being able to process orders\n","n":0.121}}},{"i":1358,"$":{"0":{"v":"Warp","n":1},"1":{"v":"\nSimilar to a VPN, in that it will encrypt your traffic, but it doesn't fake our IP like a VPN does.\n\nSince Workers are distributed across 190+ datacenters, you can get the website loading in milliseconds all around the world.\n","n":0.16}}},{"i":1359,"$":{"0":{"v":"Chrome","n":1}}},{"i":1360,"$":{"0":{"v":"Vimium","n":1},"1":{"v":"\nA good way to find the starting point for your selection is to to use search mode (`/`) to jump to your desired starting location on the page, then use visual mode to select any text you need.\n\n## Vimium\n- `<c-u>` move tab to new window (unpin)\n- `<c-n>` - pin the tab back to the main chrome window\n- `^` - go to most previous tab\n- `yt` - duplicate current tab\n- `gE` - edit the current url and open in new tab\n- `gi` - focus first (or nth) input box on page\n- `yy` - copy url that you are currently on\n- `gu` - go up a level in the url hierarchy\n","n":0.096}}},{"i":1361,"$":{"0":{"v":"Devtools","n":1}}},{"i":1362,"$":{"0":{"v":"Tabs","n":1}}},{"i":1363,"$":{"0":{"v":"Performance Tab","n":0.707},"1":{"v":"\nPerformance tab allows us to see how the Javascript actually performs on our site.\n\n## Usage\n1. Hit record (`Cmd + E`)\n2. Do the interaction on the webpage that you want to measure performance for.\n","n":0.174}}},{"i":1364,"$":{"0":{"v":"Network Tab","n":0.707},"1":{"v":"\nWe should probably have the \"disable cache\" checkbox enabled. This will give us a more realistic view of the actual user experience.\n\nWhen looking at the load amounts, there is two numbers. \n- *DOMContentLoaded* - how much data is in the browser\n    - the time it takes for the initial HTML document to be completely loaded and parsed, and for the browser to have constructed the DOM for the page.\n    - doesn't including time taken for external resources like images to finish loading.\n- *Load* - considers when all resources on the page, including images, stylesheets, scripts, and other assets, have been fully loaded and rendered by the browser.\n    - this is more significant in terms of debugging performance\n    - This is typically used to determine when it's safe to interact with or manipulate the fully loaded DOM. \n\n## Unofficial APIs\nOften companies don't offer open APIs. We can \"open it up\" to ourselves by mimicking the request as if it's coming from us interacting with the webpage.\n1. In the website's UI, do the interaction that will trigger a network request (eg. clicking a button)\n2. In the network tab, find the request and gather the API endpoint that was hit.\n3. Right-click and *copy as* fetch/cURL. We now have a fully formatted request that has all of the headers needed to make a successful request.\n\n## Image caching status\nWe can check to see if image caching is working or not (eg. if we are using Cloudflare image caching worker)\n1. In Network tab, filter for images, and find the image in question\n2. Check response headers for an indicator (in this example, `cf-cache-status=hit`) that the image came from a cached location.\n\n* * *\n\nWhen right-clicking a request, we get the option to `Copy As...`. If we `copy as Node.js fetch`, we also get anything that came along with the request (inc. all of the cookies, session Ids, headers etc).\n- a normal fetch request doesn't include these, because it assumes we are firing off the request from the browser. However, if we are making a fetch request from a server, then we would manually need to send those cookies/headers etc along.\n","n":0.053}}},{"i":1365,"$":{"0":{"v":"Lighthouse","n":1},"1":{"v":"\nWhenever you set out to improve a site's load performance, always start with a lighthouse audit.\n\n1. Establish a baseline by running an audit so we can compare it to any potential performance improvements we undertake afterwards.","n":0.167}}},{"i":1366,"$":{"0":{"v":"Devtools Commands","n":0.707},"1":{"v":"\n- swap between chrome tabs - `cmd+[`/`cmd+]`\n- move dock position - `cmd+shift+d`\n","n":0.289}}},{"i":1367,"$":{"0":{"v":"Chrome Debugger","n":0.707},"1":{"v":"\n## Debugging outbound network issues\n1. Sources tab, hit `Pause on exceptions` (stop sign with pause icon)\n2. Reload the page, and code will pause on the first exception that happens\n3. Hover over the failing line, and investigate\n\t- are args to the function call what you'd expect them to be? Does anything stand out? Why is an error being thrown?\n","n":0.131}}},{"i":1368,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Clear data for a single site\nApplication > Storage > Clear site data\n\n### Prevent Chrome from refreshing tabs\nChrome has a feature that “discards” any tabs you haven’t used for some time, in order to save memory and prevent the browser or even your PC or Mac from running slow.\n\nGo to `chrome://discards/` and toggle *Auto Discardable* off.\n\n\n","n":0.134}}},{"i":1369,"$":{"0":{"v":"Chrome Commands","n":0.707},"1":{"v":"\n- Open Clear browsing data modal - `cmd+shift+delete`\n- Remove query autosuggestion results based on history - `shift+fn+delete`\n","n":0.243}}},{"i":1370,"$":{"0":{"v":"Chatgpt","n":1},"1":{"v":"\n## Tips\n- If the response you get to a query is generic or unrelated to what you asked, follow up with \"Ask more questions to help you build context.\"\n- ask ChatGPT to generate a mnemonic to help remember information\n    - \"create an acronym to help remember...\"\n        - ex. Great Lakes can be memorized with the acronym HOMES\n    - \"create an acrostic to help remember...\"\n        - ex. Every Good Boy Deserves Fudge","n":0.119}}},{"i":1371,"$":{"0":{"v":"Caddy","n":1},"1":{"v":"\nif Caddy is running at port `:2019`, then the config file is at `:2019/config/`\n\nmost(?) caddy commands are just running GET/POST requests. For instance, `caddy stop` sends a POST to `http://localhost:2019/stop`\n\n#### Workflow\nTo administer Caddy, we can do so in 2 ways:\n1. using the API\n    - ex. POST to `https://caddyserver.mywebsite.com:443/load`\n2. using the CLI\n    - ex. run `caddy load`\n\nCaddyfile+CLI combo is the more easy-going route, but is more difficult to scale. Otherwise, if looking for more control, users typically go for the JSON+API combo.\n","n":0.111}}},{"i":1372,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n#### Start Caddy in background\n`caddy start`\n\n#### Start Caddy in foreground\n`caddy run`\n\n#### Give Caddy a new config without stopping server\n`caddy reload`\nnote: this works with `caddy start`; not `run`\n","n":0.192}}},{"i":1373,"$":{"0":{"v":"Caddyfile","n":1},"1":{"v":"\nCaddy config natively uses a JSON structure. However, because writing JSON by hand can be error-prone, we have config adapters to help us.\n- The standard config adapter coming with Caddy converts a `Caddyfile` into JSON.\n- At the end of the day, JSON is more flexible, but not quite as simple. If we stay on the well-beaten path, then we probably can just get by with a Caddyfile.\n\nWe can use a `caddy.json` config file like this:\n```json\n{\n\t\"apps\": {\n\t\t\"http\": {\n\t\t\t\"servers\": {\n\t\t\t\t\"example\": {\n\t\t\t\t\t\"listen\": [\":2015\"],\n\t\t\t\t\t\"routes\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"handle\": [{\n\t\t\t\t\t\t\t\t\"handler\": \"static_response\",\n\t\t\t\t\t\t\t\t\"body\": \"Hello, world!\"\n\t\t\t\t\t\t\t}]\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\n}\n```\n\nOr, the equivalent can be expressed in a `Caddyfile`\n```Caddyfile\n:2015\n\nrespond \"Hello, world!\"\n```\n\nIf we run `caddy adapt` in a directory where there is a `Caddyfile`, then we can see the json that the Caddyfile contents are being converted into. This is the *config adaptor* at work.\n\nIf we are serving multiple apps from a single Caddy server, then we need to use curly braces (`{}`) in the Caddyfile to wrap the details of a single app. Otherwise, curly braces aren't needed\n","n":0.079}}},{"i":1374,"$":{"0":{"v":"Directives","n":1},"1":{"v":"\n## Matchers\nA request matcher is used to filter requests.\n\nSyntax-wise, matchers immediately follow directives\n\n## Addresses\nThe address part may be any of these forms:\n- `host`\n- `host:port`\n- `:port`\n- `/path/to/unix/socket`\n    - ex. `unix//path/to/socket`\n\nsome config fields may allow us to specify a port range, like `:8080-8085`\n\n## Directives\n\n### root\nsets the root path of the site\n- the root path (ie. root directory) holds all the files related to serving an application, whether they are private or public, (with respect to being exposed to the internet).\n\nspecifying multiple `root`s in the same block is legal. \n- Each `root` directive is mutually exclusive with others in the same block.\n    - multiple `root`s will not cascade and overwrite each other.\n\n`root` is usually used together with `file_server`, since it does not enable serving static files on its own.\n```\nroot * /var/www\n```\n\n### file_server\n`file_server` works by taking the request's URI path and appending it onto the root path\n\nex. if a user visits `https://example.com/jokes`\nWith the Caddyfile:\n```Caddyfile\nroot * /var/www\n```\nCaddy will serve content from `/var/www/jokes`\n","n":0.08}}},{"i":1375,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n#### Serving static files\nie. Caddy performing the duties of a static file server\n```conf\nexample.com {\n    # if `root` omitted, pwd (of Caddyfile) is assumed\n    # `*` is a wildcard matcher, so it matches all requests.\n    root * /var/www\n    file_server\n}\n```\n\n#### Reverse proxy load balance\nThis will round-robin load balance requests to 3 different hosts on the same local network\n```conf\nreverse_proxy /api/* 192.168.0.30:80 192.168.0.31:80 192.168.0.32:80 {\n\tlb_policy round_robin\n}\n```\n","n":0.127}}},{"i":1376,"$":{"0":{"v":"C","n":1},"1":{"v":"\nC is designed to interface with the output of the computer hardware. Therefore, it is a natural language to use when building operating systems. When you write code in C, you can better intuit what the resulting assembly language will look like. C effectively enables you to write code from a hardware-first perspective.\n\nIn C, if a function signature doesn’t specify any argument, it means that the function can be called with any number of parameters or without any parameters.\n\n`main` is a function just like any other, and thus returns a value to the environment that executed the c program to begin with.\n\n### Header Files\n- purpose is to share functions and macros across source files.\n\n* * *\n\n## Aspects\n### Const\n- a *constant expression* involves only constants, and is therefore evaluated during compilation \n- an `emun` is a type of constant. By default, the first element has value 0, the second has value 1, and so on.\n\n## Operators\n### Token-pasting operator (##)\n- if 1+1=2, then 1##1=11\n\n## Concepts\n\n* * *\n\n### Symbolic Constants (`#define`)\n- `#define` creates a macro\n- Any constant defined in this manner will find all occurrences and replace it with the corresponding value *before* compilation \n\t- This contrasts with variables, in that data is actually stored inside of them (while macros are more like aliases) \n- Symbolic Constants are valauble for defining magic numbers\n\n* * *\n\n## Debugging\nErrors involving “token” almost always mean that you either missed a semicolon or your {}s, ()s, or []s aren’t matched.\n\t\t\n* * *\n\n### Indirection\n- The act of referencing something using a name, rather than using the value itself.\n- A common form of indirection is when we manipulate a value via its memory address\n\t- ex. accessing a variable by using a pointer.\n- aka Dereferencing\n\n### Handle vs Pointer vs Reference\n- While a pointer contains the address of the item it refers to, a handle is an opaque reference to an object. \n- The type of the handle is unrelated to the element referenced\n- using handles adds an extra layer of indirection, meaning that we can change the details at this level without breaking the program. The same couldn't be said for a pointer. \n- A pointer is the combination of an address in memory and the type of the object that resides in that memory location\n","n":0.052}}},{"i":1377,"$":{"0":{"v":"Preprocessors","n":1},"1":{"v":"\nC Preprocessors are the header of the file\n- contains C function declarations and macro definitions to be shared between several source files.\n\t- In essence, they are like importing modules.\n- ex. `stdio.h` – Defines core input and output functions\n- In C, all lines that start with # are processed by preprocessor which is a program invoked by the compiler\n\t- a preprocessor takes a C program and produces another C program without any `#`\n- When we use `include` directive,  the contents of included header file (after preprocessing) are copied to the current file.\n\t- `<` and `>` instruct the preprocessor to look in the standard folder\n\t- `“` and `“` instruct the preprocessor to look into the current folder\n- When we use `define` for a constant, the preprocessor produces a C program where the defined constant is searched and matching tokens are replaced with the given expression\n\t- ex. `#define max 100` — `max` is searched for in the program, and is replaced with `100`\n","n":0.079}}},{"i":1378,"$":{"0":{"v":"Makefile","n":1},"1":{"v":"\n### Purpose\n- to help decide which parts of a program need to be recompiled.\n- Makefiles allow us to give a series of instructions to run depending on what files have changed.\n- similar to the sript section of the `package.json`\n\n### Structure\n- a Makefile consists of a set of rules, which take the form:\n```\ntargets: prerequisites\n   command\n   command\n   command\n```\n- `targets` are filenames.\n- `commands` are a series of steps, normally used to make the target(s)\n- `prerequisites` are dependencies that are needed before the commands can be run.\n\n# ## Running \"make\" command\n- when we run `make` without arguments, the first target that doesn't begin with `.` is processed. This is known as the *default goal*.\n\t- To do this, it may have to process other targets, specifically ones that the first target depends on.\n- Often the *default goal* is called `all`, though this is just a convention.\n\n#### Building from source with `make`\n1. run `make`\n2. run `sudo make install`\n\n### Syntax\n- `PG_CONFIG ?= pg_config` - set PG_CONFIG variable only if it isn't already set\n\n## UE Resources\n- [Makefile Tutorial](https://makefiletutorial.com/)\n","n":0.077}}},{"i":1379,"$":{"0":{"v":"C Lang","n":0.707}}},{"i":1380,"$":{"0":{"v":"Variables","n":1},"1":{"v":"\n`external` and `static` variables are guaranteed to be initialized to zero if not explicitly declared.\n- The value of these variables is determined conceptually before the program begins execution \n\nAutomatic and register variables have undefined (i.e., garbage) initial values, if not explicitly declared.\n- The value of these variables is determined when the function or block is entered.\n\n### Static\n- Decalaring a variable or fn *static* does 2 things:\n1. it becomes scoped to wherever it was defined (scoped to source file, fn etc)\n2. When static is declared inside a function, then that variable will live on between calls of the function, giving us permanent stoage. \n- Static variables are allocated memory in data segment, not stack segment\n- Unless initialized with a value, static variables default at ‘0’\n\n### Register\n- Declaring a variable with `register` advises the compiler that it will be heavily used, placing it in a machine register for quicker access\n- Only a few variables in each function may be kept in registers, and only certain types are allowed.\n\t- Excess register declarations are harmless, however, since the word register is ignored for excess or disallowed declarations\n\t\t- The actual limit varies from machine to machine.\n\t- It is impossible to get the memory address of a register variable. \n","n":0.07}}},{"i":1381,"$":{"0":{"v":"Types","n":1},"1":{"v":"\nIn the early days of C, pretty much everything was either an int or pointer (which is why int is the default type)\n\n### Numbers\nPrefixes\n- A leading `0` on an integer constant means octal\n- a leading `0x` means hexadecimal\n\nSuffixes\n- spec:an int is by default short, and has no suffix:\n`int num = 1234;`\n- but to make a long constant (double by default), we need to append the number with `L`:\n`long num = 123456789L`\n\t- note: even if `long` is not specified, the compiler will know it is long and declare it as such \n- to make an unsigned const, append `U`\n- to make a float const, append `F`\n","n":0.098}}},{"i":1382,"$":{"0":{"v":"String","n":1},"1":{"v":"\nC does not have a native string type. Instead, the language uses arrays of char terminated with a null char (`'\\0'`)\n- Therefore, the following are equivalent:\n```c\nchar pattern[] = \"ould\";\n// and\nchar pattern[] = { ′o′, ′u′, ′l′, ′d′, ′\\0′ };\n```\n- Therefore, the physical storage required is one more than the number of characters\n\t- The `\\0` gets automatically appended for us when array is initialized. Therefore if we access the last element, we get back `\\0`\n\nThere is a standard header `<string.h>` that gives us some string utilities\n\nIn C, arrays are of fixed length at the time they are declared. For this reason, it may make sense to declare an array with a length that is far more than you need, just so you know it can handle everything that gets thrown at it. \n\n### Single vs Double Quotes\nIn C, `'x'` is an integer representing the ASCII value, while `\"x\"` is a character array with 2 elements: `x` and `\\0` \n\n## Initialization\n- Character arrays can be initialized in different ways:\n\t- `char name[13] = \"StudyTonight\";`\n\t- `char name[] = \"hello\";` (spec:wide char array)\n\t- `char name[10] = {'L','e','s','s','o','n','s','\\0'};`\n- These character arrays can either be read-only or manipulatable\n\t- read-only: `char * str = \"Hello\";`\n\t- manipulatable: `char str[] = \"Hello\";`\n\n## Format specifiers \n- `%s` - Take the next argument and print it as a **string**\n- `%d` - Take the next argument and print it as an **int**\n\n## printf (print formatted)\nThe compliment to `printf` is `scanf`, in the sense that print is stdout and scanf is stdin. \n- `scanf` terminates its input on the first white space it encounters. Tip: use \"edit set conversion code `%[..]` \" to get around this.\n\t- alternatively, use `gets()`, which will terminate once hitting `\\n`\n\n","n":0.06}}},{"i":1383,"$":{"0":{"v":"Pointer","n":1},"1":{"v":"\n`&` returns the memory address of the object\n- called \"address of\" operator\n\t- Therefore, this gets applied to the value to get the memory address\n- ie. `p = &c;`\n\n`*` either: \n1. declares a pointer variable, or\n\t- ex. `int *p;`\n2. dereferences an existing pointer (indirection through a pointer)\n- called \"indirection\" or \"dereferencing\" operator\n\t- Therefore, this gets applied to the pointer to get the value in storage\n- `int a = 5` then `int *ptr = &a;`\n\t- this means \"locate\" the address where `a` is stored, and assign its value to `ptr`\n\t\t- In other words, the value of `ptr` will now be the address where `a` is stored. \n- `*ptr = 8`\n\t- this means take the address of `ptr`, \"locate\" that address in memory, and set its value to 8.\n- `int *ptr`\n\t- \"`ptr` is a pointer that points to an object of type `int`\", or simply: \"`ptr` is a pointer to `int`.\"\n\nIf p points to the integer x, then *p can occur in any context where x could, so:\n```\n*p = *p + 10;\n// is the same as\nx = x + 10;\n```\n\n<!-- TODO: broken URL -->\n![996035c6b28b5cf88b9156c920cc0d58.png](:/454c71a68a8845c99466685e0c038c4d)\n- This picture shows how a pointer references a storage location, which holds a variable `c`. When we use the `&` operator, we are talking about the place where `c` is stored. When we use the `*` operator, we are talking about the variable `c` itself. \n\n- each pointer points to a specific data type, which is why we declare a pointer variable `int *p;`\n\t- The exception is \"pointer to void\", which can hold any kind of pointer, but cannot be dereferenced itself.\n- C does not implicitly initialize storage duration of memory locations. Therefore, we should be careful that the address that the pointer points to is valid.\n\t- For this reason, some suggest initializing pointers to `NULL` (*null pointer*/*null reference*)\n\t- null pointer shown as `0x00000000`\n- Like other languages, manipulating a function argument will have no effect on the original variable that was passed to the function. This is because when we pass an argument, a copy is made and we are merely mutating the copy. \n\t- However, we are also able to call a function, passing in the variable's *address* as the argument `passByAddr(&x)`.\nfrom within the function, if we change the value like so `*m = 14`. this changes the value at the address, so outside the function we'll notice that the value changed\n","n":0.05}}},{"i":1384,"$":{"0":{"v":"Opaque","n":1},"1":{"v":"\nAn opaque type is a type that \"wraps\" lower-level types, and is often used when either the underlying implementation is complex, or the user simply does not need to know about the inner workings\n- ex. there is a type in Swift called `CFString`. It is an opaque type that provides a series of methods that allow for string manipulation and string conversion. For example, we have a `.length` and `.indexOf` methods. The implementation details of these methods is unimportant to the user of this type, so they have been hidden. \n\t- This is the essence of an opaque type, in that a native type has had some extra functionality added to it by being \"wrapped\".\n\nAn \"opaque type\" is a type where you don't have a full definition for the struct (or class in the case of C++)\n- In C, you can tell the compiler that a type will be defined later by using a forward declaration:\n```c\n// forward declaration of struct in C and C++ \nstruct Foo;\n```\nhere, the compiler only has enough info to be able to declare pointers to `Foo`, which is sometimes all we need to do\n- Allows library and framework creators to hide implementation details, allowing the users of that library to call helper functions to create, change or destroy instances of a forward declared `struct` (also `class` in C++) \n","n":0.067}}},{"i":1385,"$":{"0":{"v":"Int","n":1},"1":{"v":"\nTypically, a `short int` is 16 bits (2 bytes), and `long int` 32 bits (4 bytes)\n- therefore, an int is a 32-bit data type\n\t- this depends on the natural size of integers on the host machine\n- in reality, 31 bits are available for the number, since 1 has to be reserved for the sign (+/-)\n- Whenever a number is being assigned to an ‘int’ type variable, it is first converted to its binary representation then it is kept in memory at specific location.\n","n":0.11}}},{"i":1386,"$":{"0":{"v":"Float","n":1},"1":{"v":"\nfloating-point types. Includes:\n- *float* - single-precision floating point\n- *double* - double-precision floating point\n","n":0.277}}},{"i":1387,"$":{"0":{"v":"Char","n":1},"1":{"v":"\nA char holds the ASCII value, rather than the character itself.\n- Therefore, by definition a `char` is just a small integer\n- We can leverage the fact that a char will evaluate to its ASCII value, by performing math on them\n\t- ex. we can test if something is a digit like with `if (val >= '0' && val < '9')`. Here, the `'0'` and `'9'` get converted to their ASCII values as the comparison is made. As it so happens, any ASCII value between 48 and 57 is a digit.\n- A char is a single byte\n","n":0.103}}},{"i":1388,"$":{"0":{"v":"Array","n":1},"1":{"v":"\nIn C, arrays can be thought of as pointers to consecutive areas of memory\n- elements in an array are stored in physically adjacent blocks of memory \n\t- spec: This is why a pointer pointing to an array only points to the first element— because we can essentially keep checking \"is the next memory block part of that array?\"\n- the name of an array is a synonym for the memory location of the first element of that array. \n\t- therefore, the assignment `p = &x[0]` is identical to `p = x`\n\t- also, what follows from this is that a reference to `x[i]` can also be written as `*(x+i)`\n\t\t- We can understand this as \"`x[5]` is the address of the 5th element beyond `x`\".\n- When an array name is passed to a function, what is passed is the location of the initial element. Within the called function, the argument is a local variable, and so an array name parameter is a pointer (a variable containing an address)\n- the syntax for accessing arrays is identical for that which can be used to dereference pointers\n- `array[i]` is equivalent to `*(array + i)`. \n\t- this fact shows that arrays are pointers to consecutive blocks of memory\n- An array can be initialized with values like so: `int days[] = { 1, 2, 3 }` \n\t- If the size of the array is omitted in initialization, the compiler will count the number of elements \n","n":0.065}}},{"i":1389,"$":{"0":{"v":"Struct","n":1},"1":{"v":"\nA struct is a composite type \n- Stands for “user defined data structure”\n\nIn C++, it is similar to a class (though by default the access level is public)\n- A struct is similar to an interface, in that we define a shape, which effectively becomes our new composite type. We can then initialize new variables that take this shape\n- The primary use of struct is for the construction of complex datatypes\n- Consider that a composite type such as this is analogous to a record in a database table. In a table, we implicitly create a type that has the same shape of the table  (in this sense the composite type is called a record)\n- All variables contained in a struct are physically located together. The consequence of this is that we can access the struct with a single pointer\n","n":0.085}}},{"i":1390,"$":{"0":{"v":"Statements","n":1},"1":{"v":"\nall statements in C end in semicolon `;`\n- braces `{`/`}` can be used to group declarations and statements together into a *compound statement* (or block)\n\t- this makes them syntactically equivalent to a single statement\n- the fact that `;` denotes the end of a statement rings true for *for loops* too. The first and third are assignments (or fn calls), while the second is a relational expression.\n\t- All are optional, so `for (;;)` is an infinite loop\n","n":0.115}}},{"i":1391,"$":{"0":{"v":"C Functions","n":0.707},"1":{"v":"\nIn C, we must declare a function prototype before calling the function so that the signature is known to the compiler \n- this is no different from declaring an int with `int e`\n- The prototype looks identical to the function, expect the body is replaced by a semi-colon:\n\n```c\nchar *do_something(char *dest, const char *src);\n```\t\n\nThis is not strictly necessary, but unless we declare it, the compiler is left to guess the signature based on how the function is called, and it is often wrong.\n- if we are passing an argument that should remain constant within the function body, we can prepend `const` to the parameter: `int strlen(const char[]);`\n","n":0.097}}},{"i":1392,"$":{"0":{"v":"C Compiler","n":0.707},"1":{"v":"\nUnlike other languages, the C compiler does not look ahead to hoist definitions. It doesn't look backward or forward, nor does it scan the file multiple times to understand relationships. The compiler only scans forward in the file exactly once. Connecting function calls to function declarations is part of the linker's job, and is only done after the file is compiled down to raw assembly instructions.\n\t- This means that as the compiler scans forward through the file, the very first time the compiler encounters the name of a function, one of two things have to be the case: It either is seeing the function declaration itself, in which case the compiler knows exactly what the function is and what types it takes as arguments and what types it returns — or it's a call to the function, and the compiler has to guess how the function will eventually be declared.\n\n#### Compilation process\nwe can get all intermediate files with `clang -Wall -save-temps filename.c -o filename`\n\n1. Pre-processing\n\t- takes the source file and handles...\n\t\t- Removal of Comments\n\t\t- Expansion of Macros\n\t\t- Expansion of the included files\n\t\t- Conditional compilation\n\t- creates `filename.i`\n\t- At the end of the file, our source code is preserved\n2. Compilation\n\t- takes the `filename.i` file and compiles it, creating assembly level instructions that assembler can understand\n\t- creates `filename.s`\n\t- compilation phase is useful because it provides a common output language for different compilers for different high-level languages\n\t\t- ex. compilers of C and Fortran produce the same assembly language.\n3. Assembly\n\t- converts assembly language to machine level instructions\n\t- creates `filename.o`\n\t\t- produces a *relocatable object file*\n4. Linking\n\t- all the linking of function calls with their definitions are done\n\t- at this phase, we take code that has already been compiled (like `printf`, which comes from the `printf.o` file), and merge it with our own `.o` file.\n\t\t- We can see the effect of this phase by running `size filename.o` and comparing it to `size filename`. We see that `filename` is much larger\n\t\t- produces a *executable object file*, which can be loaded into memory and executed by the system.\n\n#### Different compilers\nIn C, it's possible that one compiler fails while another succeeds.\n- In the gcc compiler, `main()` cannot return `void`, but Turbo C compiler allows this.\n- To ensure we are writing proper C code, we have to look at the C Standard.\n- Therefore, just because C code compiles doesn't mean it is up to C standard.  \n","n":0.05}}},{"i":1393,"$":{"0":{"v":"C#","n":1},"1":{"v":"\n# Resources\n[Float notation in C#](https://answers.unity.com/questions/282128/what-does-0f-and-5f-mean.html)\n","n":0.447}}},{"i":1394,"$":{"0":{"v":"Lang","n":1},"1":{"v":"\n### Enumerables vs Enumerators\n#### Enumerable\n\"Enumerable\" defines an object that is meant to be iterated over, passing over each element once in order.\n- arrays and objects are examples of *enumerables*\n- an Enumerable is an object like an array, list, or any other sort of collection that implements the `IEnumerable` interface\n- Enumerables standardize looping over collections, and enables the use of useful extension methods like `List.Where()` or `List.Select()`.\n\n`IEnumerable` is an interface that defines one method `GetEnumerator` which returns an `IEnumerator` interface, this in turn allows readonly access to a collection. A collection that implements `IEnumerable` can be used with a foreach statement.\n\n#### Enumerator\n\"Enumerator\" is an object that can return each item in a collection. Therefore, the enumerator knows the order of items and keeps track of where it is in sequence. It then returns the current item when it is requested.\n\n`IEnumerator` provides two methods, `MoveNext()` and `Reset()`. It also has a property `Current`. \n- `MoveNext` is the method used to step over each item, applying any kind of custom iterator logic in the process\n- `Current` is a method used to get the current item after `MoveNext` is done. You end up with an interface that defines objects that can be enumerated, and how to handle that enumeration.\n\n![](/assets/images/2021-08-24-12-52-32.png)\nin the above example:\n\n- It gets the object’s enumerator by calling its GetEnumerator method.\n- It requests each item from the enumerator and makes it available to your code as the iteration variable, which your code can read\n\nevery time you write a foreach loop, you’re using enumerators. You actually can’t use foreach on a collection that doesn’t implement `IEnumerable`. When it gets compiled, a foreach loop like this:\n\n```cs\nforeach (Int element in list)\n{\n    // your code\n}\n```\n\nGets turned into a while loop, that processes until the Enumerable is out of items, calling MoveNext each time and setting the iterator variable to .Current.\n\n```cs\nIEnumerator enumerator= list.GetEnumerator();\nwhile (list.MoveNext())\n{\n    element = (Int)enumerator.Current\n    // your code\n}\n```\n\n[resource](https://www.csharpstar.com/difference-between-ienumerator-and-ienumerable-interface-csharp/)\n\n### Coroutine\nCoroutines are special functions that can pause execution and return back to the main thread.\n- Coroutines return an enumerator\n    - therefore, Coroutines can use functions that have a return type of `IEnumerator`\n- They’re commonly used for executing long actions that can take some time to finish, without causing the application to hang while waiting on the routine. \n    - For example, in games where the framerate of the application matters a lot, large hitches even on the order of a few milliseconds would hurt the user experience.\n\nto break up execution over different frames, you’ll just need to `yield return null` whenever you’d like to pause execution and run more on the main thread.\n- to use this Enumerator, you’ll need to call the function and assign it to a variable, which you can call `MoveNext()` on at regular intervals.\n\n#### in Unity\nthe Coroutine controller in [[unity]] handles the processing of coroutines for us. All it's really doing is just calling `MoveNext()` once per frame, checking if it can process more, and handling the return value if it’s not null.\n","n":0.045}}},{"i":1395,"$":{"0":{"v":"Keywords","n":1},"1":{"v":"\n### Access modifiers\nin C# you have:\n- public\n- private\n- internal\n- protected \n\n[[explanation|paradigm.oop.keywords]]\n* * * \n\n#### override\nwhen we specify `override`, we override virtual method which was implemented scope above\n","n":0.192}}},{"i":1396,"$":{"0":{"v":"Browser","n":1},"1":{"v":"\nEvents are the native way to deal with user input in browser based web applications. Events are not part of the language of JavaScript itself, but they are part of the browser environment that JavaScript runs in\n\nBrowsers are single threaded and this single thread (The UI thread) is shared between the rendering engine and the js engine.\n- Therefore, if the thing you want to do takes a lot of time, it could halt the rendering (flow and paint).\n- In browsers there also exists \"The bucket\" where all events are first put in wait for the UI thread to be done with whatever it´s doing. As soon as the thread is done it looks in the bucket and picks the task first in line.\n    - Using `setTimeout` you create a new task in the bucket after the delay and let the thread deal with it as soon as it´s available for more work.\n\n### Ajax\nAjax is when a client-side JavaScript application running inside a web browser uses XMLHttpRequest to become an HTTP client, enabling it to make requests.\n- the request is probably for JSON, rather than HTML, as is the traditional response data format for HTTP requests from a browser.\n\n## Resources\n- [What happens when typing URL into address bar](https://systemdesign.one/what-happens-when-you-type-url-into-your-browser/)","n":0.07}}},{"i":1397,"$":{"0":{"v":"Browser Workers","n":0.707},"1":{"v":"\n### Workers in the browser\n#### Web workers\nA web worker is a JavaScript file that runs independently of the website off of the main thread of the app\n\n#### Service workers\nA service worker is a type of web worker which acts as a proxy between the browser and the server.  It also acts as a proxy between the browser and the cache.\n- Put another way, service workers act as a caching agent, and can store content for offline use.\n- They also give you more control over network requests and allow you to handle push-messaging, too. Since service workers are web workers, they run independently of your app, meaning that they can run even when the app is not open. Progressive web apps use service workers, and can thus work offline or on very slow networks.\n\nService workers are based on JavaScript promises They also use JavaScript’s fetch and cache APIs.\n\n\n","n":0.082}}},{"i":1398,"$":{"0":{"v":"Cookies","n":1},"1":{"v":"\nCookies are small data files that are placed on your computer or mobile device when you visit a website.\n\nWhen a browser sends a request to a server, all of the cookies are automatically sent along with that request.\n\nOne of the benefits of using cookies versus local storage is that the server receives all of these cookies on initial request. If we are storing a JWT in local storage, then the user will be shown the non-authenticated version of the page initially. The reason is that local storage uses JavaScript, which only gets loaded after the HTML and CSS have been already received. This is a perceivable delay, over just storing the JWT in an http-only cookie.\n\nIf you use an HTTP only cookie for authentication, the user's data will be available on the server for the initial render\n\nIn the old days of the web, cookies werecause those cookies are susceptible to cross-site forgery attacks. Once the attacker has lured the unsuspecting victim to a hostile website, they can use JS scripts to exploit the cookies and tamper with the data to send malicious requests to the server.\n- Another vulnerability regards the chances of a man-in-the-middle attack, where an attacker can intercept the session ID and perform harmful requests to the server.\n\n\n### httpOnly Cookie\na cookie that is only sent in the http requests to the server.\n- Therefore, never accessible to the client-side javascript (making it immune to XSS attacks.)\n\nCookies and local storage can be accessed by anything client-side. This includes things like extensions. This makes storing sensitive information in a cookie a security risk. However, an HTTP-only cookie is different, in that its contents can only be set and read server side. This is why it is OK to store JWTs in an HTTP-only cookie.\nThe cookie is set via the response payload from the server\n\nThe clients only real job with an HTTP-only cookie is to store the cookie itself\n","n":0.056}}},{"i":1399,"$":{"0":{"v":"DOM","n":1},"1":{"v":"\nThe DOM is just a big nested object that can be represented as a tree:\n![](./assets/images/2021-11-06-09-53-30.png)\n\nAnother way to view the tree is like this, which is a more accurate representation of what it actually looks like:\n![](./assets/images/2021-11-06-09-54-44.png)\n\nHTML becomes the DOM after the browser parses it.\n\nDOM is an API for HTML or XML documents and it creates a logical structure which can be accessed and manipulated.\n\nThe DOM was created so we could have a standard API that could be used with any programming language.\n\nThis object would include everything that's inside the scope of your browser, including `window`, `navigator`, `document`, global variables\n\nThe inspector tab looks like HTML, but in fact it is just an HTML-like representation of the DOM. The reason it looks like HTML is because someone thought it would be a good idea to show the DOM as HTML tags instead of objects. This makes sense, since the developer writes HTML, which is parsed into the DOM, and it looks similar to how the HTML was authored.\n","n":0.078}}},{"i":1400,"$":{"0":{"v":"Shadow DOM","n":0.707},"1":{"v":"\nShadow DOM is a tool used to build component-based apps and websites.\n- Shadow DOM comes in small pieces, and it doesn’t represent the whole DOM\n- Therefore, it can be seen as a subtree or as a separate DOM for an element.\n\nThe shadow DOM is mostly used when building without a front-end framework.\n\n### Shadow DOM vs Regular DOM\nThe DOM and Shadow DOM differ in how they are created and how they behave.\n- DOM nodes are placed within other elements.\n- Shadow DOM node (a scoped tree) is connected to the element but separated from the children elements\n    - the element it's attached to is called a Shadow Host.\n\nEverything that is added to the Shadow DOM is local, including styles.\n\nIf you have an element in the shadow dom and run `document.querySelector` to find it, it will not be returned.\n\nIf we are using [Web Components](https://developer.mozilla.org/en-US/docs/Web/Web_Components), then we are using the shadow DOM.\n\n* * *\n\nThe shadow DOM API is part of the browser specification, supported by most browser versions post 2018.\n\nShadow DOM should not be confused with the virtual DOM, which is an implementation used in frameworks like React and Vue.\n\nShadow DOM allows hidden DOM trees to be attached to elements in the regular DOM tree — this shadow DOM tree starts with a shadow root, underneath which can be attached to any elements you want, in the same way as the normal DOM.\n","n":0.066}}},{"i":1401,"$":{"0":{"v":"Blockchain","n":1},"1":{"v":"\nThe digital world is inherently reproducible at no cost. Data can be copied freely. The use of a blockchain removes this characteristic of infinite reproducibility from a digital asset.\n- Blockchain represents digital scarcity in the same way that packets Represent digital information\n\nBlockchain takes the concept of open source one step further where it is Open State, meaning not just the source code is shared, but the data (ie. the chain itself) is shared as well\n\nEvery scarce asset is well-suited to be block-chainified\n- ex. Stocks, bonds, mortgages, marriage certificate, passports\n","n":0.106}}},{"i":1402,"$":{"0":{"v":"BitTorrent","n":1},"1":{"v":"\nTorrent is the usual shorthand for BitTorrent\n\nTo download something via BitTorrent is to download parts of the file from various peer nodes in the network. Once all parts of the file are received, they can be reassembled to form the full file.\n\nBitTorrent is a communications protocol for P2P sharing (which is by nature decentralized)\n\nEach node in the BitTorrent network is a BitTorrent client which implements the protocol.\n\n### Terminology\n\nSEEDERS are those who has downloaded the file already or initially only one person who uploads the torrent seeds to others. You may notice that after your download is complete the torrent turns from DOWNLOADING to SEEDING.\n\nPEERS are those who are downloading and uploading at the same time. They do not possess the whole file. They only possess parts of whole.\n\nLEECHERS are those who don’t have all parts of the file and thus are not able to share with you the required part of the file.\n\n### Example: downloading the alphabet\nImagine you're joining the English alphabet, but the letter G is missing (only the original seed, The Library, has a copy of it). So you'll get A-F and H-Z (99% completion), as will anybody else joining - you pass them around until everybody has a copy of everybody else's letters. Eventually you all have the same set, 400 of you all missing G.\n\nThen one year later, The Library rejoins the swarm. By this point, 350 of the original peers have gotten bored and removed the incomplete torrent. The 50 who kept going now get the missing G and have 100% compete copies of the alphabet. Hooray! 25 of them disconnect, as does The Library, but now there are 25 seeds remaining in case any new peers join the swarm. So long as at least one of them stays online, future peers can still get 100% of the file.\n\n### Tracker\nBitTorrent trackers provide a list of files available for transfer and allow the client to find peer users, known as \"seeds\", who may transfer the files.\n\nA BitTorrent tracker is a special type of server that assists in the communication between peers using the BitTorrent protocol.\n\nThe tracker server keeps track of where file copies reside on peer machines (in other words, where the file that other torrent users are trying to download are)\n\nConnecting to a tracker is no longer needed after the initial peer-to-peer file download is started\n\n### Magnet Link\nA magnet link is a type of hyperlink that enables the downloading of files and data from [[P2P|network.internet.p2p]] sharing networks, particularly torrent networks. It works in a server-less environment and contains all the information a torrent client requires to download a specific file. Once the user clicks a magnet link, its data is sent to the desktop torrent client software, which automatically starts the download.\n- It is preferable to a .torrent file because it eliminates the need to download a tracker file and search for uploading peers.\n- Therefore, a magnet link replaces a .torrent file extension/mechanism with only a hyperlink, which consists of the magnet identifier, file name and cryptographic content hash.\n\nThe idea of magnets is instead of downloading the `.torrent` file from a webserver, you download it directly from a seed/leecher. The biggest advantage is that you might be able to download the content of the torrent, even if the tracker is down or closed for registration.\n\nThe main advantage for Bittorrent indexers (e.g. PirateBay) is that they do not have to store the torrents on their servers anymore which could be beneficial for them in several ways. It could reduce the pressure from the media creation industry and reduce hardware infrastructure expenses thanks to less tracking and downloading.\n\n## Resources\n- See the IP address used when downloading a torrent: https://torguard.net/checkmytorrentipaddress.php\n    - right-click on the Green banner and copy the link (it’s a Magnet link) Then add this link into your torrent client and start the torrent. After a few seconds, the site will show the IP address of the connection it finds.\n- VPNs that support Port Forwarding\n    - https://www.reddit.com/r/VPNTorrents/comments/s9f36q/list_of_vpns_that_allow_portforwarding_2022/","n":0.039}}},{"i":1403,"$":{"0":{"v":"Bitcoin","n":1},"1":{"v":"\nA coin is a chain of transaction records, where each record represents each time an owner transferred the coin to a new owner as payment. \n- The latest transaction record in the chain shows the coin's current owner.\n\nEach coin owner has a [[public/private key|crypt.public-key]] pair which the network uses to verify the integrity of transactions.\n\nWhen the current owner of a coin wants to transfer the coin to a new owner, they create a transaction record which contains:\n- The public key of the coin's new owner.\n- A hash of the previous transaction record in the chain.\n- A signature of the above hash signed with the current owner's private key.\n\nThis information allows the new owner to verify that it received the coin from the right owner.\n\n## The Bitcoin Blockchain\nEach block of the [[blockchain|blockchain]] is identified with a [[hash|crypt.hashing]] and contains:\n- The hash of the previous block in the chain.\n- A set of transactions.\n- A nonce\n\nWhen you make a payment, the payee won't accept it until the transaction is in the blockchain. \n- since all transactions are in the blockchain, the payee will find out if the coin has been spent before.\n\n### Adding a block to the blockchain (ie. mining)\nWhen a peer receives new transactions, it collects them into a block. Before it can add the block to the blockchain, it needs to do actual CPU work (called *proof-of-work*).\n\nThe process of finding the *nonce* is calling *mining*\n\nIt takes 10 minutes on average for a peer to mine a new block, which means that the parties involved in a transaction have to wait for about 10 minutes before it appears on the blockchain.\n- A peer broadcasts a block to all peers after finding its proof of work. \n\n# E Resources\n- https://timilearning.com/posts/mit-6.824/lecture-19-bitcoin/","n":0.059}}},{"i":1404,"$":{"0":{"v":"BIOS","n":1},"1":{"v":"\nBios/UEFI serves as an interface between a system's OS and the software\n","n":0.289}}},{"i":1405,"$":{"0":{"v":"Binary","n":1},"1":{"v":"\nBinary data can come in many different formats. For example, let’s consider a binary sequence representing a byte of data: 01110110. If this binary sequence represented a string in English using the ASCII encoding standard, it would be the letter v. However, if our computer was processing an image, that binary sequence could contain information about the color of a pixel.\n- The computer knows to process them differently because the bytes are encoded differently. \n\t- Byte encoding is the format of the byte (e.g. ascii, UTF-8).\n\t- ex. if it's encoded in UTF-8 (as is the case in Node [[buffers|js.node.types.buffer]]), then we know it's a number, letter or symbol.\n\n# Bits and Bytes\n- Bit - a zero or 1\n\t- portmanteau of \"binary digit\"\n- Byte - an atomic unit of digital information. From a memory standpoint, it is the smallest unit that can be stored and read.\n- Though 8 bits make up a byte, there is no reason for that number aside from the benefits gained by raising that number to the power of 2.\n\t- There have been many other instances of varying-bit bytes, such as 10-bit bytes, 4-bit bytes etc.\n- 1 octet equals 1 byte (as long as  the bytes exist in an 8-bit system, which is almost always)\n- by definition, binary numbers ending in 0 are even, and those ending in 1 are odd.\n\n## Bit\nthe number 255 represents 8 bits.\n- Therefore, it is the largest binary number that can be represented by 8 individual bits.\n\t- in binary, it is *11111111*\n\n### Conversion\n- the image below shows how the placement of the bit determines its numerical value on the whole.\n\t- just like in base 10, each placement to the left multiplies the number by 10, in base 2, each placement to the left multiplies the number by 2\n\t- if we add up the value of each numerical position (128, 64...), we will get 255\n![](/assets/images/2021-03-09-21-26-04.png)\n- This chart above can be used to convert to/from binary format\n\t- ex. if we want to represent 135 in binary, we just need to add each slot, with the biggest fitting number first. In this case, the 8th slot, the 3rd slot, the 2nd slot and the 1st slot (or, 10000111)\n\n### How bits are read\n- a bit can be stored by any device that can exist in one of 2 possible states\n\t- ex. lightswitch, presence/absense of a hole in a punchcard, thickness of barcode line, presence of a microscopic pit on a CD ROM\n\t\t- therefore, any of these methods can be used to represent 0's and 1's.\n\t- in the case of modern computers, that bit is stored by way of: electrical pulse, or no electrical pulse\n\n### Bit field\n- a type of data structure that directly stores bits\n- the bit field is made up of adjacently-located memory locations.\n- think of a bit field as if it were an array of bits\n\n* * *\n\nAn 8 bit machine can only show a maximum of 256 Colors on the screen at any given time\n\n* * *\n\nHexadecimal is used in computing as a more compact representation of binary\n- Every hexadecimal digit corresponds to a sequence of four binary digits, since sixteen is the fourth power of two; for example, hexadecimal 7816 is binary 11110002.\n","n":0.043}}},{"i":1406,"$":{"0":{"v":"Encoding","n":1},"1":{"v":"\nPrograms usually work with data in (at least) two different representations:\n1. In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and so on. These data structures are optimized for efficient access and manipulation by the CPU (typically using pointers).\n2. When you want to send some data to another process with which you don't share memory (e.g. write data to a file or send it over the network), you have to encode it as some kind of self-contained sequence of bytes (for example, a JSON document). Since a pointer wouldn't make sense to any other process, this sequence-of-bytes representation looks quite different from the data structures that are normally used in memory.\n\nWe need some kind of translation between the two representations. \n- The translation from the in-memory representation to a byte sequence is called encoding (also known as serialization or marshalling)\n- the reverse is called decoding (parsing, deserialization, unmarshalling).\n\nWe use encoding to ensure that data can transport through protocols and mechanisms designed for ASCII without needing casual modification and without having to worry about things like newlines, brackets, etc.\n\nSome languages come with built-in support for encoding in-memory objects into byte sequences.\n- ex. Pickle in Python, [[Buffer|js.node.types.buffer]] in Node.\n- these libraries allow in-memory objects to be saved and restored with minimal additional code\n\nHowever, these libraries bring drawbacks:\n- The encoding is often tied to a particular programming language, and reading the data in another language is very difficult. If you store or transmit data in such an encoding, you are committing yourself to your current programming language for potentially a very long time\n- In order to restore data in the same object types, the decoding process needs to be able to instantiate arbitrary classes. This is frequently a source of security problems\n\nFor these reasons it's generally a bad idea to use your language's built-in encoding for anything other than very transient purposes.\n- Instead, we should use standardized encodings that can be written and read by many programming languages, like JSON and XML.\n\n* * *\n\nUTF-8 is a superset of ASCII\n- ASCII can encode bytes with uppercase and lowercase English letters, the numbers 0-9, and a few other symbols like the exclamation mark (!) or the ampersand sign (&).\n","n":0.052}}},{"i":1407,"$":{"0":{"v":"Base64","n":1},"1":{"v":"\nBase64 is an encoding scheme for translating binary to and from text. The resulting binary represents the binary data (a sequence of 8-bit bytes)\n\nBase64 encodes binary data in a printable ASCII string format. \n- It is commonly used when there is a need to transmit binary data over a communication medium that does not correctly handle binary data and is designed to deal with textual data only.\n\nBase64 is designed to carry data stored in binary formats across channels that only reliably support text, not binary.\n\nBase64 is prevalent in the World Wide Web.\n- it is commonly used to embed image files (and other binary assets) within textual assets like HTML and CSS files\n\nWe use Base64 to transfer data over the network so that we don't run into nasty garbled text issues related to character encoding (e.g. `qÃ®Ã¼Ã¶:Ã`) \n- This problem happens because different computers across the world are handling strings in different character encodings, such as for example `UTF-8`, `ISO 8859-1`, etc.\n\nBase64 has some key differences with hashing. Notably, if you take the string \"I like dogs\" and hash it with a cryptography function like the one Bcrypt uses, you will get a completely different hash than if you change a single character in the string, for instance to \"I like dogs!\". If we are talking Base64 encoding however, the resultant strings will be almost the same. Observe that the strings are the same, except for the end, which is precicely where we find the `!`:\n- \"I like dogs\" -> \"SSBsaWtlIGRvZ3M=\"\n- \"I like dogs!\" -> \"SSBsaWtlIGRvZ3Mh\"\n\nIn cases where you encounter JSON parsing errors or unexpected behavior with payloads, especially when they contain binary or special characters, using base64 encoding can be a reliable solution to ensure data integrity during transmission and processing.\n\n### Example\nTaking the following string\n> Many hands make light work.\n\nWhen the quote is encoded into Base64, it is represented as a byte sequence of 8-bit-padded ASCII characters encoded in MIME's Base64 scheme:\n> TWFueSBoYW5kcyBtYWtlIGxpZ2h0IHdvcmsu\n\nthe encoded value of `Man` is `TWFu`\n- Encoded in ASCII, the characters `M`, `a`, and `n` are stored as the byte values `77`, `97`, and `110`, which are the 8-bit binary values `01001101`, `01100001`, and `01101110`. These three values are joined together into a 24-bit string, producing `010011010110000101101110`.\n","n":0.052}}},{"i":1408,"$":{"0":{"v":"Bitwise Operators","n":0.707},"1":{"v":"\n*Bitwise operation* - an operation done on a series of bits. Therefore, it is supported directly by the CPU and thus more efficient\n- the nature of bitwise is to treat a value as a series of bits, rather than as a numerical quantity\n- used to manipulate values for comparisons and calculations\n\t- the values must be an array of chars (string), array of ints, or a binary numeral\n- ex. NOT, AND, OR, XOR\n- ex. Say we perform the bitwise operation `4&1`. What happens is that a check is performed on each bit:\n```\n  00000011\n& 00000001\n__________\n  00000001\n```\n\nIt is *not* their main use to substitute arithmetic operations. Cryptography, computer graphics, hash functions, compression algorithms, and network protocols are just some examples where bitwise operations are extremely useful.\n- It is true that in most cases when you multiply an integer by a constant that happens to be a power of two, the compiler optimises it to use the bit-shift.\n\n*Bitmask* (or simply mask) - in the example above (with `&`) we utilized the bitwise operator to make a mask. Basically, what comes after the `&` is the mask, and it specifies which bits from the leftside will \"shine through\".\n- the mask is determined by the bitwise operator (i.e. AND, OR XOR) and the value following it.\n\t- `&` (AND) will extract a subset of bits from the value\n\t- `|` (OR) will set a subset of the bits in the value\n\t```\n\t  00110110\n\t| 11010011\n\t__________\n\t  11110111\t\n\t```\n\t- `^` (XOR) will toggle a subset of the bits in the value\n\t```\n\t  00110110\n\t^ 11010011\n\t__________\n\t  1110001\t\n\t```\n- anal. imagine a mask in Inkscape who's job is to mask anything that isn't skin. The resulting mask would be black wherever there is not skin in the photo. When we apply this mask to the photo, the resulting image would be just skin. \n\n*Shifting* (`<<`/`>>`) - the action of shifting all bits to the left or right. Shifting to the left effectively doubles the value. Conversely, shifting to the right divides the value by 2.\n- when shifting, if there is no bit to take a place, it defaults to 0\n- Declaring the first argument to be unsigned ensures that when it is right-shifted, vacated bits will be filled with zeros, not sign bits\n\n# UE Resources\n[Example: implementing a calendar in JS](https://snook.ca/archives/javascript/creative-use-bitwise-operators)","n":0.052}}},{"i":1409,"$":{"0":{"v":"Babel","n":1},"1":{"v":"\nTwo big things to be aware of in Babel are parsing and transforming:\n\n### Parsing\nParsing is the process of taking source Javascript code and converting it into an [[AST|general.lang.AST]].\n- The AST allows Babel to understand the code's structure and the relationships between different parts of the code. \n\nParsing does not modify or transform the code in any way; it simply analyzes it and creates an AST.\n\n### Transforming\nTransforming is the process of making changes to the AST generated during parsing. In Babel, transforming involves modifying the AST to apply various changes to the code, such as transpiling modern JavaScript features into older versions of JavaScript that are compatible with a wider range of browsers and environments.\n\n* * * \n\n### Gotchas\n- Babel changes do not come into effect until we manually restart our server (e.g. if we are using nodemon or something similar). Therefore, a good rule of thumb is to *always* reset the server when we make changes to our `babel.config.js` file. This way, we don't break something without instantly knowing it.","n":0.077}}},{"i":1410,"$":{"0":{"v":"Azure","n":1},"1":{"v":"\nAzure gives you many different ways to interact with its resources, such as Azure Portal, Azure SDKs, az (command-line tool) etc.\n\n## Deployment\n- To deploy Functions to the cloud, we need to create 3 different resources: Resource group, storage account, and function app\n\n### Storage Account\n- maintains state and other information about your projects\n- created with command `az storage account create`\n\n### Function app\n- provides the environment for executing your function code\n- A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources\n- created with command `az functionapp create`\n\t- When running this command, we also determine the `<APP_NAME>`. This will serve as the default DNS domain for the function app (ie. we invoke the function with an HTTP request to `<APP_NAME>.azurewebsites.net`)\n\n* * *\n\n## Artifacts\n- Artifacts allows us to create/share/manage dependencies (ex. npm packages) in our project (client-read, client-publisher)\n\t- usually we explicitly define each feed that we pull our package from. Artifacts allows us to use a single point of entry for multiple existing feeds.\n- Github Package Manager is analogous to Artifacts\n\nArtifact Feed\n- \"feed\" is an Azure-specific term\n- a feed is a construct that allows us to store, manage and group packages (like npm), and control who we share it with.\n- a feed is like an npm registry, similar to an endpoint that specifies where these packages can be found.\n- a feed gives us access to a collection of packages.\n- a feed is made up of artifacts\n\nBuild Artifacts\n- build artifact is the output of running the azure-pipeline.yml CI file.\n- Build Artifacts are different than Artifacts\n- ex. build artifact is the result of taking some input data, processing it in some way, then stamping it with a commit SHA as well as a build #, letting us track it.\n\nAzure has 2 types of pipelines: build pipeline and release pipeline.\n- the build pipeline is CI (build, test and create artifacts), while release pipeline is CD\n* * *\n\n### Resource Manager\nWhen a user sends a request from any of the Azure tools, APIs, or SDKs, Resource Manager receives the request. Then it:\n1. authenticates and authorizes the request\n2. sends the request to the Azure service\n- since it is a central API that all requests pass through, we can see logs and everything related to these services and the requests they receive.\n- the Resource Manager allows us to declaratively manage resources (as opposed to having to use scripts)\n\n#### Scope\nAzure provides four levels of scope: management groups, subscriptions, resource groups, and resources.\n![](/assets/images/2021-03-08-21-28-23.png)\n- management settings can be applied to any of these levels of scope, and the level we choose will determine how widely the setting is applied (lower levels inherit from higher).\n\n## Terminology\n**Resource**\n- Any manageable item that is available through Azure.\n- ex. VMs, storage accounts, web apps, databases, blob storage, virtual networks, resource groups, management groups etc.\n\n## Questions\n- What would the connection string be a connection to?\n\t- would it be the host of the function?\n","n":0.045}}},{"i":1411,"$":{"0":{"v":"RG (Resource Group)","n":0.577},"1":{"v":"\nA Resource group is a logical container for related resources. We would group together resources that we want to manage together as a group.\n- all resources in a RG should be a part of the same lifecycle, meaning we deploy, update, and delete them together.\n- resource groups can be stored in a different location than the resource that is comprises. This is possible because the location of the RG is merely the location of the RGs metadata about its resources.\n- created with command `az group create`\n- ex. `AzureFunctionsQuickstart-rg`\n\n#### Allow IP address to resource\nFrom within resource panel (ie. database panel) of the resource group, select `connection security` and add your IP address","n":0.095}}},{"i":1412,"$":{"0":{"v":"Axios","n":1},"1":{"v":"\n```ts\nexport class MarketstackDS {\n  private baseUrl = `http://api.marketstack.com/v1/eod?access_key=${MARKETSTACK_API_KEY}`;\n\n  private handleAxiosError(error: AxiosError) {\n    if (error.code === \"ERR_BAD_REQUEST\") {\n      throw new Error(\n        `Received Axios error: ${error.response?.status} ${error.response?.statusText}`\n      );\n    }\n  }\n\n  public async getStockData(tickerSymbol: string) {\n    try {\n      const response = await axios.get(\n        `${this.baseUrl}&symbols=${tickerSymbol}`\n      );\n      return response.data;\n    } catch (error: unknown) {\n      if (error instanceof AxiosError) this.handleAxiosError(error);\n      else console.log(error);\n    }\n  }\n}\n```","n":0.131}}},{"i":1413,"$":{"0":{"v":"Aws","n":1}}},{"i":1414,"$":{"0":{"v":"AWS Cloud Terms","n":0.577},"1":{"v":"\n### Resource\na resource is an entity that you can work with. Put another way, it is an instance of an AWS object.\n- ex. [[EC2|aws.svc.EC2]] instance, a [[CloudFormation|aws.svc.cloud-formation]] stack, or an [[S3|aws.svc.S3]] bucket.\n\n### Resource Group\nAllow us to manage multiple resources together as a group, so we don't have to move from one AWS service to another for each task.\n- If you manage large numbers of related resources, such as EC2 instances that make up an application layer, you likely need to perform bulk actions on these resources at one time\n\t- ex. applying updates/patches, upgrading applications, opening/closing ports to network traffic, collecting logs from machine etc.\n\nAll resources in a resource group are in the same AWS Region.\n\n### ARN (Amazon Resource Name)\nUniquely identify an AWS resource. Therefore, it is identifiable across all of AWS.\n- shows region and account number.\n\nFormat:\n```\narn:partition:service:region:account-id:resource-type:resource-id\n```\n\nExample:\n```\narn:aws:s3:::my_corporate_bucket/Development/*\narn:aws:ec2:us-east-1:4575734578134:instance/i-054dsfg34gdsfg38\n```\n","n":0.086}}},{"i":1415,"$":{"0":{"v":"Identity Federation","n":0.707},"1":{"v":"\nIdentity federation is a system of trust between two parties for the purpose of authenticating users.\n- the SP trusts the IdP to authenticate users and relies on the information provided by the IdP about them.\n\nFederated identities are users that can access secure AWS account resources with external identities. \n- External identities can come from a corporate identity store (such as LDAP or Windows Active Directory) or from a third party (such as Login in with Amazon, Facebook, or Google). \n- Federated identities do not sign in with the AWS Management Console or AWS access portal. The type of external identity in use determines how federated identities sign in.\n\nIdentity Federation is a way that we can manage [[federated identity|security.federated-identity]] in AWS.\n\nThe problem Identity Federation solves is, \"how do we grapple with the fact that we have multitudes of different employees, contractors\n\n- an identity provider (IdP) is responsible for user authentication.\n- a service provider (SP) controls access to resources.\n\t- an SP might be a service or application\n\nAfter authenticating a user, the IdP sends the SP a message, called an assertion, containing the user's sign-in name and other attributes that the SP needs to establish a session with the user and to determine the scope of resource access that the SP should grant.\n\nFederation is a common approach to building access control systems which manage users centrally within a central IdP and govern their access to multiple applications and services acting as SPs.\n\nAWS SSO and [[AWS|aws.terms.IAM]] are two tools from AWS that allow us to [[federate|general.terms.federated]] our workforce into AWS accounts and business applications.\n","n":0.062}}},{"i":1416,"$":{"0":{"v":"IAM (Identity and Access Management)","n":0.447},"1":{"v":"\n## IAM Identity\nAn IAM identity provides access to the resources under management by an AWS account.\n\nAn IAM identity represents a user, and can be authenticated and then authorized to perform actions in AWS.\n\nEach IAM identity can be associated with one or more policies.\n- Policies determine what actions a user, role, or member of a user group can perform, on which AWS resources, and under what conditions.\n\n### User\nAn IAM user is an entity that you create in AWS.\n- The IAM user uniquely represents the person or service who uses the IAM user to interact with AWS.\n\nA primary use for IAM users is to give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI.\n\nA user in AWS consists of a name, a password, and up to 2 access keys used to access the API/CLI.\n\nWhen you create an IAM user, you grant it permissions by making it a member of a user group that has appropriate permission policies attached\n- you can also directly attaching policies to the user, though this is less recommended.\n\nWhen you create an IAM user, you can choose to allow console or programmatic access. \n- if console access is allowed, the IAM user can sign in to the console using a user name and password. \n- if programmatic access is allowed, the user can use access keys to work with the CLI or API.\n\n#### Use an IAM user when...\n- You created an AWS account and you're the only person who works in your account.\n\t- In this case, create an IAM user for yourself and use the credentials for that user when you work with AWS.\n- Other people in your user group need to work in your AWS account, and your user group is using no other identity mechanism.\n\t- In this case, create IAM users for the individuals who need access to your AWS resources, assign appropriate permissions to each user, and give each user his or her own credentials.\n\n### User Group\nA user group is a collection of IAM users managed as a unit, allowing us to specify permissions for a collection of users\n- ex. you could have a user group called `Admins` and give that user group the types of permissions that administrators typically need\n\n### Role\nA role is a collection of policies which can be applied to a user / other service.\n\nA role can only be added at time of instance and user creation though. \n- A role cannot be added after the fact (though permissions for that role can be)\n\nAn IAM role is very similar to a user, in that it is an identity with permission policies that determine what the identity can and cannot do in AWS.\n- However, a role does not have any credentials (password or access keys) associated with it.\n\nOnce a role is given, it can interact with AWS via a CLI or SDK and does not need to authenticate or provide credentials (which is all done behind the scenes)\n\nA user can also assume a role, either from the same AWS account (via federated login with identity provider) or a different AWS account (via `AssumeRole`)\n\nA role is intended to be used by anyone who needs it, and therefore it is not necessarily associated with a single person.\n- that is, an IAM user can assume a role to temporarily take on different permissions for a specific task.\n\nIf we didn't have roles, the client (potentially itself a AWS service) accessing another AWS service would need to hold the access key and secret key of a user, and those credentials would be used to access the AWS resource\n\n#### Use an IAM role when...\n- You're creating an application that runs on an [[aws.svc.EC2]] instance, and it makes requests to AWS.\n- You're creating an app that runs on a mobile phone and that makes requests to AWS.\n- Users in your company are authenticated in your corporate network and want to be able to use AWS without having to sign in again—that is, you want to allow users to federate into AWS.\n\t- In this case, don't create IAM users. Configure a federation relationship between your enterprise identity system and AWS.\n\n### Policy\nA policy is a whitelist of permissions for an [action](https://docs.aws.amazon.com/IAM/latest/APIReference/API_Operations.html). We then take that policy and attach it to an IAM identity (ie. user, user group or role)\n- ex. `AmazonGlacierFullAccess`, `AmazonGlacierReadOnlyAccess`, `AWSLambdaRole`\n- it does not matter if the resource is accessed from the AWS Management Console, the AWS CLI, or the AWS API.\n- e.g. actions: `secretsmanager:GetSecretValue`\n\nWhen an *IAM principal* (user or role) makes a request, AWS will evaluate the policy and determine if the request is allowed or not.\n\nPolicies are stored as JSON documents, like so:\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"lambda:InvokeFunction\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n```\n\nAWS supports six types of policies ([docs](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types)): \n- identity-based policies\n- resource-based policies\n- permissions boundaries\n- Organizations SCPs\n- access control lists (ACLs)\n- session policies\n\nPolicies may be AWS managed, meaning they are created by AWS, or they may be Customer managed, meaning they are created by the AWS root user. In the latter case, we create the JSON policy ourselves, defining the permissions that it would grant when attached to an entity.\n\n#### Identity-based Policies\nIdentity-based policies are JSON permissions policy documents that control what actions an identity (users, groups of users, and roles) can perform, on which resources, and under what conditions.\n\nIdentity-based policies can be either:\n- Managed - standalone policies that can be attached to multiple users, groups and roles of the AWS account. They can either be managed by AWS or by the account holder.\n- Inline - policies that get added directly to a single user, group or role (ie. one-to-one relationship).\n\n#### Resource-based Policies\nJSON policy documents that we attach to a resource (e.g. S3 bucket)\n- this grants the specified principal (ie. user/role) permission to perform specific actions on that resource and defines under what conditions this applies\n\n## Gotchas\n- IAM role names and policy names exist at a global namespace (IAM namespace), so if we had [[lambdas|aws.svc.lambda]] in both us-east-1 and us-west-2 and wanted to create IAM policies for each, we'd have to name the policies/roles differently (e.g. add a region suffix).\n\n### AWS IAM vs AWS IAM Identity Center\nThese are 2 different AWS console applications\n\n*AWS IAM* manages access to AWS services and resources within an AWS account securely for entities (e.g. IAM users)\n\n*AWS Identity Center* manages access to all AWS services and resources within an AWS organization, as well as access to other cloud applications, e.g., Salesforce.\n- ideal for managing multiple AWS accounts\n- This service is the successor to *AWS Single Sign-on*\n\n# E Resources\n[AWS Docs](https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html)\n","n":0.03}}},{"i":1417,"$":{"0":{"v":"AWS Services","n":0.707},"1":{"v":"\n### Messaging Service\nAWS offers 3 principle messaging services:\n- [[SQS|aws.svc.SQS]]\n- [[SNS|aws.svc.SNS]]\n- [[EventBridge|aws.svc.event-bridge]]\n\n### Container Service\nAWS offers managed and unmanaged solutions\n- [[ECS|aws.svc.ECS]]\n- EKS\n\n[docs](https://aws.amazon.com/blogs/containers/amazon-ecs-vs-amazon-eks-making-sense-of-aws-container-services/)\n","n":0.224}}},{"i":1418,"$":{"0":{"v":"Xray","n":1},"1":{"v":"\nThe purpose of AWS X-Ray is to analyze and debug distributed applications. It helps us understand how the application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors.\n\nX-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.\n\nThe service collects information at each execution step of your Serverless application and then makes that information available through log files and visual maps and diagrams.\n- a good use case is to use it with [[lambdas|aws.svc.lambda]]. The data from X-ray can get ingested into a performance monitoring tool like Datadog, so that it gets treated like any other service in terms of APM coverage.\n\nX-ray gets enabled at the infrastructure level (e.g. [[terraform]], [[serverless-framework]]), as well as at the application level.\n","n":0.085}}},{"i":1419,"$":{"0":{"v":"Step Functions","n":0.707},"1":{"v":"\nAWS Step Functions is a low-code way to implement event-driven workflows that connect services, systems or people.\n- We can do this with Workflow Studio, giving us a drag and drop interface.\n\nAny AWS service can be used in the steps of the workflow.\n\nStep Functions manages state, checkpoints, and restarts for you to make sure that your workflows run in order and as expected.\n\nStep Functions offers two workflow types- Standard or Express- that can be used depending on your specific use case.\n- Standard Workflows are used to manage long-running workloads.\n- Express Workflows support high-volume event processing workloads.\n\nCan be used for:\n- building distributed applications\n- automating IT and business processes\n- build data and machine learning pipelines using AWS services\n\nFailures, retries, parallelization, service integrations, and observability are all handled out of the box for us, allowing us to focus on high-level business logic.\n","n":0.085}}},{"i":1420,"$":{"0":{"v":"SAM","n":1},"1":{"v":"\n## Overview\nAWS SAM is a toolkit around improving the developer experience of building and running serverless apps.\n\nConsists of 2 primary parts:\n1. SAM template specification\n2. SAM CLI\n\n### SAM Template Specification\nAn open-source framework for defining and managing our serverless application infrastructure code.\n\nSAM template specification is built on [[CloudFormation|aws.svc.cloud-formation]]\n- therefore we can use AWS CloudFormation syntax directly within our SAM template\n- but because it is an extension of CloudFormation, SAM offers its own unique syntax to be combined with the syntax of CloudFormation.\n\nTo provision the infrastructure specified in the template, it is deployed to CloudFormation where the SAM template syntax will be transformed into CloudFormation syntax.\n- 23 lines of SAM template code will produce 200+ lines of CloudFormation code.\n\n#### Example: Simple READ app\nImagine we want to make a simple application consisting of a lambda (exposed through API Gateway) which gets some records from DynamoDB.\n\nOur application's SAM template can be made to define:\n- the lambda\n- the API Gateway\n- the DynamoDB\n- the [[IAM|aws.terms.IAM]] permissions necessary for these services to interact with one another\n\n### SAM CLI\nSAM CLI is a command line tool to be used with SAM templates to build and run serverless applications.\n- Quickly initialize a new application project.\n    - `sam init`\n- Build your application for deployment, generating the `.aws-sam` directory (which includes all of our code and dependencies)\n    - `sam build`\n- Perform local debugging and testing, including allowing us to simulate events, test APIs, invoke functions, etc.\n    - `sam local invoke`\n- Deploy your application.\n    - `sam deploy --guided`\n- Configure CI/CD deployment pipelines.\n- Monitor and troubleshoot your application in the cloud.\n    - `sam list` to view deployed resources\n    - `sam logs`\n- Sync local changes to the cloud as you develop.\n    - `sam sync --watch` to automatically detect changes and deploy updates to the cloud\n\nSAM CLI can also work with third-party products such as [[Terraform|terraform]]\n","n":0.058}}},{"i":1421,"$":{"0":{"v":"Route53","n":1},"1":{"v":"\nRoute53 is a [[dns]] provider.\n\nIt allows us to route end users to Internet applications by translating names like www.example.com to IP addresses.\n\neffectively, Route53 connects user requests to infrastructure running in AWS, whether it be an [[aws.svc.EC2]] instance, [[aws.svc.S3]] bucket etc.\n- it can also be used to route users to infrastructure outside of AWS.\n\nBecause Route 53 knows about AWS resources, you can do things like run health checks against load balancers, configure geo-routing based on specific AWS resources, etc.\n\nA common way to do it would be to use Route53 for internal infrastructure, and something like Cloudflare for public facing resources.\n","n":0.101}}},{"i":1422,"$":{"0":{"v":"Redshift","n":1},"1":{"v":"\nAmazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver the best price performance at any scale.\n","n":0.174}}},{"i":1423,"$":{"0":{"v":"AWS Lambda","n":0.707},"1":{"v":"\nA Lambda is like an anonymous function (or a callback) that runs code in response to events.\n- Think of them like event handlers, but for web services, not components within a webpage\n\nEffectively, everything is abstracted away aside from a function interface.\n\n## Concepts\n### Function\n![[aws.svc.lambda.fn]]\n\n### Triggers\nA trigger is a resource or configuration that invokes a Lambda function. Triggers include AWS services that you can configure to invoke a function and event source mappings.\n\nA trigger can be configured to invoke your function in response to lifecycle events, external requests, or on a schedule.\n\nEach trigger acts as a client invoking your function independently. Each event that Lambda passes to your function has data from only one client or trigger.\n\n### Event\nAn event is a JSON-formatted document that contains data for a Lambda function to process. The runtime converts the event to an object and passes it to your function code.\n\n![[aws.svc.lambda.event]]\n\n## Lambda Runtime\nProcess:\n1. Our Lambdas will be stored as zipped files in [[aws.svc.S3]].\n2. The execution environment will be started (e.g. Node runtime, Python, C# etc.)\n3. execute initialization code (the code outside the main `handler` function of the Lambda, which includes imports)\n\t- variables defined here will live in between invocations of our Lambda\n4. execute the `handler` code\n\n### Execution Environment\nLambda provides a secure and isolated runtime environment for your Lambda function. It manages the processes and resources that are required to run the function.\n\nThe execution environment provides lifecycle support for the function and for any extensions associated with your function.\n\n[more on Lambda Execution Environment](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html)\n\n### Deployment\nYou deploy your Lambda function code using a deployment package. Lambda supports two types of deployment packages:\n- A .zip file archive that contains your function code and its dependencies.\n- Your Lambda + its dependencies as a [[Docker image|docker.image]]. You can then load that container into something like AWS Elastic Container Registry (ECR)\n\t- Dockerizing Lambdas is a slower approach.\n\n## Cold start\nOnly 1, 2 and 3 of the above process are relevant to cold starts.\n\nLambas will stay alive for ~5-30 min, but the time-frame is highly variable and unpredictable.\n\nA cold start happens any time a new Lambda instance is created\n- in other words, going from 0 Lambdas to 1 Lambda is a single cold start, just like going from 499 to 500 is a single cold start.\n\nCold starts are essentially unavoidable, and all we can do is reduce the frequency of their occurrence.\n\n### Concurrency controls\nThere are 2 types of concurrency controls available: provisioned concurrency and reserved concurrency\n\n#### Provisioned Concurrency\nThis is an AWS setting that allows us to keep a set number of Lambdas warm.\n- there is a cost associated with keeping Lambdas warm, and if the load exceeds the number of Lambdas we've set to keep warm, we will still have cold starts.\n\n#### Reserved Concurrency\nThis lets us guarantee a maximum number of concurrent instances for a function. When a function has reserved concurrency, no other function can use that concurrency.\n\n* * *\n\n### Lambda Extensions\nWe can include extensions to support our lambdas\n![[aws.svc.lambda.extensions]]\n\n### Concurrency\n[[Concurrency|general.concurrency]] is the number of requests that your function is serving at any given time. When your function is invoked, Lambda provisions an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is provisioned, increasing the function's concurrency.\n\n## Features\n### Event Source Mapping\nAn event source mapping is a resource in Lambda that reads items from a stream or queue and sends the items to your function in batches.\n- Each event that your function processes can contain hundreds or thousands of items.\n\nLambda reads events from [[Kinesis|aws.svc.kinesis]], [[Dynamoc|aws.svc.dynamo]], [[SQS|aws.svc.SQS]], among a few others.\n\nYou can use event source mappings to process items from a stream or queue in services that don't invoke Lambda functions directly\n\nEvent source mappings maintain a local queue of unprocessed items and handle retries if the function returns an error or is throttled.\n\n### Destination\nA destination is an AWS resource that receives invocation records for a function\n- The invocation record contains details about the request and response in JSON format, including the function's response, and the reason that the record was sent.\n\t- this is something we'd want to send to the [[DLQ|general.patterns.messaging.DLQ]] in the event a message fails to be sent.\n\nWe can configure separate destinations depending on whether invocations pass or fail.\n\n* * *\n\n### How it works\nLambda provides a Programming Model, which is common to all runtimes (Node, C# etc). It defines the interface between your code and the Lambda system:\n- You tell Lambda the entry point to your function by defining a handler in the function configuration. The runtime passes in objects to the handler that contain the invocation event and the context (such as the function name and request ID).\n\nWhen the handler finishes processing the first event, the runtime sends it another.\n- The function's class stays in memory, so clients and variables that are declared outside of the handler method in initialization code can be reused.\n- Therefore, to save processing time on subsequent events, create reusable resources like AWS SDK clients during initialization. Once initialized, each instance of your function can process thousands of requests.\n\n\n### When to use\nUnless going with a serverless architecture, Lambda is most suitable for small snippets of code that rarely change.\n- think of Lambda functions as part of the infrastructure rather than part of the application\n\t- The justification for adhering to this principle diminishes when we consider advancements like IaC (ex. [[Terraform|terraform]]).\n\nLambda can be used as a plugin system for other AWS services, for example:\n- S3 doesn’t come with an API to resize an image after uploading it to a bucket, but with Lambda, you can add that capability to S3.\n\n### Limitations\n- Your functions will suffer a cold start when a function is invoked after a period of inactivity\n- limit of 250 MB for your code bundle, including all your dependencies. This limit should not present an issue.\n\n* * *\n\n### Lambda Application\nAn AWS Lambda application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks.\n- With it, you can use [[CloudFormation|aws.svc.cloud-formation]] and other tools to collect your application's components into a single package that can be deployed and managed as one resource.\n\nApplications make your Lambda projects portable\n\n* * *\n\n## Misc\nYour function also has access to local storage in the /tmp directory. Instances of your function that are serving requests remain active for a few hours before being recycled.\n\nThe runtime captures logs from your function and sends it to Amazon CloudWatch Logs. We get:\n- the return value of the function.\n- entries detailing when functions were invoked, and when they failed.\n\nLambdas should be considered stateless, though we can use local storage and class-level objects to increase performance\n\nBy default, Lambda runs your functions in a secure VPC. Lambda owns this VPC, which isn't connected to your account's default VPC.\n- When you connect a function to a VPC in your account, the function can't access the internet unless your VPC provides access.\n\nYou can configure a function to mount an Amazon Elastic File System (Amazon EFS) file system to a local directory. With Amazon EFS, your function code can access and modify shared resources safely and at high concurrency.\n\nLambda provides runtimes for .NET (PowerShell, C\\#), Go, Java, Node.js, Python, and Ruby.\n\nIt may take 5 to 10 minutes for logs to show up in [[Cloudwatch|aws.svc.cloudwatch]] after a Lambda function invocation.\n\nLambdas are billed by GB/second, which means if we allocate more memory to our Lambdas, the cost/second will be higher.\n\nWhen using [[CommonJS|js.lang.imports#commonjs-imports,1:#*]], barrel files (`index.js` file that `exports` all of the modules within the same directory) are potentially problematic with projects that implement Lambdas, since every module exported from the `index.js` file will be imported when we try to import a module.\n\n## Resources\n- [always allocate at least 1024mb](https://blog.neverendingqs.com/2021/04/16/aws-lambda-always-allocate-1024-mb.html)","n":0.028}}},{"i":1424,"$":{"0":{"v":"Lambda Layer","n":0.707},"1":{"v":"\n## What is it?\nA layer is a `.zip` file archive that can contain additional code or data.\n- it may contain libraries (e.g. node modules), a custom runtime, data, or configuration files.\n- think of it in much same way as how a [[docker image|docker.image]] is built using layers, or how [[Kustomize|k8s.tools.kustomize]] is used to reduce redunant code between configurations. The rationale is the same.\n\n## Why use it?\nA key benefit of Lambda layers is that the layers stay warm (ie. initialized, eliminating the need for a cold start)\n- the potential for this as a performance enhancement is huge, because if our Lambda uses many third-party dependencies, they can be aggregated into a layer (or many layers) and they won't have to be downloaded on cold starts.\n\nLambda layers provide a convenient way to package libraries and other dependencies that you can use with your Lambda functions.\n\nUsing layers reduces the size of uploaded deployment archives, making it faster to deploy the code.\n\nLayers also promote code sharing and separation of responsibilities\n\n## How does it work?\n- When you add a layer to a function, Lambda extracts the layer contents into the /opt directory in your function’s execution environment\n\n### Versioning\nWhen you create a new layer, Lambda creates a new layer version with a version number of 1. Each time you publish an update to the layer, Lambda increments the version number and creates a new layer version.\n- Every layer version is identified by a unique ARN. Therefore, when adding a layer to the function, you must specify the exact layer version you want to use.\n\n### Using Layers\nthe general process of creating and using layers involves these three steps:\n1. Package your layer content into a `.zip` file\n2. Create the layer in Lambda AWS console (or via Lambda API).\n3. Add the layer to your function(s).\n\nWhen you create a layer, you must bundle all its content into a .zip file archive. You upload the .zip file archive to your layer from Amazon Simple Storage Service (Amazon S3) or your local machine. Lambda extracts the layer contents into the /opt directory when setting up the execution environment for the function.\n","n":0.054}}},{"i":1425,"$":{"0":{"v":"Lambda Function","n":0.707},"1":{"v":"\nA function is a resource that you can invoke to run your code in Lambda. A function has code to process the events that you pass into the function or that other AWS services send to the function.\n\nFunctions can be invoked directly with...\n- the Lambda console\n- the Lambda API\n- AWS SDK\n- AWS CLI\n- AWS toolkits\n\nFunctions can also be invoked *indirectly*...\n- by configuring other AWS services to invoke it\n\t- ex. invoke your function when an object is created in [[S3|aws.svc.S3]]\n- configuring Lambda to read from a stream or queue and invoke it.\n\n### Sync vs Async Invocation\n- With synchronous invocation, you wait for the function to process the event and return a response.\n- With asynchronous invocation, Lambda queues the event for processing and returns a response immediately.\n\t- Therefore, in [[Node|js.node]] it returns a [[Promise|js.lang.promises]]\n\nWhen you invoke a function synchronously, you receive errors in the response and can retry. When you invoke asynchronously, use an event source mapping, or configure another service to invoke your function, the retry requirements and the way that your function scales to handle large numbers of events can vary.\n\nWhen you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors, and can send invocation records to a downstream resource to chain together components of your application.\n\nYou can also configure Lambda to send an invocation record to another service. Lambda supports the following destinations for asynchronous invocation.\n- an [[SQS|aws.svc.SQS]] queue\n- an [[aws.svc.SNS]] topic\n- a [[Lambda|aws.svc.lambda]] function\n- Amazon EventBridge – An EventBridge event bus.\n\nSeveral AWS services, such as [[S3|aws.svc.S3]] and [[SNS|aws.svc.SNS]] invoke functions asynchronously to process events.\n","n":0.06}}},{"i":1426,"$":{"0":{"v":"Lambda Extensions","n":0.707},"1":{"v":"\nLambda Extensions API can be used to integrate Lambda with their preferred tools for monitoring, observability, security, and governance.\n\nYour extension can register for function and execution environment lifecycle events. In response to these events, you can start new processes, run logic, and control and participate in all phases of the Lambda lifecycle: initialization, invocation, and shutdown.\n\nAn extension runs as an independent process in the execution environment and can continue to run after the function invocation is fully processed.\n- Because extensions run as processes, you can write them in a different language than the function, though a compiled language is recommended so the extension is a self-contained binary.\n","n":0.097}}},{"i":1427,"$":{"0":{"v":"Lambda Event","n":0.707},"1":{"v":"\nWhen we invoke a function, we determine the structure and contents of the event:\n```json\n{\n  \"TemperatureK\": 281,\n  \"WindKmh\": -3,\n  \"HumidityPct\": 0.55,\n  \"PressureHPa\": 1020\n}\n```\n\nWhen an AWS service invokes your function, the service defines the shape of the event:\n```json\n{\n  \"Records\": [\n    {\n      \"Sns\": {\n        \"Timestamp\": \"2019-01-02T12:45:07.000Z\",\n        \"Signature\": \"tcc6faL2yUC6dgZdmrwh1Y4cGa/ebXEkAi6RibDsvpi+tE/1+82j...65r==\",\n        \"MessageId\": \"95df01b4-ee98-5cb9-9903-4c221d41eb5e\",\n        \"Message\": \"Hello from SNS!\",\n        // ...\n```\n","n":0.137}}},{"i":1428,"$":{"0":{"v":"Lambda CLI","n":0.707},"1":{"v":"\n### Invoke a Lambda\n```sh\naws lambda invoke \\\n\t--function-name <name/arn> \\\n\t--payload '{ \"name\": \"Bob\" }' \\\n\t--cli-binary-format raw-in-base64-out \\\n\t# --invocation-type <RequestResponse (sync) | Event (async)>\n\tresponse.json\n```\n\n* * *\n\n## Flags\nGet back base64 logs from command\n```sh\n--log-type Tail \\\n```\n","n":0.177}}},{"i":1429,"$":{"0":{"v":"Lake Formation","n":0.707},"1":{"v":"\nA fully managed [[Lake|data.lake]].\n\nAll we need to do is define data sources and what access and security policies you want to apply.\n- Lake Formation then helps you collect and catalog data from databases and object storage, move the data into your new [[S3|aws.svc.S3]] data lake, clean and classify your data using ML algorithms, and secure access to your sensitive data using granular controls at the column, row, and cell-levels.\n","n":0.12}}},{"i":1430,"$":{"0":{"v":"Kinesis","n":1},"1":{"v":"\nA highly-durable linked list in the cloud\n\nUse-cases are similar to SQS— you would typically use either Kinesis or SQS when you want to enqueue records for asynchronous processing.\n- use-cases are also similar to [[kafka]]\n\nDifference with SQS:\n- SQS can only have one consumer, while Kinesis can have many.\n- Once an SQS message gets consumed, it gets deleted from the queue. But Kinesis records get added to a list in a stable order, and any number of consumers can read a copy of the stream by keeping a cursor over this never-ending list.\n- Multiple consumers don’t affect each other, and if one falls behind, it doesn’t slow down the other consumers.\n- Whenever consumers read data out of Kinesis, they will always get their records in the same order.\n- Often cheaper than SQS\n- Kinesis can carry a significant operational burden with the need to provision capacity (shards).\n","n":0.083}}},{"i":1431,"$":{"0":{"v":"Fargate","n":1},"1":{"v":"\n# Fargate\n- serverless compute engine for containers\n- adds about 20% in price, but removes a lot of the admin overhead\n- removes the need to provision and manage servers\n\t- don't have to worry about scaling, patching, securing, and managing servers\n- Fargate automates how much computing power you need and will scale up/down automatically\n- with Fargate, you only interact with your containers\n![](/assets/images/2021-03-08-21-25-27.png)\n\nFargate isn't typically used for long running stuff, as it can get quite expensive quite fast.\n\n### Differences with ECS\nWith a virtual machine, someone still has to manage the hardware, but with EC2 that someone is AWS and you never even see the hardware.\n\nWith ECS on EC2, someone still has to manage the instances, but with ECS on Fargate that someone is AWS and you never even see the EC2 instances.\n\nECS has a “launch type” of either EC2 (if you want to manage the instances yourself) or Fargate (if you want AWS to manage the instances).","n":0.081}}},{"i":1432,"$":{"0":{"v":"EventBridge","n":1},"1":{"v":"\nEventBridge is a serverless [[event bus|general.arch.SOA.bus]], implementing the [[pub/sub|general.patterns.messaging.pubsub]] architectural pattern.\n- goal is to make it easier to build event-driven apps at scale using events originating from our applications.\n\nEventBridge logically separates routing using event buses, and you implement the routing logic using rules (ie. it has a built-in rules engine).\n- You can filter and transform incoming messages at the service level, and route events to multiple targets\n\t- Matched events can be sent to a host of target destination, such as [[Lambdas|aws.svc.lambda]], other Event Bridges, or even HTTP endpoints.\n- It also comes with schema registry\n\nEventBridge may be used to help us implement an [[Event-driven architecture|general.arch.event]]\n\nData Sources may include:\n- our own applications\n- SaaS applications ([list of 3rd party integrations](https://aws.amazon.com/eventbridge/integrations/))\n\t- meaning we can authorize supported SaaS providers to send events directly from their EventBridge event bus to partner event buses in your account.\n- AWS services\n\nTargets may include:\n- HTTP endpoints\n- other AWS services (e.g. other event buses, [[Lambdas|aws.svc.lambda]], [[Cloudwatch|aws.svc.cloudwatch]])\n\nA single bus of EventBridge can handle tens of thousands of concurrent events, so it is conceivable that you'd only have a single bus for one business case.\n- In other words, each user of your application wouldn't need have their own associated bus; all events no matter which users they are associated with can exist within a single bus.\n\nEventBridge provides at-least-once event delivery to targets, including retry with exponential backoff for up to 24 hours.\n\n### How it works\nEventBridge receives an event and applies a rule to route the event to a target.\n- Each event contains a json event body, as well as several metadata fields.\n\nWe the programmer provide EventBridge with a list of rules, which it uses to match events to targets\n- These matches are made based on either [event patterns](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-patterns.html), or schedules.\n\t- ex. when an [[EC2|aws.svc.EC2]] instance changes from pending to running, you can have a rule that sends the event to a Lambda function.\n\nAll events that arrive at the doorstep of EventBridge will be associated with a particular [event bus](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-bus.html)\n- rules are tied to one specific event bus.\n- EventBridge comes with a default event bus out of the box, which receives events from AWS services.\n\nEvent bridge doesn't enforce ordering.\n- if ordering is something we need, [[kafka]] might be a more suitable alternative\n\n* * *\n\n### Targets\nTargets allow us to do things like invoke a Lambda, start execution on a [[step function|aws.svc.step-functions]], start a task with [[ESC|aws.svc.ECS]] or [[Fargate|aws.svc.fargate]]\n\nThe event bridge can be provisioned and manipulated programmatically to route incoming events to various destinations.\n\nThe [[rules engine|general.arch.SOA.rules-engine]] allows for advanced pattern matching based on source, subject, version, or any key of the JSON body. Matched events can then be sent to a host of target destinations including Lambdas, other EventBridges, or even HTTP endpoints.\n\n### Rules\nThe rules engine provides the business logic of our EventBridge. This is where we (the configurer of EventBridge) can create rules that match selected events in the stream and route them to targets to take action.\n\nThis is where we the configurer of EventBridge determine which events we are interested in, and what we will do when we encounter those events.\n- namely, the event is routed to the target(s) associated with that rule.\n\nYou can also use rules to take action on a predetermined schedule. For example, you can configure rules to:\n- Automatically invoke an Lambda function to update DNS entries when an event notifies you that Amazon EC2 instance enters the running state.\n- Direct specific API records from CloudTrail to an Amazon Kinesis data stream for detailed analysis of potential security or availability risks.\n- Periodically invoke a built-in target to create a snapshot of an Amazon EBS volume.\n\n\n### Schema Registry\nEventBridge has a built-in [schema registry](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-schema.html)\n\n### Competing Solutions\n- Google PubSub\n","n":0.041}}},{"i":1433,"$":{"0":{"v":"EventBridge Testing","n":0.707},"1":{"v":"\n## Challenges\n- PubSub is inherently asynchronous. Your test can’t just make an API request and then perform an assertion on the response.\n- EventBridge has no persistent storage of events that you can query via its API.\n- How do you control the side effects caused by multiple downstream subscribers when publishing to EventBridge from your tests?\n\nWe want to test for 3 main scenarios:\n- Publisher fails to send event to EventBridge\n- Subscriber does not receive event that it should have\n- Subscriber receives event but fails to process it correctly\n\n## E Resources\n[Test that EventBridge event was fired](https://stackoverflow.com/questions/60743785/test-that-an-aws-eventbridge-or-cloudwatch-event-was-fired)\n","n":0.103}}},{"i":1434,"$":{"0":{"v":"Elastic Beanstalk","n":0.707},"1":{"v":"\nElasticBeanstalk is a PaaS that enables you to deploy and scale applications onto Amazon EC2. You retain ownership and full control over the underlying EC2 instances.\n\nPurpose is to deploy + scale web apps/services built with common web languages (Javascript, PHP etc) onto webservers like Nginx/Apache\n- Elastic beanstalk hosts all the services needed for deployment/scaling\n\nElasticBeanstalk uses [[aws.svc.cloud-formation]] under the hood to manage and provision resources for your application.\n\nElasticBeanstalk Handles things like:\n- Provisioning EC2 instances and scaling up/down as necessary\n- loadbalancing\n- updating platform with latest patches\n- app health monitoring\n- Web server (Nginx)\n\nAlso allows us to reimplement portions of the code as containerized services, allowing us to achieve a more [[distributed|deploy.distributed]] architectural design\n\nAllows to retain control over the resources powering your application, so you can decide on how much you want to manage.\n\nTo get an app working through ElasticBeanstalk, we essentially...\n1. upload source code bundle to Elastic Beanstalk console\n2. Elastic Beanstalk returns a new URL for the webapp\n\n* * *\n\nAlternatives: Azure App Service, Google Cloud App Engine, DigitalOcean App Platform\n","n":0.077}}},{"i":1435,"$":{"0":{"v":"DynamoDB","n":1},"1":{"v":"\nDynamo is a fully-managed, highly available schemaless NoSQL database.\n- The DB engine can manage structured or semi-structured data, including JSON documents.\n- supports key–value and document data structures\n\nUses tables, though this concept is loosely related to how tables are used in SQL.\n- essentially, we would include all domain primitives of a single domain in a Dynamo table, and use [[GSIs|aws.svc.dynamo#global-secondary-index-gsi,1:#*]] as our means of \"JOINing\" the domain primitives together.\n    - each GSI should represent an access pattern (ie. how that data is accessed, such as \"get all people by gender\", \"get all last names by country\")\n- One of the biggest mistakes people make with dynamo is thinking that it's just a relational database with no relations. It's not.\n\nTo get the full benefits of Dynamo, and it requires you often to design your data layer very well up-front. Dynamo is not recommended for a system that hasn't mostly stabilized in design.\n\n### When to use Dynamo\nDynamo is really good for high read:write ratio (at least 4:1), meaning Dynamo is a good candidate for high-read applications.\n- By default each DynamoDB table is allocated 40,000 read units and 40,000 write units of capacity per second. \n\nDynamo may support large, complex schemas but it gets more difficult to maintain and understand. Dynamo is a better candidate for applications with simpler schemas.\n\nDynamo offers effortless cross-region replication. Therefore, it is a good candidate for apps that distributed geographically.\n\n## Data is stored as partitions\nData is stored as a partitioned [[general.lang.data-structs.tree.B]].\n- Items are distributed across 10-GB storage units, called partitions (physical storage internal to DynamoDB)\n- Each table has one or more partitions\n- DynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each item’s location is determined by the hash value of its partition key.\n\nUnlike [[redis]], in that it is immediately consistent and highly-durable, centered around that single data structure.\n- If you put something into DynamoDB, you’ll be able to read it back immediately and, for all practical purposes, you can assume that what you have put will never get lost.\n\n## Terms\n### Items\nAnalogous with row of a SQL table\n\n### Attribute\nAnalogous with column name of a SQL table\n\n### Marshalling\nBefore we can Create/Update a record in DynamoDB, a plain JS Object needs to be converted into a DynamoDB Record.\n- Marshalling refers to our ability to convert a Javascript object into a DynamoDB Record.\n\n#### Example\n```js\nAWS.DynamoDB.Converter.unmarshall({\n    \"updated_at\":{\"N\":\"146548182\"},\n    \"uuid\":{\"S\":\"foo\"},\n    \"status\":{\"S\":\"new\"}\n})\n\n// { updated_at: 146548182, uuid: 'foo', status: 'new' }\n```\n\n* * *\n\nDynamo provides seamless integration with services such as Redshift (large scale data analysis), Cognito (identity pools), Elastic Map Reduce (EMR), Data Pipeline, Kinesis, and S3. Also, has tight integration with AWS lambda via Streams and aligns with the server-less philosophy; automatic scaling according to your application load, pay-per-what-you-use pricing, easy to get started with, and no servers to manage.\n\n* * *\n\n## Expression\n[[aws.svc.dynamo.expression]]\n\n* * *\n\n## DynamoDB in practice\n\nThe general rule of thumb is to choose Dynamo for low throughput apps as writes are expensive and consistent reads are twice the cost of eventually consistent reads\n\nWhen to use DynamoDB?\n- In case you are looking for a database that can handle simple key-value queries but those queries are very large in number\n- In case you are working with OLTP workload like online ticket booking or banking where the data needs to be highly consistent\n\nWhen not use DynamoDB?\n- In cases where you have to do computations on the data.\n\t- Relational databases run their queries close to the data, so if you’re trying to calculate the sum total value of orders per customer, then that rollup gets done while reading the data, and only the final summary (one row per customer) gets sent over the network. However, if you were to do this with DynamoDB, you’d have to get all the customer orders (one row per order), which involves a lot more data over the network, and then you have to do the rollup in your application, which is far away from the data.\n- If worried about high vendor lock-in.\n\nPricing\n\\$256/TB/month\n\nBy default, you should start with DynamoDB’s on-demand pricing and only consider the provisioned capacity as cost optimization. On-demand costs \\$1.25 per million writes, and \\$0.25 per million reads.\n- Then, if your usage grows significantly, you will almost always want to consider moving to provisioned capacity (significant cost savings).\n- if you believe that on-demand pricing is too expensive, then DynamoDB will very likely be too expensive, even with provisioned capacity. In that case, you might want to consider a relational database.\n\n## UE Resources\n- [Determining your data model in Dynamo](https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/steps.html)\n- [Dynamo whitepaper](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)","n":0.036}}},{"i":1436,"$":{"0":{"v":"Dynamo Streams","n":0.707},"1":{"v":"\nA DynamoDB Stream is a time-ordered sequence of modifications made to items in a single DynamoDB table.\n- each stream record provides information about the item-level modification that was made.\n- When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.\n\t- disabled by default\n- DynamoDB Streams are near realtime.\n\nA Dynamo Stream is basically Dynamo's way of exposing its table-level [[change logs|general.lang.data-structs.log]].\n\nData is durably stored for up to 24 hours.\n\nDynamoDB streams are a bit like a direct messaging queue integration (Kinesis/Kafka) with a table that contains all the events that are happening in the table.\n- This differs from usual streaming where data can be a complete business object. In a DynamoDB stream, users are limited to the table that triggered the stream events.\n\nDynamoDB streams can be processed with [[AWS Lambdas|aws.svc.lambda]].\n- To achieve best separation of concerns, use one Lambda function per DynamoDB Stream. It will help you keep IAM permissions minimal and code as simple as possible\n\n### Endpoints\nDynamoDB and DynamoDB Streams are accessed via different endpoints.\n- the DynamoDB endpoint lets us work with database tables and indexes\n- the DynamoDB Streams endpoint lets us work with records\n\nThe naming convention for DynamoDB Streams endpoints is `streams.dynamodb.<region>.amazonaws.com`.\n- ex. if you use the endpoint `dynamodb.us-west-2.amazonaws.com` to access DynamoDB, you would use the endpoint `streams.dynamodb.us-west-2.amazonaws.com` to access DynamoDB Streams.\n\n## Problem sets solved by DynamoDB Streams\nDynamoDB Streams are great if you want to decouple your application core business logic from effects that should happen afterward.\n- Your base code can be minimal while you can still \"plug-in\" more Lambda functions reacting to changes as your software evolves.\n\n- How do you set up a relationship across multiple tables in which, based on the value of an item from one table, you update the item in a second table?\n- How do you trigger an event based on a particular item change?\n- How do you audit or archive data?\n- How do you replicate data across multiple tables (similar to that of [[materialized views|sql.view.materialized]]/streams/replication in relational data stores)?\n\nDynamoDB Streams works particularly well with AWS Lambda due to its event-driven nature.\n\n## Components of a DynamoDB Stream\n\nThe following diagram shows the relationship between a stream, shards in the stream, and stream records in the shard.\n\n![](/assets/images/2021-12-13-14-37-18.png)\n\n### Shard\nStream records from a single stream are organized into uniquely identifiable groups called *shards*.\n- Each shard acts as a container for multiple stream records, and contains information required for accessing and iterating through these records.\n\nShards are ephemeral: They are created and deleted automatically, as needed. Any shard can also split into multiple new shards; this also occurs automatically.\n\nShards are distributed by nature.\n\nWe know ahead of time how many shards there will be. Even if new records are added to the stream after the fact, this number of shards doesn't change.\n\n#### ShardIterator\nA shard [[iterator|general.patterns.behavioural.iterator]] provides information about how to retrieve the stream records from within a shard. Use the shard iterator in a subsequent `GetRecords` request to read the stream records from the shard.\n\nEach ShardIterator has a ShardIteratorType which determines how the shard iterator is used to start reading stream records from the shard.\n- [(docs)](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_streams_GetShardIterator.html)\n\na ShardIterator specifies a location within a shard, but that location could be either the oldest/newest/etc so NextShardIterator allows you to determine where the next set of records are to then iterate through\n\n### (Stream) Record\nA record is a description of a unique event within a stream.\n\nA record contains information about a data modification to a single item in a DynamoDB table.\n- You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items.\n\n#### Sequence Number\nEach record has a sequence number that locates it in chronological order within a given shard.\n- it is unique per partition-key within its shard.\n\t- in the same partition-key, sequence numbers generally increase over time— The longer the time period between write requests, the larger the sequence numbers become.\n\t\t- however, there is no guarantee the sequence number will increase by 1.\n\nThe sequence number reflects the order in which the record was published to the stream.\n- Therefore, the sequence number will stay sequential so long as it is in the same partition.\n\n## How to access individual stream records\nTo access a stream and process the stream records within, you must do the following:\n1. Determine the unique ARN of the stream that you want to access.\n2. Determine which shards in the stream contain the stream records that you are interested in.\n3. Access the shards and retrieve the stream records that you want.\n\n## Streams API\nThe DynamoDB Streams API provides the following actions for use by application programs:\n- `ListStreams` — Returns a list of stream descriptors for the current account and endpoint. You can optionally request just the stream descriptors for a particular table name.\n- `DescribeStream` — Returns detailed information about a given stream. The output includes a list of shards associated with the stream, including the shard IDs.\n- `GetShardIterator` — Returns a shard iterator, which describes a location within a shard. You can request that the iterator provide access to the oldest point, the newest point, or a particular point in the stream.\n- `GetRecords` — Returns the stream records from within a given shard. You must provide the shard iterator returned from a GetShardIterator request.\n\n# UE Resources\n- https://www.macrometa.com/event-stream-processing/dynamodb-streams\n","n":0.034}}},{"i":1437,"$":{"0":{"v":"Keys","n":1},"1":{"v":"\n### Primary key\nDynamoDB stores and retrieves each item based on the primary key value\n\nCan be either:\n- Simple primary key, composed solely of the *partition key*\n- Composite primary key, composed of the *partition key* and the *sort key* (a.k.a. hash attribute and range attribute)\n\n![](/assets/images/2023-01-25-13-43-58.png)\n\n### Global Secondary Index (GSI)\n[Overview](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html)\n\nThe idea is that we create GSIs and then we issue Query requests against them. \n- without GSIs, we'd only be able to query against the primary key, which is highly limiting our potential to query against business logic.\n- GSIs are what makes JOINs unnecessary in DynamoDB.\n    expl: a RDBMS does a lot of the work in figuring out how to perform the query you give it. A NoSQL db like Dynamo would instead have us define how we'd like to access our data beforehand, then add GSIs that define specifically how to access that data. From there, the user just needs to query the GSI and it will get all of the data it needs\n- ex. if we had a table that stored books and authors, we could have a GSI whose business logic would encapsulate the idea of \"all books belonging to one author\".\n\nA GSI contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table.\n\nWe can have multiple GSIs, with each corresponding essentially to a particular unit of business logic.\n- ex. `BooksByAuthor-Index` and `AuthorsByBook-Index` would be two different GSIs, effectively allowing us to query all books by a single author, and all authors of a single book.\n\n[[see also|db.distributed.partitioning#term-based,1]]\n\n#### Partition Key\n- a.k.a *hash key*, *hash attribute*\n    - The term *hash attribute* derives from the use of an internal hash function in DynamoDB that evenly distributes data items across partitions, based on their partition key values.\n\nMost often the partition key is the `id` attribute of an item\n\nDynamo uses the partition key as input to the hashing function which determines which partition the data will live on.\n\n- All items with the same partition key value are stored together, in sorted order by sort key value.\n- ex. in booksByAuthors example, `hash_key = book`\n\n\n#### Sort key\n- a.k.a *range key*, *range attribute*\n    - The term *range attribute* derives from the way DynamoDB stores items with the same partition key physically close together, in sorted order by the sort key value.\n\nDetermines the attribute that the results (ie. projection) will be grouped by.\n- ex. in booksByAuthors example, `range_key = author`\n\nProviding a sort key (along with a partition key) creates a composite primary key, which gives you additional flexibility when querying data\n- ex. if you provide only the value for `Artist`, DynamoDB retrieves all of the songs by that artist. To retrieve only a subset of songs by a particular artist, you can provide a value for `Artist` along with a range of values for `SongTitle`.\n    `range_key = SongTitle`\n\nWell-designed sort keys have 2 major benefits:\n1. related information is gathered together in one place where it can be queried efficiently. \n    - This lets us retrieve commonly needed groups of related items using range queries with operators such as begins_with, between, >, <, and so on.\n2. Composite sort keys let you define hierarchical (one-to-many) relationships in your data that you can query at any level of the hierarchy.\n    - ex. in a table listing geographical locations, you might structure the sort key: `[country]#[region]#[state]#[county]#[city]#[neighborhood]`. This would let you make efficient range queries for a list of locations at any one of these levels of aggregation, from country, to a neighborhood, and everything in between.\n\n#### `projection_type`\nSpecifies which attributes we want returned from the query. \n\nCan be `INCLUDE` or `ALL`. \n- If `INCLUDE`, then we must specify a list of attributes we want (`non_key_attributes`)\n","n":0.04}}},{"i":1438,"$":{"0":{"v":"Expression","n":1},"1":{"v":"\nExpressions are used to denote the attributes that you want to read from an item.\n\nExpressions are strings that check for the validity of a particular statement.\n\nWith expressions, we can use comparator operators (`=`, `<`, `>`), as well as built-in functions like `attribute_exists()`, `contains()`, `size()` and `begins_with()`\n- ex. use `attribute_not_exists(DateShipped)` to ensure we are not updating an item that has already shipped.\n\nYou also use expressions when writing an item to indicate any conditions that must be met, and to indicate how the attributes are to be updated.\n\nBy default, all items are returned with GET operations (like `GetItem`, `Query`, or `Scan`).\n- to get only some items back, we use a *projection expression*, which is a string that identifies the attributes that you want.\n\nExpressions are used in a few different areas:\n- *condition expression* - update an item when certain conditions are true\n- *projection expression* - specify a subset of attributes you want to receive when reading Items\n- *update expression* - update a particular attribute in an existing Item.\n- *key condition expression* - query a table with a composite primary key to limit the items selected.\n- *filter expression* - filter the results of queries to make responses more efficient.\n\nSometimes we cannot accurately represent our desired statement due to DynamoDB syntax limitations or when it's easier to use variable substitution to create the statement rather than building a string.\n- we can use expression attribute names and expression attribute values to get around these limitations.\n- these allow you to define expression variables outside of the expression string itself, then use replacement syntax to use these variables in your expression.\n\n### Expression attribute names\nAn expression attribute name is a placeholder that you use in an DynamoDB expression\n- begins with `#`\n\nThere are times where we want to write an expression for a particular attribute, but can't properly represent that attribute name due to DynamoDB limitations. These limitations are:\n- the attribute is a [reserved word](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ReservedWords.html)\n- attribute name contains a `.`\n- attribute name begins with a number\n\nTo get around this, we pass in a map where the key is the placeholder to use in our expression and the value is the attribute name we want to expand to.\n```js\n{\n  \"#ts\": \"timestamp\"\n}\n```\n\nThen, we can use `#a` when we want to refer to our `Age` attribute.\n\n### Expression attribute values\nThese are substitutes for the actual values that you want to compare— values that you might not know until runtime.\n- begins with `:`\n\nSimilar to expression attribute names except that they are used for the values used to compare to values of attributes on an Item, rather than the name of the attribute.\n- unlike expression attribute names, we must also specify the type:\n```js\n{\":agelimit\": {\"N\": 21} }\n```\n\nFor example, we could have a `scan` operation that returns us all items from the `ProductCatalog` table where the color is black and the size is less than $500:\n\n```sh\naws dynamodb scan \\\n    --table-name ProductCatalog \\\n    --filter-expression \"contains(Color, :c) and Price <= :p\" \\\n    --expression-attribute-values { \\\n        \":c\": { \"S\": \"Black\" }, \\\n        \":p\": { \"N\": \"500\" } \\\n      } \\\n    }\n```\n\n\n\n","n":0.045}}},{"i":1439,"$":{"0":{"v":"Dynamo CLI","n":0.707},"1":{"v":"\n### `get-item`\n```sh\naws dynamodb get-item \\\n    --table-name TABLE_NAME \\\n    --key '{ \n        \"id\": { \n            \"S\": \"aaae3562-2f85-41b5-a003-b25e9425cd75\" \n        } \n    }'\n```\n","n":0.224}}},{"i":1440,"$":{"0":{"v":"Cloudwatch","n":1},"1":{"v":"\nCloudWatch is a monitoring and observability service.\n\nCloudWatch collects monitoring and operational data in the form of logs, metrics, and events.\n\nPurpose is to get a unified view of operational health and gain complete visibility of your AWS resources, applications, and services running on AWS and on-premises\n\nCloudWatch can be set up to...\n- detect anomalous behavior in your environments\n- set alarms\n- visualize logs and metrics side by side\n- take automated actions\n- troubleshoot issue\n- discover insights\n\n## Cloudwatch Logs\nCloudWatch Logs enables you to centralize the logs from all of your systems, applications, and [[AWS Services|aws.svc]]\nmonitor, store, and access your log files from\n\n### Log Event\nA log event is a record of some activity recorded by the application or resource being monitored.\n\nContains two properties: the timestamp of when the event occurred, and the raw event message.\n\n### Log Stream\nA log stream is a sequence of log events that share the same source. Each separate source of logs in CloudWatch Logs makes up a separate log stream.\n\na log stream is generally intended to represent the sequence of events coming from the application instance or resource being monitored\n\n### Log Group\nA log group is a group of log streams that share the same retention, monitoring, and access control settings.\n- You can define log groups and specify which streams to put into each group.\n\n## Concepts\n\n### Namespaces\nA namespace is a container for CloudWatch metrics\n- Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.\n\nThere is no default namespace. You must specify a namespace for each data point you publish to CloudWatch.\n- namespaces typically use the following naming convention: AWS/service. For example, Amazon EC2 uses the AWS/EC2 namespace\n\n### Metrics\nA metric represents a time-ordered set of data points that are published to CloudWatch.\n- Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time.\n- ex. the CPU usage of a particular EC2 instance is one metric provided by Amazon EC2. The data points themselves can come from any application or business activity from which you collect data.\n\nMetrics are uniquely defined by a name, a namespace, and zero or more dimensions.\n\n#### Resolution\nEach metric is one of the following:\n- Standard resolution, with data having a one-minute granularity\n- High resolution, with data at a granularity of one second\n\nMetrics produced by AWS services are standard resolution by default.\n\nWhen you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.\n\nHigh-resolution metrics can give you more immediate insight into your application's sub-minute activity.\n\n### Dimension\nA dimension is a name/value pair that is part of the identity of a metric. You can assign up to 10 dimensions to a metric.\n\nEvery metric has specific characteristics that describe it, and you can think of dimensions as categories for those characteristics. Dimensions help you design a structure for your statistics plan. Because dimensions are part of the unique identifier for a metric, whenever you add a unique name/value pair to one of your metrics, you are creating a new variation of that metric.\n\nAWS services that send data to CloudWatch attach dimensions to each metric.\n- You can use dimensions to filter the results that CloudWatch returns.\n\t- ex. you can get statistics for a specific EC2 instance by specifying the InstanceId dimension when you search for metrics.\n\nCloudWatch treats each unique combination of dimensions as a separate metric, even if the metrics have the same metric name.\n\nsuppose that you publish four distinct metrics named ServerStats in the DataCenterMetric namespace with the following properties:\n\n```\nDimensions: Server=Prod, Domain=Frankfurt, Unit: Count, Timestamp: 2016-10-31T12:30:00Z, Value: 105\nDimensions: Server=Beta, Domain=Frankfurt, Unit: Count, Timestamp: 2016-10-31T12:31:00Z, Value: 115\nDimensions: Server=Prod, Domain=Rio,       Unit: Count, Timestamp: 2016-10-31T12:32:00Z, Value: 95\nDimensions: Server=Beta, Domain=Rio,       Unit: Count, Timestamp: 2016-10-31T12:33:00Z, Value: 97\n```\n\nIf you publish only those four metrics, you can retrieve statistics for these combinations of dimensions:\n- `Server=Prod,Domain=Frankfurt`\n- `Server=Prod,Domain=Rio`\n- `Server=Beta,Domain=Frankfurt`\n- `Server=Beta,Domain=Rio`\n\nYou can't retrieve statistics for the following dimensions or if you specify no dimensions.\n- `Server=Prod`\n- `Server=Beta`\n- `Domain=Frankfurt`\n- `Domain=Rio`\n\n### Alarms\nYou can use an alarm to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time period, and performs one or more specified actions, based on the value of the metric relative to a threshold over time.\n\nYou can create alarms that watch metrics and send notifications or automatically make changes to the resources you are monitoring when a threshold is breached. For example, you can monitor the CPU usage and disk reads and writes of your Amazon EC2 instances and then use this data to determine whether you should launch additional instances to handle increased load. You can also use this data to stop under-used instances to save money.\n","n":0.036}}},{"i":1441,"$":{"0":{"v":"Cloudfront","n":1},"1":{"v":"\n# CloudFront\nCloudFront is a CDN in AWS which operates at Edge Locations around the world closer to the users. It can cache the static assets and deliver them to the end-users quite fast.\n\n`Lambda@Edge` functions run here\n- `Lambda@Edge` runs your code in response to events generated by CloudFront. In this scenario we can use `Lambda@Edge` to change the path pattern before forwarding a request to the origin and thus removing the context.\n\nCloudFront is able to route incoming requests with different path patterns to different origins or origin groups by configuring its cache behavior rules:\n![](/assets/images/2021-12-07-12-19-33.png)\n","n":0.104}}},{"i":1442,"$":{"0":{"v":"CloudFormation","n":1},"1":{"v":"\n# CloudFormation\nWhen working in AWS, you almost always want to use some CloudFormation (or a similar tool).\n- It lets you create and update the things you have in AWS without having to click around on the console or write fragile scripts.\n    - for instance, gives us the ability to tear down everything cleanly and recreate your AWS set up in one click\n- spec: the alternative to using CloudFormation is doing click-ops\n\nrule of thumb is to let CloudFormation deal with all the AWS things that are either static or change very rarely, like load balancers, deployment pipelines, VPC configs, security groups\n\n### Overview\n- define your AWS resources as a YAML script\n- point CloudFormation to your AWS account, and it creates all the resources you defined idempotently.\n- Updates can be rolled back.\n\nWhile CloudFormation is specific to AWS, we can use [[Terraform|terraform]], which is platform agnostic.\n\n#### Stack\n*stack* refers to all the lamdbas that are found in the project\n- for instance, if we are using serverless framework, the stack refers to all of the stuff in serverless.yml\nif we are looking for stacks, dt-fleet-event-bus stack\n","n":0.075}}},{"i":1443,"$":{"0":{"v":"Aurora","n":1},"1":{"v":"\nIf you choose to use Aurora, you have the option of base Aurora, or Aurora serverless. \n- *Base Aurora* is AWS's highly customized MySQL/Postgres compatible database that gives you the stability and performance of the commercial enterprise databases (SQL Server/Oracle) at the cost point of the leading open source databases (MySQL/Postgres). It is extremely scalable. You can use auto scaling of your read replicas so as your read traffic peaks, read replicas will scale out. Aurora is perfectly capable of handling the most demanding relation data workloads.\n\nAnything running Aurora is a diverged fork of the underlying DB.","n":0.102}}},{"i":1444,"$":{"0":{"v":"API Gateway","n":0.707},"1":{"v":"\nAPI Gateway is a fully managed service that allows us to *create*, *publish*, *maintain*, *monitor*, and *secure* APIs at any scale\n\nAllows us to create 2 types of API:\n- RESTful APIs\n- Web APIs, that enable real-time two-way communication applications\n\nHandles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including:\n- traffic management\n- CORS support\n- authorization and access control\n- throttling\n- monitoring\n- API version management\n\nAPI Gateway’s REST API type allows users to setup HTTP proxy integrations, which can be used for forwarding incoming requests with different path patterns to different origin servers according to the API specifications\n![](/assets/images/2021-12-07-12-18-22.png)\n\nSupports containerized and serverless workloads, as well as web applications.\n\nPay for the API calls you receive and the amount of data transferred out\n\nAPI Gateway is combined well with Lambda, whereby we make an endpoint and cause the endpoint to trigger the [[aws.svc.lambda]]\n\n## Authorizers\n### Lambda Authorizers\nWhen a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.\n\nA Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.\n\nThere are two types of Lambda authorizers:\n- **token-based** - the authorizer receives the caller's identity in a bearer token (e.g. JWT or OAuth)\n- **request parameter-based** - the authorizer receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables.\n\n#### API Gateway Lambda authorization workflow\n1. The client calls a method on an API Gateway API method, passing a bearer token (or request parameters).\n2. API Gateway checks whether a Lambda authorizer is configured for the method. If it is, API Gateway calls the Lambda function.\n3. The Lambda function authenticates the caller by means such as the following:\n- Calling out to an OAuth provider to get an OAuth access token.\n- Calling out to a SAML provider to get a SAML assertion.\n- Generating an IAM policy based on the request parameter values.\n- Retrieving credentials from a database.\n4. If the call succeeds, the Lambda function grants access by returning an output object containing at least an IAM policy and a principal identifier.\n5. API Gateway evaluates the policy.\n- If access is denied, API Gateway returns a suitable HTTP status code, such as 403 ACCESS_DENIED.\n- If access is allowed, API Gateway executes the method. If caching is enabled in the authorizer settings, API Gateway also caches the policy so that the Lambda authorizer function doesn't need to be invoked again.","n":0.048}}},{"i":1445,"$":{"0":{"v":"AWS Amplify","n":0.707},"1":{"v":"\nAWS Amplify is a toolbox for front-end/mobile development, with some overlap with Firebase\n\t- ex. tools for creating onboarding flow, tools for implementing AI/ML features, tools for auth\n- also includes tools to help implement cloud-based features in the app\n- includes tools to make real-time apps (ex. news feed, chat)\n","n":0.144}}},{"i":1446,"$":{"0":{"v":"VPC (Virtual Private Cloud)","n":0.5},"1":{"v":"\n## What is it?\nVirtual Private Cloud (VPC) is a service that allows us control over the virtual networking environment\n- gives control over resource placement, connectivity, security etc.\n\nYou can use Amazon VPC to create a private network for resources such as databases, cache instances, or internal services.\n\nVPC is a logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define.\n- You have complete control over your virtual networking environment, including selection of your own IP address ranges, creation of subnets, and configuration of route tables and network gateways.\n- The network config of the VPC is highly configurable\n\t- ex. you can create a public-facing subnet for your web servers that have access to the Internet, and place your backend systems such as databases or application servers in a private-facing subnet with no Internet access.\n\n### Why does it exist?\na VPC is analogous to your home local network.\n- Similar to how the devices in your home need a local IP address so the router knows where to direct traffic, your resources need to exist in a VPC so it can receive traffic.\n- If you want the LAN (VPC) to talk to the internet, you can add a NAT Gateway (just like your home router).\n\t- The rest of the internet can only respond to packets from your computers. The Internet can't initiate a connection to your computers -- because they can't even get \"your\" IP address (it's un-routable, remember). They can only talk to the NAT gateway, and that only routes existing connections from the LAN.\n- If you want your computers to respond to the Internet, you can \"poke holes in the firewall\" using various technologies such as ALB, ELB, routing, peering, etc.\n\nEverything attached to the internet needs to be in a network. If you attached network cables directly from all the compute devices to a huge router then you'd have a huge open network with minimal control over what is allowed to talk to what. Not ideal at all if one device gets compromised and then can hack everything else right?\n\nVPC separates the giant network of the AWS data centers into smaller logically isolated networks and lets you build out rules about what networked resources you want to be able to talk to what other networked resources.\n\nThings in a VPC can't talk over the network to anything else outside of the VPC or anything else in another VPC unless you explicitly allow it. This makes VPC \"secure by default\". It starts out completely isolated from the global internet and isolated from everything else inside of AWS. Then you build out the set of minimal network access rules and routing rules you'd like to have in your network.\n\nAs a side note, most of the time if you are having trouble understanding AWS networking its because of this \"secure by default\" aspect. And if your networking isn't working then 99% chance it's a missing or misconfigured security group rule, or missing networking route that means that the VPC isn't allowing this networking connection until you explicitly allow it.\n\n## Why use it?\nCompare it to your own home network, where devices within the network can communicate with each other without too many security barriers in between. In addition to that, each device has access to the internet via the [[NAT|network.lan.router.nat]], but internet traffic can't come directly in.\n- if you want something externally visible, you put it in a DMZ or public subnet.\n\nIn a microservices architecture, imagine you had a [[Serverless|serverless-framework]] repo with some [[lambdas|aws.svc.lambda]]. If those lambdas needed to communicate with any of our other services (assuming they too were in the same VPC), we would need to put them in the VPC.\n- In Serverless Framework in particular, this is just a config option in the `serverless.yml` file.\n- Lambdas are the type of thing that should be left out of the VPC unless they absolutely need to be in it. There is a cost to putting them in the VPC. They cost more $ and have slower cold start times. They also carry more potential for connectivity issues.\n\t- ex. since Elasticache (AWS Redis) must run in a VPC, if our lambda needed to connect to it, the lambda would also need to exist in the same VPC.\n\nVPC endpoints enable you to privately connect your VPC to services hosted on AWS without requiring an Internet gateway, a NAT device, VPN, or firewall proxies\n- Endpoints are horizontally scalable\n- VPC offers two different types of endpoints: gateway type endpoints and interface type endpoints.\n\t- Gateway type endpoints are available only for AWS services including S3 and DynamoDB.\n\t- Interface type endpoints provide private connectivity to services powered by PrivateLink.\n\nMultiple VPCs within a single region can communicate with each other\n![](/assets/images/2021-11-23-12-55-44.png)\n\nDifferent VPCs usually don't communicate with each other, but we can open a few ports if they really need to.\n\n## How does it work?\nProcess:\n1. create your VPC\n2. add resources to it, such as EC2 and RDS instances\n3. define how your VPCs communicate with each other across accounts, Availability Zones (AZs), or Regions.\n\nAWS VPC comprises a variety of objects:\n- *A Virtual Private Cloud*: A logically isolated virtual network in the AWS cloud. You define a VPCs IP address space from ranges you select.\n- *Subnet*: A segment of a VPC’s IP address range where you can place groups of isolated resources.\n- *Internet Gateway*: The Amazon VPC side of a connection to the public Internet.\n- *NAT Gateway*: A highly available, managed [[NAT|network.lan.router.NAT]] service for your resources in a private subnet to access the Internet.\n- *Virtual private gateway*: The Amazon VPC side of a [[VPN|network.vpn]] connection.\n- *Peering Connection*: A peering connection enables you to route traffic via private IP addresses between two peered VPCs.\n- *VPC Endpoints*: Enables private connectivity to services hosted in AWS, from within your VPC without using an Internet Gateway, [[VPN|network.VPN]], [[NAT|network.lan.router.NAT]] devices, or firewall proxies.\n- *Egress-only Internet Gateway*: A stateful gateway to provide egress only access for IPv6 traffic from the VPC to the Internet.\n\n## Concepts\n<!-- - note: is this just repeated from above? -->\nVPC networks includes the following network elements:\n\n- *Elastic network interface* – elastic network interface is a logical networking component in a VPC that represents a virtual network card.\n- *Subnet* – A range of IP addresses in your VPC. You can add AWS resources to a specified subnet. Use a public subnet for resources that must connect to the internet, and a private subnet for resources that don't connect to the internet.\n- *Security group* – use security groups to control access to the AWS resources in each subnet.\n- *Access control list (ACL)* – use a network ACL to provide additional security in a subnet. The default subnet ACL allows all inbound and outbound traffic.\n- *Route table* – contains a set of routes that AWS uses to direct the network traffic for your VPC. You can explicitly associate a subnet with a particular route table. By default, the subnet is associated with the main route table.\n- *Route* – each route in a route table specifies a range of IP addresses and the destination where Lambda sends the traffic for that range. The route also specifies a target, which is the gateway, network interface, or connection through which to send the traffic.\n- *NAT gateway* – An AWS Network Address Translation (NAT) service that controls access from a private VPC private subnet to the Internet.\n- *VPC endpoints* – You can use an Amazon VPC endpoint to create private connectivity to services hosted in AWS, without requiring access over the internet or through a NAT device, VPN connection, or AWS Direct Connect connection. For more information, see AWS PrivateLink and VPC endpoints.\n","n":0.028}}},{"i":1447,"$":{"0":{"v":"SQS (Simple Queue Service)","n":0.5},"1":{"v":"\nHighly-durable queue in the cloud\n- put messages on one end, and a consumer takes them out from the other side.\n\nMessages are consumed *almost* in FIFO, but there is no strictness to adhere to this.\n- Strictness *can* be guaranteed, but there is a performance cost.\n\nQueues are an important mechanism for providing fault tolerance in distributed systems.\n- The service durably persists messages until they are processed by a downstream consumer.\n\nSQS scales elastically, and there is no limit to the number of messages per queue.\n\nSQS requires zero capacity management.\n- no limit on the rate of messages enqueued or consumed\n- don’t have to worry about any throttling limits.\n- number of messages stored in SQS (the backlog size) is also unlimited.\n\ngreat default choice for dispatching asynchronous work.\n\n### SQS vs SNS\nSNS is \"heavier\":\n- SQS lets any authorized connection read from it, SNS requires a specific subscription. \n- SQS can be read from a local dev end, SNS will need a local tunnel like ngrok to receive. \n- By default stuff just waits on SQS, the default SNS behaviour is to immediately send the notification.","n":0.075}}},{"i":1448,"$":{"0":{"v":"SNS (Simple Notification Service)","n":0.5},"1":{"v":"\nAWS SNS is a fully managed messaging service for decoupling event-driven microservices. \n- It enables us to send messages *reliably* between parts of the infrastructure.\n\nSNS works via [[pub/sub|general.patterns.messaging.pubsub]] topics to allow services to consume relevant events.\n\nSNS can be set up to send messages to end users via email, SMS, mobile push notifications etc.\n\nSNS uses a robust retry mechanism in cases where downstream targets are unavailable.\n- If it still fails, it will utilize a [[DLQ|general.patterns.messaging.DLQ]].\n\nSNS does not retain messages so if there are no subscribers for a topic, the message is discarded.\n- comparatively, an [[SQS|aws.svc.SQS]] message is stored on the queue for up to 14 days until it is successfully processed by a subscriber.\n\nSNS uses topics to logically separate messages into channels, and your [[Lambda functions|aws.svc.lambda]] interact with these topics.\n\nSNS topics may broadcast to multiple targets (fan-out).\n- An SNS topic can have up to 12,500,000 subscribers, providing highly scalable fan-out capabilities.\n\nSNS can be used to parallelize work across Lambda functions or send messages to multiple environments (such as test or development).\n\n### vs [[EventBridge|aws.svc.event-bridge]]\nBoth can be used to decouple publishers and subscribers, filter messages or events, and provide fan-in or fan-out capabilities.\n\nEventBridge offers two capabilities not in SNS:\n- SaaS integration\n- Schema registry\n","n":0.071}}},{"i":1449,"$":{"0":{"v":"S3 (Simple Storage Service)","n":0.5},"1":{"v":"\nFundamentally, you can think of S3 as a highly-durable hash table in the cloud.\n- The key can be any string, and\n- the value any blob/object of data up to 5 TB.\n\nObject storage is used for purposes such as storing photos on Facebook, songs on Spotify, or files in online collaboration services, such as Dropbox.\n\nWhen using S3, always consider implementing it alongside a CDN like [[CloudFront|aws.svc.cloudfront]]\n- ex. if you were building a video streaming platform, you would want to cache egress data so that 2 users living in a similar geographical location would be able to benefit from content that is cached at the edge.\n\nData gets streamed at a rate of around 90 MB/s.\n\nYou can have as many parallel uploads and downloads as you want, thus, the infinite bandwidth.\n\nS3 offers built-in redundancy.\n\nAt first you can start with the default storage class and ignore all the other classes. Don't bother with the implications of the different storage classes until you really need to start saving money from S3 storage costs.\n\n### Event Notifications\nLike webhooks, allows us to receive notifications when certain events happen in your S3 bucket.\n\n### Limitations\n- cannot append to objects.\n- It doesn’t support HTTPS when used as a static website host, so you cannot host static websites\n\n* * *\n\n- Alternatives: Azure Blob\n\nPricing\n$23.55/TB/month\n$5/million uploads and $0.40/million downloads\n","n":0.068}}},{"i":1450,"$":{"0":{"v":"RDS (Relational Database Service)","n":0.5},"1":{"v":"\nDatabases shouldn't be run in a container\n- by default will spin up a single instance in a single availability zone\n\t- if we want more redundancy, we can add an active backup instance\n- doesn't support downloading postgres extensions\n","n":0.164}}},{"i":1451,"$":{"0":{"v":"Elastic Load Balancing (ELB)","n":0.5},"1":{"v":"\nComes in 3 variants:\n- Classic\n\t- Legacy software.\n- Application (ALB)\n\t- a proper reverse proxy that sits between the internet and your application\n\t- Every request to your application gets handled by the load balancer first. The load balancer then makes another request to your application and finally forwards the response from your application to the caller.\n- Network (NLB)\n\t- behave like load balancers, but they work by routing network packets rather than by proxying HTTP requests\n","n":0.117}}},{"i":1452,"$":{"0":{"v":"Elastic Container Service (ECS)","n":0.5},"1":{"v":"\nrunning containers via AWS proprietary orchestrator\n- AWS offers 2 main models for running containers: ec2 and fargate\n- ECS clusters can be run a few different ways:\n\t- run on vms\n\t- managed docker container runtime\n\t- EC2\n\t- Fargate\n- **ECS Control Plane** - The tools to manage ECS\n\t- handles autohealing\n- by default, containers behave like other Linux processes with respect to access to resources like CPU and memory. this means they get access to all of the host's CPU and memory capacity\n  - ecs provides mechanisms through which we can limit this (in the task)\n\n### Task\n**Task definition** - To prepare your application to run on Amazon ECS, you create a task definition, which is a JSON file that describes between 1 and 10 containers\n- Think of it as a blueprint for your application\n- Task definition parameters might be:\n\t- Which containers should be used?\n\t- Which ports should be opened?\n\t- Which data volumes should be used within the containers?\n\t- How are container linked together\n\n- a `task` is an instantiation of a `task definition`\n\t- the basic unit of deployment is a `task`\n- a `task` is a logical construct that models 1+ containers\n\t- therefore, the ECS API operates on tasks rather than individual containers\n- in ECS you run a task, which in turn runs a container\n- each fargate task has its own isolation boundary and doesn't share kernel, cpu resources, memory resources etc\n\n- **ECS Cluster** - a regional grouping of container instances, upon which we can run task requests\n\t- When tasks are run on Fargate, cluster resources are managed for us\n\t- ECS cluster is a regional grouping of one or more container instances on which you can run task requests\n\nECS objects and how they relate\n![](/assets/images/2021-03-08-21-29-20.png)\n","n":0.06}}},{"i":1453,"$":{"0":{"v":"Elastic Compute Cloud (EC2)","n":0.5},"1":{"v":"\nIn a sense, an EC2 instance can be thought of as an atomic building block of AWS compute.\n- An instance of EC2 is a VM on an AWS server\n\t- Therefore, you get an OS, a file system, access to the server’s hardware, etc.\n\nThe nice thing about EC2 is that the computer you get will be very similar to the computer you use to develop your software. If you can run your software on your computer, you can almost certainly run it on EC2 without any changes.\n- This is one of EC2’s main advantages compared to other types of compute platforms (such as [[Lambda|aws.svc.lambda]] ): you don’t have to adapt your application to your host.\n\n*Elastic* refers to the fact that it can scale up/down as needed automatically\n\nEC2 has dozens of options you will probably never need, and thus it comes with sensible defaults.\n-  This is the result of the highly varied workloads and use cases serviced by EC2\n\nSelecting an instance type will be the most consequential decision when provisioning an EC2 instance.\n- There are 256+, and can be narrowed to a few categories, defined by what they're optimized for:\n\t- CPU\n\t- Memory\n\t- Network\n\t- Storage\n\nIf you were building your own server, there would be an infinite number of ways to configure it, but with EC2 you get to pick an instance type from its catalog.\n- This is the tradeoff you make as opposed to getting you build your own server, but most likely it shouldn't even be a concern.\n\n### Instance profile\nJust like an [[IAM|aws.terms.IAM]] user represents a person, an *instance profile* represents EC2 instances.\n- however, the only permissions is provides is the power to assume a role.\n- So the EC2 instance runs under the EC2 instance profile, defining \"who\" the instance is. It then \"assumes\" the IAM role, which ultimately gives it any real power.\n\nWhen you create an IAM Role for EC2 using the AWS Management Console, it creates both an EC2 instance profile as well as an IAM role.\n\n### Security\nthe defaults are sensible, but we may have to modify:\n- The security group\n\t- You can think of security groups as individual firewalls for your instances. With security groups, you can control what goes in and out of your instances.\n- The VPC ACL.\n\t- You can think of VPC ACL as a network firewall. With the VPC ACL, you can control what goes in and out of your network.\n\n### Scaling\nThe auto part of Auto Scaling allows us to automatically add/remove EC2 instances (therefore horizontal scaling). In theory it works, but is often impractical.\nThe main premise of Auto Scaling is that once you decide how much headroom you want, you’ll be able to make that headroom a constant size\n\nUnless the following 2 are true, then it's probably not worth messing around with auto-scaling:\n- Are your EC2 costs high enough that any reduction in usage will be materially significant?\n- If your EC2 bill were to go down by 30%—would that be a big deal for your business?\n\n#### How many EC2 instances to run?\nCapacity Headroom - we need to have a buffer between the expected peak demand and the maximum capacity that our system can handle.\n\nPricing\n- only pay for the number of seconds your instance is running.\n","n":0.043}}},{"i":1454,"$":{"0":{"v":"AWS Regionality","n":0.707},"1":{"v":"\nAWS services are either Global, Regional or specific to the Availability Zone and cannot be accessed from outside.\n- Most AWS managed services are regional based (except for [[IAM|aws.terms.IAM]], [[aws.svc.route53]], [[aws.svc.cloudfront]], WAF etc).\n\n### Availability Zone\nAn Availability Zone (AZ) is a group of logical data centers, each one isolated from others.\n\nEach Region has multiple AZs\n- if a region might be `us-east-1`, the AZ might be `us-east-1a`\n- note: other cloud providers might define a region as a single data centre.\n\nEach AZ has independent and redundant power, cooling, and physical security and is connected via redundant, ultra-low-latency networks\n- this means that a company running its resources in different AZs get the benefit of high availability and fault-tolerance, since AZs are isolated from each other. Therefore, it's a good strategy is to spread partitions across AZs\n","n":0.087}}},{"i":1455,"$":{"0":{"v":"Code Suite","n":0.707},"1":{"v":"\n### CodeCommit\nManaged Git repo hosting, alternative to Github, Gitlab etc.\n\n### CodeBuild\n[[Continuous Integration|deploy.CI-CD]] service\n\n### CodeDeploy\nManaged deployment service that automates software deployments to a variety of compute services (e.g. [[aws.svc.EC2]] and your on-premises servers)\n- can be used to automate software deployments\n\n### CodePipeline\n[[Continuous Delivery|deploy.CI-CD]] service allowing us to automate release pipelines\n- CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change\n- CodePipeline can be integrated with Github/Gitlab","n":0.117}}},{"i":1456,"$":{"0":{"v":"Billing","n":1},"1":{"v":"\nset up billing alerts and thresholds so you can get notified if cost starts to spike for strange reasons","n":0.229}}},{"i":1457,"$":{"0":{"v":"Accounts","n":1},"1":{"v":"\n### Root user\nUpon creating a new AWS account, we begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This is the root user.\n- Never use the root account for anything real; use it to set up your account and payment method and contact methods and most importantly, your IAM 'real' users. Then secure it with MFA and never login again until you have to do something that requires root access.\n- Never ever create API keys or access credentials for root user. Those are literally the keys to your entire kingdom and losing them will be catastrophic\n\n### AWS Organization\nAWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.","n":0.087}}},{"i":1458,"$":{"0":{"v":"SDK","n":1},"1":{"v":"\nSDK is for interacting with AWS services for the most part.\n\nThe AWS SDK comes with a lot of low level code that is done already to interact with AWS services, so that you can include it in your app and do what you want without having to worry about what goes on underneath.\n- So if you have an application written in JavaScript and it needs to process messages in an SQS queue, you would use the JavaScript SDK.\n- It can also create/delete/manage infrastructure.\n\nWhen used with AWS, Terraform is implemented using the AWS Go SDK.\n\n### `lib` packages\nWhen using AWS SDK, we may come across `lib` packages (e.g. `@aws-sdk/lib-dynamodb` instead of `@aws-sdk/client-dynamodb`). These packages are higher level.\n- ex. in the `lib` version of DynamoDB package, Marshalling is handled for us.\n\n### EventBridge Example\nWe could use the [@aws-sdk/client-eventbridge](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/clients/client-eventbridge/index.html#aws-sdkclient-eventbridge) package to communicate with our [[EventBridge|aws.svc.event-bridge]].\n- For instance, you can create rules that match selected events in the stream and route them to targets to take action.\n\nImagine you wanted to create a system where vehicle fleet managers can determine which types of event they are interested in (low tire pressure, enter/exit geofence etc). When the fleet manager indicates they want to be notified of low tire pressure, the act of them selecting that event would add a rule to the rules engine, effectively subscribing them to the `LowTirePressure` event. This is accomplished via code at runtime, rather than during compile/deploy time. Effectively, the rules engine can be modified on the fly, rather than having to do it ahead of time.\n\n# Docs\n### DynamoDB\n[@aws-sdk/client-dynamodb](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/clients/client-dynamodb/index.html)\n[@aws-sdk/lib-dynamodb](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/modules/_aws_sdk_lib_dynamodb.html)","n":0.063}}},{"i":1459,"$":{"0":{"v":"CDK","n":1},"1":{"v":"\n## What i\nCDK is an [[IaC|devops.IaC]] solution for provisioning AWS infrastructure that is written in common programming languages such as Javascript and Python\n\nCDK is an alternative to [[terraform]]\n\nOur CDK code will be transformed into [[CloudFormation|aws.svc.cloud-formation]] teplates, which are used to build our AWS infrastructure.\n\nWe use the CDK to manage infrastructure, while we use the [[AWS SDKs|aws.Sdk]] to interact with that infrastructure\n- ex. you'd use the CDK to create a [[DynamoDB|aws.svc.dynamo]] table. You'd use the SDK to read and write data in that table.\n","n":0.11}}},{"i":1460,"$":{"0":{"v":"Auth","n":1},"1":{"v":"\n## Authorization vs Authentication\nBecause \"auth\" is ambiguous, we often see the distinguishing terms \"authn\" (authentication) and \"authz\" (authorization)\n\nBoth are an important part of identity and access management (IAM), but...\n- authn has to do with identity (who someone is)\n  - authn can be carried out with username/password, 2FA, public key certificates, biometric etc.\n- authz has to do with permissions (what an authenticated user is allowed to do)\n  - authz can be carried out with RBAC (role-based access control), ABAC (attribute-based access control)\n\nanal: think of authorization as a subway ticket. The ticket is in no way attached to your identity, since you just got it from the machine. You could give this ticket to someone else, and it would *authorize* them to enter the platform and board the train. Authentication on the other hand, is about identity. If Authorization is a subway ticket, then Authentication is a finger print scan.\n\n* * *\n\n### Basic Authentication\nHTTP provides its own form of basic authentication out of the box. It works by combining the username and password with a `:` separator, then base64 encoding the string.\n- dangerous, since we are passing the password on every request.\n\n* * *\n\nBecause of SSL, sending plaintext passwords from the client to the server is perfectly fine.\n","n":0.07}}},{"i":1461,"$":{"0":{"v":"Auth Tokens","n":0.707},"1":{"v":"\n### Opaque token vs Structured token\nan unencrypted structured token (like a [[JWT|auth.tokens.jwt]]) can be interpreted by anybody that holds the token, whereas opaque tokens cannot.\n\nThe opaque token is a random unique string of characters issued by the authorization server.\n- it is issued in a format that is known only to the auth server (unlike JWTs, which use a highly predictable format)\n- it contains an identifier to information stored on the authorization server.\n  - therefore, if the resource server wants to get identity information from the token, it needs to send a request to the introspection endpoint of the auth server.\n  - on the other hand, a structured token (like a JWT) contains enough information for the resource server to make its authorization decision.\n- To validate the token and retrieve the information on the token and the user, the resource server calls the authorization server and requests the token introspection.\n\nOpaque tokens should be the default choice *unless* we want our client to be able to parse the token (which turns out is a common requirement with webapps)\n- ex. a service framework in which you only pass the token to the service as a blind claim; you have no idea what it says, but you know it works and identifies you. It is only up to the service to confirm it's you via the secret key or cipher.\n","n":0.067}}},{"i":1462,"$":{"0":{"v":"Refresh Token","n":0.707},"1":{"v":"\nThe Refresh Token contains basically only the session ID which can be used to look up in the database to see if it’s an active session or not. If it is, then it can generate a new [[access token|auth.jwt.access-token]] and that can be sent to the user\n- A refresh token is a special token that is used to generate additional jwt tokens. This allows you to have short-lived access tokens (JWTs) without having to collect credentials every time one expires. The server sends the client this token alongside the access (JWT) and/or ID tokens as part of a user's initial authentication flow\n\t- The refresh token should be saved in the database in the relevant row of the user's table.\n\t\t- Therefore, we can handle the renewing login with Postgres.\n\t- the refresh token can be sent from the server to the client as an `HTTPOnly` cookie\n- A refresh token is not capable of authenticating a user on its own— its only use is to look up active sessions. If it finds one, then the refresh token can be used to generate a new access token.\n- Every time a new access token is generated, a refresh token should be generated as well\n- a Refresh token is what allows us to login to a website, close the browser, and still be logged in upon reopening it.\n\t- When a new session starts (reopening the browser tab), the app is able to see that there is no JWT in memory, so it triggers a *silent refresh*\n- Imagine we set a jwt to have a lifetime of 15 minutes. Without the refresh token, this means that the server would send back an http 401: unauthorized every 15 minutes (probably at which point your app will log the user out and display the login screen)\n- A refresh token has 2 properties:\n\t1. It can be used to make an API call (say, `/refresh_token`) to fetch a new JWT token before the previous JWT expires.\n\t2. It can be safely persisted across sessions on the client!\n\nRefresh tokens don't usually expire (or are long-lived) and is single use only.\n\n### Why use refresh tokens?\nUsing refresh tokens in conjunction with access tokens helps us alleviate the problem of stale tokens. e\n\nrefresh tokens are exchanged for an [[access token|auth.tokens.access]]","n":0.052}}},{"i":1463,"$":{"0":{"v":"JWT","n":1},"1":{"v":"\n## What is it?\nA JWT is simply a JSON payload containing a particular [[claim|auth.tokens.jwt.claim]] (typically about the permission a user has)\n- The key property of JWTs is that in order to confirm if they are valid we only need to look at the token itself (we don't have to contact a third-party service or keep it in-memory between requests).\n\nWhen used as an authentication token, a JWT is a security token format that is used for exchanging authentication and authorization data between parties\n\nThe most common implementations of JWT that we will see are as [[access tokens|auth.tokens.access]] and [[refresh tokens|auth.tokens.refresh]]\n\nThe JWT should include the roles of the user, so that the backend knows what that user can do solely from the JWT itself.\n\na JWT is not encrypted. So any information that we put in the token is still readable to anyone who intercepts the token.\n- therefore, we should never put anything in the JWT that a bad actor could leverage directly.\n\nResource servers that verify the signature of JWTs are performing [[Authorization|auth#authorization-vs-authentication]] (not Authentication), since identity is not verified; only access rights.\n\na JWT is actually Base64Url, not [[binary.encoding.base64]]\n- this is virtually the same, except a couple characters are different so that it can exist as a URL param.\n  - ex. `=` is displayed as `%3D`\n\n### Flow\n1. User sends credentials to auth service. \n2. If the credentials are valid, the authentication service generates a JWT with the user's identity and any necessary claims (e.g., user roles or permissions) and signs it with a secret key known only to the auth service.\n3. The auth service returns the JWT to the client\n4. The client includes the JWT in subsequent requests for resources (e.g. to invoice service)\n5. The invoice service extracts the JWT from the request and verifies the signature of the JWT using the secret key shared from the auth service.\n6. If the signature is valid, the invoice service decodes the JWT to extract the user's identity and any relevant claims.\n7. The invoice service checks to see if the claim offers sufficient permissions to access the resource. If it does, then it accesses it and returns it to the client.\n\n### Construction\nA JWT is made of 3 parts: the Header, the Payload and the Signature\n\n#### Payload (claim)\n![[auth.tokens.jwt.claim]]\n\n#### Header\nThe receiver of the JWT needs to know what type of signature is used (here `RS256`).\n```json\n{\n  \"alg\": \"RS256\",\n  \"typ\": \"JWT\"\n}\n```\n\n#### Signature\n![[auth.tokens.jwt.signature]]\n\n## Why use it?\nThe biggest advantage of JWTs (when compared to [[user session management|auth.session]] using an in-memory random token) is that they enable the delegation of the authentication logic to a third-party server, such as:\n- a centralized in-house auth server \n- LDAP (Lightweight Directory Access Protocol)\n- third-party authentication provider like Auth0\n\nThis allows a system where:\n- The external authentication server can be completely separate from our application server. There is no secret key that has to be shared over the network. This means all the application server has to do is check the JWT.\n    - the authentication server has the private key, while the application server has the public key.\n- No direct live link between the application server and the authentication server is needed\n- The application server can be completely stateless (since we don't need to keep tokens in-memory between requests). The authentication server can issue the token, send it back and then immediately discard it.\n\nThe only way for an attacker to impersonate a user would be to either \n- steal both its username and personal login password \n- steal the secret signing key from the Authentication server.\n\nJWTs are tamper-proof.\n- in the days of [[cookie|browser.cookies]]-based authentication, if an attacker gained access to that cookie, they could tamper it and cause that client to issue hostile requests to the server. If an attacker tried to tamper with a JWT, it would destroy the hash and would no longer match the decoded data of the JWT.\n\nJWTs allow servers to be decoupled from authentication.\n- Consider the *Basic Authentication* method of HTTP. In this scheme, we pass the `username:password` as a [[base64|binary.encoding.base64]] encoded string to the server with each request. This means that the server needs to know how to verify user account credentials. This coupling is remedied by having JWTs, since a querying client just has to present the valid JWT in order to be considered a legitimate user. Now, the server doesn't need any knowledge about authentication, it just has to know how to verify the JWT.\n\n## How do they work?\n1. User logs in with email and password\n2. The Authentication server issues a JWT to the client\n3. The client goes to view a list of private data, so a request to the resource server is made.\n4. The resource server validates the content of the payload by inspecting the signature. \n    - validation can be done by using *asynchronous cryptographic signatures* or by using a shared signing key.\n\n![](/assets/images/2022-12-14-13-10-02.png)\n\nThere are multiple types of signature, so one of the things that the receiver needs to know is for example which type of signature to look for.\n- this is found in the header\n\nhashing the header and payload makes it much smaller. therefore hashing is not an functional part of it (ie. it's not about making it more secure); it's a performance part.\n\n### Analogy: Money vs Cheque\nWhen we give a $20 bill to someone, there is not a second thought as to where this money came from or if the bearer is the legitimate owner. The person who we are giving this money to does not need to verify with anyone of its legitimacy, and it is simply trusted\n\nOn the other hand, when we pay with a cheque, a call needs to be made to the central authority (the bank) to verify if this cheque-holder is who they say they are.\n\nIn this analogy, the cash is like a JWT: the system is designed where we can just trust the bearer of the money.\n\n* * *\n\n### Token (ex. JWT)\n- When user logs in, the server creates a JWT with the secret, and sends it to the client. The client stores the JWT in local storage, and includes that JWT in every request.\n- The biggest difference here is that the user’s state is not stored on the server, as the state is stored inside the token on the client side instead\n\nThe whole point of JWTs is to not require centralized coordination.\n- this is why there is no `/logout` endpoint to hit when we use JWTs for authentication. Instead, we just need to delete it from local storage.\n\nA JWT is just a regular javascript object that is stringified, hashed, and cryptographically signed.\n- Your token is signed with the secret, known only by the server. If someone changes the token on client side, it would fail validation and the server side framework would reject it. Therefore you can trust your token. Of course, the jwtSecret should be a secret only known by your authentication server and resource server.\n\nJWTs are agnostic to what form of authentication you are using (ex. email, OAuth etc). Regardless, the response will contain the JWT.\n\nYou generate the token only if you trust the user who requested it.\n- You trust the token as long as it has not expired and can be verified with the secret.\n- The information in a JWT can be read by anyone, so do not put private information in a JWT. What makes JWTs secure is that unless they were signed by our secret, we can not accept the information inside the JWT as truth.\n\nJWTs guarantee that the bearer of the token also owns the data that he is requesting.\n- However JWTs don't guarantee encryption, which is why HTTPS is required. Otherwise, a man in the middle could take that server response (with the jwt) and use it to authenticate itself on your behalf, gaining access to all data.\n\nJWTs come with a death sentence— that is, by nature they have an expiry date.\n- this value is stored in the `iat` property. This can be thought of as the `date_of_death` property.\n- The server determines the lifespan of the JWT, since it controls the expiry date. Therefore, JWTs give a uniform lifespan to all JWTs, but like God, it has control over ending your life prematurely in order to deny access.\n\nJWTs are stateless (while [[sessions|auth.session]] are stateful). This fact enables them to be verified on the server, without having to make a database call. This in principle makes them faster than using sessions (since sessions need to be stored).\n- In reality, it's likely that the times we need to authenticate ourseleves with the JWT are also times that we need to interact with the database. This makes \"saving trips to the database\" more of a pipe-dream than a reality. At the end of the day, if we need to make a database interaction *and* authenticate ourselves, we are quicker just using a session rather than authenticating with a JWT (due to their size)\n  - spec: The assumption here is that Sessions are stored in the same database as our app resources. If instead we have a separate session manager service, then we will always have to make 2 requests.\n\nJWTs are huge. Storing a userid in a cookie is 6 bytes, while storing in the JWT (along with headers+secret) makes it 304 bytes.\n\n![](/assets/images/2021-03-08-16-41-50.png)\n\n- unlike a cookie, a JWT can contain an unlimited amount of data\n\n### Logout\nThere is no `/logout` endpoint to hit, as all we need to do is delete the JWT kept on the client.\n- This means the token is still valid even after you logout. This is why keeping a short expiry date is important\n\n### Analogy\n*\"Pretend I’m blind and hard of hearing. Let’s also pretend that last week you bought me lunch, and now I need your bank account number to pay you back. If I ask you for your bank account number in person, and someone else shouts their bank account number, I might accidentally send them the money I owe you.\nThat’s because I heard someone shout a bank account number, and I trusted that it was you, even though in this case, it wasn’t.\nJWTs were designed to prevent this sort of thing from happening. JWTs give people an easy way to pass data between each other, while at the same time verifying who created the data in the first place.\nIf I received 1,000,000 different JWTs that contained a bank account number, I’d easily be able to tell which one actually came from you.*\"\n\n## E Resources\n- [Good high level overview. Includes links to other good content](https://blog.logrocket.com/jwt-authentication-best-practices/#:~:text=A%20JWT%20needs%20to%20be,storage%20(or%20session%20storage).)\n- [Full guide](https://blog.angular-university.io/angular-jwt/)\n\ne being used to store user information that was used to authenticate them with a resource server. This is dangerous, b","n":0.024}}},{"i":1464,"$":{"0":{"v":"JWT Signature","n":0.707},"1":{"v":"\nThe signature is the key part of the JWT and is the part that provides its level of security.\n- The signature is what enables a fully stateless server to be sure that a given HTTP request belongs to a given user, just by looking at a JWT token present in the request itself, and without forcing the password to be sent each time with the request.\n- The signature is what proves that the payload is correct, and that it was actually sent by a given third party.\n\nWhile we can take the JWT header and payload and decode it with a base64 decoder, we cannot do the same with the signature. \n- in other words, it is not JSON; it is a cryptographic signature.\n\nThe JWT signature is created using the header, the claims AND the secret. Therefore, this unique combination creates a hash, and if something in the claim were to change, then the signature would be different and would no longer match up. \n- What this means is that if the 1st and 2nd set of the JWT don't change, than neither will the 3rd (of course assuming the secret remains unchanging)\n- Therefore, the signature of a JWT can only be produced by someone in possession of both the payload (plus the header) and a given secret key.\n- in practice, a JWT will always be different between instances of the same user signing in, since the `exp` variable will always be different\n\n<!-- TODO: broken image -->\n![8ded9f2b6239d7464fa7e85badce77c6.png](:/fa5c3e7e2d95477e85f51bed49c6b4d9)\n\nThe signature is a MAC (Message Authentication Code)\n\nHere is how the signature is used to ensure Authentication:\n1. the user submits the username and password to an Authentication server, which might be our Application server, but it's typically a separate server\n2. the Authentication server validates the username and password combination and creates a JWT token with a payload containing the user technical identifier and an expiration timestamp\n3. the Authentication server then takes a secret key, and uses it to sign the Header plus Payload and sends it back to the user browser\n4. the browser takes the signed JWT and starts sending it with each HTTP request to our Application server\n5. the signed JWT acts effectively as a temporary user credential, that replaces the permanent credential wich is the username and password combination\n\nFrom there, here is what our Application server does with the JWT token:\n1. our Application server checks the JWT signature and confirms that indeed someone in possession of the secret key signed this particular Payload. They know this because they can take the JWT header plus the payload and hash it together with the password (if HS256, the password must be the same as the possessed by the original issuer)\n2. The Payload identifies a particular user via a technical identifier\n3. Only the Authentication server is in possession of the private key, and the Authentication server only gives out tokens to users that submit the correct password\n4. therefore our Application server can safely be sure that this token was indeed given to this particular user by the Authentication server, meaning that it's indeed the user as it had the right password\n6. The server proceeds with processing the HTTP request assuming that it indeed it belongs to that user\n\n### Signature types\nThere are many types of signature for JWTs, but two main types: HS256 and RS256.\n\n#### HS256\nHS256 is a [[cryptographic hashing function|crypt.hashing]]\n- It is a symmetric algorithm, which means that there is only one private key that must be kept secret, and it is shared between the two parties\n- can be brute forced if the input secret key is weak (could also be said about many other key-based technologies)\n- requires the existence of a previously agreed upon secret between the original issuer of the JWT and any other server consuming JWTs (e.g. application server). Therefore, to change the secret is non-trivial.\n  - this means everyone who has the secret password can create JWTs. This also means there are more places where the secret can be stolen.\n\n#### RS256 (preferred)\nRS256 uses a [[public-key/private-key|crypt.public-key]] paradigm. The implication is that while only the authentication provider can sign (create) JWTs, the servers that consume JWTs can only validate them.\n- It is an asymmetric algorithm, which means that there are two keys: one public key and one private key that must be kept secret\n  - The auth server has the private key used to generate the signature, and the consumer of the JWT retrieves a public key from the metadata endpoints provided by the auth server and uses it to validate the JWT signature.\n- the auth server cannot validate tokens.\n\nRS256 is preferred, since:\n- you are sure that only the holder of the private key (ie. the auth server) can sign tokens, while anyone can check if the token is valid using the public key.\n- if the private key is compromised, you can implement key rotation without having to re-deploy your application or API with the new secret (which you would have to do if using HS256).\n\nRS256 signatures use RSA keys (which uses one key to encrypt and another to decrypt)\n- this isn't a hashing function, since the operation is reversible.","n":0.034}}},{"i":1465,"$":{"0":{"v":"JSON Web Key","n":0.577},"1":{"v":"\nThe JSON Web Key (JWK) is a JSON object that contains a well-known public key which can be be used to validate the signature of a signed JWT.\n- The members of the object represent properties of the key, including its value.\n- If the issuer of your JWT used an asymmetric key (like RS256) to sign the JWT, it will likely host a file called a JSON Web Key Set (JWKS). The JWKS is a JSON object that contains the property keys, which in turn holds an array of JWK objects.\n\nThe service may only use one JWK for validating web tokens, however the JWKS may contain multiple keys if the service rotates signing certificates.\n- we can use a service like Auth0 or Okta to store our JWKs, which will be located at an endpoint hosted at their domain. \n- Any time your application validates a JWT, it will attempt to retrieve the JWK(S) from the issuer in order to ensure the JWT signature matches the content.\n\nJWKs enable us to rotate keys.\n\nExample of a JWK representing an RSA public key:\n```json\n{\n  \"alg\": \"RSA\",\n  \"kid\": \"1\",\n  \"use\": \"sig\",\n  \"n\": \"qDABcd7-bvFz1MC1Upu5r1QhIwe8U7Gd0bY5\",\n  \"e\": \"AQAB\"\n}\n```\n\nWhere:\n- `alg`: specifies the type of cryptographic key. In this example, it's \"RSA\".\n\t- sometimes `kty` is used, for Key Type\n- `kid`: Key ID, a unique identifier for the key.\n- `use`: How the key is intended to be used. In this example, it's \"sig\" for signing.\n- `n`: Modulus, a large integer value that is part of the RSA public key.\n- `e`: Exponent, a small integer value that is part of the RSA public key.\n\n## JSON Web Key Set (JWKS)\nA JWKS is an object holding a set of JWKs containing the [[public keys|crypt.public-key]] used to verify any JWT issued by the Authorization Server and signed using the RS256 signing algorithm.\n\nThe auth server (e.g. AWS Cognito, Auth0) exposes a JWKS endpoint for each tenant, which contains the JWK used to verify all JWTs issued by the auth server for that particular tenant.\n\nExample JWKS:\n```json\n{\n  \"keys\": [\n    {\n      \"alg\": \"RSA\",\n      \"kid\": \"1\",\n      \"use\": \"sig\",\n      \"n\": \"qDABcd7-bvFz1MC1Upu5r1QhIwe8U7Gs3hhdk93d\",\n      \"e\": \"AQAB\"\n    },\n    {\n      \"alg\": \"RSA\",\n      \"kid\": \"2\",\n      \"use\": \"sig\",\n      \"n\": \"t5yOfCnYfJzFREvSxqOUHzUzL5KkHzzx2JgYB49\",\n      \"e\": \"AQAB\"\n    }\n  ]\n}\n```","n":0.053}}},{"i":1466,"$":{"0":{"v":"JWT Claim","n":0.707},"1":{"v":"\nthe core of a JWT, since they are the data contained in the JWT\n- claims are pieces of information that are \"claimed\" about a subject (most often a user). In other words, they are just properties of an object.\n\t- ex. name, sub, admin\n\t- ex. \"the holder of this token is able to create/read/update/delete a specified resource\"\n- the claim refers to the key, not the value\n- usually not encrypted, meaning if we don't use https, this information is potentially compromised.\n- anyone will be able to decode them and to read them, we cannot store any sensitive data in here\n\t- not an issue because of the secret\n```json\n{\n\t\"sub\": \"1234567890\",\n\t\"name\": \"John Doe\",\n\t\"admin\": true\n}\n```\n- when the server receives a JWT from an HTTP request's authorization header, like so:\n\n> Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhIjoxLCJiIjoyLCJjIjozfQ.hxhGCCCmGV9nT1slief1WgEsOsfdnlVizNrODxfh1M8\n\nit will verify the token using the secret, and then will serialize the claims in that token to the database. \n- If using Postgres, this would enable access to the data in `current_settings`, e.g. `current_setting('request.jwt.claim.email', true)`\n\nClaims can be used as a means to differentiate users. Imagine we are building a postgres database and decide that all signed-in users will have the group role `user_login`. We can use the claims in the jwt to distinguish users\n\n*Registered claim* - recommended pre-defined claims\n- **iss** (issuer)\n- **exp** (expiration time)\n- **sub** (subject)\n- **aud** (audience)\n\t\n## Properties\n- `iss` means the issuing entity, in this case, our authentication server\n- `iat` is the timestamp of creation of the JWT (in seconds since Epoch)\n- `sub` contains the technical identifier of the user\n- `exp` contains the token expiration timestamp","n":0.063}}},{"i":1467,"$":{"0":{"v":"Access Token","n":0.707},"1":{"v":"\nAn access token is used to inform an API that the bearer of the token has been authorized to access the API\n\nThe access token doesn't have any concept of sessions. All it cares about is user information. If we are leaving out refresh tokens, then we would only care if the user has an access token or not. If they do, then we trust that they are who they say they are, and we allow access.\n\nThe access token contains only the session ID and the user ID, and it expires when the user closes that browser window.\n\nAccess tokens are typically short lived (compared to the typically longer-lived [[auth.tokens.refresh]])\n- the benefit here is that if the access token is intercepted, the damage can be limited, since the token will expire imminently.","n":0.088}}},{"i":1468,"$":{"0":{"v":"Session","n":1},"1":{"v":"\n### General flow\n1. When user logs in with email and password, \n2. The authentication server verifies the identity of the user\n3. If successful, the session server creates an entry in its database (ie. a session) for the user, associating the session with the JWT [[access token|auth.tokens.access]] and defining its TTL (time to live).\n4. The server then responds to the client, providing it with the sessionId. \n5. The client then stores this sessionId somehow (e.g. in a [[cookie|browser.cookies]]). \n6. The client can now send info in that cookie with each request to the server, meaning that any server can check to see if there is an active session or not.\n\n![](/assets/images/2022-12-14-13-09-43.png)\n\nThe session server database (which stores the session) will associate the session with the refresh and access tokens\n\nSessions are stateful (unlike [[JWTs|auth.tokens.jwt]], which are stateless)\n- This means that one of the communicating parties must keep track of (store) information about the current state, and save information about session history.\n\nSessions piggyback off of stateless HTTP requests to make a stateful connection.\n- anal: the way TCP piggybacks on top of IP\n\nWhen there is an active session, the server responds to the request with an [[ETag|protocol.http.etag]] (an id corresponding to a particular \"version\" of data). If the data arrives and it is identical to the ETag, then we know it is the same as the cached version.\n\nIn a session-based workflow, a session is active until the user hits the logout button (or remains logged out for a longer period of time)\n\nWe can store additional data into the session as needed, such as the user’s set of permissions or anything else that is potentially useful.\n\nsessions are not an option for *mobile apps*.\n\n## Considerations\nSince sessions are stored server-side, administrators of the app retain full power and can rescind privileges of a session simply by destroying it and forcing the user to log in again.\n\nSession-based workflows introduce scalability concerns due to the inherent bottleneck behind which your data sits.\n- when using sessions, the resource servers must verify the session information with each request. It must therefore make its own request to the Session server, which will verify the session information.\n  - spec: This problem could be solved by using something like [[apollo.federation]], passing a token with the graphql request and having the gateway server perform the session lookup for you\n\nBroadly speaking, session-based authentication methods are better suited for user-to-server connections, while [[token|auth.tokens]]-based methods are better suited for server-to-server connections.\n\n### Sessions vs Tokens\n- The session authentication method is based on the concept of the ID being shared with the client through a cookie file, while the rest of the details are on the session file, stored on the server.\n- The token-based authentication method is based on the concept that possessing a token is the only thing that a user needs to have their requests authorized by the server, which must only verify a signature. The token is secure to use because it cannot be tampered with.\n- Both methods have inherent vulnerabilities that can be most easily resolved with different workarounds. In the end, it is up to the individual needs of the development team and their applications.\n\n## Persistent Session Store\na place to store all session-related data (but persistent so it doesn't get erased when the user closes the tab or app)\n- session store is like the next evolution after cookies. Cookies were being used to remember user's preferences and some searching data about them. When those client-side cookies reached their capacity, session storage starting taking over for that sort of data storage about a user.\n\n# Resources\n[A new session should be created during user authentication](https://rules.sonarsource.com/typescript/type/Vulnerability/RSPEC-5876)\n","n":0.041}}},{"i":1469,"$":{"0":{"v":"Passport","n":1},"1":{"v":"\n## Serializing/Deserializing\n```\npassport.serializeUser(function(user, done) {\n    done(null, user.id); \n   // where is this user.id going? Are we supposed to access this anywhere?\n});\n\n// used to deserialize the user\npassport.deserializeUser(function(id, done) {\n    User.findById(id, function(err, user) {\n        done(err, user);\n    });\n});\n```\n\n- the `user.id` we pass as an arg to `done` (inside `serializeUser`) is saved in the session. We can later use it to retrieve the whole user object (`deseralizeUser`)\n- `serializeUser` determined which data in the `User` object shoould be saved in the session \n\t- the result of `serializeUser` is attached to the session as `req.session.passport.user`\n- the `id` that is passed to `deseralizeUser` is used to find the session object that has already been stored. The key it looks for is the same key provided to the `done` funtion in `serializeUser`\n\t- the fetched object is attached to the request object as `req.user`\n","n":0.086}}},{"i":1470,"$":{"0":{"v":"*Arrs","n":1},"1":{"v":"\n*Arrs refers to the combined programs of Lidarr, Prowlarr, Radarr, Readarr and Sonarr.\n- these programs are designed to automatically grab, sort, organize, and monitor your Music, Movie, E-Book, or TV Show collections\n\n## Radarr, Sonarr, Lidarr, Readarr\nThese are search tools\n\nRadarr, Sonarr, Lidarr and Readarr allow us to search for movies, tv shows, music and ebooks.\n- these programs talk to Prowlarr, which looks around for the content and returns the location so the download client (e.g. qBittorrent) can download it.\n\nTheir job is to perform searches and track indexers for desired things, then submit download jobs to a download client (ie. a torrent or Usenet client)\n\nThese search tools will be notified by the download client when the download is complete, and automatically name the files and move them to their configured location.\n\nThe search tools will monitor your download client's active downloads associated with that category name (e.g movies, tv, series, music). \n- This monitoring occurs via your download client's API.\n\nWhen using torrents, the downloaded content will remain in the download folder so that it can be seeded. However, it is hardlinked to the media folder, so it does not take up any additional space.\n- you can confirm that the files are indeed hard-linked by verifying that the inode number is the same for each file (`ls -i`)\n\n## Prowlarr\nProwlarr crawls the internet and finds torrent files for us, negating our need to search for them manually\n\n### Indexer\nAn index is simply a torrent website that will be searched.\n\nExamples: ThePirateBay, KickassTorrents\n\n## Overseerr\nOverseer automates the accepting of requests, and feeds those into Radarr/Sonarr/Lidarr, which automates looking for those requests","n":0.062}}},{"i":1471,"$":{"0":{"v":"Arduino","n":1},"1":{"v":"\nArduino abstracts the circuit board part of electronics away from us, by giving us a pre-made integrated circuit\n- Arduino allows us to perform logical computations\n    - ex. When I press *this*, *that* happens\n\n### Parts of Arduino\nHardware\n1. a Printed Circuit Board (PCB), with contains the Integrated Circuit\n2. Pin headers, so we can easily connect to other devices\n3. USB port to connect to a computer\n\nSoftware\n1. IDE\n","n":0.125}}},{"i":1472,"$":{"0":{"v":"Apollo","n":1},"1":{"v":"\n# Overview\nIdea is that all the lower-level networking tasks (HTTP requests etc) as well as storing the data (caching) should be abstracted away and the declaration of data dependencies should be the dominant part.\n- This is precisely what GraphQL client libraries like Relay or Apollo will enable you to do. They provide the abstraction that you need to be able to focus on the important parts of your application rather than having to deal with the repetitive implementation of infrastructure.\n\nApollo is completely separate from the view-layer, making it framework agnostic.\n\nApollo allows us to send queries from the view-layer\n\n### As state management\n[high-level overview](https://www.apollographql.com/blog/dispatch-this-using-apollo-client-3-as-a-state-management-solution/)\n\n## Alternatives\n- Relay - Relay is more opinionated, Apollo is unopinionated\n- [Open source alternative to Apollo](https://formidable.com/open-source/urql/)\n","n":0.092}}},{"i":1473,"$":{"0":{"v":"Apollo Rover CLI","n":0.577},"1":{"v":"\nRover CLI lets us manage manage our graphs, allowing us to publish, fetch and compose graphs.\n- ex. if we have a subgraph that we want to compose into our supergraph, we can run `rover supergraph compose --config ./supergraph.yml`","n":0.162}}},{"i":1474,"$":{"0":{"v":"Apollo Federation","n":0.707},"1":{"v":"\nThe idea of Apollo Federation is that the organization should expose a single supergraph that provides a unified interface for querying all of our backing data sources. \n- However, we wouldn't want to do this with a single Graphql Server, so we'll string many subgraphs together into what's called a Gateway. This gateway acts as a proxy to the subgraphs.\n![](/assets/images/2022-02-10-09-10-43.png)\n\nThe Federated approach differs from achieving the same result via *schema stitching*, since it uses a declarative programming model that enables each subgraph to implement only the part of your composed supergraph that it's responsible for.\n\nApollo Federation encourages [[Separation of Concerns|general.principles.SoC]], allowing different teams to work on different products/features within a single graph without interfering others.\n\nEach subgraph should define the types and fields that it is capable of (and should be responsible for) populating from its back-end data store.\n","n":0.085}}},{"i":1475,"$":{"0":{"v":"Apollo Client","n":0.707}}},{"i":1476,"$":{"0":{"v":"Testing","n":1},"1":{"v":"\nInstead of wrapping our component with `<ApolloProvider />` (as we normally do), we wrap it with `<MockedProvider />`, which gives us benefits:\n- mocks all network calls, allowing us to test in isolation and get consistent results with each query (`mocks` prop)\n\n### `mocks`\nthe `mocks` array takes objects with specific requests and their associated results. \n\n```ts\nconst mocks = [\n  {\n    request: {\n      query: GET_DOG_QUERY,\n      variables: {\n        name: 'Buck',\n      },\n    },\n    result: {\n      data: {\n        dog: { id: '1', name: 'Buck', breed: 'bulldog' },\n      },\n    }\n  },\n];\n```\n\nWhen the provider receives a `GET_DOG_QUERY` with matching variables, it returns the corresponding object from the result key.","n":0.1}}},{"i":1477,"$":{"0":{"v":"Apollo Links","n":0.707},"1":{"v":"\nA system of modular components for GraphQL networking\n\t- Each link represents a subset of functionality that can be composed with other links to create complex control flows of data.\n\t- In a few words, Apollo Links are chainable \"units\" that you can snap together to define how each GraphQL request is handled by your GraphQL client.\n- When you fire a GraphQL request, each Link's functionality is applied one after another, allowing you to control the request lifecycle in a way that makes sense for your application\n\t- ex. Links can provide retrying, polling, and batching\n- At a basic level, a link is a function that takes an operation and returns an observable\n\t- an operation is an object with information like: query, variables, context\n- links are required when creating an Apollo client instance.\n- the core of a link is the `request` method, which accepts the `operation` (ex. query, mutation) as an argument\n\t- this method is called every time execute is run on that link chain (typically each time an operation is passed through the link chain)\n\n## HTTP Link\n- most common Apollo Link\n- The http link is a terminating link that fetches GraphQL results from a GraphQL endpoint using http\n\t- This can be used for authentication, persisted queries, dynamic uris, and other granular updates.\n\n## State link\n- allows us to use graphql query operations on client state. \n\n## Auth link\n- allows us to add an authorization header to each request\n","n":0.065}}},{"i":1478,"$":{"0":{"v":"Apollo React Hooks","n":0.577},"1":{"v":"\n# Query & Mutation\n- `useQuery` will execute on component mount, giving us back the loading, error and data state. Therefore, it is declarative \n- `useLazyQuery` will execute the query on command. This is perfect to use in events other than component mount, like on button click \n\t- To be clear, you could put this in `useEffect` and it would operate similarly to `useQuery`, since `useEffect` gets run on component mount\n- `useMutation` will return a function that we can execute to perform the mutation \n\n### Errors\nwe need to stringify errors:\n`console.log('error', JSON.stringify(err, null, 2))`\nspec: alternatively, we can use `apollo-link-error`\n\n","n":0.102}}},{"i":1479,"$":{"0":{"v":"Apollo Core","n":0.707},"1":{"v":"\nthe core part of apollo that is framework-agnostic. In other owrds it doesn't care about react and has no concept of hooks or anytihng like that\n","n":0.196}}},{"i":1480,"$":{"0":{"v":"Apollo Client Cache","n":0.577},"1":{"v":"\nApollo caches the result of every graphql query in a normalized [[cache|general.arch.cache]].\n- The cache normalizes query response objects before it saves them to its internal data store.\n- though like the result of a graphql query, the object can be arbitrarily deep. However, references are used, so that the same object is never stored twice.\n\n### Normalization\nNormalization happens in the following steps:\n1. The cache generates a CacheId for every identifiable object included in the response. \n\t- The CacheId will be in format: `<__typename>:<id>` (ex. `Bucket:232`)\n2. All the objects are stored by that generated ID in a flat lookup table.\n3. Whenever an incoming object is stored with the same CacheId as an existing object, the fields of those objects are merged.\n\t- this means that the only time anything is overwritten is when the field names are the same. If the incoming object has different fieldnames than the existing one, they will be preserved \n\t- ex. this has the further implication that if we had one query that returned a list of books (but only the title and author), then we subsequently clicked on a book and another query was made to get all the metadata data (description, page count, reviews etc.), that single book would not have been overwritten. The additional fields from the second query would have simply been merged.\n\nThe apollo cache takes the following shape:\n```json\n{\n  \"__typename\": \"Person\",\n  \"id\": \"cGVvcGxlOjE=\",\n  \"name\": \"Luke Skywalker\",\n  \"homeworld\": {\n    \"__ref\": \"Planet:cGxhbmV0czox\"\n  }\n}\n```\n\n## Interacting directly with the Cache\nWe can read/write to the Apollo cache without interacting with our Graphql server.\n\nWe can do this in a few different ways:\n- using regular Graphql queries to manage both remote and local data\n\t- `readQuery` / `writeQuery` / `updateQuery`\n- using Graphql fragments to access the fields of a cached object without having to compose an entire query to reach that object.\n\t- `readFragment` / `writeFragment` / `updateFragment`\n- modify the cache directly without using Graphql at all\n\t- `cache.modify`\n\n### Manually updating the cache\nWhen a single resource is updated with a mutation, Apollo handles caching for us (as long as we return the `id` and fields that were updated). \n- creating, deleting, or updating many at once will require us to manually update the cache.\n\n![[apollo.client.cache.api]]\n\n### Local-only fields\nWe can store and retrieve fields that only exist in the cache by using the `@client` decorator on our graphql query.\n```gql\nquery ProductDetails($productId: ID!) {\n  product(id: $productId) {\n    name\n    price\n    isInCart @client # This is a local-only field\n  }\n}\n```\n\n* * *\n\n### Type Policies\nBy defining type policies, we can determine how the cache interacts with specific types in the schema\n- done by mapping a `__typename` to the whole `TypePolicy` object.\n- in other words, the `typePolicies` object has `key`-`values` of `__typename`-`TypePolicy Object`\n\neach field in a `typePolicies` object is a type's `__typename`\n\nwe can customize how a particular field within our Apollo cache is written to and read. For this, we have 2 methods: `merge` and `read`.\n- with `read`, the cache calls that function whenever your client queries for the field. In the query response, the field is populated with the read function’s return value, instead of the field’s cached value.\n\t- Read is useful for manipulating values when they’re read from the cache, for example, things like formatting strings, dates, etc.\n- with `merge`, the cache calls that function whenever the field is about to be written with an incoming value. When the write occurs, the field’s new value is set to the merge function’s return value, instead of the original incoming value.\n\t- Merge can take incoming data and manipulate it before merging it with existing data. Suppose you want to merge arrays or non-normalized objects.\n- to define the policy for a single field, we need to first know which TypePolicy object the field corresponds to.\n\n`FieldPolicy` lets us customize how individual fields in the Apollo Client cache are read and written.\n","n":0.04}}},{"i":1481,"$":{"0":{"v":"Cook","n":1},"1":{"v":"\n### Delete item from cache after useMutation `update` function\n```ts\nconst handleDeleteBook = (bookId: string) => {\n  deleteBookMutation({\n    variables: {\n      bookId,\n    },\n    update: (cache) => {\n      const normalizedId = cache.identify({\n        id: bookId,\n        __typename: 'Book',\n      });\n\n      cache.evict({ id: normalizedId });\n      cache.gc();\n    },\n  });\n};\n```\n\n## Functions\n\n### Delete from cache\n```ts\nexport function deleteFromCache(\n  thisCache: ApolloCache<DeleteBookMutation>,\n  thisId: string,\n): void {\n  const normalizedId = thisCache.identify({\n    id: thisId,\n    __typename: 'Book',\n  })\n  thisCache.evict({ id: normalizedId })\n  thisCache.gc()\n}\n```\n\n### Update cache\n```ts\nexport function updateFromCache(\n  thisCache: ApolloCache<UpdateBookMutation>,\n  book: Book,\n  bookInput: BookInput,\n): void {\n  thisCache.writeFragment({\n    id: `Book:${book.id}`,\n    fragment: gql`\n      fragment MyBook on Book {\n        title\n      }\n    `,\n    data: {\n      title: bookInput.title ? bookInput.title : '',\n    },\n  })\n}\n```","n":0.1}}},{"i":1482,"$":{"0":{"v":"API","n":1},"1":{"v":"\n`useMutation` has an `update` function, whose purpose is to modify your cached data to match the modifications that a mutation makes to your back-end data\n- this method allows us to interact with the cache as if we were interacting with a graphql API. \n\t- ex. we can make general queries, as well as use fragments to help\n- we can interact directly with the cache with `readQuery, writeQuery, readFragment, writeFragment`, which are methods on the `ApolloClient` class\n- we can get the cached version of an entity with `cache.identify`\n\t- an input `{ id: 1, title: '', mediaItems: [{...}] }` gives us `Nugget:1`\n\n## Apollo `cache` object\n- Here, queries mirror what our gql queries would look like to target the same data from a Graphql server. Therefore, it must be a complete and valid query.\n- fragments on the other hand provide more *random access* (as opposed to *sequential access*) to our cached data.\n\t- therefore, we need to provide the cacheId as an argument to the fragment methods.\n\n### `readQuery`\n- like a regular graphql query, only it is performed on the cache, rather than the GraphQL API\n\n### `writeQuery`\n- uses the same shape as `readQuery`, but requires us to include a `data` option with the modifications we wish to make\n- the shape of our query is not enforced, and we can write fields that aren't present in our Graphql schema.\n- [docs](https://www.apollographql.com/docs/react/caching/cache-interaction/#writequery)\n\n### `readFragment`\nIf the object in the cache is missing fields that exist on the fragment, then we will be returned `null`.\n\n### `writeFragment`\nJust like `readFragment`, but like `writeQuery`, it requires a `data` object with the fields that we are writing.\n\n### `updateQuery` / `updateFragment`\nAllows us to read and write cached data in a single method call.\n- ApolloClient 3.5 and later\n- [docs](https://www.apollographql.com/docs/react/caching/cache-interaction/#using-updatequery-and-updatefragment)\n\n### `cache.modify` \n- [docs](https://www.apollographql.com/docs/react/caching/cache-interaction/#using-cachemodify)\n\n`modify` is a method we can execute on our cache that lets us modify individual fields directly \n- differs from `writeQuery` and `writeFragment` in that it will circumvent any `merge` function we have defined, meaning that fields are always overwritten with exactly the values you specify.\n- cannot write fields that do not already exist on the object in the cache.\n\n### `cache.identify`\nReturns the cacheId for the specified object","n":0.053}}},{"i":1483,"$":{"0":{"v":"Batch","n":1},"1":{"v":"\nIn GraphQL apps, batching usually takes one of two forms. \n1. take all operations and combine them into a single operation using the `alias` feature in GraphQL. \n    - This approach is not recommended, however, since this removes the ease of tracking metrics on a per-operation basis and adds additional complexity to the client.\n\n2. send an array of operations to a GraphQL server, having the server recognize the request as an array of operations instead of a single one, and handle each operation separately. \n    - This method still requires only a single round-trip, while retaining the ability to track single operation performance. Apollo Client handles batching like this using `apollo-link-batch-http`. It’s also worth noting that this method requires the server to be able to receive an array of operations as well as single operations. Luckily, `apollo-server` supports this out of the box.\n","n":0.084}}},{"i":1484,"$":{"0":{"v":"API","n":1},"1":{"v":"\nAn api is really an interesting thing, because it allows you to do the work once, and have its effect (result) be replicated ad infinitum. Consider the way the cursor works in WoW. The cursor points over something, and somehow it knows what it is hovering over. Well, For a mouse to able to locate something in that sense, it has to have the coordinates of the item (ex. a chest). If the coordinates that you are hovering over match the coordinates of the chest, then you know you are hovering over the chest. The function to determine this is complex, but it is at the end of the day reducible to a function. We can still walk away with an atomic piece that can be transported anywhere. So imagine we take the functionality of determining where the coorinate of the cursor is and wrap it up into a function. Now all we need to do is run that function every time the mouse moves. We have just created an API that easily replicable across the entire application. \n\nwe can make a simplifying assumption in the case of dataflow through an API: it is reasonable to assume that all the servers will be updated first, and all the clients second. Thus, you only need backward compatibility on requests, and forward compatibility on responses.\n\n## Best Practices\n- follow the single responsibility principle to build the API and [[paginate|api.pagination]] the API response by default\n- simplify the API by reducing the number of API parameters and define default parameter values on heavily used APIs\n- provide consistent API endpoints through naming conventions, input parameters, and output responses\n- include meaningful error messages in the API response for the ease of troubleshooting\n- rate limit the API and design for scale\n- avoid breaking changes to the API\n\n### Robustness principle (Postel's law)\n\"be conservative (strict) in what you send, be liberal in what you accept\"\n- programs that send messages to other machines (or to other programs on the same machine) should conform completely to the specifications, but programs that receive messages should accept non-conformant input as long as the meaning is clear.\n\n# UE Resources\n[difference between API and SDK](https://nordicapis.com/what-is-the-difference-between-an-api-and-an-sdk/#:~:text=By%20definition%2C%20an%20SDK%20is,to%20allow%20communication%20between%20applications.)\n","n":0.053}}},{"i":1485,"$":{"0":{"v":"Pagination","n":1},"1":{"v":"\n*page window* is the amount of items returned per page (ie. per request)\n\n## Limit/Offset Pagination\n- `offset` -> with this request, what is our starting point?\n- `limit` -> how many records will we be retrieving per request?\n\nA naive approach to pagination is to think like this: \"We want page 3, with a page size of 10, so we should load 10 items, starting after item 20\". This might look like this:\n```sql\nSELECT * FROM posts ORDER BY created_at LIMIT 10 OFFSET 20;\n```\nThis approach is naive because it has some major downfalls\n- If an item was added to the list while the user is switching pages, we will inevitably be skipping over items\n    - this issue can be mitigated with proper ordering. For instance, if we order by `created_at`, then new items will always appear at the end.\n- If an item was added at the top of the list while switching pages, then we might see the same item twice.\n- The rows skipped by an `OFFSET` clause still have to be computed inside the server; therefore a large `OFFSET` might be inefficient.\n\nThese previous examples shows the limitation of the analogy of pagination as pages in a book, since the data set is not static. This goes to show that for certain apps, the very concepts of page1 and page2 don't really make any sense, because the set of data and the boundaries between loaded sections is constantly changing.\n\n## Cursor-Based Pagination\n> \"Cursor-based pagination is the most efficient method of paging and should always be used where possible.\"\n\n- see [[db.strategies.cursors]]\n\nWhat if we could just specify the place in the list we want to begin, and then specify how many items we want to fetch? Then it doesn’t matter how many items were added to the top of the list in the meanwhile, since we have a constant pointer (cursor) to the specific spot where we left off.\n\nAn API request that implements cursor-based pagination needs 2 things to fetch more data: \n1. the current cursor position (the cursor parameter), obtained from the previous API response\n2. the number of items to fetch\n\nbenefits of cursor-based pagination:\n- scalable for large datasets because the [[database index|db.strategies.index]] is leveraged to prevent a full table scan\n- pagination window is not affected when high-frequency writes to the database occur as the next cursor remains the same\n","n":0.051}}},{"i":1486,"$":{"0":{"v":"API Gateway","n":0.707},"1":{"v":"\nAn API gateway acts as a [[reverse proxy|webserver.proxy.reverse]] to accept all API calls, aggregate the various services required to fulfill them, and return the appropriate result.\n\nAPI gateways can be used to unify access control across all the APIs that sit behind\n- ex. authentication, rate limiting, and analytics\n\nApplications that are built using [[microservices|general.arch.microservice]] most likely use an API gateway, since a single API call could result in dozens of calls to distinct APIs\n\nHaving an API gateway allows querying users to continue to access the API in the same fashion indefinitely, even if APIs behind the gateway are modified.\n\nAn API Gateway will use a [Circuit Breaker](https://microservices.io/patterns/reliability/circuit-breaker.html) to invoke services\n\nIn implementing an API gateway, an event-driven/reactive approach is best if it must scale to scale to handle high loads.\n- AWS offers [[AWS API Gateway|aws.svc.api-gateway]]\n\n### Downsides\n- Increased complexity - the API gateway is yet another moving part that must be developed, deployed and managed\n- Increased response time due to the additional network hop through the API gateway - however, for most applications the cost of an extra roundtrip is insignificant.\n\n* * * \n\n### Backends for Frontends\nThe *Backends for frontends* pattern is a variation on the API gateway pattern\n\nThis pattern defines a separate API gateway for each kind of client\n- ex. API gateways for web application, mobile application etc.\n\n![](/assets/images/2023-02-06-12-10-18.png)\n\n### API Composition\nIn systems that apply both the Microservices pattern and the *Database per service* pattern, it is no longer straightforward to implement queries that join data from multiple services.\n- this is the role of an *API Composer*, which invokes the services that own the data and performs an in-memory join of the results.\n- [[general.patterns.architectural.CQRS]] is an alternative approach to this problem\n\n\n## UE Resources\n- [Netflix API gateway](https://netflixtechblog.com/optimizing-the-netflix-api-5c9ac715cf19)","n":0.06}}},{"i":1487,"$":{"0":{"v":"API Compatability","n":0.707},"1":{"v":"\nThe need to maintain backward and forward compatibility appears in many [[distributed|deploy.distributed]] systems\n\n- *Backward compatibility* is making new code compatible with existing data. \n- *Forward compatibility* is making existing code compatible with new data.\n\nBackward compatibility is straightforward. It’s the ability to open old documents in new versions of the program.\n\nForward compatibility, the ability to open documents in formats invented in the future, is rarer. We can see forward compatibility in web browsers, which are written so the features added to HTML won’t break the ability to render new sites in old browsers.\n\nThe publisher of an API must balance their desire to make changes against their customers' reluctance to change something that works for them.\n- The result is strong pressure to preserve backward compatibility over time, often across many versions.\n\nAn API publisher can intuit that an endpoint can respond with additional data, trusting existing clients to ignore it, but not require additional data in requests, because existing clients won’t know to send it. \n\nStripe [solved this problem](https://stripe.com/blog/api-versioning) by building a middleware system through which all requests and responses to its API travel. It translates between the latest version of the Stripe system and the API version that the client is using. As a result, Stripe developers don't have to worry as much about nuances with old API versions, since the middleware ensures the old version will be converted to comply with the latest version. When Stripe developers want to create a new API version, they just need to add a new stack to the middleware.\n![](/assets/images/2022-03-18-13-01-30.png)\n\n## Tools\n- [Cambria](https://github.com/inkandswitch/cambria-project)\n  - helps us achieve campatability between schema versions— You specify (in YAML or JSON) a lens, which specifies a data transformation.\n  - takes lessons from Stripe's middleware approach to compatability (described above)\n  - [Intro and overview of Cambria](https://www.inkandswitch.com/cambria/)\n\n## E Resources\nhttps://www.inkandswitch.com/cambria/","n":0.058}}},{"i":1488,"$":{"0":{"v":"REST","n":1},"1":{"v":"\nREST is a set of architectural constraints for an API.\n\nREST is typically built on top of [[protocol.http]]\n\nWhen a client makes a request to a REST API, the request serves as a representation of the state of the resource.\n\nThe API is client-facing. Therefore, any questions about what an API endpoint does should ultimately fall back to the user's expectations. \n- ex. soft-delete is a common implementation, where a column on a database item called `isDeleted` is simply switched to `true`, rather than actually removing an item from a table. From an API design standpoint, the question then becomes \"which [[http method|protocol.http.methods]] is this, a DELETE request, or PATCH request?\". If a DELETE request is for *deleting resources*, then surely we'd want to use a PATCH request. However, the logic is that the API hides implementation details behind its interface. The implementation of a soft-delete is of no importance to the client using the API, therefore none of the interface should reflect any kind of implementation details. A DELETE http request is the right choice here because of this.\n\n## Terminology\n- *resource* - any piece of information that can be accessed from the API\n    - Each resource has a unique name, called the *resource identifier*.","n":0.07}}},{"i":1489,"$":{"0":{"v":"Apache","n":1},"1":{"v":"\nApache is an HTTP server (just like NGINX)\n\n# How it works\n- Apache listens to the IP addresses identified in its config file (HTTPd.conf). Whenever it receives a request, it analyzes the headers, and takes action; considering the rules specified for it in the Config file,\n\n## Four main directories on apache server\n- `htdocs` contains the files to be served to the client upon receiving HTTP requests\n\t- files and sub-directories under htdocs are available to the public \n- `conf` contains all server configuration files \n\t- similar to Dockerfile, it is a manifesto of instructions for the apache server\n- `logs` contains server logs\n- `cgi-bin` contains CGI scripts\n\n# E Resources\n[primer on apache](https://code.tutsplus.com/tutorials/an-introduction-to-apache--net-25786)\n","n":0.096}}},{"i":1490,"$":{"0":{"v":"Apache Reverse Proxy","n":0.577},"1":{"v":"\nWe need to load the proxy and proxy_http modules in Apache, and the most basic configuration would look like this (for one domain):\n\n```xml\n<VirtualHost *:80>\n    ServerName foo.example.com\n    ProxyPass / http://192.168.2.x\n</VirtualHost>\n```\n","n":0.186}}},{"i":1491,"$":{"0":{"v":"Apache Hadoop","n":0.707},"1":{"v":"\n### What is it and what is it for?\nSome computations can have their performance improved by splitting the task up evenly among many computers.\n- ex. if we have 100 million numbers and need to find the largest one, we can either give it to one computer to do it, or we can break it into parts (say 100 different groups), and give each group to a different computer. Each computer then solves the problem of finding the largest number among 1 million numbers, and then of the resulting set of 100 numbers, we find the largest one. Doing it this way (ie. in parallel) is exponentially faster.\n- problems such as these are called [Embarrassingly Parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel)\n    - the method of breaking it down (mapping) into pieces and then joining the individual results to form a global result (reducing) is called *MapReduce*\n\nHadoop is an open source software that makes doing [[MapReduce|general.patterns.map-reduce]] type programming easier. You dont have to worry about installing the program on your 100 machines, breaking your initial data into pieces, copying it to all 100 machines, copying results over from 100 machines, etc. All the housekeeping is managed by Hadoop. Once you setup a hadoop cluster over the 100 machines, you can give it any program and data and it takes care of all the behind the scenes work and give you back the result.\n\nHadoop has often been used for implementing [[ETL|db.strategies.etl]] processes\n- data from transaction processing systems is dumped into the distributed filesystem in some raw form, and then MapReduce jobs are written to clean up that data, transform it into a relational form, and import it into an MPP data warehouse for analytic purposes\n\nThe biggest limitation of [[unix]] tools is that they run only on a single machine—and that’s where tools like Hadoop come in.\n","n":0.058}}},{"i":1492,"$":{"0":{"v":"Android","n":1},"1":{"v":"\n### Hermes\n- a engine for optimizing RN apps on Android\n","n":0.316}}},{"i":1493,"$":{"0":{"v":"Android Commands","n":0.707},"1":{"v":"\n# Android Emulator\n- open developer menu - `cmd+m`\n","n":0.354}}},{"i":1494,"$":{"0":{"v":"ADB","n":1},"1":{"v":"\n# Adb reverse and port forwarding\n`adb -s 192.168.76.101:5555 reverse tcp:8081 tcp:8081`\nIn either case, it's basically just port forwarding. It's probably called \"reverse\" because it's actually setting up a [[reverse proxy|webserver.proxy.reverse]] in which a http server running on your phone accepts connections on a port and wires them to your computer... Or vice versa. The port is specified twice because you have the ability to control the listening port on both sides of the proxy.\n\nWhen the RN packager is running, there is an active web server accessible in your browser at 127.0.0.1:8081. It's from this server that the JS bundle for your application is served and refreshed as you make changes. Without the reverse proxy, your phone wouldn't be able to connect to that address.\n\nNow when your phone tries to access `http://localhost:3000` your request will be routed to localhost:3000 of your laptop. Just recompile your app to use localhost:3000 as the API endpoint.\n\nif we had written `adb reverse tcp:80 tcp:3000`, it would redirect your phone's port 80 to your computer's port 3000\n","n":0.076}}},{"i":1495,"$":{"0":{"v":"Alfred","n":1}}},{"i":1496,"$":{"0":{"v":"Alfred Workflows","n":0.707},"1":{"v":"\nEach item in a workflow is called an `Object`\n- an input can be a `trigger`, `input`, `action`, `utility`, or `output`\n\nEach workflow has its own folder, found in `Alfred.alfredpreferences/workflows/user.workflow.20D99320-DB8C-43F0-8128-B0981143AE62`\n- To access this folder, click into a workflow object and look for this symbol in the resulting window.\n- this is currently stored in Onedrive so multiple machines can sync the same Alfred config.\n- We can store scripts in here. For instance, we could have a node script, and then call that script with a simple bash script (ScriptFilter workflow object):\n\t- `./recentdownloads 'added_reverse'`\n\t\t- the current directory here would be the workflow folder.\n\n![](/assets/images/2021-11-30-22-10-28.png)\n\n## Workflow Object types\n- *Triggers*: Activate Alfred from a hotkey, another Alfred feature or an external source.\n- *Inputs*: Keyword-based objects used to perform an action, on its own or followed by a query.\n- *Actions*: The objects that do most of the work in your workflows; opening or revealing files and web searches, running scripts and performing commands.\n- *Utilities*: Utilities give you control over how your objects are connected together and how the arguments output by the previous object is passed on to the next object.\n- *Outputs*: Collect the information from the earlier objects in your workflow to pop up a Notification Centre message, show output in Large Type, copy to clipboard or run a script containing the result of your workflow.\n\n### Placeholders\n- `{query}` - the string that the user adds after the keyword.\n- `{arg}` - the arg is what is passed from the previous object.\n\n### Variables and Arguments Utility\nWe can modify arguments. For instance, `{query}` evaluates to whatever was typed in the Alfred launcher. We can modify this to be `- {query}`, for instance if we were creating a bulleted list.\n- otherwise, we can just pass the argument through.\n\n![](/assets/images/2021-11-29-21-58-21.png)\n\nIn this example, I've set two variables; The first variable named task will save the {query} input, allowing me to re-use it later in the workflow with {var:task}\n![](/assets/images/2021-11-29-21-58-11.png)\n\n#### Custom Environment Variables\nIcon at top right of workflow window:\n![](/assets/images/2021-12-19-10-34-56.png)\n\n## Debugging\n### Bash script\nredirect stdout to stderr to see logs in the debug console:\n```\n>&2 echo $query\n```\n","n":0.054}}},{"i":1497,"$":{"0":{"v":"YML","n":1},"1":{"v":"\nYAML stands for *Yaml Ain't a Markup Language*.\n\nYAML is excellent for configuration files and alike thanks to anchors and aliases that prevent you from having sparse chunks of text, as well as comments which allow you to clearly explain the logic behind a given setup, etc.,\n\nRather than parse and stringify, YAML uses the following terms:\n- *load* - a load is the process of translating a character stream to data structures (the actual data you operate with).\n  - Parsing is a step of that process, yet the YAML processor has additional tasks to perform than to simply parse a source.\n- *dump* - the reverse operation of serializing data back to text.\n\nYAML doesn't support extending arrays, so when we override an array value, we need to provide all the contents of the array, not just the diff\n\n## Directives\nA directive is an instruction you can supply to the YAML processor. One can pick from two directives:\n- `YAML`\n- `TAG`\n\n`---` is not a \"document start\" marker. This particular notation stands for \"directives end\".\n- YAML allows you to specify multiple documents in a single stream (e.g. file), by separating them with `—--`\n\n## Code reuse\n### Anchors (`&`) and Aliases (`*`)\ncan be used to duplicate content across your yml file.\n- with it, we can duplicate or inherit properties\n\nAnchors are identified by an `&` character, and aliased by an `*` character.\n\nYou can use YAML anchors to merge YAML arrays.\n\nBelow `&flag` identifies the Apple item, and is later referenced.\n```yml\n- &flag Apple\n- *flag\n```\n\nBecomes this when the yml is parsed:\n```yml\n- Apple\n- Apple\n```\n\n### Overrides (`<<`) a.k.a. map merging\nThis is a special key that indicates key-values from another mapping should be merged into this mapping.\n\nThis example demonstrates 2 features:\n- Anchors can be used with non-scalar values, such as whole objects (here the `nodeinfo` object is referred to in the `echoit` container)\n- Overrides, which allows us to merge the key-value pairs of `&function` into the `echoit` object\n    - note: that the `image` key-value gets overwritten, since we define it even though it already exists in `&function`\n```yml\n# docker-compose.yml\nservices:\n  nodeinfo: &function\n    image: functions/nodeinfo:latest\n    labels:\n      function: \"true\"\n\nechoit:\n    <<: *function\n    image: functions/alpine:health\n```\n\n* * *\n\n### Strings in YAML\nWhen putting a string over multiple lines\n- `|` will preserve all new lines\n- `>` will fold them into a single line\n\n```yml\n|\nWinnie-the-Pooh\nВинни-ПухCOPY\n```\n\nThe equivalent string in a JSON document would be:\n```json\n\"Winnie-the-Pooh\\nВинни-Пух\\n\"COPY\n```\n\nNow, if the folded style was used, the output would be considerably different:\n```yml\n>\nWinnie-the-Pooh\nВинни-ПухCOPY\n```\nWould result in:\n```json\n\"Winnie-the-Pooh Винни-Пух\\n\"\n```","n":0.051}}},{"i":1498,"$":{"0":{"v":"Raspberry Pi","n":0.707},"1":{"v":"\n- [Setup headless RPi (inc. WiFi)](https://brandonb.ca/raspberry-pi-zero-w-headless-setup-on-macos)\n- [Build your own NAS w/ RPi](https://pimylifeup.com/raspberry-pi-nas/)\n","n":0.289}}},{"i":1499,"$":{"0":{"v":"PDF","n":1},"1":{"v":"\nPDFs are popular in part because the creator can put locks on the document, allowing them to prevent you from doing things like copy the text or edit the fields without having been authenticated first.\n- You can get through a locked PDF relatively easily. There is a software called CutePDF Printer for Windows, and if you run the PDF through it, the lock will effectively be removed.\n\n# Resources\n- [Remove the lock on PDFs preventing us from copying text](https://www.howtogeek.com/666814/how-to-print-to-pdf-on-mac/)\n- https://superuser.com/questions/47462/cant-copy-text-from-a-pdf-file\n","n":0.113}}},{"i":1500,"$":{"0":{"v":"Operating System","n":0.707},"1":{"v":"\nAll user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.\n\n### Parts of the OS\nThe [[kernel|os.kernel]] is the brain of the OS, but other parts provided by the OS are:\n- User interface\n- security\n- networking ability\n  \n\n### User space (userland) and Kernel space\nTo mediate access to the same resources, the [[kernel|os.kernel]] has special rights, reflected in the distinction of kernel space from user space.\n\n#### Kernel space\nKernel space is strictly reserved for running a privileged operating system kernel, kernel extensions, and most device drivers. Anything running with this privilege is fully trusted and has full control over the OS.\n\nKernel space can be accessed by user module only through the use of system calls.\n\n#### User space\nUser space is the memory area where application software and some drivers execute. Processes running here are subject to the Principle of Least Privilege.\n- *userland* refers to all code that runs outside the operating system's kernel. These are programs and libraries that the operating system uses to interact with the kernel.\n- ex. bash, vim, VSCode, WoW etc.\n\n","n":0.072}}},{"i":1501,"$":{"0":{"v":"Thread","n":1},"1":{"v":"\n## What is it?\nA \"thread\" is a series of instructions that must be run in a row, but your computer may switch which thread is being run at any time. \n- Threads are most commonly a part of [[OS]] or a part of a language runtime.\n\nThreads are the unit of execution on a processor. When a program is run on your computer, that starts up a process. That process can then be made up of one or more threads which execute different tasks.\n\n[[scheduled|os.scheduling]] work is carried out on threads.\n- Therefore a thread is the smallest amount of instructions that can be handled by a scheduler.\n\t- think of it as the atomic unit from the scheduler's perspective.\n\nA [[process|os.process]] is an instance of a program that is being executed on 1 or more threads.\n- multiple threads can exist on a process simultaneously.\n\t- the number of threads depends on the programming language and source code that the process is running on.\n- threads within a process share resources like memory, but threads in different processes do not share resources.\n- a process can be forked (split) into many child processes\n\n### Analogy: Cooks in a kitchen\nJust as a group of cooks can work together in a kitchen to prepare multiple dishes at once, threads can work together in a computer to perform multiple tasks simultaneously. Each cook in the kitchen can focus on a specific task, such as chopping vegetables, cooking meat, or preparing a sauce, and by working together, they can prepare a meal more efficiently and quickly than a single cook could do alone. \n- Similarly, threads can be assigned different tasks to perform simultaneously, such as processing user input, updating a graphical user interface, or running a background task, and by working together, they can improve the performance and responsiveness of a computer program.\n\nHowever, just as the cooks in a kitchen need to coordinate with each other to avoid getting in each other's way or duplicating efforts, threads in a computer also need to be managed and coordinated to avoid conflicts or inefficiencies. This is typically handled by the [[OS]] or programming language runtime, which schedules and manages the execution of threads and ensures that they have access to the resources they need to perform their tasks.\n\n## Why use them?\n**concurrency**: Threads enable [[concurrency|general.concurrency]] which is important in distributed systems. Concurrency allows us to schedule multiple tasks on a single processor. These tasks are run in an interleaved manner and essentially share CPU time between themselves. \n- ex. with I/O concurrency, instead of waiting for an I/O operation to complete before continuing execution (thereby rendering the CPU idle), threads allow us to perform other tasks while we wait.\n\n**Parallelism**: We can perform multiple tasks in parallel on several cores. Unlike with just concurrency where only one task is making progress at a time (depending on which has its share of CPU time at that instant), parallelism allows multiple tasks to make process at the same time since they are executing on different CPU cores.\n\n**Convenience**: Threads provide a convenient way to execute short-lived tasks in the background \n- ex. a master node continuously polling a worker to check if it's alive.\n\n### Multi-process vs Multi-threaded\nIf a single process in a multi-process application crashes, that process alone dies. The buffer is flushed, and all the other child processes continue happily along.  In a multi-threaded environment, when one thread dies, they all die.\n- On some OSs, processes tend to be more expensive than threads (though on Linux, the difference is not that great)\n\n### Thread safety\n*Thread safe* means that you can access the resource in multiple threads without worrying about the final state.\n- Something thread safe is designed to deal with the possibility the same section of code will be run at the same time by two different threads or the same variable will be used or altered by a different thread.\n\t- anal: [[general.concurrency]]\n\nImagine you have a list of names. You have two threads that want to add to the list of names. The *non*-thread safe version can go like this:\n```log\nThread 1 reads the list of names [a,b,c]\nThread 2 reads the list of names [a,b,c]\nThread 1 adds d to its list and writes it back out [a,b,c,d]\nThread 2 adds e to its list and writes it back out [a,b,c,e]\n```\n\nBecause thread 1's read and write operation got interrupted, \"d\" is not in the final list. If the `addName()` operation was thread safe, this wouldn't have been possible. It would go like this:\n\n```log\nThread 1 adds d to the list [a,b,c,d]\nThread 2 adds e to the list [a,b,c,d,e]\n```\n","n":0.036}}},{"i":1502,"$":{"0":{"v":"Process","n":1},"1":{"v":"\nSince a process itself forms an environment, [[environment variables|unix.environments]] are associated with a single process.\n- ex. when we say `process.env` we are talking about getting environment variables that exist during a specific process\n\nIf a program is a set of instructions to carry out a specific task, then a process is just **a program in execution**\n\na **Signal** is the way that a process can communicate with the OS\n- Signal and interrupt are basically same, but with a small distinction:\n    - interrupts are generated by the processor and handled by the \n    - signals are generated by the kernel and handled by the process.\n\nEvery time a program wants to do something (like process a key press, or open a file, or exit) that program needs to ask its parent to do it for it (the parent is the kernel)\n\n### How a process comes to life\nA process comes into existence via actions facilitated by the [[kernel scheduler|os.kernel.scheduler]].\n\n### How a process requests service of the kernel\nA process can request the kernel's services by either making a system call or passing a [[message|general.patterns.messaging]]\n- The OS will implement one or the other, but not both.\n\nBy accessing these services, the process can do things like\n- Accessing hardware related services\n    - ex. if Zoom needs to access the Camera, or VSCode needs to access your file system\n- Spawning new processes\n- Communicating with integral kernal services, like [[scheduling|os.kernel.scheduler]]\n\nThese system calls serve as interfaces between a process and the OS.\n\nSystem calls are analogous to calling REST endpoints. The OS provides a library of user functions, which a process can call.\n\nFrom the perspective of the application making the system call, it is identical to an ordinary procedure call.\n\n# Debugging\n- `strace`/`truss`, `ltrace` and `gdb` are generally good ideas for looking at why a process is stuck. (truss -u on Solaris is particularly helpful; I find ltrace too often presents arguments to library calls in an unusable format.) Solaris also has useful /proc-based tools, some of which have been ported to Linux. (pstack is often helpful).\n","n":0.055}}},{"i":1503,"$":{"0":{"v":"Kernel","n":1},"1":{"v":"\nThe kernel is the part of the OS that serves as the master control program of all processes thought of as back-end.\n- it is the lowest level of [[general.terms.abstraction]] in the [[os]].\n- It provides an interface to hardware devices, and manages system processes, memory and other resources\n\nThe OS runs the kernel as fully trusted software. Once root access (kernel space) is granted to the kernel, a user can command the OS to do anything.\n- therefore, the kernel is what must implement protection mechanisms that cannot be subverted by untrusted software executing in [[user space|os#user-space-userland-and-kernel-space,1]].\n\nThe [[operating system|os]] uses the kernel to perform grunt work involved in bridging the gap between the computer's hardware and its software.\n- the kernel talks to the hardware via drivers (which are loaded as kernel extensions) and/or firmware\n- ex. when an application wants to change the system volume, it submits a request to the kernel to perform the action. Because of the speaker drivers which it has access to via the kernel module (like a built-in extension; provides the same benefit), the kernel can control the volume\n\nThe kernel provides services that...\n- handle I/O\n- maniupulate the file system\n- start and stop programs\n- schedule access to avoid conflicts when programs try to access the same resource or device simultaneously. \n- handle other common \"low-level\" tasks that most programs share, \n\nThe kernel also plays a big part in resource management, managing resources like memory and CPU.\n- the kernel must manage how memory is used, which means it has to be fully aware of how it is being used. It has to know exactly where in memory any process's resources will reside.\n- naturally, the kernel tries to optimize the usage of the resources it manages. It is fine if memory usage is high, even for moderate usage. The kernel knows how to remove things from memory in order to make space for the higher priority data.\n\nFailures can lead to deadlocks, resulting in the whole system halting because resources used by one application are needed by another.\n\nTechnically speaking, [[Linux|linux]] is not an operating system; it refers to the kernel itself. This misunderstanding is somewhat justified, given how integral the kernel is to the [[os]]\n- examples of operating systems in Linux are Ubuntu, Mint, Debian etc.\n\nLike any piece of software, the kernel is versioned. If we want to see our kernel and its version, issue:\n```sh\nuname\nuname -r\n```\n","n":0.05}}},{"i":1504,"$":{"0":{"v":"Scheduler","n":1},"1":{"v":"\nScheduling is the action of assigning resources to perform tasks. \n- The resources may be processors, network links or expansion cards. \n- The tasks may be threads, processes or data flows.\n\nSchedulers are designed so as to keep all computer resources busy\n- therefore, they analaglous to [[load balancers|deploy.distributed.load-balancer]]\n\nSchedulers also manage how each user shares system resources effectively.\n\nScheduling is fundamental to computation itself, and an intrinsic part of the execution model of a computer system\n\n* * *\n\n### Kernel thread\nA kernel thread is a \"lightweight\" unit of kernel scheduling\n\nAt least one kernel thread exists within each process. \n- If multiple kernel threads exist within a process, then they share the same memory and file resources. \n\nKernel threads are preemptively multitasked if the operating system's process scheduler is preemptive. Kernel threads do not own resources except for a stack, a copy of the registers including the program counter, and thread-local storage (if any), and are thus relatively cheap to create and destroy.\n- Thread switching is also relatively cheap\n\n* * *\n\n### Cooperative multitasking (non-preemptive multitasking)\n- In this scheduling paradigm, the OS has its role as a scheduler reduced. Instead of the scheduler determining which process' turn it is to get run, the processes themselves voluntarily perform this duty. That is, each process knows when its turn is over, and cedes processing power to the next process in line.\n- In this scheme, the process scheduler of an operating system is known as a cooperative scheduler, having its role reduced down to starting the processes and letting them return control back to it voluntarily\n\nThis paradigm is widely used in memory-constrained embedded systems\n- the concept of scheduling makes it possible to have computer multitasking with a single [[CPU|hardware.cpu]].","n":0.06}}},{"i":1505,"$":{"0":{"v":"Device Driver","n":0.707},"1":{"v":"\nA driver is software that is run by the [[kernel|os.kernel]] that gives it the ability to control a particular piece of hardware (e.g. keyboard, audio interface)\n\nA driver communicates with a device via a [[computer bus|hardware.bus]]\n1. When a calling program invokes a routine in the driver, the driver issues commands to the device (drives it). \n2. Once the device sends data back to the driver, the driver may invoke routines in the original calling program.\n\nDrivers are hardware dependent and [[operating-system|os]] specific\n","n":0.112}}},{"i":1506,"$":{"0":{"v":"Scheduling","n":1},"1":{"v":"\nScheduling is the action of assigning resources to perform tasks\n- The resources may be processors, network links or expansion cards. \n- The tasks may be threads, processes or data flows.\n","n":0.183}}},{"i":1507,"$":{"0":{"v":"CMS","n":1},"1":{"v":"\nTraditionally, CMSs managed all content on a website, including \n- content, \n- images, \n- HTML, \n- CSS\n\nThis made it impossible to reuse the content because it was commingled with code.\n","n":0.183}}},{"i":1508,"$":{"0":{"v":"Headless CMS","n":0.707},"1":{"v":"\n- a Headless CMS can be thought of an an API-first CMs\n\nany type of back-end content management system where the content repository “body” is separated or decoupled from the presentation layer “head\".\n- if the presentation layer of a website is the “head” of a CMS, then cutting off that presentation layer creates a headless CMS.\n\nThe idea is *not* that we have no head, but that we get to *choose* our head. In traditional CMSs, there is an integrated head, meaning the presentation layer is coupled to the data-handling layer. Suppose we already have a React app set up, so the visual layer is already taken care of. Now, all we need is the editing interface to alter our content, and the database to store it. A Headless CMS provides these for us.\n\nAll Headless CMSs pretty much boil down to some database backend with a web-based user interface, and content made accessible through an API.\n\n## Providers\n- https://forestry.io/\n- [Keystone](https://keystonejs.com/)\n- [Strapi](https://strapi.io/)\n- [Prismic](https://prismic.io/)\n- [Sanity](https://www.sanity.io/)\n","n":0.079}}},{"i":1509,"$":{"0":{"v":"CGI (Common Gateway Interface)","n":0.5},"1":{"v":"\n### What is it?\nCGI is a specification for web servers (like Nginx, Apache) to execute command line programs that run on a server and generate web pages dynamically.\n- Such programs are known as CGI scripts or simply as CGIs\n- Normally, a CGI script executes at the time a request is made and generates HTML.\n- ex. an HTTP GET or POST request from the client may send HTML form data to the CGI program (via STDIN). Simultaneously, the CGI program will receive other data like URL path and HTTP headers as a list of environment variables. \n\n### Purpose\n- The HTTP server (ex. Express) will have a directory that is designated as a document collection (ie. each file in the directory is a document).\n\t- These files can be sent to users accessing the website. \n- ex. if our website was www.example.com and we had a document collection stored at `/usr/local/apache/htdocs` of the local filesystem, the Web server will respond to a request for `http://example.com/index.html` by sending to the browser the (pre-written) file `/usr/local/apache/htdocs/index.html`\n\n## FastCGI\n- this is an alternative approach and variation of CGI.\n- it is a protocol for interfacing interactive programs with a web server\n- purpose is to reduce overhead by allowing a single, long-running process to handle more than one user request\n\t- result is that the server can handle more web page requests per unit of time.\n- FastCGI applications remain independent of the web server.\n- FastCGI can be implemented in any language that supports network sockets\n\t- ex. Node, PHP, Python\n- both Nginx and Apache implement FastCGI\n","n":0.063}}},{"i":1510,"$":{"0":{"v":"CAN","n":1},"1":{"v":"\nThis note is for CAN systems on modern vehicles.\n\n## CAN (Controller Area Network)\nThe CAN Bus acts as a central networking system, allowing any ECU to communicate to the rest of the system.\n- in the car, nodes (ie. electronic control units, or ECU) are connected via the CAN Bus.\n  - these ECUs might be the Engine Control Unit, Airbags, Audio system etc\n- anal: the CAN Bus is analogous to the nervous system in the human body. That is, it facilitates communicates between all parts\n\nAn OBD-II (on-board diagnostics) tool can be used to identify what is wrong with your car.\n- the DTCs (diagnostic trouble codes) are retrieved by CAN loggers.\n- OBD-II is also used for real-time vehicle telematics and post analysis\n- OBD-II is vehicle make agnostic\n\n### CAN Frame (ie. CAN Message)\nThe data in a CAN frame can be broken up into eight one-byte values, sixty-four one-bit values, one sixty-four bit value, or any combination of these\n\nIncludes\n- CAN-ID, which includes message priority, as well as functional address (e.g. RMP, wheel speed etc.)\n- RTR (Remote Transmission Request), which allows ECUs to request messages from other ECUs\n- the data itself\n\nExample frame:\n- 0CF00400 FF FF FF 68 13 FF FF FF\n  - if you have a DBC that contains decoding rules for the CAN ID (here, `0CF00400`), you can extract parameters (ie. signals) from the data bytes\n- Decoded: \n```json\n{\n  \"message\": \"EEC1\",\n  \"signal\": \"EngineSpeed\",\n  \"value\": 621,\n  \"unit\": \"RPM\"\n}\n```\n\n## DBC (CAN Bus Databases)\nthe type of data that communicates over a CAN bus can be read and understood using DBC files.\n- DBC files can help identify the data within the CAN frame by describing it.\n- DBC files contain information for decoding raw CAN bus to physical values (ie. human-readable form). Thus, functioning as a signal library.\n\nFor a DBC file, the signal is not an electrical input or output, but it is a physical parameter, such as temperature, engine speed, voltages etc.\n\na DBC file helps in understanding what data is communicating through the CAN bus\n\nThe DBC file contains the following information.\n- The CAN ID of the message in which this signal is present\n- The position where the signal is present in the CAN message\n- The byte order of the signal\n- The conversion details of the signal\n- Unit of the signal\n\nex. you could have a DBC file that contains decoding rules for Speed and EngineSpeed. Therefore, you could extract time-based Speed and EngineSpeed information from cars/trucks, tractors etc.\n\nUsing dedicated software, we can upload the data log files (ie. all of the CAN Frames, of which there are probably millions), along with the DBC file.\n- the output might be as simple as a `.csv` of all the decoded CAN frames.\n\nProprietary DBC files are often used by OEM manufacturers for decoding their CAN data for blackbox logging.","n":0.047}}}]}
