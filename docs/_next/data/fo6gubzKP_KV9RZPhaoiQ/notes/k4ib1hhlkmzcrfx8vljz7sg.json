{"pageProps":{"note":{"id":"k4ib1hhlkmzcrfx8vljz7sg","title":"Neural Network","desc":"","updated":1692806939461,"created":1688841508824,"custom":{},"fname":"ml.deep-learning.nn","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"2799e534d23d458d7edee23b3fb9ad45","links":[{"type":"wiki","from":{"fname":"ml.deep-learning.nn","id":"k4ib1hhlkmzcrfx8vljz7sg","vaultName":"tech"},"value":"statistics.regression","alias":"linear regression","position":{"start":{"line":17,"column":31,"offset":898},"end":{"line":17,"column":74,"offset":941},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"statistics.regression"}},{"type":"wiki","from":{"fname":"ml.deep-learning.nn","id":"k4ib1hhlkmzcrfx8vljz7sg","vaultName":"tech"},"value":"statistics.regression","alias":"regression","position":{"start":{"line":35,"column":29,"offset":2714},"end":{"line":35,"column":65,"offset":2750},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"statistics.regression"}},{"type":"ref","from":{"fname":"ml.deep-learning.nn","id":"k4ib1hhlkmzcrfx8vljz7sg","vaultName":"tech"},"value":"ml.deep-learning.nn.types","position":{"start":{"line":67,"column":1,"offset":4239},"end":{"line":67,"column":31,"offset":4269},"indent":[]},"xvault":false,"to":{"fname":"ml.deep-learning.nn.types"}},{"from":{"fname":"pytorch","id":"i2it5id8qwtg27n4usg8bo1","vaultName":"tech"},"type":"backlink","position":{"start":{"line":25,"column":33,"offset":804},"end":{"line":25,"column":101,"offset":872},"indent":[]},"value":"ml.deep-learning.nn"},{"from":{"fname":"ml.deep-learning","id":"r0w4lqir8cq5ez9inlimel8","vaultName":"tech"},"type":"backlink","position":{"start":{"line":4,"column":3,"offset":150},"end":{"line":4,"column":42,"offset":189},"indent":[]},"value":"ml.deep-learning.nn"},{"from":{"fname":"ml.terminology","id":"r9fy49qm440b41yo8sqfwz5","vaultName":"tech"},"type":"backlink","position":{"start":{"line":36,"column":19,"offset":1850},"end":{"line":36,"column":58,"offset":1889},"indent":[]},"value":"ml.deep-learning.nn"},{"from":{"fname":"ml.deep-learning.nn.functions","id":"v3rh8rqc5aorupfno1w4otw","vaultName":"tech"},"type":"backlink","position":{"start":{"line":91,"column":9,"offset":5995},"end":{"line":91,"column":47,"offset":6033},"indent":[]},"value":"ml.deep-learning.nn"}],"anchors":{"overview":{"type":"header","text":"Overview","value":"overview","line":8,"column":0,"depth":2},"node-aka-neuron":{"type":"header","text":"Node (a.k.a Neuron)","value":"node-aka-neuron","line":20,"column":0,"depth":3},"weight":{"type":"header","text":"Weight","value":"weight","line":31,"column":0,"depth":3},"bias":{"type":"header","text":"Bias","value":"bias","line":48,"column":0,"depth":3},"layers":{"type":"header","text":"Layers","value":"layers","line":55,"column":0,"depth":3},"how-a-neural-network-learns-simplified":{"type":"header","text":"How a neural network learns (simplified)","value":"how-a-neural-network-learns-simplified","line":65,"column":0,"depth":3},"types-of-neural-network":{"type":"header","text":"Types of Neural Network","value":"types-of-neural-network","line":72,"column":0,"depth":2},"example-number-recognition":{"type":"header","text":"Example: number recognition","value":"example-number-recognition","line":77,"column":0,"depth":3},"ue-resources":{"type":"header","text":"UE Resources","value":"ue-resources","line":90,"column":0,"depth":2},"e-resources":{"type":"header","text":"E Resources","value":"e-resources","line":94,"column":0,"depth":2}},"children":["v3rh8rqc5aorupfno1w4otw","hgm9om14ascftd0kof1fbzb"],"parent":"r0w4lqir8cq5ez9inlimel8","data":{}},"body":"<h1 id=\"neural-network\">Neural Network<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#neural-network\"></a></h1>\n<h2 id=\"overview\">Overview<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#overview\"></a></h2>\n<p>Neural networks are preferred when you have unstructured data (ie. data that is not neatly formatted in tables)</p>\n<ul>\n<li>that's not to say that they can't work well with structured data. It's more the case that traditional machine learning algorithms like decision trees or SVMs often handle structured data well.</li>\n</ul>\n<p>At a basic level, a neural network is comprised of four main components: </p>\n<ul>\n<li>inputs\n<ul>\n<li>ex. your inputs may have a binary value of 0 or 1</li>\n</ul>\n</li>\n<li>weights - Larger weights make a single input’s contribution to the output more significant</li>\n<li>a bias or threshold - the output value of any node must be above the threshold for data to be sent to the next layer of the network\n<ul>\n<li>a threshold value of 5 would translate to a bias value of –5.</li>\n</ul>\n</li>\n<li>an output (<code>y-hat</code>)</li>\n</ul>\n<h3 id=\"node-aka-neuron\">Node (a.k.a Neuron)<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#node-aka-neuron\"></a></h3>\n<p>Each node corresponds to a neuron in a biological neural network</p>\n<p>Think of each node as its own <a title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\">linear regression (Private)</a> model.</p>\n<ul>\n<li>since linear regression can be used to predict future events</li>\n</ul>\n<p>Think of a neuron as a function that takes in the outputs of all nodes from the previous layer and returns a number between 0 and 1.</p>\n<ul>\n<li>this number is called the <em>activation</em>, and it is the output of the <em>activation function</em></li>\n<li>the activations in one layer determine the activations in the subsequent layer.</li>\n<li>note: While many activation functions produce values between 0 and 1 (like the sigmoid function), not all do. For instance, the ReLU (Rectified Linear Unit) function produces values between 0 and infinity, and the tanh function outputs values between -1 and 1.</li>\n</ul>\n<h3 id=\"weight\">Weight<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#weight\"></a></h3>\n<p>Each connection between 2 nodes is assigned a weight, which indicates the strength of that connection, and how much influence the input has on the node.</p>\n<p>Each node (the current node) receives input from <em>every</em> node of the previous layer. To each of these inputs we apply a weight that affects how much of a contribution that particular node of the previous layer has on the current node</p>\n<p>Weight is a fundamental concept to neural networks because inputs naturally have a different magnitude of effort on the outcome</p>\n<ul>\n<li>ex. in a model that predicts housing prices, the recency of a paint job and the number of bedrooms are both inputs to the model that affect the price, but the latter has a much bigger impact on the ultimate price of the house.</li>\n</ul>\n<p>Weights are the primary mechanism by which neural networks learn. During training, the network gets feedback on its predictions in the form of a loss or error. To minimize this error, the model uses optimization techniques (like gradient descent) to adjust the weights. Over time, the model gradually makes more and more accurate predictions.</p>\n<p>The main difference between <a title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\">regression (Private)</a> and a neural network is the impact of change on a single weight. </p>\n<ul>\n<li>In regression, you can change a weight without affecting the other inputs in a function. this isn’t the case with neural networks. Since the output of one layer is passed into the next layer of the network, a single change can have a cascading effect on the other neurons in the network.</li>\n</ul>\n<p>The neural network figures out the weights on its own</p>\n<p>a.k.a representation, patterns, numbers, features</p>\n<h3 id=\"bias\">Bias<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#bias\"></a></h3>\n<p>The bias is a value that is applied to the weighted sum of all neurons from the previous layer, adjusting the importance of that neuron. </p>\n<ul>\n<li>In other words, the bias gives us some indication of whether or not that neuron tends to be active or inactive.</li>\n<li>More specifically, bias shifts the activation function along the input axis, essentially determining the threshold at which the neuron begins to activate.</li>\n</ul>\n<p><img src=\"/assets/images/2023-08-10-21-26-46.png\" style=\"max-width:200px;\"></p>\n<h3 id=\"layers\">Layers<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#layers\"></a></h3>\n<p>Neural networks are composed of layers. Generally they are:</p>\n<ul>\n<li>input layer</li>\n<li>hidden layers</li>\n<li>output layer</li>\n</ul>\n<p><img src=\"/assets/images/2023-07-02-20-17-39.png\"></p>\n<p>Each layer is usually a combination of linear functions (ie. straight lines) and non-linear functions (ie. non-straight lines)</p>\n<h3 id=\"how-a-neural-network-learns-simplified\">How a neural network learns (simplified)<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#how-a-neural-network-learns-simplified\"></a></h3>\n<p>A neural network learns by:</p>\n<ol>\n<li>starting with random numbers</li>\n<li>perform tensor operations</li>\n<li>update random numbers to try and make them better representations of the data</li>\n<li>repeat</li>\n</ol>\n<h2 id=\"types-of-neural-network\">Types of Neural Network<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#types-of-neural-network\"></a></h2>\n<p></p><p></p><div class=\"portal-container\">\n<div class=\"portal-head\">\n<div class=\"portal-backlink\">\n<div class=\"portal-title\">From <span class=\"portal-text-title\">Types</span></div>\n<a href=\"/notes/hgm9om14ascftd0kof1fbzb\" class=\"portal-arrow\">Go to text <span class=\"right-arrow\">→</span></a>\n</div>\n</div>\n<div id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\">\n<div class=\"portal-parent-fader-top\"></div>\n<div class=\"portal-parent-fader-bottom\"></div><p>Data is passed from one layer to the next in what's called a <strong>Feedforward neural network (FNN)</strong> </p>\n<ul>\n<li>so-called because connections between the nodes do not form a cycle.</li>\n</ul>\n<p>Other types of Neural Network:</p>\n<ul>\n<li><strong>Convolutional Neural Networks (CNNs)</strong> are commonly used as the model architecture in image classification tasks</li>\n<li><strong>Recurrent Neural Networks (RNNs)</strong> are commonly used in natural language processing tasks</li>\n<li><strong>Transformer</strong> are commonly used in natural language and speech</li>\n<li><strong>Fully Connected Neural Networks (FCNN)</strong></li>\n</ul>\n</div></div><p></p><p></p>\n<hr>\n<h3 id=\"example-number-recognition\">Example: number recognition<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#example-number-recognition\"></a></h3>\n<p>Imagine we have a model that takes an image (28x28 pixels) of a handwritten number and tells us what number it is. </p>\n<ul>\n<li><em>First layer</em> - Our neural network will start with 784 neurons (<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>28</mn><mi>x</mi><mn>28</mn></mrow><annotation encoding=\"application/x-tex\">28x28</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">28</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">28</span></span></span></span></span>), and each value between 0 and 1 will represent its grayscale level (from black to white). These 784 neurons make up the first layer of our neural network.</li>\n<li><em>Last layer</em> - Our last layer will be composed of 10 neurons, each representing a digit from 0-9. </li>\n<li><em>Hidden layers</em> - Though hidden layers are a black box, it helps to think of them in terms of how they <em>might</em> behave. Generally when solving problems with computers, it's helpful to break problems down into smaller problems, then use the smaller building block solutions to amount to a bigger solution. In this case, it's difficult to recognize the number <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>9</mn></mrow><annotation encoding=\"application/x-tex\">9</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">9</span></span></span></span></span>. However, it's easier to recognize a shape with a loop and a tail (the top and bottom part of the number, respectively). There are however, different numbers that have loops (<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">6</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">6</span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">8</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">8</span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0</span></span></span></span></span>) and different numbers that have tails (<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">7</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">7</span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span>). So we might think, \"What if each node in our second last layer represented a different component shape? Then the activation functions of the <em>loop node</em> and the <em>tail node</em> would output a number close to 1, which would be enough information to tell us that we have the number <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>9</mn></mrow><annotation encoding=\"application/x-tex\">9</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">9</span></span></span></span></span>\".\n<ul>\n<li>of course, then the question becomes \"how do we recognize shapes like loops and tails?\". We can continue to break down these shapes to reveal straight-ish edges. That is, we can think of the upper loop of the number <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>9</mn></mrow><annotation encoding=\"application/x-tex\">9</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">9</span></span></span></span></span> as being made up of ~5 straight-ish edges. When our neural network receives the handwritten <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>9</mn></mrow><annotation encoding=\"application/x-tex\">9</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">9</span></span></span></span></span>, it can break it down first into edges, then determine which shapes those edges make, and then finally determine which number it is based on which shapes it has found.</li>\n<li><a href=\"https://youtu.be/aircAruvnKk?t=443\">video demonstration</a></li>\n</ul>\n</li>\n</ul>\n<p>The activation of these neurons (often simply a number between 0-1) represents how much the model thinks that the input image is that particular number</p>\n<ul>\n<li>ex. if our input drawing was of a 9, then the final neuron in the last layer will have the highest activation value</li>\n</ul>\n<p>In this example, the <em>weights</em> of each node activation tell us the pixel pattern that that node is picking up on, while the <em>bias</em> tells us how high the weighted sum needs to be before we consider the neuron to be meaningfully active</p>\n<h2 id=\"ue-resources\">UE Resources<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#ue-resources\"></a></h2>\n<ul>\n<li><a href=\"http://neuralnetworksanddeeplearning.com/\">Neural Networks and Deep Learning Book</a>\n<ul>\n<li>has a lot of code examples and gives a good fundamental overview. Recommended by 3Blue1Brown</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"e-resources\">E Resources<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#e-resources\"></a></h2>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=aircAruvnKk&#x26;t=707s\">But what is a neural network?</a>\n<ul>\n<li>explains the fundamentals of a neural network, including layers, neurons, biases, weights etc.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<strong>Children</strong>\n<ol>\n<li><a href=\"/notes/v3rh8rqc5aorupfno1w4otw\">Functions</a></li>\n<li><a href=\"/notes/hgm9om14ascftd0kof1fbzb\">Types</a></li>\n</ol>\n<hr>\n<strong>Backlinks</strong>\n<ul>\n<li><a href=\"/notes/i2it5id8qwtg27n4usg8bo1\">Pytorch</a></li>\n<li><a href=\"/notes/r0w4lqir8cq5ez9inlimel8\">Deep Learning</a></li>\n<li><a href=\"/notes/r9fy49qm440b41yo8sqfwz5\">Terminology</a></li>\n<li><a href=\"/notes/v3rh8rqc5aorupfno1w4otw\">Functions</a></li>\n</ul>","noteIndex":{"id":"olZIVfSs2uLLr3BppFh4K","title":"Digital Garden","desc":"","updated":1674517603573,"created":1615482407722,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"effb007003ca6a91d7fd0c293e1d2436","links":[{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"testing.method.unit","alias":"unit testing","position":{"start":{"line":18,"column":121,"offset":1146},"end":{"line":18,"column":157,"offset":1182},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"testing.method.unit"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"general.arch.microservice","alias":"microservices","position":{"start":{"line":18,"column":188,"offset":1213},"end":{"line":18,"column":231,"offset":1256},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"general.arch.microservice"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"paradigm.oop","alias":"OOP","position":{"start":{"line":36,"column":227,"offset":2718},"end":{"line":36,"column":247,"offset":2738},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"paradigm.oop"}}],"anchors":{"tags":{"type":"header","text":"Tags","value":"tags","line":46,"column":0,"depth":3},"resources":{"type":"header","text":"Resources","value":"resources","line":55,"column":0,"depth":2},"ue-unexamined-resources":{"type":"header","text":"UE (Unexamined) Resources","value":"ue-unexamined-resources","line":56,"column":0,"depth":3},"e-examined-resources":{"type":"header","text":"E (Examined) Resources","value":"e-examined-resources","line":59,"column":0,"depth":3},"resources-1":{"type":"header","text":"Resources","value":"resources-1","line":62,"column":0,"depth":3}},"children":["40ubf88tephzbdjte8cdsa0","zFMjbn3xihVNHjUIdZCD1","IK6NOKemuDjhfstJBovKL","ZaTr28eWk1DrXOEsc1YVb","Vi0WYVLZunVM9iR8XZJn3","ngAbg7gluvbt1bY1UIRsi","RCPPXSakm5TvKka8kOmVp","LIcuGYV0DDt1VWbvH6Sed","MPx8ykTP57I40WTZvTP7f","ZU5WmdTG1bHoE8RcmZXZG","jqWiyYJff92RjXuUQt9PQ","KihijM8OQvZ4pASkkhqzL","F9vyqvls3OBCujtukqKhy","k6jxm2b3edgkhbordpuz6v1","facc2b01-755a-409f-99f6-57bef2d1501f","bZumdyapJ2H0wWWOmJ45i","elqpgoe2r951si4xrhujppj","m5ov42Vm6mR7RQWTvl1NW","cw8cerc0cos8lh871oz8rtf","PZxxZ4iESzSlbbHJKxuAZ","UDu62Pa6BaRqlG8uGoMhy","u3zcndycwqessho5h6x0nz9","ZgCUp366YrF2Tyky2NT73","DVpVUmavSoVDA7UIlPzLX","nMCtMXVvjBsJk3iw1zoIO","ANfx9Z4a6ZA0uJuSHtbIJ","BkG557LKUYbH1DnxprEVT","1TaH8zDTM98FZ9SYaeXLM","02v7ymc144e5c4pv3edkud7","dc79799f-a55d-48ab-a8be-c48adb1b19c0","f6feef57-b8f5-451f-a09a-7b63f6a07183","4sz47Y0LKs1Si73rWtyyh","g7ulqi8no93ezeocbesc3ll","5a9fb1df-478e-4687-9be0-5cb97e61ec57","1374e9e9-1cbc-4e1f-b1ca-66b8569533dd","D8Z3rjXkSj2EOymXQXF4Z","np3c1ykvnjqv5xoombpfwqz","f529cc34-aba0-45ca-ad7e-02ddda318941","0zcauha3il2NqtxZazIo7","9bbd6f68-03b2-41f4-92e4-2ca313e8b450","5a2ab598-fa7e-4471-8bda-9f5831b679ae","uV6w4mZPoohWyZV4Xaad0","QHXEIyeZGIGMVi5Q52UWI","RgE0mZLaUjPftFPZsiAoe","mytCOts26Pidush65tdRW","fwUzxfLSPMH1eL8oBoLWx","wazdsda6h25x66edvfmeuiv","TbW7PM9bg1y5TGkiWwQ8b","xiSIDeEtIc8X0lpUQlppI","0jxgntiLNHWFuCbzqtFGF","GkdMprLUe4QQULBxmGN6V","4SYc6v5hxY5g6Ip6kjpwO","czi7ilt2i1uoqm1f2otbntj","fqVQpS9FBiXgKsZX3R3sJ","L5JUZlGAGvTxrsEBB7DY8","ttyri4pwfyn5lcx0vp9804o","TeTedoeS2LdHPR632eCpM","ulicRRwo3lSFzh3tMfWH9","p7d24vyb8m00ombzn34t50c","e65188a1-177e-4ab2-99f9-75f85d537621","fajYnbVhCRDi74xn0H30R","bArGdGwqo3iFkyKMyE7qR","4Dcp7gbEVoLLgfu7bXFai","5d700782-fb81-416e-83f9-5dd88260e350","zubgzhNFE6KlTgXcjTz6O","8lpdfWa0cbSq1XJQbcYcY","2psluywdc416t7vrql0m058","17I8ZksXvqCH1mRtZDjHp","kF916Ow84qpJJeMRkWMIo","tMLkLcrIHHBz56xVmBLkP","CSePBQ6q7qhowKESqVwt7","laZ4OfLhZNK1Kuy6GaWUr","z5IJblOknQhMzZ5QZh4ye","N36FHxfxzwJfxDY3miWyX","3bb25f58-2b50-4fa3-af55-48ea9f88a081","7x07qjgbitozdfszqzuy7ix","u93Rz4fEWGu6VBR30Zraf","9hjMHnKvYT4jLKvuDSXaV","qC5GxCZBmNb4Ip6c0kU8x","LgW7mTIALODoXc54B3p6S","UYPxfHBFWX7fb6hHU5bB6","74lYtC8NKpCzcyFOZTfR1","sEoBNAEuaTxwSmTXDonZt","dj0jr9mpvs62e2pkg3zc3yy","z8ie0xjogb6ht7gzowav5xr","qxSOd6SPN8qf9ZUojVFDX","WQoMTf6VXBaxCgksXAVsj","2Mw4XgfyCHXHNOX5yoCIY","qcdt2f7jo51muquo5r95dpo","0XJqmcdtcMZu66glBI5O8","lK8r8BXS4ThiUTe4xKIZe","93de42ca-53ea-460a-baa7-b9ec5c47cb1e","Rxs2jaGpdFzqYtP7lAJFJ","aO8W81Z0PyIb6Hs7nOHPW","lhzisalkebu4w5n01np07i0","7b5l4b6fi65n7sv9org5q1l","I01hENHnh8Tqu3Ok8sLzG","03o3n0hz9v9jtb7j889zpd4","evqsPNutOaZ8hcBCqxFQu","zhhxcjZUHdU8uRLwGb9Zh","G1aFACZB2ooWGMGwyd3ZW","YWy1C4tgoaCcw1m8JJsr7","Szj3o5iaNxPpesiCqwrbu","613syb18hb3v0u1ydvor7ru","XhvCDW3fIw6h6MhY5ticq","Q70g7SusFZBQXzkuQifv4","AzfWDH3wp7jFpL2EYxBcW","y0fwpZ9qMqirsLiFyOciU","ZF8xj8wwDUqKlrwTrCFZ1","tAJvhqhdfyZZa87QHq1TU","4hRmipi8lxpBLyzWu5JVB","iTg0C7QjvnmqBZeEigJNs","3babc3d2-79ae-470a-9c06-ab8bba2e684e","bF3UsMFya3fMeXWDspVov","ULkfbL9WpktbVYnzhl6Jw","PpNOO8JYWe6dM8wruSa7x","di40pCxDn7IiqE8lFdD46","rmW0mkerqV35I8QPji6lM","3IFIK1ByzeIxZCByryGLN","iImkYAKfkw3beAl6pLbDn","ecDe8DNWrkeQTwpTEvHje","i2it5id8qwtg27n4usg8bo1","Yqhdd9mSJGN7OJOeyoSD2","Ws5tah8tpeyn9tK8VBTg8","gWAg15uBJgkS2B0wcpMAa","l7V3v2ep1YdDCt7DOr7Ci","yM2PJBdqJnHpD63cPA6sW","qn0bre7eLbi3QMbCfWkUi","fSu0KxFL41IRouotqmbHs","0gtg24Mj1a1bQFPRGQNlO","7iQPBMltLPLbFEz2qbjPu","yoh4pwoXcfELInGKRdYf6","dCGCWXgAmiOZXbdULT1m6","jMavlje07sNa6hSEIE8WA","Xxm0JE4dKHxrQAaZfzvxD","nRb6Im4Kcmc2ZWE7K1jZ1","PAEBZCyFBZJyR7OoMZ41E","PQ6km8RgRCuyICBPOYz8f","x5tm1nfjzyawwzedy3yitgd","2bhftt8rGuxYu4pFgNqru","hjYIZpHQWuXfeEoGeJEKW","K2M9bQqVq2eQfm29eslKL","X8obW1iKwYsvNgKWGyCzy","yJwSC7hqYIezTFHf5i0Ev","c4Z7ETcOHUILRMH32Sfjw","qiR6dIu857b9M9kTqjOyK","fc2coz74cnfy5czzofx4h5x","Ku1OgHMhELajzo61Gx7ye","LUrfhDWo8wuwZu7CN9TV8","osu6JGOnvXJ5gt3tpqWZY","1a6173cd-cf13-4b34-a522-8350bf9a364f","S2sBltrPfd8a7ICuD7CuH","GLQ2pmkJUNUa93THBDVsD","md6xitz4exia2joa06i490b","oWCuBXOg6JWfZzjmKxmNl","jOmhZ8ovLYTPbpM1vqSDx","p9bov84s0isgkl1ysaw93kk","FraC6xzLy1ei91l1ICyc9","6ceBas2RE9Q4787GDngH7","734cd78d-0bc9-426b-803d-1efc84dfffe5","k4Bb09px6r0FxIRs49SXV","oaG3H1S9IUBO644nGZigu","Ka7agQJkUMRSWN0uFdkWK","si3z090WsiLasMhJBa1Az","hs6rwzt4mogiicoc4gcykbi","ljKAVERmdEiKLK9hXGKBm","zxt3lhonfdhglvijd17ua8c","dd7dopve1dudqkoibkqvti4","923tgifqf59ovv5yldtyi0a","vrjwp01goqw47fqctm4f4lo","c99gdmmppju3r1tth8cb2jx","z2pvn5qxdz84zgygqzxage8"],"parent":null,"data":{},"body":"This Dendron vault of tech knowledge is organized according to domains and their sub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git would include things like `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner`, and so on.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the \"aha\" moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nAs a result, I have allowed myself to use potentially arcane concepts to explain other ones. For example, in my note on [[unit testing|testing.method.unit]], I have made reference to the [[microservices|general.arch.microservice]] note. If these notes were made with the public in mind, this would be a very bad strategy, given that you'd have to understand microservices to be able to draw that same parallel that I've already drawn. Since these notes are written for myself, I have been fine with taking these liberties.\n\nWhat I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course this is all\nvague sounding, and really depends on the domain along with the ends I am\ntrying to reach.\n\nTo achieve this, the system should be steadfast to:\n- be able to put information in relatively easily, without too much thought\n\trequired to its location. While location is important, Dendron makes it easy\n\tto relocate notes, if it becomes apparent that a different place makes more\n\tsense.\n- be able to extract the information that is needed, meaning there is a\n\thigh-degree in confidence in the location of the information. The idea is\n\tthat information loses a large amount of its value when it is unfindable.\n\tTherefore, a relatively strict ideology should be used when determining\n\twhere a piece of information belongs.\n\t- Some concepts might realistically belong to multiple domains. For instance, the concept of *access modifiers* can be found in both `C#` and `Typescript`. Therefore, this note should be abstracted to a common place, such as [[OOP|paradigm.oop]].\n\nThis Dendron vault is the sister component to the [General Second Brain](https://tech.kyletycholiz.com).\n\n### Tags\nThroughout the garden, I have made use of tags, which give semantic meaning to the pieces of information.\n\n- `ex.` - Denotes an *example* of the preceding piece of information\n- `spec:` - Specifies that the preceding information has some degree of *speculation* to it, and may not be 100% factual. Ideally this gets clarified over time as my understanding develops.\n- `anal:` - Denotes an *analogy* of the preceding information. Often I will attempt to link concepts to others that I have previously learned.\n- `mn:` - Denotes a *mnemonic*\n- `expl:` - Denotes an *explanation*\n\n## Resources\n### UE (Unexamined) Resources\nOften, I come across sources of information that I believe to be high-quality. They may be recommendations or found in some other way. No matter their origin, I may be in a position where I don't have the time to fully examine them (and properly extract notes), or I may not require the information at that moment in time. In cases like these, I will add reference to a section of the note called **UE Resources**. The idea is that in the future when I am ready to examine them, I have a list of resources that I can start with. This is an alternative strategy to compiling browser bookmarks, which I've found can quickly become untenable.\n\n### E (Examined) Resources\nOnce a resource has been thoroughly examined and has been mined for notes, it will be moved from *UE Resources* to *E Resources*. This is to indicate that (in my own estimation), there is nothing more to be gained from the resource that is not already in the note.\n\n### Resources\nThis heading is for inexhaustible resources. \n- A prime example would be a quality website that continually posts articles.  - Another example would be a tool, such as software that measures frequencies in a room to help acoustically treat it.\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.83.0","vaults":[{"fsPath":"../main/tech","name":"tech"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"The Tech Digital Garden of Kyle Tycholiz"},"github":{"cname":"tech.kyletycholiz.com","enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableMermaid":true,"siteUrl":"https://tech.kyletycholiz.com","duplicateNoteBehavior":{"action":"useVault","payload":["tech"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}