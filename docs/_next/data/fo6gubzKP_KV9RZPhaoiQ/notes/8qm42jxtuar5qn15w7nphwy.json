{"pageProps":{"note":{"id":"8qm42jxtuar5qn15w7nphwy","title":"Tensors","desc":"","updated":1692807161077,"created":1689029383059,"custom":{},"fname":"pytorch.tensors","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"bb37f2a5388222d121de54bf748b1f5e","links":[{"type":"wiki","from":{"fname":"pytorch.tensors","id":"8qm42jxtuar5qn15w7nphwy","vaultName":"tech"},"value":"math.algebra.linear","alias":"Matrix multiplication","position":{"start":{"line":90,"column":11,"offset":4522},"end":{"line":90,"column":78,"offset":4589},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"math.algebra.linear","anchorHeader":"matrix-multiplication"}},{"type":"wiki","from":{"fname":"pytorch.tensors","id":"8qm42jxtuar5qn15w7nphwy","vaultName":"tech"},"value":"math.algebra.linear","alias":"rules of matrix multiplication","position":{"start":{"line":105,"column":149,"offset":4989},"end":{"line":105,"column":209,"offset":5049},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"math.algebra.linear","anchorHeader":"rules"}},{"type":"wiki","from":{"fname":"pytorch.tensors","id":"8qm42jxtuar5qn15w7nphwy","vaultName":"tech"},"value":"math.algebra.linear.tensors","alias":"see tensors Dendron node for mathematical definition","position":{"start":{"line":160,"column":1,"offset":7808},"end":{"line":160,"column":85,"offset":7892},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"math.algebra.linear.tensors"}}],"anchors":{"tensor-attributes":{"type":"header","text":"Tensor attributes","value":"tensor-attributes","line":32,"column":0,"depth":2},"shape":{"type":"header","text":"Shape","value":"shape","line":35,"column":0,"depth":3},"data-type":{"type":"header","text":"Data Type","value":"data-type","line":54,"column":0,"depth":3},"device":{"type":"header","text":"Device","value":"device","line":60,"column":0,"depth":3},"gradient-requirement":{"type":"header","text":"Gradient Requirement","value":"gradient-requirement","line":66,"column":0,"depth":3},"tensor-manipulation":{"type":"header","text":"Tensor Manipulation","value":"tensor-manipulation","line":80,"column":0,"depth":2},"tensor-operations":{"type":"header","text":"Tensor Operations","value":"tensor-operations","line":82,"column":0,"depth":3},"transpose":{"type":"header","text":"Transpose","value":"transpose","line":98,"column":0,"depth":4},"tensor-aggregation-tensor-methods":{"type":"header","text":"Tensor Aggregation (tensor methods)","value":"tensor-aggregation-tensor-methods","line":113,"column":0,"depth":3},"reshaping-stacking-squeezing-and-unsqueezing-tensors":{"type":"header","text":"Reshaping, Stacking, Squeezing and Unsqueezing Tensors","value":"reshaping-stacking-squeezing-and-unsqueezing-tensors","line":116,"column":0,"depth":3},"indexing":{"type":"header","text":"Indexing","value":"indexing","line":135,"column":0,"depth":3},"resources":{"type":"header","text":"Resources","value":"resources","line":165,"column":0,"depth":2}},"children":[],"parent":"i2it5id8qwtg27n4usg8bo1","data":{}},"body":"<h1 id=\"tensors\">Tensors<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensors\"></a></h1>\n<p>Tensors are the primary data structure used for representing and manipulating data in deep learning models.</p>\n<ul>\n<li>they are the numerical representation of data as it is used as an input to the model, and it is also the numerical representation that is output by the model (ie. representation outputs)\n<ul>\n<li>therefore, the inputs and outputs of a neural network are tensors</li>\n</ul>\n</li>\n<li>ex. Imagine we have a machine learning model that predicts if an image is of a shirt or not. In this case, the tensors would typically hold the image data in the form of numerical values. The image data is represented as pixel intensities, which can be transformed into tensors.\n<ul>\n<li>if we consider what an image is, it's simply an assortment of pixels, where each pixel is some combination of RGB. If we had a pixel art of Mario which was 16x12, then we could create a tensor of shape <code>(16, 12, 3)</code>, where the first dimension represents the height, the second represents the width, and the third represents the color channel (ie. each pixel's degree of RGB). With this tensor, we could recreate the image.</li>\n</ul>\n</li>\n</ul>\n<p>Tensors are often created internally by Pytorch (e.g., the weights of a neural network), but we as developers sometimes create tensors manually, especially when importing data or during preprocessing.</p>\n<p>Many neural networks perform their learning by starting off with tensors full of random numbers, and then adjusting those numbers to better represent the data (by taking the source data, such as an image, and encoding its data and replacing the random numbers within the tensor).</p>\n<ul>\n<li>this is done to help break the symmetry and prevent the model from getting stuck in a suboptimal solution.</li>\n<li>it also helps with reinforcement learning algorithms in order to introduce exploration during the learning process. By adding random noise to the actions taken by an agent, it can explore different states and actions, enabling it to discover new and potentially better strategies.</li>\n</ul>\n<p><img src=\"/assets/images/2023-07-09-10-19-06.png\"></p>\n<p>Tensors are similar to an <code>ndarray</code> (N-dimensional array), due to certain characteristics:</p>\n<ul>\n<li>it is multidimensional\n<ul>\n<li>Tensors can have different dimensions, including 0-dimensional (scalar), 1-dimensional (vector), 2-dimensional (matrix), and higher-dimensional arrays.</li>\n</ul>\n</li>\n<li>each element is of the same type</li>\n<li>each sub-array is of the same (fixed) length</li>\n</ul>\n<p>The main difference between a tensor and an ndarray is that a tensor can run on GPUs and TPUs, in addition to CPUs</p>\n<p>The variable for tensors that are 0 or 1 dimension (ie. scalars or vectors) is lowercase, while the variable for tensors that are more than 2 (ie. matrices and tensors) is UPPERCASE.</p>\n<h2 id=\"tensor-attributes\">Tensor attributes<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensor-attributes\"></a></h2>\n<p>A tensor on PyTorch has attributes:</p>\n<h3 id=\"shape\">Shape<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#shape\"></a></h3>\n<p>the size of the tensor; ie. the size of each layer of nested <code>[]</code> </p>\n<ul>\n<li>ex. a tensor of shape <code>[2, 3, 4]</code> would have 2 elements in its outermost brackets, each of which would have 3 elements (ie. 3 arrays), and each element of that array would have 4 elements:</li>\n</ul>\n<pre class=\"language-py\"><code class=\"language-py\"><span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">[</span><span class=\"token number\">9</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">[</span><span class=\"token number\">9</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">]</span>\n</code></pre>\n<ul>\n<li><code>TENSOR.shape</code> (or alias <code>TENSOR.size()</code>)</li>\n</ul>\n<h3 id=\"data-type\">Data Type<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#data-type\"></a></h3>\n<p>the type of data stored in the tensor</p>\n<ul>\n<li><code>TENSOR.dtype</code></li>\n<li>default is <code>float32</code></li>\n<li>convert type: <code>TENSOR.type(torch.float32)</code> OR  <code>TENSOR.to(torch.float32)</code></li>\n</ul>\n<h3 id=\"device\">Device<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#device\"></a></h3>\n<p>the device in which the tensor is stored</p>\n<ul>\n<li><code>TENSOR.device</code></li>\n<li>default is <code>cpu</code></li>\n<li>moving tensor from CPU to GPU: <code>TENSOR.to('cuda:0')</code></li>\n</ul>\n<h3 id=\"gradient-requirement\">Gradient Requirement<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#gradient-requirement\"></a></h3>\n<p>the state of whether or not gradients need to be computed for the tensor</p>\n<pre><code>- `TENSOR.requires_grad`\n</code></pre>\n<ul>\n<li>spec: gradient is the same as slope if we are considering a straight line in a graph</li>\n</ul>\n<hr>\n<p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.</p>\n<pre class=\"language-py\"><code class=\"language-py\">data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\nx_data <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n</code></pre>\n<h2 id=\"tensor-manipulation\">Tensor Manipulation<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensor-manipulation\"></a></h2>\n<h3 id=\"tensor-operations\">Tensor Operations<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensor-operations\"></a></h3>\n<p>The neural network will combine these tensor operations in some way in order to find patterns in numbers of a dataset</p>\n<p>All operations (except matrix multiplication) are element-wise, meaning the operator (ร, รท etc.) is applied to each element in place</p>\n<p>Operations include:</p>\n<ul>\n<li>addition</li>\n<li>subtraction</li>\n<li>multiplication\n<ul>\n<li><code>torch.mul(TENSOR, 10)</code></li>\n</ul>\n</li>\n<li>division</li>\n<li>matrix multiplication\n<ul>\n<li><code>torch.matmul(TENSOR1, TENSOR2)</code> or <code>torch.mm()</code> or <code>TENSOR1 @ TENSOR2</code></li>\n<li>matrix multiplication is the most common type of operation found in neural networks</li>\n<li>see <a title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\">Matrix multiplication (Private)</a> Dendron node</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"transpose\">Transpose<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#transpose\"></a></h4>\n<p>Transposing a matrix is simply swapping the row contents with its column contents.</p>\n<pre class=\"language-py\"><code class=\"language-py\">tensor_A\n<span class=\"token comment\"># tensor([[ 7,  8,  9],</span>\n<span class=\"token comment\">#        [10, 11, 12]])</span>\ntensor_A<span class=\"token punctuation\">.</span>T\n<span class=\"token comment\"># tensor([[ 7, 10],</span>\n<span class=\"token comment\">#        [ 8, 11],</span>\n<span class=\"token comment\">#        [ 9, 12]])</span>\n</code></pre>\n<p>We do this since we cannot multiply two matrices unless # rows of MatrixA = # columns of MatrixB, and # columns of MatrixA = # rows of MatrixB (see <a title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\">rules of matrix multiplication (Private)</a>), we need to transpose one of our matrices if the shapes don't line up properly.</p>\n<h3 id=\"tensor-aggregation-tensor-methods\">Tensor Aggregation (tensor methods)<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensor-aggregation-tensor-methods\"></a></h3>\n<p>Finding the <code>.min()</code>, <code>.max()</code>, <code>.mean()</code>, <code>.sum()</code> etc. of certain tensor values.</p>\n<h3 id=\"reshaping-stacking-squeezing-and-unsqueezing-tensors\">Reshaping, Stacking, Squeezing and Unsqueezing Tensors<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#reshaping-stacking-squeezing-and-unsqueezing-tensors\"></a></h3>\n<p>Each of these methods allows us to manipulate our tensors in order to change their shape/dimension.</p>\n<ul>\n<li>\n<p>we often use these to solve issues that might arise due to tensors with mismatched shapes. </p>\n</li>\n<li>\n<p><strong>Reshaping</strong> - reshapes an input tensor to a defined shape</p>\n<ul>\n<li>we do this because in order for multiple tensors to work together, we need their shape to correspond to one another   </li>\n<li>the new shape of the reshaped tensor must be compatible with the original tensor\n<ul>\n<li>ex. if we have a tensor <code>[1, 2, 3, 4, 5, 6]</code>, we cannot reshape it into a 2x2 tensor. We could however put it into a 2x3.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>View</strong> - return a view of an input tensor of certain shape, but keep the same memory as the original tensor</p>\n<ul>\n<li>therefore if we assign a new variable <code>Z</code> to the view of an existing tensor and then change <code>Z</code>, it will also change the original tensor</li>\n</ul>\n</li>\n<li>\n<p><strong>Stacking</strong> - combine multiple tensors, either on top of each other (vstack) or side-by-side (hstack)</p>\n</li>\n<li>\n<p><strong>Squeeze</strong> - removes all <code>1</code> dimensions from a tensor</p>\n<ul>\n<li><code>1</code> dimension is a dimension (ie. an array) has a single element in it</li>\n<li>ex. if we have a tensor of shape <code>[2, 1, 2, 1, 2]</code> and squeeze it, it will become <code>[2, 2, 2]</code></li>\n</ul>\n</li>\n<li>\n<p><strong>Unsqueeze</strong> - add a <code>1</code> dimension to a target tensor</p>\n<ul>\n<li>we must specify the <code>dim</code> argument, which specified which index to add </li>\n</ul>\n</li>\n<li>\n<p><strong>Permute</strong> - returns a view that rearranges (ie. swaps) the dimensions of a tensor in a specified way</p>\n<ul>\n<li>ex. if we have a tensor (<code>x</code>) of shape <code>(2, 3, 5)</code> and permute it as <code>(2, 0, 1)</code>, the new tensor will have shape <code>(5, 2, 3)</code></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"indexing\">Indexing<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#indexing\"></a></h3>\n<p>Sometimes you'll want to select specific data from tensors </p>\n<ul>\n<li>ex. only the first column or second row.</li>\n</ul>\n<p>Aside from how indexing normally works (e.g. using <code>[0][0]</code> to access first element of the first element), we can also use <code>:</code> to specify \"all values in this dimension\"</p>\n<pre class=\"language-py\"><code class=\"language-py\">x <span class=\"token operator\">=</span> tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token punctuation\">[</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">9</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Get all values of 0th dimension and the 0 index of 1st dimension</span>\nx<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token comment\"># tensor([[1, 2, 3]])</span>\n\n<span class=\"token comment\"># Get all values of 0th &#x26; 1st dimensions but only index 1 of 2nd dimension</span>\nx<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token comment\"># tensor([[2, 5, 8]])</span>\n\n<span class=\"token comment\"># Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension</span>\nx<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token comment\"># tensor([5])</span>\n\n<span class=\"token comment\"># Get index 0 of 0th and 1st dimension and all values of 2nd dimension (same as x[0][0])</span>\nx<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span> <span class=\"token comment\"># tensor([1, 2, 3])</span>\n\n<span class=\"token comment\"># Get the first 10 values of the 0 dimension</span>\nx<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># Get all values after the 10th index</span>\nx<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span>\n</code></pre>\n<h2 id=\"resources\">Resources<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#resources\"></a></h2>\n<p><a title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\">see tensors Dendron node for mathematical definition (Private)</a></p>","noteIndex":{"id":"olZIVfSs2uLLr3BppFh4K","title":"Digital Garden","desc":"","updated":1674517603573,"created":1615482407722,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"effb007003ca6a91d7fd0c293e1d2436","links":[{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"testing.method.unit","alias":"unit testing","position":{"start":{"line":18,"column":121,"offset":1146},"end":{"line":18,"column":157,"offset":1182},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"testing.method.unit"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"general.arch.microservice","alias":"microservices","position":{"start":{"line":18,"column":188,"offset":1213},"end":{"line":18,"column":231,"offset":1256},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"general.arch.microservice"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"paradigm.oop","alias":"OOP","position":{"start":{"line":36,"column":227,"offset":2718},"end":{"line":36,"column":247,"offset":2738},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"paradigm.oop"}}],"anchors":{"tags":{"type":"header","text":"Tags","value":"tags","line":46,"column":0,"depth":3},"resources":{"type":"header","text":"Resources","value":"resources","line":55,"column":0,"depth":2},"ue-unexamined-resources":{"type":"header","text":"UE (Unexamined) Resources","value":"ue-unexamined-resources","line":56,"column":0,"depth":3},"e-examined-resources":{"type":"header","text":"E (Examined) Resources","value":"e-examined-resources","line":59,"column":0,"depth":3},"resources-1":{"type":"header","text":"Resources","value":"resources-1","line":62,"column":0,"depth":3}},"children":["40ubf88tephzbdjte8cdsa0","zFMjbn3xihVNHjUIdZCD1","IK6NOKemuDjhfstJBovKL","ZaTr28eWk1DrXOEsc1YVb","Vi0WYVLZunVM9iR8XZJn3","ngAbg7gluvbt1bY1UIRsi","RCPPXSakm5TvKka8kOmVp","LIcuGYV0DDt1VWbvH6Sed","MPx8ykTP57I40WTZvTP7f","ZU5WmdTG1bHoE8RcmZXZG","jqWiyYJff92RjXuUQt9PQ","KihijM8OQvZ4pASkkhqzL","F9vyqvls3OBCujtukqKhy","k6jxm2b3edgkhbordpuz6v1","facc2b01-755a-409f-99f6-57bef2d1501f","bZumdyapJ2H0wWWOmJ45i","elqpgoe2r951si4xrhujppj","m5ov42Vm6mR7RQWTvl1NW","cw8cerc0cos8lh871oz8rtf","PZxxZ4iESzSlbbHJKxuAZ","UDu62Pa6BaRqlG8uGoMhy","u3zcndycwqessho5h6x0nz9","ZgCUp366YrF2Tyky2NT73","DVpVUmavSoVDA7UIlPzLX","nMCtMXVvjBsJk3iw1zoIO","ANfx9Z4a6ZA0uJuSHtbIJ","BkG557LKUYbH1DnxprEVT","1TaH8zDTM98FZ9SYaeXLM","02v7ymc144e5c4pv3edkud7","dc79799f-a55d-48ab-a8be-c48adb1b19c0","f6feef57-b8f5-451f-a09a-7b63f6a07183","4sz47Y0LKs1Si73rWtyyh","g7ulqi8no93ezeocbesc3ll","5a9fb1df-478e-4687-9be0-5cb97e61ec57","1374e9e9-1cbc-4e1f-b1ca-66b8569533dd","D8Z3rjXkSj2EOymXQXF4Z","np3c1ykvnjqv5xoombpfwqz","f529cc34-aba0-45ca-ad7e-02ddda318941","0zcauha3il2NqtxZazIo7","9bbd6f68-03b2-41f4-92e4-2ca313e8b450","5a2ab598-fa7e-4471-8bda-9f5831b679ae","uV6w4mZPoohWyZV4Xaad0","QHXEIyeZGIGMVi5Q52UWI","RgE0mZLaUjPftFPZsiAoe","mytCOts26Pidush65tdRW","fwUzxfLSPMH1eL8oBoLWx","wazdsda6h25x66edvfmeuiv","TbW7PM9bg1y5TGkiWwQ8b","xiSIDeEtIc8X0lpUQlppI","0jxgntiLNHWFuCbzqtFGF","GkdMprLUe4QQULBxmGN6V","4SYc6v5hxY5g6Ip6kjpwO","czi7ilt2i1uoqm1f2otbntj","fqVQpS9FBiXgKsZX3R3sJ","L5JUZlGAGvTxrsEBB7DY8","ttyri4pwfyn5lcx0vp9804o","TeTedoeS2LdHPR632eCpM","ulicRRwo3lSFzh3tMfWH9","p7d24vyb8m00ombzn34t50c","e65188a1-177e-4ab2-99f9-75f85d537621","fajYnbVhCRDi74xn0H30R","bArGdGwqo3iFkyKMyE7qR","4Dcp7gbEVoLLgfu7bXFai","5d700782-fb81-416e-83f9-5dd88260e350","zubgzhNFE6KlTgXcjTz6O","8lpdfWa0cbSq1XJQbcYcY","2psluywdc416t7vrql0m058","17I8ZksXvqCH1mRtZDjHp","kF916Ow84qpJJeMRkWMIo","tMLkLcrIHHBz56xVmBLkP","CSePBQ6q7qhowKESqVwt7","laZ4OfLhZNK1Kuy6GaWUr","z5IJblOknQhMzZ5QZh4ye","N36FHxfxzwJfxDY3miWyX","3bb25f58-2b50-4fa3-af55-48ea9f88a081","7x07qjgbitozdfszqzuy7ix","u93Rz4fEWGu6VBR30Zraf","9hjMHnKvYT4jLKvuDSXaV","qC5GxCZBmNb4Ip6c0kU8x","LgW7mTIALODoXc54B3p6S","UYPxfHBFWX7fb6hHU5bB6","74lYtC8NKpCzcyFOZTfR1","sEoBNAEuaTxwSmTXDonZt","dj0jr9mpvs62e2pkg3zc3yy","z8ie0xjogb6ht7gzowav5xr","qxSOd6SPN8qf9ZUojVFDX","WQoMTf6VXBaxCgksXAVsj","2Mw4XgfyCHXHNOX5yoCIY","qcdt2f7jo51muquo5r95dpo","0XJqmcdtcMZu66glBI5O8","lK8r8BXS4ThiUTe4xKIZe","93de42ca-53ea-460a-baa7-b9ec5c47cb1e","Rxs2jaGpdFzqYtP7lAJFJ","aO8W81Z0PyIb6Hs7nOHPW","lhzisalkebu4w5n01np07i0","7b5l4b6fi65n7sv9org5q1l","I01hENHnh8Tqu3Ok8sLzG","03o3n0hz9v9jtb7j889zpd4","evqsPNutOaZ8hcBCqxFQu","zhhxcjZUHdU8uRLwGb9Zh","G1aFACZB2ooWGMGwyd3ZW","YWy1C4tgoaCcw1m8JJsr7","Szj3o5iaNxPpesiCqwrbu","613syb18hb3v0u1ydvor7ru","XhvCDW3fIw6h6MhY5ticq","Q70g7SusFZBQXzkuQifv4","AzfWDH3wp7jFpL2EYxBcW","y0fwpZ9qMqirsLiFyOciU","ZF8xj8wwDUqKlrwTrCFZ1","tAJvhqhdfyZZa87QHq1TU","4hRmipi8lxpBLyzWu5JVB","iTg0C7QjvnmqBZeEigJNs","3babc3d2-79ae-470a-9c06-ab8bba2e684e","bF3UsMFya3fMeXWDspVov","ULkfbL9WpktbVYnzhl6Jw","PpNOO8JYWe6dM8wruSa7x","di40pCxDn7IiqE8lFdD46","rmW0mkerqV35I8QPji6lM","3IFIK1ByzeIxZCByryGLN","iImkYAKfkw3beAl6pLbDn","ecDe8DNWrkeQTwpTEvHje","i2it5id8qwtg27n4usg8bo1","Yqhdd9mSJGN7OJOeyoSD2","Ws5tah8tpeyn9tK8VBTg8","gWAg15uBJgkS2B0wcpMAa","l7V3v2ep1YdDCt7DOr7Ci","yM2PJBdqJnHpD63cPA6sW","qn0bre7eLbi3QMbCfWkUi","fSu0KxFL41IRouotqmbHs","0gtg24Mj1a1bQFPRGQNlO","7iQPBMltLPLbFEz2qbjPu","yoh4pwoXcfELInGKRdYf6","dCGCWXgAmiOZXbdULT1m6","jMavlje07sNa6hSEIE8WA","Xxm0JE4dKHxrQAaZfzvxD","nRb6Im4Kcmc2ZWE7K1jZ1","PAEBZCyFBZJyR7OoMZ41E","PQ6km8RgRCuyICBPOYz8f","x5tm1nfjzyawwzedy3yitgd","2bhftt8rGuxYu4pFgNqru","hjYIZpHQWuXfeEoGeJEKW","K2M9bQqVq2eQfm29eslKL","X8obW1iKwYsvNgKWGyCzy","yJwSC7hqYIezTFHf5i0Ev","c4Z7ETcOHUILRMH32Sfjw","qiR6dIu857b9M9kTqjOyK","fc2coz74cnfy5czzofx4h5x","Ku1OgHMhELajzo61Gx7ye","LUrfhDWo8wuwZu7CN9TV8","osu6JGOnvXJ5gt3tpqWZY","1a6173cd-cf13-4b34-a522-8350bf9a364f","S2sBltrPfd8a7ICuD7CuH","GLQ2pmkJUNUa93THBDVsD","md6xitz4exia2joa06i490b","oWCuBXOg6JWfZzjmKxmNl","jOmhZ8ovLYTPbpM1vqSDx","p9bov84s0isgkl1ysaw93kk","FraC6xzLy1ei91l1ICyc9","6ceBas2RE9Q4787GDngH7","734cd78d-0bc9-426b-803d-1efc84dfffe5","k4Bb09px6r0FxIRs49SXV","oaG3H1S9IUBO644nGZigu","Ka7agQJkUMRSWN0uFdkWK","si3z090WsiLasMhJBa1Az","hs6rwzt4mogiicoc4gcykbi","ljKAVERmdEiKLK9hXGKBm","zxt3lhonfdhglvijd17ua8c","dd7dopve1dudqkoibkqvti4","923tgifqf59ovv5yldtyi0a","vrjwp01goqw47fqctm4f4lo","c99gdmmppju3r1tth8cb2jx","z2pvn5qxdz84zgygqzxage8"],"parent":null,"data":{},"body":"This Dendron vault of tech knowledge is organized according to domains and their sub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git would include things like `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner`, and so on.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the \"aha\" moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nAs a result, I have allowed myself to use potentially arcane concepts to explain other ones. For example, in my note on [[unit testing|testing.method.unit]], I have made reference to the [[microservices|general.arch.microservice]] note. If these notes were made with the public in mind, this would be a very bad strategy, given that you'd have to understand microservices to be able to draw that same parallel that I've already drawn. Since these notes are written for myself, I have been fine with taking these liberties.\n\nWhat I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course this is all\nvague sounding, and really depends on the domain along with the ends I am\ntrying to reach.\n\nTo achieve this, the system should be steadfast to:\n- be able to put information in relatively easily, without too much thought\n\trequired to its location. While location is important, Dendron makes it easy\n\tto relocate notes, if it becomes apparent that a different place makes more\n\tsense.\n- be able to extract the information that is needed, meaning there is a\n\thigh-degree in confidence in the location of the information. The idea is\n\tthat information loses a large amount of its value when it is unfindable.\n\tTherefore, a relatively strict ideology should be used when determining\n\twhere a piece of information belongs.\n\t- Some concepts might realistically belong to multiple domains. For instance, the concept of *access modifiers* can be found in both `C#` and `Typescript`. Therefore, this note should be abstracted to a common place, such as [[OOP|paradigm.oop]].\n\nThis Dendron vault is the sister component to the [General Second Brain](https://tech.kyletycholiz.com).\n\n### Tags\nThroughout the garden, I have made use of tags, which give semantic meaning to the pieces of information.\n\n- `ex.` - Denotes an *example* of the preceding piece of information\n- `spec:` - Specifies that the preceding information has some degree of *speculation* to it, and may not be 100% factual. Ideally this gets clarified over time as my understanding develops.\n- `anal:` - Denotes an *analogy* of the preceding information. Often I will attempt to link concepts to others that I have previously learned.\n- `mn:` - Denotes a *mnemonic*\n- `expl:` - Denotes an *explanation*\n\n## Resources\n### UE (Unexamined) Resources\nOften, I come across sources of information that I believe to be high-quality. They may be recommendations or found in some other way. No matter their origin, I may be in a position where I don't have the time to fully examine them (and properly extract notes), or I may not require the information at that moment in time. In cases like these, I will add reference to a section of the note called **UE Resources**. The idea is that in the future when I am ready to examine them, I have a list of resources that I can start with. This is an alternative strategy to compiling browser bookmarks, which I've found can quickly become untenable.\n\n### E (Examined) Resources\nOnce a resource has been thoroughly examined and has been mined for notes, it will be moved from *UE Resources* to *E Resources*. This is to indicate that (in my own estimation), there is nothing more to be gained from the resource that is not already in the note.\n\n### Resources\nThis heading is for inexhaustible resources. \n- A prime example would be a quality website that continually posts articles.  - Another example would be a tool, such as software that measures frequencies in a room to help acoustically treat it.\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.83.0","vaults":[{"fsPath":"../main/tech","name":"tech"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"The Tech Digital Garden of Kyle Tycholiz"},"github":{"cname":"tech.kyletycholiz.com","enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableMermaid":true,"siteUrl":"https://tech.kyletycholiz.com","duplicateNoteBehavior":{"action":"useVault","payload":["tech"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}