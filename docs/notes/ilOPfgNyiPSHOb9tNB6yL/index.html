<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>CAP-theorem</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Publishable tech notes"/><meta property="og:title" content="CAP-theorem"/><meta property="og:description" content="Publishable tech notes"/><meta property="og:url" content="https://tycholiz.github.io/Digital-Garden/notes/ilOPfgNyiPSHOb9tNB6yL/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="10/12/2021"/><meta property="article:modified_time" content="10/13/2021"/><link rel="canonical" href="https://tycholiz.github.io/Digital-Garden/notes/ilOPfgNyiPSHOb9tNB6yL/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/Digital-Garden/_next/static/css/804c5fc85a5d7e04.css" as="style"/><link rel="stylesheet" href="/Digital-Garden/_next/static/css/804c5fc85a5d7e04.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/Digital-Garden/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/Digital-Garden/_next/static/chunks/webpack-d53c65e15128d807.js" defer=""></script><script src="/Digital-Garden/_next/static/chunks/framework-dc33c0b5493501f0.js" defer=""></script><script src="/Digital-Garden/_next/static/chunks/main-9409ea3202b3ee44.js" defer=""></script><script src="/Digital-Garden/_next/static/chunks/pages/_app-585c2573c1834e35.js" defer=""></script><script src="/Digital-Garden/_next/static/chunks/155-0a8d44b6408c244d.js" defer=""></script><script src="/Digital-Garden/_next/static/chunks/pages/notes/%5Bid%5D-30a63e62504f2dc5.js" defer=""></script><script src="/Digital-Garden/_next/static/s-Pdpb0Y8fVIEUakhzEMN/_buildManifest.js" defer=""></script><script src="/Digital-Garden/_next/static/s-Pdpb0Y8fVIEUakhzEMN/_ssgManifest.js" defer=""></script><script src="/Digital-Garden/_next/static/s-Pdpb0Y8fVIEUakhzEMN/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div class="ant-col ant-col-xs-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"></div><div class="ant-col ant-col-xs-12"></div><div style="margin-left:4px;display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout site-layout" style="margin-top:64px"><section class="ant-layout site-layout" style="flex-direction:row"><section class="ant-layout site-layout-sidebar" style="flex:0 0 auto;width:calc((100% - 992px) / 2 + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></section><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="cap-theorem"><a aria-hidden="true" class="anchor-heading" href="#cap-theorem"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>CAP-theorem</h1>
<p>CAP theorem states that it is impossible for a distributed system to simultaneously provide all three of: consistency, availability, and partition tolerance. Only 2 can be achieved.</p>
<ul>
<li>We cannot build a general data store that is continually available, sequentially consistent, and tolerant to any partition failures.</li>
<li>Therefore, the theorem can really be stated as: "In the presence of a network partition, a distributed system must choose either Consistency or Availability."
<ul>
<li>note: this seems to be at least somewhat of a controversial tone, given that SQL databases occupy CA. This view above would state that CA systems are incoherent, though this opinion might not appear to be reputable, given the prevalence of relational databases.</li>
<li>regarding CA systems then, what CAP theorem would argue is that these systems have an inherent weakness, which is that in the case of a network partition, they will be forced to give up either consistency or availability.</li>
</ul>
</li>
</ul>
<h3 id="consistency-c"><a aria-hidden="true" class="anchor-heading" href="#consistency-c"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Consistency (C)</h3>
<p>All nodes see the same data at the same time. This means users can read or write from/to any node in the system and will receive the same data. It is equivalent to having a single up-to-date copy of the data.</p>
<p>Depending on the business needs of the application, we may wish to make the trade-off of sacrificing consistency for availability. Say we are implementing a facebook newsfeed. It is acceptable for the application to miss some data points here and there. For instance, if a friend of yours uploads a new post, it's not of critical importance that you get that data right away. That is, if one client is getting its data from a data store that is not up-to-date, then it's not the end of the world (assuming everything can be made up-to-date in a timely manner).</p>
<h3 id="availability-a"><a aria-hidden="true" class="anchor-heading" href="#availability-a"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Availability (A)</h3>
<p>Availability means every request received by a non-failing node in the system must result in a response. Even when severe network failures occur, every request must terminate. In simple terms, availability refers to a system’s ability to remain accessible even if one or more nodes in the system go down.</p>
<p>In an AP (Availability/Partition-tolerant) system, the system is essentially saying “I will get you to a node, but I do not know how good the data you find there will be”; or “I can be available and the data I show will be good, but not complete.”</p>
<h3 id="partition-tolerance-p"><a aria-hidden="true" class="anchor-heading" href="#partition-tolerance-p"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Partition tolerance (P)</h3>
<p>a.k.a robustness</p>
<p>Partition tolerance is the ability of a data processing system to continue processing data even if a network partition causes communication errors between subsystems</p>
<ul>
<li>A single node failure should not cause the entire system to collapse.</li>
<li>A partition is a communication break (or a network failure) between any two nodes in the system, i.e., both nodes are up but cannot communicate with each other. </li>
</ul>
<p>A partition-tolerant system continues to operate even if there are partitions (ie. communication breakdowns) in the system. Such a system can sustain any network failure that does not result in the failure of the entire network. Data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages.</p>
<p><img src="/Digital-Garden/assets/images/2021-10-12-10-38-14.png"></p>
<h2 id="pacelc-theorem"><a aria-hidden="true" class="anchor-heading" href="#pacelc-theorem"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>PACELC Theorem</h2>
<p>One place where the CAP theorem is silent is what happens when there is no network partition? What choices does a distributed system have when there is no partition?</p>
<p>The PACELC theorem states that in a system that replicates data:</p>
<ul>
<li>if there is a partition (‘P’), a distributed system can tradeoff between availability and consistency (i.e., ‘A’ and ‘C’);</li>
<li>else (‘E’), when the system is running normally in the absence of partitions, the system can tradeoff between latency (‘L’) and consistency (‘C’).
<img src="/Digital-Garden/assets/images/2021-10-12-10-43-34.png"></li>
</ul>
<p>The first part of the theorem (PAC) is the same as the CAP theorem, and the ELC is the extension. The whole thesis is assuming we maintain high availability by replication. So, when there is a failure, CAP theorem prevails. But if not, we still have to consider the tradeoff between consistency and latency of a replicated system.</p>
<h3 id="examples"><a aria-hidden="true" class="anchor-heading" href="#examples"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Examples</h3>
<ul>
<li>Dynamo and Cassandra are PA/EL systems: They choose availability over consistency when a partition occurs; otherwise, they choose lower latency.</li>
<li>BigTable and HBase are PC/EC systems: They will always choose consistency, giving up availability and lower latency.</li>
<li>MongoDB can be considered PA/EC (default configuration): MongoDB works in a primary/secondaries configuration. In the default configuration, all writes and reads are performed on the primary. As all replication is done asynchronously (from primary to secondaries), when there is a network partition in which primary is lost or becomes isolated on the minority side, there is a chance of losing data that is unreplicated to secondaries, hence there is a loss of consistency during partitions. Therefore it can be concluded that in the case of a network partition, MongoDB chooses availability, but otherwise guarantees consistency. Alternately, when MongoDB is configured to write on majority replicas and read from the primary, it could be categorized as PC/EC.</li>
</ul>
<hr>
<h2 id="backlinks"><a aria-hidden="true" class="anchor-heading" href="#backlinks"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Backlinks</h2>
<ul>
<li><a href="/Digital-Garden/notes/ZF8xj8wwDUqKlrwTrCFZ1">Nosql</a></li>
<li><a href="/Digital-Garden/notes/vutujFFWxQu6TshWVuMpI">Distributed Computing</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#consistency-c" title="Consistency (C)">Consistency (C)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#availability-a" title="Availability (A)">Availability (A)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#partition-tolerance-p" title="Partition tolerance (P)">Partition tolerance (P)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#pacelc-theorem" title="PACELC Theorem">PACELC Theorem</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#examples" title="Examples">Examples</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"ilOPfgNyiPSHOb9tNB6yL","title":"CAP-theorem","desc":"","updated":1634160427288,"created":1634060006478,"custom":{},"fname":"deploy.distributed.CAP-theorem","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"ca21e756eb1d8daf6ad228dcc6d8ade8","links":[{"from":{"fname":"nosql","vaultName":"tech"},"type":"backlink","position":{"start":{"line":69,"column":53,"offset":4816},"end":{"line":69,"column":99,"offset":4862},"indent":[]},"value":"deploy.distributed.CAP-theorem"},{"from":{"fname":"deploy.distributed","vaultName":"tech"},"type":"backlink","position":{"start":{"line":28,"column":14,"offset":2032},"end":{"line":28,"column":60,"offset":2078},"indent":[]},"value":"deploy.distributed.CAP-theorem"}],"anchors":{"consistency-c":{"type":"header","text":"Consistency (C)","value":"consistency-c","line":14,"column":0,"depth":3},"availability-a":{"type":"header","text":"Availability (A)","value":"availability-a","line":19,"column":0,"depth":3},"partition-tolerance-p":{"type":"header","text":"Partition tolerance (P)","value":"partition-tolerance-p","line":24,"column":0,"depth":3},"pacelc-theorem":{"type":"header","text":"PACELC Theorem","value":"pacelc-theorem","line":35,"column":0,"depth":2},"examples":{"type":"header","text":"Examples","value":"examples","line":45,"column":0,"depth":3}},"children":[],"parent":"vutujFFWxQu6TshWVuMpI","data":{}},"body":"\u003ch1 id=\"cap-theorem\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#cap-theorem\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eCAP-theorem\u003c/h1\u003e\n\u003cp\u003eCAP theorem states that it is impossible for a distributed system to simultaneously provide all three of: consistency, availability, and partition tolerance. Only 2 can be achieved.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe cannot build a general data store that is continually available, sequentially consistent, and tolerant to any partition failures.\u003c/li\u003e\n\u003cli\u003eTherefore, the theorem can really be stated as: \"In the presence of a network partition, a distributed system must choose either Consistency or Availability.\"\n\u003cul\u003e\n\u003cli\u003enote: this seems to be at least somewhat of a controversial tone, given that SQL databases occupy CA. This view above would state that CA systems are incoherent, though this opinion might not appear to be reputable, given the prevalence of relational databases.\u003c/li\u003e\n\u003cli\u003eregarding CA systems then, what CAP theorem would argue is that these systems have an inherent weakness, which is that in the case of a network partition, they will be forced to give up either consistency or availability.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"consistency-c\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#consistency-c\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eConsistency (C)\u003c/h3\u003e\n\u003cp\u003eAll nodes see the same data at the same time. This means users can read or write from/to any node in the system and will receive the same data. It is equivalent to having a single up-to-date copy of the data.\u003c/p\u003e\n\u003cp\u003eDepending on the business needs of the application, we may wish to make the trade-off of sacrificing consistency for availability. Say we are implementing a facebook newsfeed. It is acceptable for the application to miss some data points here and there. For instance, if a friend of yours uploads a new post, it's not of critical importance that you get that data right away. That is, if one client is getting its data from a data store that is not up-to-date, then it's not the end of the world (assuming everything can be made up-to-date in a timely manner).\u003c/p\u003e\n\u003ch3 id=\"availability-a\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#availability-a\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eAvailability (A)\u003c/h3\u003e\n\u003cp\u003eAvailability means every request received by a non-failing node in the system must result in a response. Even when severe network failures occur, every request must terminate. In simple terms, availability refers to a system’s ability to remain accessible even if one or more nodes in the system go down.\u003c/p\u003e\n\u003cp\u003eIn an AP (Availability/Partition-tolerant) system, the system is essentially saying “I will get you to a node, but I do not know how good the data you find there will be”; or “I can be available and the data I show will be good, but not complete.”\u003c/p\u003e\n\u003ch3 id=\"partition-tolerance-p\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#partition-tolerance-p\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePartition tolerance (P)\u003c/h3\u003e\n\u003cp\u003ea.k.a robustness\u003c/p\u003e\n\u003cp\u003ePartition tolerance is the ability of a data processing system to continue processing data even if a network partition causes communication errors between subsystems\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA single node failure should not cause the entire system to collapse.\u003c/li\u003e\n\u003cli\u003eA partition is a communication break (or a network failure) between any two nodes in the system, i.e., both nodes are up but cannot communicate with each other. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA partition-tolerant system continues to operate even if there are partitions (ie. communication breakdowns) in the system. Such a system can sustain any network failure that does not result in the failure of the entire network. Data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/Digital-Garden/assets/images/2021-10-12-10-38-14.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"pacelc-theorem\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#pacelc-theorem\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePACELC Theorem\u003c/h2\u003e\n\u003cp\u003eOne place where the CAP theorem is silent is what happens when there is no network partition? What choices does a distributed system have when there is no partition?\u003c/p\u003e\n\u003cp\u003eThe PACELC theorem states that in a system that replicates data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eif there is a partition (‘P’), a distributed system can tradeoff between availability and consistency (i.e., ‘A’ and ‘C’);\u003c/li\u003e\n\u003cli\u003eelse (‘E’), when the system is running normally in the absence of partitions, the system can tradeoff between latency (‘L’) and consistency (‘C’).\n\u003cimg src=\"/Digital-Garden/assets/images/2021-10-12-10-43-34.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe first part of the theorem (PAC) is the same as the CAP theorem, and the ELC is the extension. The whole thesis is assuming we maintain high availability by replication. So, when there is a failure, CAP theorem prevails. But if not, we still have to consider the tradeoff between consistency and latency of a replicated system.\u003c/p\u003e\n\u003ch3 id=\"examples\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#examples\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eExamples\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDynamo and Cassandra are PA/EL systems: They choose availability over consistency when a partition occurs; otherwise, they choose lower latency.\u003c/li\u003e\n\u003cli\u003eBigTable and HBase are PC/EC systems: They will always choose consistency, giving up availability and lower latency.\u003c/li\u003e\n\u003cli\u003eMongoDB can be considered PA/EC (default configuration): MongoDB works in a primary/secondaries configuration. In the default configuration, all writes and reads are performed on the primary. As all replication is done asynchronously (from primary to secondaries), when there is a network partition in which primary is lost or becomes isolated on the minority side, there is a chance of losing data that is unreplicated to secondaries, hence there is a loss of consistency during partitions. Therefore it can be concluded that in the case of a network partition, MongoDB chooses availability, but otherwise guarantees consistency. Alternately, when MongoDB is configured to write on majority replicas and read from the primary, it could be categorized as PC/EC.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"backlinks\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#backlinks\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eBacklinks\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/Digital-Garden/notes/ZF8xj8wwDUqKlrwTrCFZ1\"\u003eNosql\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/Digital-Garden/notes/vutujFFWxQu6TshWVuMpI\"\u003eDistributed Computing\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"olZIVfSs2uLLr3BppFh4K","title":"Root","desc":"","updated":1618676699718,"created":1615482407722,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"777fb4dab9bcf63cbfe772245d7e4871","links":[],"anchors":{"digital-garden":{"type":"header","text":"Digital Garden","value":"digital-garden","line":8,"column":0,"depth":1}},"children":["zFMjbn3xihVNHjUIdZCD1","IK6NOKemuDjhfstJBovKL","RS8qp1GxwtiJopCUHTTtq","ZaTr28eWk1DrXOEsc1YVb","Vi0WYVLZunVM9iR8XZJn3","ngAbg7gluvbt1bY1UIRsi","RCPPXSakm5TvKka8kOmVp","MPx8ykTP57I40WTZvTP7f","ZU5WmdTG1bHoE8RcmZXZG","jqWiyYJff92RjXuUQt9PQ","KihijM8OQvZ4pASkkhqzL","F9vyqvls3OBCujtukqKhy","facc2b01-755a-409f-99f6-57bef2d1501f","m5ov42Vm6mR7RQWTvl1NW","YYt62mSW964cwb5nP0hva","PZxxZ4iESzSlbbHJKxuAZ","UDu62Pa6BaRqlG8uGoMhy","nMCtMXVvjBsJk3iw1zoIO","ANfx9Z4a6ZA0uJuSHtbIJ","BkG557LKUYbH1DnxprEVT","1TaH8zDTM98FZ9SYaeXLM","dc79799f-a55d-48ab-a8be-c48adb1b19c0","f6feef57-b8f5-451f-a09a-7b63f6a07183","4sz47Y0LKs1Si73rWtyyh","5a9fb1df-478e-4687-9be0-5cb97e61ec57","1374e9e9-1cbc-4e1f-b1ca-66b8569533dd","f529cc34-aba0-45ca-ad7e-02ddda318941","0zcauha3il2NqtxZazIo7","9bbd6f68-03b2-41f4-92e4-2ca313e8b450","5a2ab598-fa7e-4471-8bda-9f5831b679ae","uV6w4mZPoohWyZV4Xaad0","RgE0mZLaUjPftFPZsiAoe","mytCOts26Pidush65tdRW","fwUzxfLSPMH1eL8oBoLWx","TbW7PM9bg1y5TGkiWwQ8b","xiSIDeEtIc8X0lpUQlppI","0jxgntiLNHWFuCbzqtFGF","GkdMprLUe4QQULBxmGN6V","4SYc6v5hxY5g6Ip6kjpwO","fqVQpS9FBiXgKsZX3R3sJ","L5JUZlGAGvTxrsEBB7DY8","963c4b54-02bd-4943-a15f-52cf59ffd0f4","TeTedoeS2LdHPR632eCpM","e65188a1-177e-4ab2-99f9-75f85d537621","fajYnbVhCRDi74xn0H30R","bArGdGwqo3iFkyKMyE7qR","4Dcp7gbEVoLLgfu7bXFai","5d700782-fb81-416e-83f9-5dd88260e350","zubgzhNFE6KlTgXcjTz6O","8lpdfWa0cbSq1XJQbcYcY","17I8ZksXvqCH1mRtZDjHp","kF916Ow84qpJJeMRkWMIo","tMLkLcrIHHBz56xVmBLkP","CSePBQ6q7qhowKESqVwt7","laZ4OfLhZNK1Kuy6GaWUr","z5IJblOknQhMzZ5QZh4ye","N36FHxfxzwJfxDY3miWyX","3bb25f58-2b50-4fa3-af55-48ea9f88a081","aOqNcxZh8qN4MoBJcvGTc","u93Rz4fEWGu6VBR30Zraf","9hjMHnKvYT4jLKvuDSXaV","LgW7mTIALODoXc54B3p6S","UYPxfHBFWX7fb6hHU5bB6","qxSOd6SPN8qf9ZUojVFDX","WQoMTf6VXBaxCgksXAVsj","2Mw4XgfyCHXHNOX5yoCIY","UFrwsXzC1yr7Ge8FF8Vbo","a8edc4ac-028b-40ec-872a-e4a005b04b2a","0XJqmcdtcMZu66glBI5O8","lK8r8BXS4ThiUTe4xKIZe","93de42ca-53ea-460a-baa7-b9ec5c47cb1e","Rxs2jaGpdFzqYtP7lAJFJ","aO8W81Z0PyIb6Hs7nOHPW","I01hENHnh8Tqu3Ok8sLzG","evqsPNutOaZ8hcBCqxFQu","zhhxcjZUHdU8uRLwGb9Zh","G1aFACZB2ooWGMGwyd3ZW","YWy1C4tgoaCcw1m8JJsr7","Szj3o5iaNxPpesiCqwrbu","XhvCDW3fIw6h6MhY5ticq","Q70g7SusFZBQXzkuQifv4","AzfWDH3wp7jFpL2EYxBcW","y0fwpZ9qMqirsLiFyOciU","ZF8xj8wwDUqKlrwTrCFZ1","tAJvhqhdfyZZa87QHq1TU","4hRmipi8lxpBLyzWu5JVB","iTg0C7QjvnmqBZeEigJNs","3babc3d2-79ae-470a-9c06-ab8bba2e684e","bF3UsMFya3fMeXWDspVov","ULkfbL9WpktbVYnzhl6Jw","PpNOO8JYWe6dM8wruSa7x","di40pCxDn7IiqE8lFdD46","rmW0mkerqV35I8QPji6lM","3IFIK1ByzeIxZCByryGLN","iImkYAKfkw3beAl6pLbDn","tho65KN0ZpjQlVvP1fEzM","ecDe8DNWrkeQTwpTEvHje","Yqhdd9mSJGN7OJOeyoSD2","NEhsoOfR7J6o01ielAuUm","Ws5tah8tpeyn9tK8VBTg8","Bqpifx2HX3vjLhX9yvhTV","gWAg15uBJgkS2B0wcpMAa","l7V3v2ep1YdDCt7DOr7Ci","yM2PJBdqJnHpD63cPA6sW","qn0bre7eLbi3QMbCfWkUi","fSu0KxFL41IRouotqmbHs","0gtg24Mj1a1bQFPRGQNlO","7iQPBMltLPLbFEz2qbjPu","yoh4pwoXcfELInGKRdYf6","dCGCWXgAmiOZXbdULT1m6","jMavlje07sNa6hSEIE8WA","Xxm0JE4dKHxrQAaZfzvxD","PAEBZCyFBZJyR7OoMZ41E","PQ6km8RgRCuyICBPOYz8f","2bhftt8rGuxYu4pFgNqru","hjYIZpHQWuXfeEoGeJEKW","K2M9bQqVq2eQfm29eslKL","X8obW1iKwYsvNgKWGyCzy","yJwSC7hqYIezTFHf5i0Ev","c4Z7ETcOHUILRMH32Sfjw","qiR6dIu857b9M9kTqjOyK","8vflbCVkwZtYcqdDDTNAz","Ku1OgHMhELajzo61Gx7ye","LUrfhDWo8wuwZu7CN9TV8","osu6JGOnvXJ5gt3tpqWZY","1a6173cd-cf13-4b34-a522-8350bf9a364f","ZgCUp366YrF2Tyky2NT73","S2sBltrPfd8a7ICuD7CuH","oWCuBXOg6JWfZzjmKxmNl","jOmhZ8ovLYTPbpM1vqSDx","FraC6xzLy1ei91l1ICyc9","6ceBas2RE9Q4787GDngH7","734cd78d-0bc9-426b-803d-1efc84dfffe5","k4Bb09px6r0FxIRs49SXV","oaG3H1S9IUBO644nGZigu","Ka7agQJkUMRSWN0uFdkWK","si3z090WsiLasMhJBa1Az","ljKAVERmdEiKLK9hXGKBm","LIcuGYV0DDt1VWbvH6Sed","MgUSrpNCIwDOW4fX3vPOF","bZumdyapJ2H0wWWOmJ45i","QHXEIyeZGIGMVi5Q52UWI","i5Fya3Vzzm0rBT0ctByuo"],"parent":null,"data":{},"body":"\n# Digital Garden\n\nThis Wiki of Personal Information is organized according to domains and their\nsub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git, would be `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner` etc.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the \"aha\" moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nThe ability I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course, this is all\nvague sounding, and really depends on the domain, along with the ends I am\ntrying to reach.\n\nTo achieve this, the system should be steadfast to:\n- be able to put information in relatively easily, without too much thought\n\trequired to make that determination.\n- be able to extract the information that we need, meaning there is a\n\thigh-degree in confidence in the location of the information. The idea is\n\tthat information loses a large amount of its value when it is unfindable.\n\tTherefore, a relatively strict ideology should be used when determining\n\twhere a piece of information belongs.\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":3,"useFMTitle":true,"useNoteTitleForLink":true,"noLegacyNoteRef":true,"mermaid":true,"useKatex":true,"dev":{"enablePreviewV2":true},"site":{"assetsPrefix":"/Digital-Garden","siteUrl":"https://tycholiz.github.io","copyAssets":true,"siteHierarchies":["root"],"siteRootDir":"docs","usePrettyRefs":true,"title":"Dendron","description":"Publishable tech notes","duplicateNoteBehavior":{"action":"useVault","payload":["tech"]},"siteLastModified":true,"gh_edit_branch":"main","usePrettyLinks":true,"siteNotesDir":"notes","siteFaviconPath":"favicon.ico","gh_edit_link":true,"gh_edit_link_text":"Edit this page on GitHub","gh_root":"docs/","gh_edit_view_mode":"edit","writeStubs":true,"siteIndex":"root"},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{}},"workspace":{"dendronVersion":"0.67.1","vaults":[{"fsPath":"../main/tech","name":"tech"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"enableUserTags":true,"enableHashTags":true},"usePrettyRefs":true}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"ilOPfgNyiPSHOb9tNB6yL"},"buildId":"s-Pdpb0Y8fVIEUakhzEMN","assetPrefix":"/Digital-Garden","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>