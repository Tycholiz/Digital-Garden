<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>Machine Learning</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="The Tech Digital Garden of Kyle Tycholiz"/><meta property="og:title" content="Machine Learning"/><meta property="og:description" content="The Tech Digital Garden of Kyle Tycholiz"/><meta property="og:url" content="https://tech.kyletycholiz.com/notes/03o3n0hz9v9jtb7j889zpd4/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2/28/2023"/><meta property="article:modified_time" content="8/29/2023"/><link rel="canonical" href="https://tech.kyletycholiz.com/notes/03o3n0hz9v9jtb7j889zpd4/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6b338472289fe290.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/fo6gubzKP_KV9RZPhaoiQ/_buildManifest.js" defer=""></script><script src="/_next/static/fo6gubzKP_KV9RZPhaoiQ/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="machine-learning">Machine Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#machine-learning"></a></h1>
<p>Machine learning is the ability for computers to learn something without explicitly being programmed to know that thing.</p>
<ul>
<li>it is done by turning data into numbers (which are then stored in <a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">tensors (Private)</a>), and then passing it into a neural network (which tries to find patterns in those numbers)
<ul>
<li>in fact, you can use machine learning for pretty much anything as long as you can convert the inputs into numbers and write the machine learning algorithm that will allow the machine to find the patterns.</li>
</ul>
</li>
<li>The whole premise of machine learning is to learn a representation of the input and how it maps to the output
<ul>
<li>ex. imagine we had an input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span> and an ouput <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></span>. We are curious to see how these 2 variables relate to each other. Using machine learning, we could reveal that a simple regression formula (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">Y = a + bX</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span>) lies beneath the relationship. In this case, machine learning is about discovering the nature of that relationship with only the input and the output.</li>
</ul>
</li>
</ul>
<p>Whereas traditional programming is focused on defining inputs and rules in order to arrive at an output, machine learning is focused on defining inputs and an output, and figuring out the rules necessary to arrive at that output</p>
<ul>
<li>ex. we are building a program to cook a roast chicken. The traditional programming paradigm would have us defining the ingredients (ie. inputs) and defining the cooking steps (ie. rules), which yields the cooked meal (ie. output). In machine learning, we define the ingredients, define the output, and the machine learning algorithm will figure out how to make the meal itself.</li>
</ul>
<p></p><p></p><div class="portal-container">
<div class="portal-head">
<div class="portal-backlink">
<div class="portal-title">From <span class="portal-text-title">GPU</span></div>
<a href="/notes/7iLURlousehIzpOtpQ6eX" class="portal-arrow">Go to text <span class="right-arrow">â†’</span></a>
</div>
</div>
<div id="portal-parent-anchor" class="portal-parent" markdown="1">
<div class="portal-parent-fader-top"></div>
<div class="portal-parent-fader-bottom"></div><h3 id="use-in-machine-learning">Use in Machine Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#use-in-machine-learning"></a></h3>
<p>Though originally GPUs were built for graphics, Nvidia provides a software platform called CUDA (Compute Unified Device Architecture) which is an API that enables software (like Pytorch) to use GPU for general purpose computing tasks.</p>
<!-- Refine this explanation -->
<p>GPUs are so fast because they are so efficient for matrix multiplication and convolution</p>
<p>CPUs are latency optimized while GPUs are bandwidth optimized. You can visualize this as a CPU being a Ferrari and a GPU being a big truck. The task of both is to pick up packages from a random location A and to transport those packages to another random location B. The CPU (Ferrari) can fetch some memory (packages) in your RAM quickly while the GPU (big truck) is slower in doing that (much higher latency). However, the CPU (Ferrari) needs to go back and forth many times to do its job (location A <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>â†’</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">â†’</span></span></span></span></span> pick up 2 packages <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>â†’</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">â†’</span></span></span></span></span> location B ... repeat) while the GPU can fetch much more memory at once (location A <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>â†’</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">â†’</span></span></span></span></span> pick up 100 packages <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>â†’</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">â†’</span></span></span></span></span> location B ... repeat).</p>
<p>So, in other words, the CPU is good at fetching small amounts of memory quickly (5 <em> 3 </em> 7) while the GPU is good at fetching large amounts of memory (Matrix multiplication: (A<em>B)</em>C). The best CPUs have about 50GB/s while the best GPUs have 750GB/s memory bandwidth. So the more memory your computational operations require, the more significant the advantage of GPUs over CPUs. But there is still the latency that may hurt performance in the case of the GPU. A big truck may be able to pick up a lot of packages with each tour, but the problem is that you are waiting a long time until the next set of packages arrives. Without solving this problem, GPUs would be very slow even for large amounts of data. So how is this solved?</p>
<p>If you ask a big truck to make many tours to fetch packages you will always wait for a long time for the next load of packages once the truck has departed to do the next tour â€” the truck is just slow. However, if you now use a fleet of either Ferraris and big trucks (thread parallelism), and you have a big job with many packages (large chunks of memory such as matrices) then you will wait for the first truck a bit, but after that you will have no waiting time at all â€” unloading the packages takes so much time that all the trucks will queue in unloading location B so that you always have direct access to your packages (memory). This effectively hides latency so that GPUs offer high bandwidth while hiding their latency under thread parallelism â€” so for large chunks of memory GPUs provide the best memory bandwidth while having almost no drawback due to latency via thread parallelism. This is the second reason why GPUs are faster than CPUs for deep learning. As a side note, you will also see why more threads do not make sense for CPUs: A fleet of Ferraris has no real benefit in any scenario.</p>
<p>But the advantages for the GPU do not end here. This is the first step where the memory is fetched from the main memory (RAM) to the local memory on the chip (L1 cache and registers). This second step is less critical for performance but still adds to the lead for GPUs. All computation that ever is executed happens in registers which are directly attached to the execution unit (a core for CPUs, a stream processor for GPUs). Usually, you have the fast L1 and register memory very close to the execution engine, and you want to keep these memories small so that access is fast. Increased distance to the execution engine dramatically reduces memory access speed, so the larger the distance to access it the slower it gets. If you make your memory larger and larger, then, in turn, it gets slower to access its memory (on average, finding what you want to buy in a small store is faster than finding what you want to buy in a huge store, even if you know where that item is). So the size is limited for register files - we are just at the limits of physics here and every nanometer counts, we want to keep them small.</p>
<p>The advantage of the GPU is here that it can have a small pack of registers for every processing unit (stream processor, or SM), of which it has many. Thus we can have in total a lot of register memory, which is very small and thus very fast. This leads to the aggregate GPU registers size being more than 30 times larger compared to CPUs and still twice as fast which translates to up to 14MB register memory that operates at a whopping 80TB/s. As a comparison, the CPU L1 cache only operates at about 5TB/s which is quite slow and has the size of roughly 1MB; CPU registers usually have sizes of around 64-128KB and operate at 10-20TB/s. Of course, this comparison of numbers is a bit flawed because registers operate a bit differently than GPU registers (a bit like apples and oranges), but the difference in size here is more crucial than the difference in speed, and it does make a difference.</p>
<p>As a side note, full register utilization in GPUs seems to be difficult to achieve at first because it is the smallest unit of computation which needs to be fine-tuned by hand for good performance. However, NVIDIA has developed helpful compiler tools which indicate when you are using too much or too few registers per stream processor. It is easy to tweak your GPU code to make use of the right amount of registers and L1 cache for fast performance. This gives GPUs an advantage over other architectures like Xeon Phis where this utilization is complicated to achieve and painful to debug which in the end makes it difficult to maximize performance on a Xeon Phi.</p>
<p>What this means, in the end, is that you can store a lot of data in your L1 caches and register files on GPUs to reuse convolutional and matrix multiplication tiles. For example the best matrix multiplication algorithms use 2 tiles of 64x32 to 96x64 numbers for 2 matrices in L1 cache, and a 16x16 to 32x32 number register tile for the outputs sums per thread block (1 thread block = up to 1024 threads; you have 8 thread blocks per stream processor, there are 60 stream processors in total for the entire GPU). If you have a 100MB matrix, you can split it up in smaller matrices that fit into your cache and registers, and then do matrix multiplication with three matrix tiles at speeds of 10-80TB/s â€” that is fast! This is the third reason why GPUs are so much faster than CPUs, and why they are so well suited for deep learning.</p>
<p>Keep in mind that the slower memory always dominates performance bottlenecks. If 95% of your memory movements take place in registers (80TB/s), and 5% in your main memory (0.75TB/s), then you still spend most of the time on memory access of main memory (about six times as much).</p>
<p>Thus in order of importance: (1) High bandwidth main memory, (2) hiding memory access latency under thread parallelism, and (3) large and fast register and L1 memory which is easily programmable are the components which make GPUs so well suited for deep learning.</p>
</div></div><p></p><p></p>
<h1 id="machine-learning-paradigms">Machine Learning Paradigms<a aria-hidden="true" class="anchor-heading icon-link" href="#machine-learning-paradigms"></a></h1>
<p>There are three basic machine learning paradigms:</p>
<ul>
<li>Supervised Learning</li>
<li>Unsupervised Learning</li>
<li>Reinforcement Learning</li>
</ul>
<p><img src="/assets/images/2023-07-08-21-35-07.png"></p>
<h2 id="supervised-learning">Supervised Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#supervised-learning"></a></h2>
<p>Supervised learning is using labeled datasets to train algoritms to classify data or predict outcomes. It does this by iteratively making predictions on the data and adjusting for the correct answer.</p>
<ul>
<li>"labeled" means that the rows in the dataset are tagged or classified in some interesting way that tells us something interesting about that data
<ul>
<li>ex. "is this a picture of a t-shirt?", "does the picture of the plant have mites on the leaves?"</li>
</ul>
</li>
</ul>
<p>The goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output.</p>
<ul>
<li>it is the job of <a href="/notes/v3rh8rqc5aorupfno1w4otw#backpropagation">backpropagation</a> to train a neural network to learn the appropriate internal representation of how an input relates to an output.</li>
</ul>
<p>In supervised learning, the goal is to predict outcomes for new data. You know up front the type of results to expect.</p>
<p>Supervised learning models tend to be more accurate, but also require much more human intervention</p>
<ul>
<li>ex. a supervised learning model can predict how long your commute will be based on the time of day, weather conditions and so on. But first, youâ€™ll have to train it to know that rainy weather extends the driving time.</li>
</ul>
<h3 id="classification-model">Classification Model<a aria-hidden="true" class="anchor-heading icon-link" href="#classification-model"></a></h3>
<p>This method involves us recognizing and grouping ideas/objects into predefined categories</p>
<ul>
<li>ex. customer retention - we can make a model that will help us identify customer that are about to churn, allowing us to take action to retain them. We can do this by analyzing their activity</li>
<li>ex. classify spam in a separate folder from your inbox.</li>
<li>ex. sentiment analysis to determine the sentiment or emotional polarity of a piece of text, such as a review or a social media post. </li>
</ul>
<p>Classification models are more suitable when the task involves assigning discrete labels</p>
<ul>
<li>ex. Is a given email message spam or not spam?</li>
<li>ex. Is this an image of a dog, a cat, or a hamster?</li>
</ul>
<p>Common types of classification algorithms are:</p>
<ul>
<li>linear classifiers</li>
<li>support vector machines</li>
<li>decision trees</li>
<li>random forest</li>
</ul>
<h3 id="regression-model">Regression Model<a aria-hidden="true" class="anchor-heading icon-link" href="#regression-model"></a></h3>
<p>This method involves us building an equation using various input values with their specific weights, determined by their overall value of the impact on their outcome. In this way, it helps us understand the relationship between dependent and independent variables</p>
<ul>
<li>ex. airlines use these models to determine how much they should charge for a particular flights, using various input factors such as days before departure, day of week, destination etc.</li>
<li>ex. Weather forecasting - well-suited for regression, since they can estimate a numerical value based on the input features. </li>
</ul>
<p>Regression models are helpful for predicting numerical values based on different data points, such as sales revenue projections for a given business.</p>
<p>Regression models are more suitable when the output is a continuous value</p>
<ul>
<li>ex. What is the value of a house in California?</li>
<li>ex. What is the probability that a user will click on this ad?</li>
</ul>
<p>Popular regression algorithms are: </p>
<ul>
<li>linear regression</li>
<li>logistic regression</li>
<li>polynomial regression</li>
</ul>
<h2 id="unsupervised-learning">Unsupervised Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#unsupervised-learning"></a></h2>
<p>Using machine learning algorithms to analyze and cluster unlabelled datasets, allowing us to discover hidden patterns/groupings without the need for human intervention.</p>
<p>With an unsupervised learning algorithm, the goal is to get insights from large volumes of new data. The machine learning itself determines what is different or interesting from the dataset.</p>
<p>Unsupervised models still require some human intervention for validating output variables. </p>
<ul>
<li>ex. an unsupervised learning model can identify that online shoppers often purchase groups of products at the same time. However, a data analyst would need to validate that it makes sense for a recommendation engine to group baby clothes with an order of diapers, applesauce and sippy cups.</li>
</ul>
<p>Unsupervised learning models are used for three main tasks: </p>
<ul>
<li>clustering</li>
<li>association</li>
<li>dimensionality reduction</li>
</ul>
<h3 id="clustering">Clustering<a aria-hidden="true" class="anchor-heading icon-link" href="#clustering"></a></h3>
<ul>
<li>ex. customer segmentation - it is not always clear how individual customers are similar or different from one another. Clustering algorithms can take into account a variety of information on the customer, such as their purchase history, social media activity, geography, demographic etc., with the goal being to segment similar customers into separate buckets so the company be more targetted with their efforts</li>
</ul>
<h3 id="association">Association<a aria-hidden="true" class="anchor-heading icon-link" href="#association"></a></h3>
<ul>
<li>ex. recommendation engines</li>
</ul>
<h3 id="dimensionality-reduction">Dimensionality Reduction<a aria-hidden="true" class="anchor-heading icon-link" href="#dimensionality-reduction"></a></h3>
<p>Techniques that reduce the number of input variables in a dataset so we don't let redundant parameters overrepresent the impact on the outcome.</p>
<h2 id="reinforcement-learning">Reinforcement Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#reinforcement-learning"></a></h2>
<p>Reinforcement Learning is semi-supervised learning where we typically have an agent take actions in an environment. The environment will then reward the agent for correct moves, or punish it for incorrect moves</p>
<ul>
<li>Through many iterations of this, we can teach a system a particular task</li>
<li>ex. with self-driving cars</li>
</ul>
<h2 id="transfer-learning">Transfer Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#transfer-learning"></a></h2>
<p>Take a pattern that one model has learned from a certain dataset, and apply those learnings to a different model to give us a head start.</p>
<ul>
<li>More specifically, it's about leveraging the knowledge learned from one task to aid the learning process in another related task. This can be done within the same model or across models.</li>
<li>ex. for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks.</li>
</ul>
<h1 id="machine-learning-approaches">Machine Learning Approaches<a aria-hidden="true" class="anchor-heading icon-link" href="#machine-learning-approaches"></a></h1>
<h3 id="seq2seq-sequence-to-sequence">Seq2Seq (Sequence to Sequence)<a aria-hidden="true" class="anchor-heading icon-link" href="#seq2seq-sequence-to-sequence"></a></h3>
<p>Here, we put one sequence into the model, and get one out</p>
<ul>
<li>ex. with Google Translate, we put in our sequence of source language text and get our target language out</li>
<li>ex. with speech recognition, we put in our sequence of audio waves and get some text out</li>
</ul>
<h3 id="classificationregression">Classification/regression<a aria-hidden="true" class="anchor-heading icon-link" href="#classificationregression"></a></h3>
<p>Classification is about predicting if something is one thing or another (e.g. if an email is spam or not)</p>
<ul>
<li>Computer Vision
<ul>
<li>ex. recognizing a truck within a security camera picture. 
<ul>
<li>In this case, the regression predicts where the corners of the box should be (predicting a number is what regression does), and the classification part would be the machine recognizing whether or not the particular vehicle was the one that did the hit and run.
<img src="/assets/images/2023-07-09-08-40-22.png"></li>
</ul>
</li>
</ul>
</li>
<li>Natural Language Processing</li>
</ul>
<hr>
<h2 id="datasets">Datasets<a aria-hidden="true" class="anchor-heading icon-link" href="#datasets"></a></h2>
<p>The idea is that we want to split up our data and assign different portions to different sets.</p>
<ul>
<li>the main reason to split datasets into Training, Validation, and Test sets is to ensure that we don't overfit our model to the data it was trained on.</li>
</ul>
<h3 id="training-set">Training Set<a aria-hidden="true" class="anchor-heading icon-link" href="#training-set"></a></h3>
<p>The training set is the data that the model learns from</p>
<p>This usually encompasses around 60-80% of our data</p>
<h3 id="validation-set">Validation Set<a aria-hidden="true" class="anchor-heading icon-link" href="#validation-set"></a></h3>
<p>The validation set is the data that the model gets tuned to</p>
<p>Validation sets are used often, but are not required like training sets and test sets are.</p>
<p>The validation set helps tune hyperparameters and choose the best version of the model</p>
<p>This usually encompasses around 10-20% of our data</p>
<h3 id="test-set">Test Set<a aria-hidden="true" class="anchor-heading icon-link" href="#test-set"></a></h3>
<p>The test set is the data that the model gets evaluated on to test what it has learned (ie. it is the final evaluation)</p>
<p>The test set should always be kept separate from all other data, since we want our model to learn on training data and then evaluate it on test data to get an indication of how well it generalizes to unseen examples.</p>
<p>This usually encompasses around 10-20% of our data</p>
<h2 id="ue-resources">UE Resources<a aria-hidden="true" class="anchor-heading icon-link" href="#ue-resources"></a></h2>
<ul>
<li><a href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/">Why are machine learning operations run on GPUs?</a></li>
</ul>
<hr>
<strong>Children</strong>
<ol>
<li><a href="/notes/r0w4lqir8cq5ez9inlimel8">Deep Learning</a></li>
<li><a href="/notes/r9fy49qm440b41yo8sqfwz5">Terminology</a></li>
</ol>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/i2it5id8qwtg27n4usg8bo1">Pytorch</a></li>
<li><a href="/notes/r0w4lqir8cq5ez9inlimel8">Deep Learning</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#machine-learning-paradigms" title="Machine Learning Paradigms">Machine Learning Paradigms</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#supervised-learning" title="Supervised Learning">Supervised Learning</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#classification-model" title="Classification Model">Classification Model</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#regression-model" title="Regression Model">Regression Model</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#unsupervised-learning" title="Unsupervised Learning">Unsupervised Learning</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#clustering" title="Clustering">Clustering</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#association" title="Association">Association</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#dimensionality-reduction" title="Dimensionality Reduction">Dimensionality Reduction</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#reinforcement-learning" title="Reinforcement Learning">Reinforcement Learning</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#transfer-learning" title="Transfer Learning">Transfer Learning</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#machine-learning-approaches" title="Machine Learning Approaches">Machine Learning Approaches</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#seq2seq-sequence-to-sequence" title="Seq2Seq (Sequence to Sequence)">Seq2Seq (Sequence to Sequence)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#classificationregression" title="Classification/regression">Classification/regression</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#datasets" title="Datasets">Datasets</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#training-set" title="Training Set">Training Set</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#validation-set" title="Validation Set">Validation Set</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#test-set" title="Test Set">Test Set</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#ue-resources" title="UE Resources">UE Resources</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"03o3n0hz9v9jtb7j889zpd4","title":"Machine Learning","desc":"","updated":1693281926948,"created":1677596264613,"custom":{},"fname":"ml","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"988dc2619ca328fe7faee673f4d245ed","links":[{"type":"wiki","from":{"fname":"ml","id":"03o3n0hz9v9jtb7j889zpd4","vaultName":"tech"},"value":"math.algebra.linear.tensors","alias":"tensors","position":{"start":{"line":3,"column":69,"offset":190},"end":{"line":3,"column":108,"offset":229},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"math.algebra.linear.tensors"}},{"type":"wiki","from":{"fname":"ml","id":"03o3n0hz9v9jtb7j889zpd4","vaultName":"tech"},"value":"ml.deep-learning.nn.functions","alias":"backpropagation","position":{"start":{"line":27,"column":20,"offset":2474},"end":{"line":27,"column":85,"offset":2539},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ml.deep-learning.nn.functions","anchorHeader":"backpropagation"}},{"type":"ref","from":{"fname":"ml","id":"03o3n0hz9v9jtb7j889zpd4","vaultName":"tech"},"value":"hardware.GPU","position":{"start":{"line":11,"column":1,"offset":1617},"end":{"line":11,"column":42,"offset":1658},"indent":[]},"xvault":false,"to":{"fname":"hardware.GPU","anchorHeader":"use-in-machine-learning"}},{"from":{"fname":"pytorch","id":"i2it5id8qwtg27n4usg8bo1","vaultName":"tech"},"type":"backlink","position":{"start":{"line":35,"column":82,"offset":1814},"end":{"line":35,"column":124,"offset":1856},"indent":[]},"value":"ml"},{"from":{"fname":"ml.deep-learning","id":"r0w4lqir8cq5ez9inlimel8","vaultName":"tech"},"type":"backlink","position":{"start":{"line":12,"column":60,"offset":904},"end":{"line":12,"column":106,"offset":950},"indent":[]},"value":"ml"}],"anchors":{"machine-learning-paradigms":{"type":"header","text":"Machine Learning Paradigms","value":"machine-learning-paradigms","line":19,"column":0,"depth":1},"supervised-learning":{"type":"header","text":"Supervised Learning","value":"supervised-learning","line":27,"column":0,"depth":2},"classification-model":{"type":"header","text":"Classification Model","value":"classification-model","line":40,"column":0,"depth":3},"regression-model":{"type":"header","text":"Regression Model","value":"regression-model","line":56,"column":0,"depth":3},"unsupervised-learning":{"type":"header","text":"Unsupervised Learning","value":"unsupervised-learning","line":72,"column":0,"depth":2},"clustering":{"type":"header","text":"Clustering","value":"clustering","line":85,"column":0,"depth":3},"association":{"type":"header","text":"Association","value":"association","line":88,"column":0,"depth":3},"dimensionality-reduction":{"type":"header","text":"Dimensionality Reduction","value":"dimensionality-reduction","line":91,"column":0,"depth":3},"reinforcement-learning":{"type":"header","text":"Reinforcement Learning","value":"reinforcement-learning","line":94,"column":0,"depth":2},"transfer-learning":{"type":"header","text":"Transfer Learning","value":"transfer-learning","line":99,"column":0,"depth":2},"machine-learning-approaches":{"type":"header","text":"Machine Learning Approaches","value":"machine-learning-approaches","line":104,"column":0,"depth":1},"seq2seq-sequence-to-sequence":{"type":"header","text":"Seq2Seq (Sequence to Sequence)","value":"seq2seq-sequence-to-sequence","line":105,"column":0,"depth":3},"classificationregression":{"type":"header","text":"Classification/regression","value":"classificationregression","line":110,"column":0,"depth":3},"datasets":{"type":"header","text":"Datasets","value":"datasets","line":120,"column":0,"depth":2},"training-set":{"type":"header","text":"Training Set","value":"training-set","line":124,"column":0,"depth":3},"validation-set":{"type":"header","text":"Validation Set","value":"validation-set","line":129,"column":0,"depth":3},"test-set":{"type":"header","text":"Test Set","value":"test-set","line":138,"column":0,"depth":3},"ue-resources":{"type":"header","text":"UE Resources","value":"ue-resources","line":145,"column":0,"depth":2}},"children":["r0w4lqir8cq5ez9inlimel8","r9fy49qm440b41yo8sqfwz5"],"parent":"olZIVfSs2uLLr3BppFh4K","data":{}},"body":"\u003ch1 id=\"machine-learning\"\u003eMachine Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#machine-learning\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eMachine learning is the ability for computers to learn something without explicitly being programmed to know that thing.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit is done by turning data into numbers (which are then stored in \u003ca title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\"\u003etensors (Private)\u003c/a\u003e), and then passing it into a neural network (which tries to find patterns in those numbers)\n\u003cul\u003e\n\u003cli\u003ein fact, you can use machine learning for pretty much anything as long as you can convert the inputs into numbers and write the machine learning algorithm that will allow the machine to find the patterns.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe whole premise of machine learning is to learn a representation of the input and how it maps to the output\n\u003cul\u003e\n\u003cli\u003eex. imagine we had an input \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eX\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eX\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e and an ouput \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eY\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eY\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\"\u003eY\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e. We are curious to see how these 2 variables relate to each other. Using machine learning, we could reveal that a simple regression formula (\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eY\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmi\u003eX\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eY = a + bX\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\"\u003eY\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e) lies beneath the relationship. In this case, machine learning is about discovering the nature of that relationship with only the input and the output.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhereas traditional programming is focused on defining inputs and rules in order to arrive at an output, machine learning is focused on defining inputs and an output, and figuring out the rules necessary to arrive at that output\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. we are building a program to cook a roast chicken. The traditional programming paradigm would have us defining the ingredients (ie. inputs) and defining the cooking steps (ie. rules), which yields the cooked meal (ie. output). In machine learning, we define the ingredients, define the output, and the machine learning algorithm will figure out how to make the meal itself.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"portal-container\"\u003e\n\u003cdiv class=\"portal-head\"\u003e\n\u003cdiv class=\"portal-backlink\"\u003e\n\u003cdiv class=\"portal-title\"\u003eFrom \u003cspan class=\"portal-text-title\"\u003eGPU\u003c/span\u003e\u003c/div\u003e\n\u003ca href=\"/notes/7iLURlousehIzpOtpQ6eX\" class=\"portal-arrow\"\u003eGo to text \u003cspan class=\"right-arrow\"\u003eâ†’\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\"\u003e\n\u003cdiv class=\"portal-parent-fader-top\"\u003e\u003c/div\u003e\n\u003cdiv class=\"portal-parent-fader-bottom\"\u003e\u003c/div\u003e\u003ch3 id=\"use-in-machine-learning\"\u003eUse in Machine Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#use-in-machine-learning\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThough originally GPUs were built for graphics, Nvidia provides a software platform called CUDA (Compute Unified Device Architecture) which is an API that enables software (like Pytorch) to use GPU for general purpose computing tasks.\u003c/p\u003e\n\u003c!-- Refine this explanation --\u003e\n\u003cp\u003eGPUs are so fast because they are so efficient for matrix multiplication and convolution\u003c/p\u003e\n\u003cp\u003eCPUs are latency optimized while GPUs are bandwidth optimized. You can visualize this as a CPU being a Ferrari and a GPU being a big truck. The task of both is to pick up packages from a random location A and to transport those packages to another random location B. The CPU (Ferrari) can fetch some memory (packages) in your RAM quickly while the GPU (big truck) is slower in doing that (much higher latency). However, the CPU (Ferrari) needs to go back and forth many times to do its job (location A \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo\u003eâ†’\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\rightarrow\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.3669em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003eâ†’\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e pick up 2 packages \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo\u003eâ†’\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\rightarrow\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.3669em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003eâ†’\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e location B ... repeat) while the GPU can fetch much more memory at once (location A \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo\u003eâ†’\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\rightarrow\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.3669em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003eâ†’\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e pick up 100 packages \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo\u003eâ†’\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\rightarrow\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.3669em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003eâ†’\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e location B ... repeat).\u003c/p\u003e\n\u003cp\u003eSo, in other words, the CPU is good at fetching small amounts of memory quickly (5 \u003cem\u003e 3 \u003c/em\u003e 7) while the GPU is good at fetching large amounts of memory (Matrix multiplication: (A\u003cem\u003eB)\u003c/em\u003eC). The best CPUs have about 50GB/s while the best GPUs have 750GB/s memory bandwidth. So the more memory your computational operations require, the more significant the advantage of GPUs over CPUs. But there is still the latency that may hurt performance in the case of the GPU. A big truck may be able to pick up a lot of packages with each tour, but the problem is that you are waiting a long time until the next set of packages arrives. Without solving this problem, GPUs would be very slow even for large amounts of data. So how is this solved?\u003c/p\u003e\n\u003cp\u003eIf you ask a big truck to make many tours to fetch packages you will always wait for a long time for the next load of packages once the truck has departed to do the next tour â€” the truck is just slow. However, if you now use a fleet of either Ferraris and big trucks (thread parallelism), and you have a big job with many packages (large chunks of memory such as matrices) then you will wait for the first truck a bit, but after that you will have no waiting time at all â€” unloading the packages takes so much time that all the trucks will queue in unloading location B so that you always have direct access to your packages (memory). This effectively hides latency so that GPUs offer high bandwidth while hiding their latency under thread parallelism â€” so for large chunks of memory GPUs provide the best memory bandwidth while having almost no drawback due to latency via thread parallelism. This is the second reason why GPUs are faster than CPUs for deep learning. As a side note, you will also see why more threads do not make sense for CPUs: A fleet of Ferraris has no real benefit in any scenario.\u003c/p\u003e\n\u003cp\u003eBut the advantages for the GPU do not end here. This is the first step where the memory is fetched from the main memory (RAM) to the local memory on the chip (L1 cache and registers). This second step is less critical for performance but still adds to the lead for GPUs. All computation that ever is executed happens in registers which are directly attached to the execution unit (a core for CPUs, a stream processor for GPUs). Usually, you have the fast L1 and register memory very close to the execution engine, and you want to keep these memories small so that access is fast. Increased distance to the execution engine dramatically reduces memory access speed, so the larger the distance to access it the slower it gets. If you make your memory larger and larger, then, in turn, it gets slower to access its memory (on average, finding what you want to buy in a small store is faster than finding what you want to buy in a huge store, even if you know where that item is). So the size is limited for register files - we are just at the limits of physics here and every nanometer counts, we want to keep them small.\u003c/p\u003e\n\u003cp\u003eThe advantage of the GPU is here that it can have a small pack of registers for every processing unit (stream processor, or SM), of which it has many. Thus we can have in total a lot of register memory, which is very small and thus very fast. This leads to the aggregate GPU registers size being more than 30 times larger compared to CPUs and still twice as fast which translates to up to 14MB register memory that operates at a whopping 80TB/s. As a comparison, the CPU L1 cache only operates at about 5TB/s which is quite slow and has the size of roughly 1MB; CPU registers usually have sizes of around 64-128KB and operate at 10-20TB/s. Of course, this comparison of numbers is a bit flawed because registers operate a bit differently than GPU registers (a bit like apples and oranges), but the difference in size here is more crucial than the difference in speed, and it does make a difference.\u003c/p\u003e\n\u003cp\u003eAs a side note, full register utilization in GPUs seems to be difficult to achieve at first because it is the smallest unit of computation which needs to be fine-tuned by hand for good performance. However, NVIDIA has developed helpful compiler tools which indicate when you are using too much or too few registers per stream processor. It is easy to tweak your GPU code to make use of the right amount of registers and L1 cache for fast performance. This gives GPUs an advantage over other architectures like Xeon Phis where this utilization is complicated to achieve and painful to debug which in the end makes it difficult to maximize performance on a Xeon Phi.\u003c/p\u003e\n\u003cp\u003eWhat this means, in the end, is that you can store a lot of data in your L1 caches and register files on GPUs to reuse convolutional and matrix multiplication tiles. For example the best matrix multiplication algorithms use 2 tiles of 64x32 to 96x64 numbers for 2 matrices in L1 cache, and a 16x16 to 32x32 number register tile for the outputs sums per thread block (1 thread block = up to 1024 threads; you have 8 thread blocks per stream processor, there are 60 stream processors in total for the entire GPU). If you have a 100MB matrix, you can split it up in smaller matrices that fit into your cache and registers, and then do matrix multiplication with three matrix tiles at speeds of 10-80TB/s â€” that is fast! This is the third reason why GPUs are so much faster than CPUs, and why they are so well suited for deep learning.\u003c/p\u003e\n\u003cp\u003eKeep in mind that the slower memory always dominates performance bottlenecks. If 95% of your memory movements take place in registers (80TB/s), and 5% in your main memory (0.75TB/s), then you still spend most of the time on memory access of main memory (about six times as much).\u003c/p\u003e\n\u003cp\u003eThus in order of importance: (1) High bandwidth main memory, (2) hiding memory access latency under thread parallelism, and (3) large and fast register and L1 memory which is easily programmable are the components which make GPUs so well suited for deep learning.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\u003ch1 id=\"machine-learning-paradigms\"\u003eMachine Learning Paradigms\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#machine-learning-paradigms\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eThere are three basic machine learning paradigms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSupervised Learning\u003c/li\u003e\n\u003cli\u003eUnsupervised Learning\u003c/li\u003e\n\u003cli\u003eReinforcement Learning\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2023-07-08-21-35-07.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"supervised-learning\"\u003eSupervised Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#supervised-learning\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eSupervised learning is using labeled datasets to train algoritms to classify data or predict outcomes. It does this by iteratively making predictions on the data and adjusting for the correct answer.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"labeled\" means that the rows in the dataset are tagged or classified in some interesting way that tells us something interesting about that data\n\u003cul\u003e\n\u003cli\u003eex. \"is this a picture of a t-shirt?\", \"does the picture of the plant have mites on the leaves?\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit is the job of \u003ca href=\"/notes/v3rh8rqc5aorupfno1w4otw#backpropagation\"\u003ebackpropagation\u003c/a\u003e to train a neural network to learn the appropriate internal representation of how an input relates to an output.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn supervised learning, the goal is to predict outcomes for new data. You know up front the type of results to expect.\u003c/p\u003e\n\u003cp\u003eSupervised learning models tend to be more accurate, but also require much more human intervention\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. a supervised learning model can predict how long your commute will be based on the time of day, weather conditions and so on. But first, youâ€™ll have to train it to know that rainy weather extends the driving time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"classification-model\"\u003eClassification Model\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#classification-model\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThis method involves us recognizing and grouping ideas/objects into predefined categories\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. customer retention - we can make a model that will help us identify customer that are about to churn, allowing us to take action to retain them. We can do this by analyzing their activity\u003c/li\u003e\n\u003cli\u003eex. classify spam in a separate folder from your inbox.\u003c/li\u003e\n\u003cli\u003eex. sentiment analysis to determine the sentiment or emotional polarity of a piece of text, such as a review or a social media post. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eClassification models are more suitable when the task involves assigning discrete labels\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. Is a given email message spam or not spam?\u003c/li\u003e\n\u003cli\u003eex. Is this an image of a dog, a cat, or a hamster?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCommon types of classification algorithms are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elinear classifiers\u003c/li\u003e\n\u003cli\u003esupport vector machines\u003c/li\u003e\n\u003cli\u003edecision trees\u003c/li\u003e\n\u003cli\u003erandom forest\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"regression-model\"\u003eRegression Model\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#regression-model\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThis method involves us building an equation using various input values with their specific weights, determined by their overall value of the impact on their outcome. In this way, it helps us understand the relationship between dependent and independent variables\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. airlines use these models to determine how much they should charge for a particular flights, using various input factors such as days before departure, day of week, destination etc.\u003c/li\u003e\n\u003cli\u003eex. Weather forecasting - well-suited for regression, since they can estimate a numerical value based on the input features. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eRegression models are helpful for predicting numerical values based on different data points, such as sales revenue projections for a given business.\u003c/p\u003e\n\u003cp\u003eRegression models are more suitable when the output is a continuous value\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. What is the value of a house in California?\u003c/li\u003e\n\u003cli\u003eex. What is the probability that a user will click on this ad?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePopular regression algorithms are: \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elinear regression\u003c/li\u003e\n\u003cli\u003elogistic regression\u003c/li\u003e\n\u003cli\u003epolynomial regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"unsupervised-learning\"\u003eUnsupervised Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#unsupervised-learning\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eUsing machine learning algorithms to analyze and cluster unlabelled datasets, allowing us to discover hidden patterns/groupings without the need for human intervention.\u003c/p\u003e\n\u003cp\u003eWith an unsupervised learning algorithm, the goal is to get insights from large volumes of new data. The machine learning itself determines what is different or interesting from the dataset.\u003c/p\u003e\n\u003cp\u003eUnsupervised models still require some human intervention for validating output variables. \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. an unsupervised learning model can identify that online shoppers often purchase groups of products at the same time. However, a data analyst would need to validate that it makes sense for a recommendation engine to group baby clothes with an order of diapers, applesauce and sippy cups.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUnsupervised learning models are used for three main tasks: \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eclustering\u003c/li\u003e\n\u003cli\u003eassociation\u003c/li\u003e\n\u003cli\u003edimensionality reduction\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"clustering\"\u003eClustering\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#clustering\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eex. customer segmentation - it is not always clear how individual customers are similar or different from one another. Clustering algorithms can take into account a variety of information on the customer, such as their purchase history, social media activity, geography, demographic etc., with the goal being to segment similar customers into separate buckets so the company be more targetted with their efforts\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"association\"\u003eAssociation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#association\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eex. recommendation engines\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"dimensionality-reduction\"\u003eDimensionality Reduction\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#dimensionality-reduction\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eTechniques that reduce the number of input variables in a dataset so we don't let redundant parameters overrepresent the impact on the outcome.\u003c/p\u003e\n\u003ch2 id=\"reinforcement-learning\"\u003eReinforcement Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#reinforcement-learning\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning is semi-supervised learning where we typically have an agent take actions in an environment. The environment will then reward the agent for correct moves, or punish it for incorrect moves\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThrough many iterations of this, we can teach a system a particular task\u003c/li\u003e\n\u003cli\u003eex. with self-driving cars\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"transfer-learning\"\u003eTransfer Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#transfer-learning\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eTake a pattern that one model has learned from a certain dataset, and apply those learnings to a different model to give us a head start.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMore specifically, it's about leveraging the knowledge learned from one task to aid the learning process in another related task. This can be done within the same model or across models.\u003c/li\u003e\n\u003cli\u003eex. for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"machine-learning-approaches\"\u003eMachine Learning Approaches\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#machine-learning-approaches\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch3 id=\"seq2seq-sequence-to-sequence\"\u003eSeq2Seq (Sequence to Sequence)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#seq2seq-sequence-to-sequence\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eHere, we put one sequence into the model, and get one out\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. with Google Translate, we put in our sequence of source language text and get our target language out\u003c/li\u003e\n\u003cli\u003eex. with speech recognition, we put in our sequence of audio waves and get some text out\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"classificationregression\"\u003eClassification/regression\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#classificationregression\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eClassification is about predicting if something is one thing or another (e.g. if an email is spam or not)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eComputer Vision\n\u003cul\u003e\n\u003cli\u003eex. recognizing a truck within a security camera picture. \n\u003cul\u003e\n\u003cli\u003eIn this case, the regression predicts where the corners of the box should be (predicting a number is what regression does), and the classification part would be the machine recognizing whether or not the particular vehicle was the one that did the hit and run.\n\u003cimg src=\"/assets/images/2023-07-09-08-40-22.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNatural Language Processing\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"datasets\"\u003eDatasets\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#datasets\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe idea is that we want to split up our data and assign different portions to different sets.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe main reason to split datasets into Training, Validation, and Test sets is to ensure that we don't overfit our model to the data it was trained on.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"training-set\"\u003eTraining Set\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#training-set\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe training set is the data that the model learns from\u003c/p\u003e\n\u003cp\u003eThis usually encompasses around 60-80% of our data\u003c/p\u003e\n\u003ch3 id=\"validation-set\"\u003eValidation Set\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#validation-set\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe validation set is the data that the model gets tuned to\u003c/p\u003e\n\u003cp\u003eValidation sets are used often, but are not required like training sets and test sets are.\u003c/p\u003e\n\u003cp\u003eThe validation set helps tune hyperparameters and choose the best version of the model\u003c/p\u003e\n\u003cp\u003eThis usually encompasses around 10-20% of our data\u003c/p\u003e\n\u003ch3 id=\"test-set\"\u003eTest Set\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#test-set\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe test set is the data that the model gets evaluated on to test what it has learned (ie. it is the final evaluation)\u003c/p\u003e\n\u003cp\u003eThe test set should always be kept separate from all other data, since we want our model to learn on training data and then evaluate it on test data to get an indication of how well it generalizes to unseen examples.\u003c/p\u003e\n\u003cp\u003eThis usually encompasses around 10-20% of our data\u003c/p\u003e\n\u003ch2 id=\"ue-resources\"\u003eUE Resources\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#ue-resources\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/\"\u003eWhy are machine learning operations run on GPUs?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cstrong\u003eChildren\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"/notes/r0w4lqir8cq5ez9inlimel8\"\u003eDeep Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/r9fy49qm440b41yo8sqfwz5\"\u003eTerminology\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/i2it5id8qwtg27n4usg8bo1\"\u003ePytorch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/r0w4lqir8cq5ez9inlimel8\"\u003eDeep Learning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"olZIVfSs2uLLr3BppFh4K","title":"Digital Garden","desc":"","updated":1674517603573,"created":1615482407722,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"effb007003ca6a91d7fd0c293e1d2436","links":[{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"testing.method.unit","alias":"unit testing","position":{"start":{"line":18,"column":121,"offset":1146},"end":{"line":18,"column":157,"offset":1182},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"testing.method.unit"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"general.arch.microservice","alias":"microservices","position":{"start":{"line":18,"column":188,"offset":1213},"end":{"line":18,"column":231,"offset":1256},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"general.arch.microservice"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"paradigm.oop","alias":"OOP","position":{"start":{"line":36,"column":227,"offset":2718},"end":{"line":36,"column":247,"offset":2738},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"paradigm.oop"}}],"anchors":{"tags":{"type":"header","text":"Tags","value":"tags","line":46,"column":0,"depth":3},"resources":{"type":"header","text":"Resources","value":"resources","line":55,"column":0,"depth":2},"ue-unexamined-resources":{"type":"header","text":"UE (Unexamined) Resources","value":"ue-unexamined-resources","line":56,"column":0,"depth":3},"e-examined-resources":{"type":"header","text":"E (Examined) Resources","value":"e-examined-resources","line":59,"column":0,"depth":3},"resources-1":{"type":"header","text":"Resources","value":"resources-1","line":62,"column":0,"depth":3}},"children":["40ubf88tephzbdjte8cdsa0","zFMjbn3xihVNHjUIdZCD1","IK6NOKemuDjhfstJBovKL","ZaTr28eWk1DrXOEsc1YVb","Vi0WYVLZunVM9iR8XZJn3","ngAbg7gluvbt1bY1UIRsi","RCPPXSakm5TvKka8kOmVp","LIcuGYV0DDt1VWbvH6Sed","MPx8ykTP57I40WTZvTP7f","ZU5WmdTG1bHoE8RcmZXZG","jqWiyYJff92RjXuUQt9PQ","KihijM8OQvZ4pASkkhqzL","F9vyqvls3OBCujtukqKhy","k6jxm2b3edgkhbordpuz6v1","facc2b01-755a-409f-99f6-57bef2d1501f","bZumdyapJ2H0wWWOmJ45i","elqpgoe2r951si4xrhujppj","m5ov42Vm6mR7RQWTvl1NW","cw8cerc0cos8lh871oz8rtf","PZxxZ4iESzSlbbHJKxuAZ","UDu62Pa6BaRqlG8uGoMhy","u3zcndycwqessho5h6x0nz9","ZgCUp366YrF2Tyky2NT73","DVpVUmavSoVDA7UIlPzLX","nMCtMXVvjBsJk3iw1zoIO","ANfx9Z4a6ZA0uJuSHtbIJ","BkG557LKUYbH1DnxprEVT","1TaH8zDTM98FZ9SYaeXLM","02v7ymc144e5c4pv3edkud7","dc79799f-a55d-48ab-a8be-c48adb1b19c0","f6feef57-b8f5-451f-a09a-7b63f6a07183","4sz47Y0LKs1Si73rWtyyh","g7ulqi8no93ezeocbesc3ll","5a9fb1df-478e-4687-9be0-5cb97e61ec57","1374e9e9-1cbc-4e1f-b1ca-66b8569533dd","D8Z3rjXkSj2EOymXQXF4Z","np3c1ykvnjqv5xoombpfwqz","f529cc34-aba0-45ca-ad7e-02ddda318941","0zcauha3il2NqtxZazIo7","9bbd6f68-03b2-41f4-92e4-2ca313e8b450","5a2ab598-fa7e-4471-8bda-9f5831b679ae","uV6w4mZPoohWyZV4Xaad0","QHXEIyeZGIGMVi5Q52UWI","RgE0mZLaUjPftFPZsiAoe","mytCOts26Pidush65tdRW","fwUzxfLSPMH1eL8oBoLWx","wazdsda6h25x66edvfmeuiv","TbW7PM9bg1y5TGkiWwQ8b","xiSIDeEtIc8X0lpUQlppI","0jxgntiLNHWFuCbzqtFGF","GkdMprLUe4QQULBxmGN6V","4SYc6v5hxY5g6Ip6kjpwO","czi7ilt2i1uoqm1f2otbntj","fqVQpS9FBiXgKsZX3R3sJ","L5JUZlGAGvTxrsEBB7DY8","ttyri4pwfyn5lcx0vp9804o","TeTedoeS2LdHPR632eCpM","ulicRRwo3lSFzh3tMfWH9","p7d24vyb8m00ombzn34t50c","e65188a1-177e-4ab2-99f9-75f85d537621","fajYnbVhCRDi74xn0H30R","bArGdGwqo3iFkyKMyE7qR","4Dcp7gbEVoLLgfu7bXFai","5d700782-fb81-416e-83f9-5dd88260e350","zubgzhNFE6KlTgXcjTz6O","8lpdfWa0cbSq1XJQbcYcY","2psluywdc416t7vrql0m058","17I8ZksXvqCH1mRtZDjHp","kF916Ow84qpJJeMRkWMIo","tMLkLcrIHHBz56xVmBLkP","CSePBQ6q7qhowKESqVwt7","laZ4OfLhZNK1Kuy6GaWUr","z5IJblOknQhMzZ5QZh4ye","N36FHxfxzwJfxDY3miWyX","3bb25f58-2b50-4fa3-af55-48ea9f88a081","7x07qjgbitozdfszqzuy7ix","u93Rz4fEWGu6VBR30Zraf","9hjMHnKvYT4jLKvuDSXaV","qC5GxCZBmNb4Ip6c0kU8x","LgW7mTIALODoXc54B3p6S","UYPxfHBFWX7fb6hHU5bB6","74lYtC8NKpCzcyFOZTfR1","sEoBNAEuaTxwSmTXDonZt","dj0jr9mpvs62e2pkg3zc3yy","z8ie0xjogb6ht7gzowav5xr","qxSOd6SPN8qf9ZUojVFDX","WQoMTf6VXBaxCgksXAVsj","2Mw4XgfyCHXHNOX5yoCIY","qcdt2f7jo51muquo5r95dpo","0XJqmcdtcMZu66glBI5O8","lK8r8BXS4ThiUTe4xKIZe","93de42ca-53ea-460a-baa7-b9ec5c47cb1e","Rxs2jaGpdFzqYtP7lAJFJ","aO8W81Z0PyIb6Hs7nOHPW","lhzisalkebu4w5n01np07i0","7b5l4b6fi65n7sv9org5q1l","I01hENHnh8Tqu3Ok8sLzG","03o3n0hz9v9jtb7j889zpd4","evqsPNutOaZ8hcBCqxFQu","zhhxcjZUHdU8uRLwGb9Zh","G1aFACZB2ooWGMGwyd3ZW","YWy1C4tgoaCcw1m8JJsr7","Szj3o5iaNxPpesiCqwrbu","613syb18hb3v0u1ydvor7ru","XhvCDW3fIw6h6MhY5ticq","Q70g7SusFZBQXzkuQifv4","AzfWDH3wp7jFpL2EYxBcW","y0fwpZ9qMqirsLiFyOciU","ZF8xj8wwDUqKlrwTrCFZ1","tAJvhqhdfyZZa87QHq1TU","4hRmipi8lxpBLyzWu5JVB","iTg0C7QjvnmqBZeEigJNs","3babc3d2-79ae-470a-9c06-ab8bba2e684e","bF3UsMFya3fMeXWDspVov","ULkfbL9WpktbVYnzhl6Jw","PpNOO8JYWe6dM8wruSa7x","di40pCxDn7IiqE8lFdD46","rmW0mkerqV35I8QPji6lM","3IFIK1ByzeIxZCByryGLN","iImkYAKfkw3beAl6pLbDn","ecDe8DNWrkeQTwpTEvHje","i2it5id8qwtg27n4usg8bo1","Yqhdd9mSJGN7OJOeyoSD2","Ws5tah8tpeyn9tK8VBTg8","gWAg15uBJgkS2B0wcpMAa","l7V3v2ep1YdDCt7DOr7Ci","yM2PJBdqJnHpD63cPA6sW","qn0bre7eLbi3QMbCfWkUi","fSu0KxFL41IRouotqmbHs","0gtg24Mj1a1bQFPRGQNlO","7iQPBMltLPLbFEz2qbjPu","yoh4pwoXcfELInGKRdYf6","dCGCWXgAmiOZXbdULT1m6","jMavlje07sNa6hSEIE8WA","Xxm0JE4dKHxrQAaZfzvxD","nRb6Im4Kcmc2ZWE7K1jZ1","PAEBZCyFBZJyR7OoMZ41E","PQ6km8RgRCuyICBPOYz8f","x5tm1nfjzyawwzedy3yitgd","2bhftt8rGuxYu4pFgNqru","hjYIZpHQWuXfeEoGeJEKW","K2M9bQqVq2eQfm29eslKL","X8obW1iKwYsvNgKWGyCzy","yJwSC7hqYIezTFHf5i0Ev","c4Z7ETcOHUILRMH32Sfjw","qiR6dIu857b9M9kTqjOyK","fc2coz74cnfy5czzofx4h5x","Ku1OgHMhELajzo61Gx7ye","LUrfhDWo8wuwZu7CN9TV8","osu6JGOnvXJ5gt3tpqWZY","1a6173cd-cf13-4b34-a522-8350bf9a364f","S2sBltrPfd8a7ICuD7CuH","GLQ2pmkJUNUa93THBDVsD","md6xitz4exia2joa06i490b","oWCuBXOg6JWfZzjmKxmNl","jOmhZ8ovLYTPbpM1vqSDx","p9bov84s0isgkl1ysaw93kk","FraC6xzLy1ei91l1ICyc9","6ceBas2RE9Q4787GDngH7","734cd78d-0bc9-426b-803d-1efc84dfffe5","k4Bb09px6r0FxIRs49SXV","oaG3H1S9IUBO644nGZigu","Ka7agQJkUMRSWN0uFdkWK","si3z090WsiLasMhJBa1Az","hs6rwzt4mogiicoc4gcykbi","ljKAVERmdEiKLK9hXGKBm","zxt3lhonfdhglvijd17ua8c","dd7dopve1dudqkoibkqvti4","923tgifqf59ovv5yldtyi0a","vrjwp01goqw47fqctm4f4lo","c99gdmmppju3r1tth8cb2jx","z2pvn5qxdz84zgygqzxage8"],"parent":null,"data":{},"body":"This Dendron vault of tech knowledge is organized according to domains and their sub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git would include things like `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner`, and so on.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the \"aha\" moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nAs a result, I have allowed myself to use potentially arcane concepts to explain other ones. For example, in my note on [[unit testing|testing.method.unit]], I have made reference to the [[microservices|general.arch.microservice]] note. If these notes were made with the public in mind, this would be a very bad strategy, given that you'd have to understand microservices to be able to draw that same parallel that I've already drawn. Since these notes are written for myself, I have been fine with taking these liberties.\n\nWhat I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course this is all\nvague sounding, and really depends on the domain along with the ends I am\ntrying to reach.\n\nTo achieve this, the system should be steadfast to:\n- be able to put information in relatively easily, without too much thought\n\trequired to its location. While location is important, Dendron makes it easy\n\tto relocate notes, if it becomes apparent that a different place makes more\n\tsense.\n- be able to extract the information that is needed, meaning there is a\n\thigh-degree in confidence in the location of the information. The idea is\n\tthat information loses a large amount of its value when it is unfindable.\n\tTherefore, a relatively strict ideology should be used when determining\n\twhere a piece of information belongs.\n\t- Some concepts might realistically belong to multiple domains. For instance, the concept of *access modifiers* can be found in both `C#` and `Typescript`. Therefore, this note should be abstracted to a common place, such as [[OOP|paradigm.oop]].\n\nThis Dendron vault is the sister component to the [General Second Brain](https://tech.kyletycholiz.com).\n\n### Tags\nThroughout the garden, I have made use of tags, which give semantic meaning to the pieces of information.\n\n- `ex.` - Denotes an *example* of the preceding piece of information\n- `spec:` - Specifies that the preceding information has some degree of *speculation* to it, and may not be 100% factual. Ideally this gets clarified over time as my understanding develops.\n- `anal:` - Denotes an *analogy* of the preceding information. Often I will attempt to link concepts to others that I have previously learned.\n- `mn:` - Denotes a *mnemonic*\n- `expl:` - Denotes an *explanation*\n\n## Resources\n### UE (Unexamined) Resources\nOften, I come across sources of information that I believe to be high-quality. They may be recommendations or found in some other way. No matter their origin, I may be in a position where I don't have the time to fully examine them (and properly extract notes), or I may not require the information at that moment in time. In cases like these, I will add reference to a section of the note called **UE Resources**. The idea is that in the future when I am ready to examine them, I have a list of resources that I can start with. This is an alternative strategy to compiling browser bookmarks, which I've found can quickly become untenable.\n\n### E (Examined) Resources\nOnce a resource has been thoroughly examined and has been mined for notes, it will be moved from *UE Resources* to *E Resources*. This is to indicate that (in my own estimation), there is nothing more to be gained from the resource that is not already in the note.\n\n### Resources\nThis heading is for inexhaustible resources. \n- A prime example would be a quality website that continually posts articles.  - Another example would be a tool, such as software that measures frequencies in a room to help acoustically treat it.\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.83.0","vaults":[{"fsPath":"../main/tech","name":"tech"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"The Tech Digital Garden of Kyle Tycholiz"},"github":{"cname":"tech.kyletycholiz.com","enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableMermaid":true,"siteUrl":"https://tech.kyletycholiz.com","duplicateNoteBehavior":{"action":"useVault","payload":["tech"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"03o3n0hz9v9jtb7j889zpd4"},"buildId":"fo6gubzKP_KV9RZPhaoiQ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>