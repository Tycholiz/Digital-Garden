<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>Neural Network</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="The Tech Digital Garden of Kyle Tycholiz"/><meta property="og:title" content="Neural Network"/><meta property="og:description" content="The Tech Digital Garden of Kyle Tycholiz"/><meta property="og:url" content="https://tech.kyletycholiz.com/notes/k4ib1hhlkmzcrfx8vljz7sg/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="7/8/2023"/><meta property="article:modified_time" content="8/23/2023"/><link rel="canonical" href="https://tech.kyletycholiz.com/notes/k4ib1hhlkmzcrfx8vljz7sg/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6b338472289fe290.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/fo6gubzKP_KV9RZPhaoiQ/_buildManifest.js" defer=""></script><script src="/_next/static/fo6gubzKP_KV9RZPhaoiQ/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="neural-network">Neural Network<a aria-hidden="true" class="anchor-heading icon-link" href="#neural-network"></a></h1>
<h2 id="overview">Overview<a aria-hidden="true" class="anchor-heading icon-link" href="#overview"></a></h2>
<p>Neural networks are preferred when you have unstructured data (ie. data that is not neatly formatted in tables)</p>
<ul>
<li>that's not to say that they can't work well with structured data. It's more the case that traditional machine learning algorithms like decision trees or SVMs often handle structured data well.</li>
</ul>
<p>At a basic level, a neural network is comprised of four main components: </p>
<ul>
<li>inputs
<ul>
<li>ex. your inputs may have a binary value of 0 or 1</li>
</ul>
</li>
<li>weights - Larger weights make a single input’s contribution to the output more significant</li>
<li>a bias or threshold - the output value of any node must be above the threshold for data to be sent to the next layer of the network
<ul>
<li>a threshold value of 5 would translate to a bias value of –5.</li>
</ul>
</li>
<li>an output (<code>y-hat</code>)</li>
</ul>
<h3 id="node-aka-neuron">Node (a.k.a Neuron)<a aria-hidden="true" class="anchor-heading icon-link" href="#node-aka-neuron"></a></h3>
<p>Each node corresponds to a neuron in a biological neural network</p>
<p>Think of each node as its own <a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">linear regression (Private)</a> model.</p>
<ul>
<li>since linear regression can be used to predict future events</li>
</ul>
<p>Think of a neuron as a function that takes in the outputs of all nodes from the previous layer and returns a number between 0 and 1.</p>
<ul>
<li>this number is called the <em>activation</em>, and it is the output of the <em>activation function</em></li>
<li>the activations in one layer determine the activations in the subsequent layer.</li>
<li>note: While many activation functions produce values between 0 and 1 (like the sigmoid function), not all do. For instance, the ReLU (Rectified Linear Unit) function produces values between 0 and infinity, and the tanh function outputs values between -1 and 1.</li>
</ul>
<h3 id="weight">Weight<a aria-hidden="true" class="anchor-heading icon-link" href="#weight"></a></h3>
<p>Each connection between 2 nodes is assigned a weight, which indicates the strength of that connection, and how much influence the input has on the node.</p>
<p>Each node (the current node) receives input from <em>every</em> node of the previous layer. To each of these inputs we apply a weight that affects how much of a contribution that particular node of the previous layer has on the current node</p>
<p>Weight is a fundamental concept to neural networks because inputs naturally have a different magnitude of effort on the outcome</p>
<ul>
<li>ex. in a model that predicts housing prices, the recency of a paint job and the number of bedrooms are both inputs to the model that affect the price, but the latter has a much bigger impact on the ultimate price of the house.</li>
</ul>
<p>Weights are the primary mechanism by which neural networks learn. During training, the network gets feedback on its predictions in the form of a loss or error. To minimize this error, the model uses optimization techniques (like gradient descent) to adjust the weights. Over time, the model gradually makes more and more accurate predictions.</p>
<p>The main difference between <a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">regression (Private)</a> and a neural network is the impact of change on a single weight. </p>
<ul>
<li>In regression, you can change a weight without affecting the other inputs in a function. this isn’t the case with neural networks. Since the output of one layer is passed into the next layer of the network, a single change can have a cascading effect on the other neurons in the network.</li>
</ul>
<p>The neural network figures out the weights on its own</p>
<p>a.k.a representation, patterns, numbers, features</p>
<h3 id="bias">Bias<a aria-hidden="true" class="anchor-heading icon-link" href="#bias"></a></h3>
<p>The bias is a value that is applied to the weighted sum of all neurons from the previous layer, adjusting the importance of that neuron. </p>
<ul>
<li>In other words, the bias gives us some indication of whether or not that neuron tends to be active or inactive.</li>
<li>More specifically, bias shifts the activation function along the input axis, essentially determining the threshold at which the neuron begins to activate.</li>
</ul>
<p><img src="/assets/images/2023-08-10-21-26-46.png" style="max-width:200px;"></p>
<h3 id="layers">Layers<a aria-hidden="true" class="anchor-heading icon-link" href="#layers"></a></h3>
<p>Neural networks are composed of layers. Generally they are:</p>
<ul>
<li>input layer</li>
<li>hidden layers</li>
<li>output layer</li>
</ul>
<p><img src="/assets/images/2023-07-02-20-17-39.png"></p>
<p>Each layer is usually a combination of linear functions (ie. straight lines) and non-linear functions (ie. non-straight lines)</p>
<h3 id="how-a-neural-network-learns-simplified">How a neural network learns (simplified)<a aria-hidden="true" class="anchor-heading icon-link" href="#how-a-neural-network-learns-simplified"></a></h3>
<p>A neural network learns by:</p>
<ol>
<li>starting with random numbers</li>
<li>perform tensor operations</li>
<li>update random numbers to try and make them better representations of the data</li>
<li>repeat</li>
</ol>
<h2 id="types-of-neural-network">Types of Neural Network<a aria-hidden="true" class="anchor-heading icon-link" href="#types-of-neural-network"></a></h2>
<p></p><p></p><div class="portal-container">
<div class="portal-head">
<div class="portal-backlink">
<div class="portal-title">From <span class="portal-text-title">Types</span></div>
<a href="/notes/hgm9om14ascftd0kof1fbzb" class="portal-arrow">Go to text <span class="right-arrow">→</span></a>
</div>
</div>
<div id="portal-parent-anchor" class="portal-parent" markdown="1">
<div class="portal-parent-fader-top"></div>
<div class="portal-parent-fader-bottom"></div><p>Data is passed from one layer to the next in what's called a <strong>Feedforward neural network (FNN)</strong> </p>
<ul>
<li>so-called because connections between the nodes do not form a cycle.</li>
</ul>
<p>Other types of Neural Network:</p>
<ul>
<li><strong>Convolutional Neural Networks (CNNs)</strong> are commonly used as the model architecture in image classification tasks</li>
<li><strong>Recurrent Neural Networks (RNNs)</strong> are commonly used in natural language processing tasks</li>
<li><strong>Transformer</strong> are commonly used in natural language and speech</li>
<li><strong>Fully Connected Neural Networks (FCNN)</strong></li>
</ul>
</div></div><p></p><p></p>
<hr>
<h3 id="example-number-recognition">Example: number recognition<a aria-hidden="true" class="anchor-heading icon-link" href="#example-number-recognition"></a></h3>
<p>Imagine we have a model that takes an image (28x28 pixels) of a handwritten number and tells us what number it is. </p>
<ul>
<li><em>First layer</em> - Our neural network will start with 784 neurons (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mi>x</mi><mn>28</mn></mrow><annotation encoding="application/x-tex">28x28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">28</span><span class="mord mathnormal">x</span><span class="mord">28</span></span></span></span></span>), and each value between 0 and 1 will represent its grayscale level (from black to white). These 784 neurons make up the first layer of our neural network.</li>
<li><em>Last layer</em> - Our last layer will be composed of 10 neurons, each representing a digit from 0-9. </li>
<li><em>Hidden layers</em> - Though hidden layers are a black box, it helps to think of them in terms of how they <em>might</em> behave. Generally when solving problems with computers, it's helpful to break problems down into smaller problems, then use the smaller building block solutions to amount to a bigger solution. In this case, it's difficult to recognize the number <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn></mrow><annotation encoding="application/x-tex">9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">9</span></span></span></span></span>. However, it's easier to recognize a shape with a loop and a tail (the top and bottom part of the number, respectively). There are however, different numbers that have loops (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn></mrow><annotation encoding="application/x-tex">6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">6</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">8</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></span>) and different numbers that have tails (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn></mrow><annotation encoding="application/x-tex">7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">7</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span>). So we might think, "What if each node in our second last layer represented a different component shape? Then the activation functions of the <em>loop node</em> and the <em>tail node</em> would output a number close to 1, which would be enough information to tell us that we have the number <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn></mrow><annotation encoding="application/x-tex">9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">9</span></span></span></span></span>".
<ul>
<li>of course, then the question becomes "how do we recognize shapes like loops and tails?". We can continue to break down these shapes to reveal straight-ish edges. That is, we can think of the upper loop of the number <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn></mrow><annotation encoding="application/x-tex">9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">9</span></span></span></span></span> as being made up of ~5 straight-ish edges. When our neural network receives the handwritten <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn></mrow><annotation encoding="application/x-tex">9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">9</span></span></span></span></span>, it can break it down first into edges, then determine which shapes those edges make, and then finally determine which number it is based on which shapes it has found.</li>
<li><a href="https://youtu.be/aircAruvnKk?t=443">video demonstration</a></li>
</ul>
</li>
</ul>
<p>The activation of these neurons (often simply a number between 0-1) represents how much the model thinks that the input image is that particular number</p>
<ul>
<li>ex. if our input drawing was of a 9, then the final neuron in the last layer will have the highest activation value</li>
</ul>
<p>In this example, the <em>weights</em> of each node activation tell us the pixel pattern that that node is picking up on, while the <em>bias</em> tells us how high the weighted sum needs to be before we consider the neuron to be meaningfully active</p>
<h2 id="ue-resources">UE Resources<a aria-hidden="true" class="anchor-heading icon-link" href="#ue-resources"></a></h2>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning Book</a>
<ul>
<li>has a lot of code examples and gives a good fundamental overview. Recommended by 3Blue1Brown</li>
</ul>
</li>
</ul>
<h2 id="e-resources">E Resources<a aria-hidden="true" class="anchor-heading icon-link" href="#e-resources"></a></h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=aircAruvnKk&#x26;t=707s">But what is a neural network?</a>
<ul>
<li>explains the fundamentals of a neural network, including layers, neurons, biases, weights etc.</li>
</ul>
</li>
</ul>
<hr>
<strong>Children</strong>
<ol>
<li><a href="/notes/v3rh8rqc5aorupfno1w4otw">Functions</a></li>
<li><a href="/notes/hgm9om14ascftd0kof1fbzb">Types</a></li>
</ol>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/i2it5id8qwtg27n4usg8bo1">Pytorch</a></li>
<li><a href="/notes/r0w4lqir8cq5ez9inlimel8">Deep Learning</a></li>
<li><a href="/notes/r9fy49qm440b41yo8sqfwz5">Terminology</a></li>
<li><a href="/notes/v3rh8rqc5aorupfno1w4otw">Functions</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#overview" title="Overview">Overview</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#node-aka-neuron" title="Node (a.k.a Neuron)">Node (a.k.a Neuron)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#weight" title="Weight">Weight</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#bias" title="Bias">Bias</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#layers" title="Layers">Layers</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#how-a-neural-network-learns-simplified" title="How a neural network learns (simplified)">How a neural network learns (simplified)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#types-of-neural-network" title="Types of Neural Network">Types of Neural Network</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#example-number-recognition" title="Example: number recognition">Example: number recognition</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#ue-resources" title="UE Resources">UE Resources</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#e-resources" title="E Resources">E Resources</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"k4ib1hhlkmzcrfx8vljz7sg","title":"Neural Network","desc":"","updated":1692806939461,"created":1688841508824,"custom":{},"fname":"ml.deep-learning.nn","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"2799e534d23d458d7edee23b3fb9ad45","links":[{"type":"wiki","from":{"fname":"ml.deep-learning.nn","id":"k4ib1hhlkmzcrfx8vljz7sg","vaultName":"tech"},"value":"statistics.regression","alias":"linear regression","position":{"start":{"line":17,"column":31,"offset":898},"end":{"line":17,"column":74,"offset":941},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"statistics.regression"}},{"type":"wiki","from":{"fname":"ml.deep-learning.nn","id":"k4ib1hhlkmzcrfx8vljz7sg","vaultName":"tech"},"value":"statistics.regression","alias":"regression","position":{"start":{"line":35,"column":29,"offset":2714},"end":{"line":35,"column":65,"offset":2750},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"statistics.regression"}},{"type":"ref","from":{"fname":"ml.deep-learning.nn","id":"k4ib1hhlkmzcrfx8vljz7sg","vaultName":"tech"},"value":"ml.deep-learning.nn.types","position":{"start":{"line":67,"column":1,"offset":4239},"end":{"line":67,"column":31,"offset":4269},"indent":[]},"xvault":false,"to":{"fname":"ml.deep-learning.nn.types"}},{"from":{"fname":"pytorch","id":"i2it5id8qwtg27n4usg8bo1","vaultName":"tech"},"type":"backlink","position":{"start":{"line":25,"column":33,"offset":804},"end":{"line":25,"column":101,"offset":872},"indent":[]},"value":"ml.deep-learning.nn"},{"from":{"fname":"ml.deep-learning","id":"r0w4lqir8cq5ez9inlimel8","vaultName":"tech"},"type":"backlink","position":{"start":{"line":4,"column":3,"offset":150},"end":{"line":4,"column":42,"offset":189},"indent":[]},"value":"ml.deep-learning.nn"},{"from":{"fname":"ml.terminology","id":"r9fy49qm440b41yo8sqfwz5","vaultName":"tech"},"type":"backlink","position":{"start":{"line":36,"column":19,"offset":1850},"end":{"line":36,"column":58,"offset":1889},"indent":[]},"value":"ml.deep-learning.nn"},{"from":{"fname":"ml.deep-learning.nn.functions","id":"v3rh8rqc5aorupfno1w4otw","vaultName":"tech"},"type":"backlink","position":{"start":{"line":91,"column":9,"offset":5995},"end":{"line":91,"column":47,"offset":6033},"indent":[]},"value":"ml.deep-learning.nn"}],"anchors":{"overview":{"type":"header","text":"Overview","value":"overview","line":8,"column":0,"depth":2},"node-aka-neuron":{"type":"header","text":"Node (a.k.a Neuron)","value":"node-aka-neuron","line":20,"column":0,"depth":3},"weight":{"type":"header","text":"Weight","value":"weight","line":31,"column":0,"depth":3},"bias":{"type":"header","text":"Bias","value":"bias","line":48,"column":0,"depth":3},"layers":{"type":"header","text":"Layers","value":"layers","line":55,"column":0,"depth":3},"how-a-neural-network-learns-simplified":{"type":"header","text":"How a neural network learns (simplified)","value":"how-a-neural-network-learns-simplified","line":65,"column":0,"depth":3},"types-of-neural-network":{"type":"header","text":"Types of Neural Network","value":"types-of-neural-network","line":72,"column":0,"depth":2},"example-number-recognition":{"type":"header","text":"Example: number recognition","value":"example-number-recognition","line":77,"column":0,"depth":3},"ue-resources":{"type":"header","text":"UE Resources","value":"ue-resources","line":90,"column":0,"depth":2},"e-resources":{"type":"header","text":"E Resources","value":"e-resources","line":94,"column":0,"depth":2}},"children":["v3rh8rqc5aorupfno1w4otw","hgm9om14ascftd0kof1fbzb"],"parent":"r0w4lqir8cq5ez9inlimel8","data":{}},"body":"\u003ch1 id=\"neural-network\"\u003eNeural Network\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#neural-network\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#overview\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eNeural networks are preferred when you have unstructured data (ie. data that is not neatly formatted in tables)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethat's not to say that they can't work well with structured data. It's more the case that traditional machine learning algorithms like decision trees or SVMs often handle structured data well.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAt a basic level, a neural network is comprised of four main components: \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003einputs\n\u003cul\u003e\n\u003cli\u003eex. your inputs may have a binary value of 0 or 1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eweights - Larger weights make a single input’s contribution to the output more significant\u003c/li\u003e\n\u003cli\u003ea bias or threshold - the output value of any node must be above the threshold for data to be sent to the next layer of the network\n\u003cul\u003e\n\u003cli\u003ea threshold value of 5 would translate to a bias value of –5.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ean output (\u003ccode\u003ey-hat\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"node-aka-neuron\"\u003eNode (a.k.a Neuron)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#node-aka-neuron\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eEach node corresponds to a neuron in a biological neural network\u003c/p\u003e\n\u003cp\u003eThink of each node as its own \u003ca title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\"\u003elinear regression (Private)\u003c/a\u003e model.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esince linear regression can be used to predict future events\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThink of a neuron as a function that takes in the outputs of all nodes from the previous layer and returns a number between 0 and 1.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethis number is called the \u003cem\u003eactivation\u003c/em\u003e, and it is the output of the \u003cem\u003eactivation function\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003ethe activations in one layer determine the activations in the subsequent layer.\u003c/li\u003e\n\u003cli\u003enote: While many activation functions produce values between 0 and 1 (like the sigmoid function), not all do. For instance, the ReLU (Rectified Linear Unit) function produces values between 0 and infinity, and the tanh function outputs values between -1 and 1.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"weight\"\u003eWeight\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#weight\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eEach connection between 2 nodes is assigned a weight, which indicates the strength of that connection, and how much influence the input has on the node.\u003c/p\u003e\n\u003cp\u003eEach node (the current node) receives input from \u003cem\u003eevery\u003c/em\u003e node of the previous layer. To each of these inputs we apply a weight that affects how much of a contribution that particular node of the previous layer has on the current node\u003c/p\u003e\n\u003cp\u003eWeight is a fundamental concept to neural networks because inputs naturally have a different magnitude of effort on the outcome\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. in a model that predicts housing prices, the recency of a paint job and the number of bedrooms are both inputs to the model that affect the price, but the latter has a much bigger impact on the ultimate price of the house.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWeights are the primary mechanism by which neural networks learn. During training, the network gets feedback on its predictions in the form of a loss or error. To minimize this error, the model uses optimization techniques (like gradient descent) to adjust the weights. Over time, the model gradually makes more and more accurate predictions.\u003c/p\u003e\n\u003cp\u003eThe main difference between \u003ca title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\"\u003eregression (Private)\u003c/a\u003e and a neural network is the impact of change on a single weight. \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIn regression, you can change a weight without affecting the other inputs in a function. this isn’t the case with neural networks. Since the output of one layer is passed into the next layer of the network, a single change can have a cascading effect on the other neurons in the network.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe neural network figures out the weights on its own\u003c/p\u003e\n\u003cp\u003ea.k.a representation, patterns, numbers, features\u003c/p\u003e\n\u003ch3 id=\"bias\"\u003eBias\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#bias\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe bias is a value that is applied to the weighted sum of all neurons from the previous layer, adjusting the importance of that neuron. \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIn other words, the bias gives us some indication of whether or not that neuron tends to be active or inactive.\u003c/li\u003e\n\u003cli\u003eMore specifically, bias shifts the activation function along the input axis, essentially determining the threshold at which the neuron begins to activate.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2023-08-10-21-26-46.png\" style=\"max-width:200px;\"\u003e\u003c/p\u003e\n\u003ch3 id=\"layers\"\u003eLayers\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#layers\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eNeural networks are composed of layers. Generally they are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003einput layer\u003c/li\u003e\n\u003cli\u003ehidden layers\u003c/li\u003e\n\u003cli\u003eoutput layer\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2023-07-02-20-17-39.png\"\u003e\u003c/p\u003e\n\u003cp\u003eEach layer is usually a combination of linear functions (ie. straight lines) and non-linear functions (ie. non-straight lines)\u003c/p\u003e\n\u003ch3 id=\"how-a-neural-network-learns-simplified\"\u003eHow a neural network learns (simplified)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#how-a-neural-network-learns-simplified\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA neural network learns by:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003estarting with random numbers\u003c/li\u003e\n\u003cli\u003eperform tensor operations\u003c/li\u003e\n\u003cli\u003eupdate random numbers to try and make them better representations of the data\u003c/li\u003e\n\u003cli\u003erepeat\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"types-of-neural-network\"\u003eTypes of Neural Network\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#types-of-neural-network\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"portal-container\"\u003e\n\u003cdiv class=\"portal-head\"\u003e\n\u003cdiv class=\"portal-backlink\"\u003e\n\u003cdiv class=\"portal-title\"\u003eFrom \u003cspan class=\"portal-text-title\"\u003eTypes\u003c/span\u003e\u003c/div\u003e\n\u003ca href=\"/notes/hgm9om14ascftd0kof1fbzb\" class=\"portal-arrow\"\u003eGo to text \u003cspan class=\"right-arrow\"\u003e→\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\"\u003e\n\u003cdiv class=\"portal-parent-fader-top\"\u003e\u003c/div\u003e\n\u003cdiv class=\"portal-parent-fader-bottom\"\u003e\u003c/div\u003e\u003cp\u003eData is passed from one layer to the next in what's called a \u003cstrong\u003eFeedforward neural network (FNN)\u003c/strong\u003e \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eso-called because connections between the nodes do not form a cycle.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOther types of Neural Network:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConvolutional Neural Networks (CNNs)\u003c/strong\u003e are commonly used as the model architecture in image classification tasks\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRecurrent Neural Networks (RNNs)\u003c/strong\u003e are commonly used in natural language processing tasks\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTransformer\u003c/strong\u003e are commonly used in natural language and speech\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFully Connected Neural Networks (FCNN)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-number-recognition\"\u003eExample: number recognition\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#example-number-recognition\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eImagine we have a model that takes an image (28x28 pixels) of a handwritten number and tells us what number it is. \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eFirst layer\u003c/em\u003e - Our neural network will start with 784 neurons (\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e28\u003c/mn\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmn\u003e28\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e28x28\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e28\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"mord\"\u003e28\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e), and each value between 0 and 1 will represent its grayscale level (from black to white). These 784 neurons make up the first layer of our neural network.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLast layer\u003c/em\u003e - Our last layer will be composed of 10 neurons, each representing a digit from 0-9. \u003c/li\u003e\n\u003cli\u003e\u003cem\u003eHidden layers\u003c/em\u003e - Though hidden layers are a black box, it helps to think of them in terms of how they \u003cem\u003emight\u003c/em\u003e behave. Generally when solving problems with computers, it's helpful to break problems down into smaller problems, then use the smaller building block solutions to amount to a bigger solution. In this case, it's difficult to recognize the number \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e9\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e9\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e9\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e. However, it's easier to recognize a shape with a loop and a tail (the top and bottom part of the number, respectively). There are however, different numbers that have loops (\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e6\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e6\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e6\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e, \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e8\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e8\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e8\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e, \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e0\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e0\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e) and different numbers that have tails (\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e7\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e7\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e7\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e, \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e). So we might think, \"What if each node in our second last layer represented a different component shape? Then the activation functions of the \u003cem\u003eloop node\u003c/em\u003e and the \u003cem\u003etail node\u003c/em\u003e would output a number close to 1, which would be enough information to tell us that we have the number \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e9\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e9\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e9\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\".\n\u003cul\u003e\n\u003cli\u003eof course, then the question becomes \"how do we recognize shapes like loops and tails?\". We can continue to break down these shapes to reveal straight-ish edges. That is, we can think of the upper loop of the number \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e9\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e9\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e9\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e as being made up of ~5 straight-ish edges. When our neural network receives the handwritten \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e9\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e9\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e9\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e, it can break it down first into edges, then determine which shapes those edges make, and then finally determine which number it is based on which shapes it has found.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/aircAruvnKk?t=443\"\u003evideo demonstration\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe activation of these neurons (often simply a number between 0-1) represents how much the model thinks that the input image is that particular number\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. if our input drawing was of a 9, then the final neuron in the last layer will have the highest activation value\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this example, the \u003cem\u003eweights\u003c/em\u003e of each node activation tell us the pixel pattern that that node is picking up on, while the \u003cem\u003ebias\u003c/em\u003e tells us how high the weighted sum needs to be before we consider the neuron to be meaningfully active\u003c/p\u003e\n\u003ch2 id=\"ue-resources\"\u003eUE Resources\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#ue-resources\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://neuralnetworksanddeeplearning.com/\"\u003eNeural Networks and Deep Learning Book\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003ehas a lot of code examples and gives a good fundamental overview. Recommended by 3Blue1Brown\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"e-resources\"\u003eE Resources\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#e-resources\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=aircAruvnKk\u0026#x26;t=707s\"\u003eBut what is a neural network?\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eexplains the fundamentals of a neural network, including layers, neurons, biases, weights etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cstrong\u003eChildren\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"/notes/v3rh8rqc5aorupfno1w4otw\"\u003eFunctions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/hgm9om14ascftd0kof1fbzb\"\u003eTypes\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/i2it5id8qwtg27n4usg8bo1\"\u003ePytorch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/r0w4lqir8cq5ez9inlimel8\"\u003eDeep Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/r9fy49qm440b41yo8sqfwz5\"\u003eTerminology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/v3rh8rqc5aorupfno1w4otw\"\u003eFunctions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"olZIVfSs2uLLr3BppFh4K","title":"Digital Garden","desc":"","updated":1674517603573,"created":1615482407722,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"effb007003ca6a91d7fd0c293e1d2436","links":[{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"testing.method.unit","alias":"unit testing","position":{"start":{"line":18,"column":121,"offset":1146},"end":{"line":18,"column":157,"offset":1182},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"testing.method.unit"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"general.arch.microservice","alias":"microservices","position":{"start":{"line":18,"column":188,"offset":1213},"end":{"line":18,"column":231,"offset":1256},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"general.arch.microservice"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"paradigm.oop","alias":"OOP","position":{"start":{"line":36,"column":227,"offset":2718},"end":{"line":36,"column":247,"offset":2738},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"paradigm.oop"}}],"anchors":{"tags":{"type":"header","text":"Tags","value":"tags","line":46,"column":0,"depth":3},"resources":{"type":"header","text":"Resources","value":"resources","line":55,"column":0,"depth":2},"ue-unexamined-resources":{"type":"header","text":"UE (Unexamined) Resources","value":"ue-unexamined-resources","line":56,"column":0,"depth":3},"e-examined-resources":{"type":"header","text":"E (Examined) Resources","value":"e-examined-resources","line":59,"column":0,"depth":3},"resources-1":{"type":"header","text":"Resources","value":"resources-1","line":62,"column":0,"depth":3}},"children":["40ubf88tephzbdjte8cdsa0","zFMjbn3xihVNHjUIdZCD1","IK6NOKemuDjhfstJBovKL","ZaTr28eWk1DrXOEsc1YVb","Vi0WYVLZunVM9iR8XZJn3","ngAbg7gluvbt1bY1UIRsi","RCPPXSakm5TvKka8kOmVp","LIcuGYV0DDt1VWbvH6Sed","MPx8ykTP57I40WTZvTP7f","ZU5WmdTG1bHoE8RcmZXZG","jqWiyYJff92RjXuUQt9PQ","KihijM8OQvZ4pASkkhqzL","F9vyqvls3OBCujtukqKhy","k6jxm2b3edgkhbordpuz6v1","facc2b01-755a-409f-99f6-57bef2d1501f","bZumdyapJ2H0wWWOmJ45i","elqpgoe2r951si4xrhujppj","m5ov42Vm6mR7RQWTvl1NW","cw8cerc0cos8lh871oz8rtf","PZxxZ4iESzSlbbHJKxuAZ","UDu62Pa6BaRqlG8uGoMhy","u3zcndycwqessho5h6x0nz9","ZgCUp366YrF2Tyky2NT73","DVpVUmavSoVDA7UIlPzLX","nMCtMXVvjBsJk3iw1zoIO","ANfx9Z4a6ZA0uJuSHtbIJ","BkG557LKUYbH1DnxprEVT","1TaH8zDTM98FZ9SYaeXLM","02v7ymc144e5c4pv3edkud7","dc79799f-a55d-48ab-a8be-c48adb1b19c0","f6feef57-b8f5-451f-a09a-7b63f6a07183","4sz47Y0LKs1Si73rWtyyh","g7ulqi8no93ezeocbesc3ll","5a9fb1df-478e-4687-9be0-5cb97e61ec57","1374e9e9-1cbc-4e1f-b1ca-66b8569533dd","D8Z3rjXkSj2EOymXQXF4Z","np3c1ykvnjqv5xoombpfwqz","f529cc34-aba0-45ca-ad7e-02ddda318941","0zcauha3il2NqtxZazIo7","9bbd6f68-03b2-41f4-92e4-2ca313e8b450","5a2ab598-fa7e-4471-8bda-9f5831b679ae","uV6w4mZPoohWyZV4Xaad0","QHXEIyeZGIGMVi5Q52UWI","RgE0mZLaUjPftFPZsiAoe","mytCOts26Pidush65tdRW","fwUzxfLSPMH1eL8oBoLWx","wazdsda6h25x66edvfmeuiv","TbW7PM9bg1y5TGkiWwQ8b","xiSIDeEtIc8X0lpUQlppI","0jxgntiLNHWFuCbzqtFGF","GkdMprLUe4QQULBxmGN6V","4SYc6v5hxY5g6Ip6kjpwO","czi7ilt2i1uoqm1f2otbntj","fqVQpS9FBiXgKsZX3R3sJ","L5JUZlGAGvTxrsEBB7DY8","ttyri4pwfyn5lcx0vp9804o","TeTedoeS2LdHPR632eCpM","ulicRRwo3lSFzh3tMfWH9","p7d24vyb8m00ombzn34t50c","e65188a1-177e-4ab2-99f9-75f85d537621","fajYnbVhCRDi74xn0H30R","bArGdGwqo3iFkyKMyE7qR","4Dcp7gbEVoLLgfu7bXFai","5d700782-fb81-416e-83f9-5dd88260e350","zubgzhNFE6KlTgXcjTz6O","8lpdfWa0cbSq1XJQbcYcY","2psluywdc416t7vrql0m058","17I8ZksXvqCH1mRtZDjHp","kF916Ow84qpJJeMRkWMIo","tMLkLcrIHHBz56xVmBLkP","CSePBQ6q7qhowKESqVwt7","laZ4OfLhZNK1Kuy6GaWUr","z5IJblOknQhMzZ5QZh4ye","N36FHxfxzwJfxDY3miWyX","3bb25f58-2b50-4fa3-af55-48ea9f88a081","7x07qjgbitozdfszqzuy7ix","u93Rz4fEWGu6VBR30Zraf","9hjMHnKvYT4jLKvuDSXaV","qC5GxCZBmNb4Ip6c0kU8x","LgW7mTIALODoXc54B3p6S","UYPxfHBFWX7fb6hHU5bB6","74lYtC8NKpCzcyFOZTfR1","sEoBNAEuaTxwSmTXDonZt","dj0jr9mpvs62e2pkg3zc3yy","z8ie0xjogb6ht7gzowav5xr","qxSOd6SPN8qf9ZUojVFDX","WQoMTf6VXBaxCgksXAVsj","2Mw4XgfyCHXHNOX5yoCIY","qcdt2f7jo51muquo5r95dpo","0XJqmcdtcMZu66glBI5O8","lK8r8BXS4ThiUTe4xKIZe","93de42ca-53ea-460a-baa7-b9ec5c47cb1e","Rxs2jaGpdFzqYtP7lAJFJ","aO8W81Z0PyIb6Hs7nOHPW","lhzisalkebu4w5n01np07i0","7b5l4b6fi65n7sv9org5q1l","I01hENHnh8Tqu3Ok8sLzG","03o3n0hz9v9jtb7j889zpd4","evqsPNutOaZ8hcBCqxFQu","zhhxcjZUHdU8uRLwGb9Zh","G1aFACZB2ooWGMGwyd3ZW","YWy1C4tgoaCcw1m8JJsr7","Szj3o5iaNxPpesiCqwrbu","613syb18hb3v0u1ydvor7ru","XhvCDW3fIw6h6MhY5ticq","Q70g7SusFZBQXzkuQifv4","AzfWDH3wp7jFpL2EYxBcW","y0fwpZ9qMqirsLiFyOciU","ZF8xj8wwDUqKlrwTrCFZ1","tAJvhqhdfyZZa87QHq1TU","4hRmipi8lxpBLyzWu5JVB","iTg0C7QjvnmqBZeEigJNs","3babc3d2-79ae-470a-9c06-ab8bba2e684e","bF3UsMFya3fMeXWDspVov","ULkfbL9WpktbVYnzhl6Jw","PpNOO8JYWe6dM8wruSa7x","di40pCxDn7IiqE8lFdD46","rmW0mkerqV35I8QPji6lM","3IFIK1ByzeIxZCByryGLN","iImkYAKfkw3beAl6pLbDn","ecDe8DNWrkeQTwpTEvHje","i2it5id8qwtg27n4usg8bo1","Yqhdd9mSJGN7OJOeyoSD2","Ws5tah8tpeyn9tK8VBTg8","gWAg15uBJgkS2B0wcpMAa","l7V3v2ep1YdDCt7DOr7Ci","yM2PJBdqJnHpD63cPA6sW","qn0bre7eLbi3QMbCfWkUi","fSu0KxFL41IRouotqmbHs","0gtg24Mj1a1bQFPRGQNlO","7iQPBMltLPLbFEz2qbjPu","yoh4pwoXcfELInGKRdYf6","dCGCWXgAmiOZXbdULT1m6","jMavlje07sNa6hSEIE8WA","Xxm0JE4dKHxrQAaZfzvxD","nRb6Im4Kcmc2ZWE7K1jZ1","PAEBZCyFBZJyR7OoMZ41E","PQ6km8RgRCuyICBPOYz8f","x5tm1nfjzyawwzedy3yitgd","2bhftt8rGuxYu4pFgNqru","hjYIZpHQWuXfeEoGeJEKW","K2M9bQqVq2eQfm29eslKL","X8obW1iKwYsvNgKWGyCzy","yJwSC7hqYIezTFHf5i0Ev","c4Z7ETcOHUILRMH32Sfjw","qiR6dIu857b9M9kTqjOyK","fc2coz74cnfy5czzofx4h5x","Ku1OgHMhELajzo61Gx7ye","LUrfhDWo8wuwZu7CN9TV8","osu6JGOnvXJ5gt3tpqWZY","1a6173cd-cf13-4b34-a522-8350bf9a364f","S2sBltrPfd8a7ICuD7CuH","GLQ2pmkJUNUa93THBDVsD","md6xitz4exia2joa06i490b","oWCuBXOg6JWfZzjmKxmNl","jOmhZ8ovLYTPbpM1vqSDx","p9bov84s0isgkl1ysaw93kk","FraC6xzLy1ei91l1ICyc9","6ceBas2RE9Q4787GDngH7","734cd78d-0bc9-426b-803d-1efc84dfffe5","k4Bb09px6r0FxIRs49SXV","oaG3H1S9IUBO644nGZigu","Ka7agQJkUMRSWN0uFdkWK","si3z090WsiLasMhJBa1Az","hs6rwzt4mogiicoc4gcykbi","ljKAVERmdEiKLK9hXGKBm","zxt3lhonfdhglvijd17ua8c","dd7dopve1dudqkoibkqvti4","923tgifqf59ovv5yldtyi0a","vrjwp01goqw47fqctm4f4lo","c99gdmmppju3r1tth8cb2jx","z2pvn5qxdz84zgygqzxage8"],"parent":null,"data":{},"body":"This Dendron vault of tech knowledge is organized according to domains and their sub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git would include things like `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner`, and so on.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the \"aha\" moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nAs a result, I have allowed myself to use potentially arcane concepts to explain other ones. For example, in my note on [[unit testing|testing.method.unit]], I have made reference to the [[microservices|general.arch.microservice]] note. If these notes were made with the public in mind, this would be a very bad strategy, given that you'd have to understand microservices to be able to draw that same parallel that I've already drawn. Since these notes are written for myself, I have been fine with taking these liberties.\n\nWhat I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course this is all\nvague sounding, and really depends on the domain along with the ends I am\ntrying to reach.\n\nTo achieve this, the system should be steadfast to:\n- be able to put information in relatively easily, without too much thought\n\trequired to its location. While location is important, Dendron makes it easy\n\tto relocate notes, if it becomes apparent that a different place makes more\n\tsense.\n- be able to extract the information that is needed, meaning there is a\n\thigh-degree in confidence in the location of the information. The idea is\n\tthat information loses a large amount of its value when it is unfindable.\n\tTherefore, a relatively strict ideology should be used when determining\n\twhere a piece of information belongs.\n\t- Some concepts might realistically belong to multiple domains. For instance, the concept of *access modifiers* can be found in both `C#` and `Typescript`. Therefore, this note should be abstracted to a common place, such as [[OOP|paradigm.oop]].\n\nThis Dendron vault is the sister component to the [General Second Brain](https://tech.kyletycholiz.com).\n\n### Tags\nThroughout the garden, I have made use of tags, which give semantic meaning to the pieces of information.\n\n- `ex.` - Denotes an *example* of the preceding piece of information\n- `spec:` - Specifies that the preceding information has some degree of *speculation* to it, and may not be 100% factual. Ideally this gets clarified over time as my understanding develops.\n- `anal:` - Denotes an *analogy* of the preceding information. Often I will attempt to link concepts to others that I have previously learned.\n- `mn:` - Denotes a *mnemonic*\n- `expl:` - Denotes an *explanation*\n\n## Resources\n### UE (Unexamined) Resources\nOften, I come across sources of information that I believe to be high-quality. They may be recommendations or found in some other way. No matter their origin, I may be in a position where I don't have the time to fully examine them (and properly extract notes), or I may not require the information at that moment in time. In cases like these, I will add reference to a section of the note called **UE Resources**. The idea is that in the future when I am ready to examine them, I have a list of resources that I can start with. This is an alternative strategy to compiling browser bookmarks, which I've found can quickly become untenable.\n\n### E (Examined) Resources\nOnce a resource has been thoroughly examined and has been mined for notes, it will be moved from *UE Resources* to *E Resources*. This is to indicate that (in my own estimation), there is nothing more to be gained from the resource that is not already in the note.\n\n### Resources\nThis heading is for inexhaustible resources. \n- A prime example would be a quality website that continually posts articles.  - Another example would be a tool, such as software that measures frequencies in a room to help acoustically treat it.\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.83.0","vaults":[{"fsPath":"../main/tech","name":"tech"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"The Tech Digital Garden of Kyle Tycholiz"},"github":{"cname":"tech.kyletycholiz.com","enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableMermaid":true,"siteUrl":"https://tech.kyletycholiz.com","duplicateNoteBehavior":{"action":"useVault","payload":["tech"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"k4ib1hhlkmzcrfx8vljz7sg"},"buildId":"fo6gubzKP_KV9RZPhaoiQ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>