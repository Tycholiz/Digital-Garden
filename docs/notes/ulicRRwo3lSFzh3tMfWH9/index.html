<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>Apache Flink</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="The Tech Digital Garden of Kyle Tycholiz"/><meta property="og:title" content="Apache Flink"/><meta property="og:description" content="The Tech Digital Garden of Kyle Tycholiz"/><meta property="og:url" content="https://tech.kyletycholiz.com/notes/ulicRRwo3lSFzh3tMfWH9/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2/22/2022"/><meta property="article:modified_time" content="1/23/2023"/><link rel="canonical" href="https://tech.kyletycholiz.com/notes/ulicRRwo3lSFzh3tMfWH9/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6b338472289fe290.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/fo6gubzKP_KV9RZPhaoiQ/_buildManifest.js" defer=""></script><script src="/_next/static/fo6gubzKP_KV9RZPhaoiQ/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="apache-flink">Apache Flink<a aria-hidden="true" class="anchor-heading icon-link" href="#apache-flink"></a></h1>
<h2 id="what-is-it">What is it?<a aria-hidden="true" class="anchor-heading icon-link" href="#what-is-it"></a></h2>
<p>Apache Flink is a stream processing framework that is scalable and fault-tolerant.</p>
<ul>
<li>it is based on the idea that it should not be hard to express simple computations (e.g. calculating an average and then grouping by a certain attribute) while still be able to scale indefinitely, and in a fault-tolerant manner.</li>
<li>spec: Flink is <a href="/notes/2k8qfv0q9ce2cai8hf5313m">MapReduce</a> like <a href="/notes/YxwqHEq2TsfL7ShAfW2Rw">Apache Hadoop</a>, but with streaming data.
<ul>
<li>might not be true.</li>
</ul>
</li>
</ul>
<p>Flink is a dataflow engine.</p>
<ul>
<li>Like MapReduce, they work by repeatedly calling a user-defined function to process one record at a time on a single thread. They parallelize work by partitioning inputs, and they copy the output of one function over the network to become the input to another function.</li>
<li>Unlike in MapReduce, these functions need not take the strict roles of alternating map and reduce, but instead can be assembled in more flexible ways (these functions are called operators)</li>
</ul>
<p>Flink's operations can be stateful</p>
<ul>
<li>therefore, the processing of one event can depend on the accumulated effect of all the events that came before it.</li>
</ul>
<p>The dataflows in Flink applications form directed graphs that start with one or more sources (e.g. <a href="/notes/WQoMTf6VXBaxCgksXAVsj">Kafka</a>, <a href="/notes/mP0bWcHx4tIFklY9Ge8vh">Kinesis</a>), and end in one or more sinks (e.g. Cassandra, <a href="/notes/gEztUcJYazBs8J8k0gi7o">DynamoDB</a>, <a href="/notes/xiSIDeEtIc8X0lpUQlppI">Elastic Search</a>).</p>
<ul>
<li>the reason these sources are eligible to be data sources to Flink is because they support low-latency, high throughput parallel reads in combination with rewind and replay – the prerequisites for high performance and fault tolerance.</li>
</ul>
<p>Flink can enrich data in its streams by using REST APIs and/or making databases queries.</p>
<p>Flink can be written in Java, Python, Scala and SQL</p>
<h3 id="stream-execution-environment">Stream Execution Environment<a aria-hidden="true" class="anchor-heading icon-link" href="#stream-execution-environment"></a></h3>
<p>Every Flink application needs an execution environment. Streaming applications need to use a <code>StreamExecutionEnvironment</code>.</p>
<p>ex.</p>
<pre class="language-java"><code class="language-java"><span class="token keyword">final</span> <span class="token class-name">StreamExecutionEnvironment</span> env <span class="token operator">=</span>
  <span class="token class-name">StreamExecutionEnvironment</span><span class="token punctuation">.</span><span class="token function">getExecutionEnvironment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>Calls to the DataStream API made in your application will build a job graph that is attached to the <code>StreamExecutionEnvironment</code></p>
<ul>
<li>When <code>env.execute()</code> is called, this graph is packaged up and sent to the <code>JobManager</code>, which parallelizes the job and distributes slices of it to the <code>TaskManagers</code> for execution. Each parallel slice of your job will be executed in a task slot.</li>
<li><code>JobManager</code> determines which TaskManager should run the job
<ul>
<li>therefore <code>env.execute()</code> must be called in order for our program to run</li>
</ul>
</li>
</ul>
<p><img src="/assets/images/2023-01-06-21-05-34.png"></p>
<h3 id="apis-of-flink">APIs of Flink<a aria-hidden="true" class="anchor-heading icon-link" href="#apis-of-flink"></a></h3>
<p>Flink has 2 core APIs (DataStream and Dataset), and other high-level APIs built on top of them:</p>
<ol>
<li><em>DataStream API</em> for bounded or unbounded streams of data </li>
<li><em>DataSet API</em> for bounded data sets.</li>
<li><em>Table API</em>, which is a SQL-like DSL (ie. expression language) for relational stream and batch processing that can be easily embedded in Flink's DataStream and DataSet APIs.</li>
<li><em>Async I/O API</em>, which allows users to use asynchronous request clients with data streams.</li>
</ol>
<h4 id="datastream-api">DataStream API<a aria-hidden="true" class="anchor-heading icon-link" href="#datastream-api"></a></h4>
<p>enables transformations (e.g. filters, aggregations, <a href="/notes/3XI9apb2EuUcxKWcVvna4">window functions</a>) on bounded or unbounded streams of data.</p>
<p>available in Java and Scala</p>
<h4 id="dataset-api">DataSet API<a aria-hidden="true" class="anchor-heading icon-link" href="#dataset-api"></a></h4>
<p>enables transformations (e.g., filters, mapping, joining, grouping) on bounded datasets.</p>
<p>As of Flink 1.12 the DataSet API has been soft deprecated. In its place, the Table API and SQL should be used.</p>
<ul>
<li>alternatively, use the DataStream API with BATCH execution mode.</li>
</ul>
<p>available in Java, Scala and Python (experimental)</p>
<h4 id="table-api">Table API<a aria-hidden="true" class="anchor-heading icon-link" href="#table-api"></a></h4>
<p>A SQL-like expression language for relational stream and batch processing that can be embedded in Flink's Java and Scala DataSet and DataStream APIs</p>
<ul>
<li>therefore, a relational Table abstraction is used. Thus, we can do things like <a href="/notes/PTD1zh7cfaEyqc4x5v0Qj">select</a>, aggregate, and <a href="/notes/74d0c3a8-73ff-4964-bc57-7ceb6f8b8cb5">join</a></li>
<li>Tables have a schema attached</li>
</ul>
<p>Table API uses DataStream and DataSet APIs under the hood.</p>
<h4 id="async-io-api">Async I/O API<a aria-hidden="true" class="anchor-heading icon-link" href="#async-io-api"></a></h4>
<p>Async I/O API is a high-level API that handles the integration with DataStreams, well as handling order, event time, fault tolerance, retry support, etc.</p>
<p>If we were to synchronously access a database within our Flink app, that database interaction would result in massive latency costs.</p>
<ul>
<li>therefore, whenever we make database calls in our Flink app, we must use a database client that supports async requests.
<ul>
<li>in the unlikely event that the database client doesn't support async requests, we can turn a synchronous client into a limited concurrent client by creating multiple clients and handling the synchronous calls with a thread pool. Naturally, this approach is usually less efficient than a proper asynchronous client.</li>
</ul>
</li>
</ul>
<p>(In addition to the async db client) three parts are needed to implement a stream transformation with asynchronous I/O against the database:</p>
<ul>
<li>An implementation of AsyncFunction that dispatches the requests</li>
<li>A callback that takes the result of the operation and hands it to the ResultFuture (similar to a <a href="/notes/FVbSefFiCJPpdtkRRsogA">Promise</a>)</li>
<li>Applying the async I/O operation on a DataStream as a transformation with or without retry</li>
</ul>
<h2 id="how-does-it-work">How does it work?<a aria-hidden="true" class="anchor-heading icon-link" href="#how-does-it-work"></a></h2>
<h3 id="parallelism">Parallelism<a aria-hidden="true" class="anchor-heading icon-link" href="#parallelism"></a></h3>
<p>Programs in Flink are inherently parallel and distributed across a cluster.</p>
<ul>
<li>Flink will basically partition the incoming stream of data and then perform some computation on that subset of data.</li>
<li>Flink allows us to specify how much parallelism you want for each of these subtasks. To scale up is to simply increase the parallelism of the bottleneck subtask.</li>
<li>During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks (each of which is independent from one another, and executes in different threads)</li>
<li>The number of operator subtasks is the parallelism of that particular operator.</li>
<li>The set of parallel instances of a stateful operator is effectively a sharded key-value store.
<ul>
<li>Each parallel instance is responsible for handling events for a specific group of keys, and the state for those keys is kept locally.</li>
</ul>
</li>
</ul>
<p>The diagram below shows a job. Notice:</p>
<ul>
<li>The job runs with a parallelism of two across the first three operators in the job graph, terminating in a sink that has a parallelism of one. </li>
<li>The third operator is stateful.</li>
<li>A fully-connected network shuffle is occurring between the second and third operators. 
<ul>
<li>This is being done to partition the stream by some key, so that all of the events that need to be processed together, will be.</li>
</ul>
</li>
</ul>
<p><img src="/assets/images/2023-01-06-20-44-57.png"></p>
<p>In production, your application will run in a remote cluster or set of containers.</p>
<h3 id="state">State<a aria-hidden="true" class="anchor-heading icon-link" href="#state"></a></h3>
<p>Flink offers some compelling features for the state it manages:</p>
<ul>
<li>local: Flink state is kept local to the machine that processes it, and can be accessed at memory speed, helping the application achieve high throughput and low-latency. </li>
<li>durable: Flink state is fault-tolerant (ie. it is automatically checkpointed at regular intervals, and is restored upon failure)</li>
<li>vertically scalable: Flink state can be kept in embedded RocksDB instances that scale by adding more local disk</li>
<li>horizontally scalable: Flink state is redistributed as your cluster grows and shrinks</li>
<li>queryable: Flink state can be queried externally via the Queryable State API.</li>
</ul>
<p>You can choose to keep state on the JVM heap, or if it is too large, in efficiently organized on-disk data structures.</p>
<p><img src="/assets/images/2023-01-06-20-48-22.png"></p>
<h4 id="state-snapshots">State Snapshots<a aria-hidden="true" class="anchor-heading icon-link" href="#state-snapshots"></a></h4>
<p>Flink is able to provide fault-tolerance with <em>state snapshots</em> and <em>stream replay</em></p>
<p><em>State Snapshots</em> capture the entire state of the distributed pipeline, recording offsets into the input queues as well as the state throughout the job graph that has resulted from having ingested the data up to that point.</p>
<p>When a failure occurs, the sources are rewound, the state is restored, and processing is resumed. As depicted above, these state snapshots are captured asynchronously, without impeding the ongoing processing.</p>
<h3 id="fault-tolerance-checkpoints-and-savepoints">Fault-Tolerance: checkpoints and savepoints<a aria-hidden="true" class="anchor-heading icon-link" href="#fault-tolerance-checkpoints-and-savepoints"></a></h3>
<p>Distributed <em>checkpoints</em> provide Flink with a lightweight fault-tolerance mechanism.</p>
<ul>
<li>A checkpoint is an automatic, asynchronous snapshot of the state of an application and the position in a source stream.</li>
</ul>
<p>In the case of a failure, a Flink program with checkpointing enabled will, upon recovery, resume processing from the last completed checkpoint, ensuring that Flink maintains exactly-once state semantics within an application.</p>
<p>The checkpointing mechanism exposes hooks for application code to include external systems into the checkpointing mechanism as well (like opening and committing transactions with a database system).</p>
<p>Flink also includes a mechanism called <em>savepoints</em>, which are manually-triggered checkpoints.</p>
<ul>
<li>A user can generate a savepoint, stop a running Flink program, then resume the program from the same application state and position in the stream.</li>
</ul>
<h2 id="time">Time<a aria-hidden="true" class="anchor-heading icon-link" href="#time"></a></h2>
<p>Flink explicitly supports three different notions of time:</p>
<ul>
<li>event time: the time when an event occurred, as recorded by the device producing (or storing) the event
<ul>
<li>use when needing to compute reproducible results (e.g. calculatemaximum price a stock reached in a day) so that the result won’t depend on when the calculation is performed.</li>
</ul>
</li>
<li>ingestion time: a timestamp recorded by Flink at the moment it ingests the event</li>
<li>processing time: the time when a specific operator in your pipeline is processing the event</li>
</ul>
<h3 id="event-time">Event Time<a aria-hidden="true" class="anchor-heading icon-link" href="#event-time"></a></h3>
<p>If you want to use event time, you will also need to supply a <em>Timestamp Extractor</em> and <em>Watermark Generator</em> that Flink will use to track the progress of event time.</p>
<h4 id="watermark">Watermark<a aria-hidden="true" class="anchor-heading icon-link" href="#watermark"></a></h4>
<p>Imagine we are building a stream sorter, which simply takes in a stream of events and sorts them by their chronological occurrence (by timestamp). An inherent problem here is that since streams are continuous, we will never really be 100% sure if we have the right order, since events with lower timestamps may technically arrive later on in the stream. At the same time, we can't wait forever. Watermarks solve this problem</p>
<p>Watermarks allow Flink to implement the policy that defines when, for any given timestamped event, to stop waiting for the arrival of earlier events.</p>
<ul>
<li><em>watermark generators</em> insert special timestamped elements into the stream (called watermarks). 
<ul>
<li>A watermark for time <code>t</code> is an assertion that the stream is (probably) now complete up through time <code>t</code>.</li>
<li>therefore, our stream sorter program will stop waiting for an event with timestamp prior to 2 once a watermark arrives with a timestamp of 2, or greater.</li>
</ul>
</li>
</ul>
<p>Watermarks give you control over the tradeoff between latency and completeness</p>
<ul>
<li>Unlike in batch processing, where one has the luxury of being able to have complete knowledge of the input before producing any results, with streaming you must eventually stop waiting to see more of the input, and produce some sort of result.</li>
</ul>
<h5 id="watermarking-strategies">Watermarking strategies<a aria-hidden="true" class="anchor-heading icon-link" href="#watermarking-strategies"></a></h5>
<p>"Bounded-out-of-orderness watermarking" - The most simple strategy for watermarking is to assume that there is a maximum delay that we will wait for any "chronologically prior events" to show up</p>
<ul>
<li>for most applications a fixed delay works well enough.</li>
</ul>
<hr>
<h3 id="example-stock-market-aggregator">Example: Stock market aggregator<a aria-hidden="true" class="anchor-heading icon-link" href="#example-stock-market-aggregator"></a></h3>
<p>Problem statement: Imagine we have a program that consumes a stream of stock market trades and we want to get data on how many trades happen by industry each minute</p>
<ul>
<li>this sort of problem has scalability problems built-in, since we can expect there to be a large amount of data, and we will have no forewarning of when trade volume unexpectedly increases.</li>
</ul>
<p>Implementation: Configure Flink to:</p>
<ul>
<li>partition the input stream based on the industry name of the stock</li>
<li>apply a moving average on a window of 1 minute.</li>
</ul>
<p>Consider that reading from the stream and applying an <code>AVG</code> calculation are 2 different tasks. Flink allows us to scale up each independently. Therefore, if we determine that trade volume has increased and there are more trades happening per minute, we simply scale up the number of readers. On the other hand, if the number of industries to group by has increased, we simply scale up the number of operators that calculate the <code>AVG</code>.</p>
<h2 id="e-resources">E Resources<a aria-hidden="true" class="anchor-heading icon-link" href="#e-resources"></a></h2>
<ul>
<li><a href="https://medium.com/archsaber/a-simple-introduction-to-apache-flink-2a603119041e">https://medium.com/archsaber/a-simple-introduction-to-apache-flink-2a603119041e</a></li>
</ul>
<p>2 types of clusters:</p>
<ul>
<li>session cluster</li>
<li>application cluster</li>
</ul>
<hr>
<strong>Children</strong>
<ol>
<li><a href="/notes/tn4l8xw8kesm4rzzig6dpap">Datastream</a></li>
</ol>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/i2wJebVFZ4oLX54iz19Hg">ETL Pipeline</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#what-is-it" title="What is it?">What is it?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#stream-execution-environment" title="Stream Execution Environment">Stream Execution Environment</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#apis-of-flink" title="APIs of Flink">APIs of Flink</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#datastream-api" title="DataStream API">DataStream API</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#dataset-api" title="DataSet API">DataSet API</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#table-api" title="Table API">Table API</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#async-io-api" title="Async I/O API">Async I/O API</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#how-does-it-work" title="How does it work?">How does it work?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#parallelism" title="Parallelism">Parallelism</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#state" title="State">State</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#state-snapshots" title="State Snapshots">State Snapshots</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#fault-tolerance-checkpoints-and-savepoints" title="Fault-Tolerance: checkpoints and savepoints">Fault-Tolerance: checkpoints and savepoints</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#time" title="Time">Time</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#event-time" title="Event Time">Event Time</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#watermark" title="Watermark">Watermark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#watermarking-strategies" title="Watermarking strategies">Watermarking strategies</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#example-stock-market-aggregator" title="Example: Stock market aggregator">Example: Stock market aggregator</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#e-resources" title="E Resources">E Resources</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"ulicRRwo3lSFzh3tMfWH9","title":"Apache Flink","desc":"","updated":1674511332687,"created":1645571616617,"custom":{},"fname":"flink","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"8b7d2214bdac9a7457a39276971e9a65","links":[{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"general.patterns.map-reduce","alias":"MapReduce","position":{"start":{"line":5,"column":18,"offset":346},"end":{"line":5,"column":59,"offset":387},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"general.patterns.map-reduce"}},{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"apache.hadoop","position":{"start":{"line":5,"column":65,"offset":393},"end":{"line":5,"column":82,"offset":410},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"apache.hadoop"}},{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"kafka","position":{"start":{"line":15,"column":100,"offset":1204},"end":{"line":15,"column":109,"offset":1213},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"kafka"}},{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"aws.svc.kinesis","position":{"start":{"line":15,"column":111,"offset":1215},"end":{"line":15,"column":130,"offset":1234},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"aws.svc.kinesis"}},{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"aws.svc.dynamo","alias":"DynamoDB","position":{"start":{"line":15,"column":179,"offset":1283},"end":{"line":15,"column":206,"offset":1310},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"aws.svc.dynamo"}},{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"elastic-search","position":{"start":{"line":15,"column":208,"offset":1312},"end":{"line":15,"column":226,"offset":1330},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"elastic-search"}},{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"pg.lang.func.window","alias":"window functions","position":{"start":{"line":46,"column":54,"offset":3106},"end":{"line":46,"column":94,"offset":3146},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"pg.lang.func.window"}},{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"sql.clause.select","alias":"select","position":{"start":{"line":60,"column":82,"offset":3802},"end":{"line":60,"column":110,"offset":3830},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"sql.clause.select"}},{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"sql.join","alias":"join","position":{"start":{"line":60,"column":127,"offset":3847},"end":{"line":60,"column":144,"offset":3864},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"sql.join"}},{"type":"wiki","from":{"fname":"flink","id":"ulicRRwo3lSFzh3tMfWH9","vaultName":"tech"},"value":"js.lang.promises","alias":"Promise","position":{"start":{"line":74,"column":100,"offset":5014},"end":{"line":74,"column":128,"offset":5042},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"js.lang.promises"}},{"from":{"fname":"db.strategies.etl","id":"i2wJebVFZ4oLX54iz19Hg","vaultName":"tech"},"type":"backlink","position":{"start":{"line":17,"column":3,"offset":820},"end":{"line":17,"column":25,"offset":842},"indent":[]},"value":"flink"}],"anchors":{"what-is-it":{"type":"header","text":"What is it?","value":"what-is-it","line":8,"column":0,"depth":2},"stream-execution-environment":{"type":"header","text":"Stream Execution Environment","value":"stream-execution-environment","line":28,"column":0,"depth":3},"apis-of-flink":{"type":"header","text":"APIs of Flink","value":"apis-of-flink","line":44,"column":0,"depth":3},"datastream-api":{"type":"header","text":"DataStream API","value":"datastream-api","line":51,"column":0,"depth":4},"dataset-api":{"type":"header","text":"DataSet API","value":"dataset-api","line":56,"column":0,"depth":4},"table-api":{"type":"header","text":"Table API","value":"table-api","line":64,"column":0,"depth":4},"async-io-api":{"type":"header","text":"Async I/O API","value":"async-io-api","line":71,"column":0,"depth":4},"how-does-it-work":{"type":"header","text":"How does it work?","value":"how-does-it-work","line":83,"column":0,"depth":2},"parallelism":{"type":"header","text":"Parallelism","value":"parallelism","line":84,"column":0,"depth":3},"state":{"type":"header","text":"State","value":"state","line":103,"column":0,"depth":3},"state-snapshots":{"type":"header","text":"State Snapshots","value":"state-snapshots","line":115,"column":0,"depth":4},"fault-tolerance-checkpoints-and-savepoints":{"type":"header","text":"Fault-Tolerance: checkpoints and savepoints","value":"fault-tolerance-checkpoints-and-savepoints","line":122,"column":0,"depth":3},"time":{"type":"header","text":"Time","value":"time","line":133,"column":0,"depth":2},"event-time":{"type":"header","text":"Event Time","value":"event-time","line":140,"column":0,"depth":3},"watermark":{"type":"header","text":"Watermark","value":"watermark","line":143,"column":0,"depth":4},"watermarking-strategies":{"type":"header","text":"Watermarking strategies","value":"watermarking-strategies","line":154,"column":0,"depth":5},"example-stock-market-aggregator":{"type":"header","text":"Example: Stock market aggregator","value":"example-stock-market-aggregator","line":162,"column":0,"depth":3},"e-resources":{"type":"header","text":"E Resources","value":"e-resources","line":172,"column":0,"depth":2}},"children":["tn4l8xw8kesm4rzzig6dpap"],"parent":"olZIVfSs2uLLr3BppFh4K","data":{}},"body":"\u003ch1 id=\"apache-flink\"\u003eApache Flink\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#apache-flink\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"what-is-it\"\u003eWhat is it?\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#what-is-it\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eApache Flink is a stream processing framework that is scalable and fault-tolerant.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit is based on the idea that it should not be hard to express simple computations (e.g. calculating an average and then grouping by a certain attribute) while still be able to scale indefinitely, and in a fault-tolerant manner.\u003c/li\u003e\n\u003cli\u003espec: Flink is \u003ca href=\"/notes/2k8qfv0q9ce2cai8hf5313m\"\u003eMapReduce\u003c/a\u003e like \u003ca href=\"/notes/YxwqHEq2TsfL7ShAfW2Rw\"\u003eApache Hadoop\u003c/a\u003e, but with streaming data.\n\u003cul\u003e\n\u003cli\u003emight not be true.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFlink is a dataflow engine.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLike MapReduce, they work by repeatedly calling a user-defined function to process one record at a time on a single thread. They parallelize work by partitioning inputs, and they copy the output of one function over the network to become the input to another function.\u003c/li\u003e\n\u003cli\u003eUnlike in MapReduce, these functions need not take the strict roles of alternating map and reduce, but instead can be assembled in more flexible ways (these functions are called operators)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFlink's operations can be stateful\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etherefore, the processing of one event can depend on the accumulated effect of all the events that came before it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe dataflows in Flink applications form directed graphs that start with one or more sources (e.g. \u003ca href=\"/notes/WQoMTf6VXBaxCgksXAVsj\"\u003eKafka\u003c/a\u003e, \u003ca href=\"/notes/mP0bWcHx4tIFklY9Ge8vh\"\u003eKinesis\u003c/a\u003e), and end in one or more sinks (e.g. Cassandra, \u003ca href=\"/notes/gEztUcJYazBs8J8k0gi7o\"\u003eDynamoDB\u003c/a\u003e, \u003ca href=\"/notes/xiSIDeEtIc8X0lpUQlppI\"\u003eElastic Search\u003c/a\u003e).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe reason these sources are eligible to be data sources to Flink is because they support low-latency, high throughput parallel reads in combination with rewind and replay – the prerequisites for high performance and fault tolerance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFlink can enrich data in its streams by using REST APIs and/or making databases queries.\u003c/p\u003e\n\u003cp\u003eFlink can be written in Java, Python, Scala and SQL\u003c/p\u003e\n\u003ch3 id=\"stream-execution-environment\"\u003eStream Execution Environment\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#stream-execution-environment\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eEvery Flink application needs an execution environment. Streaming applications need to use a \u003ccode\u003eStreamExecutionEnvironment\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eex.\u003c/p\u003e\n\u003cpre class=\"language-java\"\u003e\u003ccode class=\"language-java\"\u003e\u003cspan class=\"token keyword\"\u003efinal\u003c/span\u003e \u003cspan class=\"token class-name\"\u003eStreamExecutionEnvironment\u003c/span\u003e env \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\n  \u003cspan class=\"token class-name\"\u003eStreamExecutionEnvironment\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token function\"\u003egetExecutionEnvironment\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCalls to the DataStream API made in your application will build a job graph that is attached to the \u003ccode\u003eStreamExecutionEnvironment\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen \u003ccode\u003eenv.execute()\u003c/code\u003e is called, this graph is packaged up and sent to the \u003ccode\u003eJobManager\u003c/code\u003e, which parallelizes the job and distributes slices of it to the \u003ccode\u003eTaskManagers\u003c/code\u003e for execution. Each parallel slice of your job will be executed in a task slot.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eJobManager\u003c/code\u003e determines which TaskManager should run the job\n\u003cul\u003e\n\u003cli\u003etherefore \u003ccode\u003eenv.execute()\u003c/code\u003e must be called in order for our program to run\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2023-01-06-21-05-34.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"apis-of-flink\"\u003eAPIs of Flink\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#apis-of-flink\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFlink has 2 core APIs (DataStream and Dataset), and other high-level APIs built on top of them:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003eDataStream API\u003c/em\u003e for bounded or unbounded streams of data \u003c/li\u003e\n\u003cli\u003e\u003cem\u003eDataSet API\u003c/em\u003e for bounded data sets.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eTable API\u003c/em\u003e, which is a SQL-like DSL (ie. expression language) for relational stream and batch processing that can be easily embedded in Flink's DataStream and DataSet APIs.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eAsync I/O API\u003c/em\u003e, which allows users to use asynchronous request clients with data streams.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"datastream-api\"\u003eDataStream API\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#datastream-api\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eenables transformations (e.g. filters, aggregations, \u003ca href=\"/notes/3XI9apb2EuUcxKWcVvna4\"\u003ewindow functions\u003c/a\u003e) on bounded or unbounded streams of data.\u003c/p\u003e\n\u003cp\u003eavailable in Java and Scala\u003c/p\u003e\n\u003ch4 id=\"dataset-api\"\u003eDataSet API\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#dataset-api\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eenables transformations (e.g., filters, mapping, joining, grouping) on bounded datasets.\u003c/p\u003e\n\u003cp\u003eAs of Flink 1.12 the DataSet API has been soft deprecated. In its place, the Table API and SQL should be used.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ealternatively, use the DataStream API with BATCH execution mode.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eavailable in Java, Scala and Python (experimental)\u003c/p\u003e\n\u003ch4 id=\"table-api\"\u003eTable API\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#table-api\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eA SQL-like expression language for relational stream and batch processing that can be embedded in Flink's Java and Scala DataSet and DataStream APIs\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etherefore, a relational Table abstraction is used. Thus, we can do things like \u003ca href=\"/notes/PTD1zh7cfaEyqc4x5v0Qj\"\u003eselect\u003c/a\u003e, aggregate, and \u003ca href=\"/notes/74d0c3a8-73ff-4964-bc57-7ceb6f8b8cb5\"\u003ejoin\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTables have a schema attached\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTable API uses DataStream and DataSet APIs under the hood.\u003c/p\u003e\n\u003ch4 id=\"async-io-api\"\u003eAsync I/O API\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#async-io-api\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eAsync I/O API is a high-level API that handles the integration with DataStreams, well as handling order, event time, fault tolerance, retry support, etc.\u003c/p\u003e\n\u003cp\u003eIf we were to synchronously access a database within our Flink app, that database interaction would result in massive latency costs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etherefore, whenever we make database calls in our Flink app, we must use a database client that supports async requests.\n\u003cul\u003e\n\u003cli\u003ein the unlikely event that the database client doesn't support async requests, we can turn a synchronous client into a limited concurrent client by creating multiple clients and handling the synchronous calls with a thread pool. Naturally, this approach is usually less efficient than a proper asynchronous client.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(In addition to the async db client) three parts are needed to implement a stream transformation with asynchronous I/O against the database:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn implementation of AsyncFunction that dispatches the requests\u003c/li\u003e\n\u003cli\u003eA callback that takes the result of the operation and hands it to the ResultFuture (similar to a \u003ca href=\"/notes/FVbSefFiCJPpdtkRRsogA\"\u003ePromise\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eApplying the async I/O operation on a DataStream as a transformation with or without retry\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"how-does-it-work\"\u003eHow does it work?\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#how-does-it-work\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"parallelism\"\u003eParallelism\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#parallelism\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003ePrograms in Flink are inherently parallel and distributed across a cluster.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFlink will basically partition the incoming stream of data and then perform some computation on that subset of data.\u003c/li\u003e\n\u003cli\u003eFlink allows us to specify how much parallelism you want for each of these subtasks. To scale up is to simply increase the parallelism of the bottleneck subtask.\u003c/li\u003e\n\u003cli\u003eDuring execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks (each of which is independent from one another, and executes in different threads)\u003c/li\u003e\n\u003cli\u003eThe number of operator subtasks is the parallelism of that particular operator.\u003c/li\u003e\n\u003cli\u003eThe set of parallel instances of a stateful operator is effectively a sharded key-value store.\n\u003cul\u003e\n\u003cli\u003eEach parallel instance is responsible for handling events for a specific group of keys, and the state for those keys is kept locally.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe diagram below shows a job. Notice:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe job runs with a parallelism of two across the first three operators in the job graph, terminating in a sink that has a parallelism of one. \u003c/li\u003e\n\u003cli\u003eThe third operator is stateful.\u003c/li\u003e\n\u003cli\u003eA fully-connected network shuffle is occurring between the second and third operators. \n\u003cul\u003e\n\u003cli\u003eThis is being done to partition the stream by some key, so that all of the events that need to be processed together, will be.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2023-01-06-20-44-57.png\"\u003e\u003c/p\u003e\n\u003cp\u003eIn production, your application will run in a remote cluster or set of containers.\u003c/p\u003e\n\u003ch3 id=\"state\"\u003eState\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#state\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFlink offers some compelling features for the state it manages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elocal: Flink state is kept local to the machine that processes it, and can be accessed at memory speed, helping the application achieve high throughput and low-latency. \u003c/li\u003e\n\u003cli\u003edurable: Flink state is fault-tolerant (ie. it is automatically checkpointed at regular intervals, and is restored upon failure)\u003c/li\u003e\n\u003cli\u003evertically scalable: Flink state can be kept in embedded RocksDB instances that scale by adding more local disk\u003c/li\u003e\n\u003cli\u003ehorizontally scalable: Flink state is redistributed as your cluster grows and shrinks\u003c/li\u003e\n\u003cli\u003equeryable: Flink state can be queried externally via the Queryable State API.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can choose to keep state on the JVM heap, or if it is too large, in efficiently organized on-disk data structures.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2023-01-06-20-48-22.png\"\u003e\u003c/p\u003e\n\u003ch4 id=\"state-snapshots\"\u003eState Snapshots\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#state-snapshots\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eFlink is able to provide fault-tolerance with \u003cem\u003estate snapshots\u003c/em\u003e and \u003cem\u003estream replay\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eState Snapshots\u003c/em\u003e capture the entire state of the distributed pipeline, recording offsets into the input queues as well as the state throughout the job graph that has resulted from having ingested the data up to that point.\u003c/p\u003e\n\u003cp\u003eWhen a failure occurs, the sources are rewound, the state is restored, and processing is resumed. As depicted above, these state snapshots are captured asynchronously, without impeding the ongoing processing.\u003c/p\u003e\n\u003ch3 id=\"fault-tolerance-checkpoints-and-savepoints\"\u003eFault-Tolerance: checkpoints and savepoints\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#fault-tolerance-checkpoints-and-savepoints\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eDistributed \u003cem\u003echeckpoints\u003c/em\u003e provide Flink with a lightweight fault-tolerance mechanism.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA checkpoint is an automatic, asynchronous snapshot of the state of an application and the position in a source stream.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the case of a failure, a Flink program with checkpointing enabled will, upon recovery, resume processing from the last completed checkpoint, ensuring that Flink maintains exactly-once state semantics within an application.\u003c/p\u003e\n\u003cp\u003eThe checkpointing mechanism exposes hooks for application code to include external systems into the checkpointing mechanism as well (like opening and committing transactions with a database system).\u003c/p\u003e\n\u003cp\u003eFlink also includes a mechanism called \u003cem\u003esavepoints\u003c/em\u003e, which are manually-triggered checkpoints.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA user can generate a savepoint, stop a running Flink program, then resume the program from the same application state and position in the stream.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"time\"\u003eTime\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#time\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eFlink explicitly supports three different notions of time:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eevent time: the time when an event occurred, as recorded by the device producing (or storing) the event\n\u003cul\u003e\n\u003cli\u003euse when needing to compute reproducible results (e.g. calculatemaximum price a stock reached in a day) so that the result won’t depend on when the calculation is performed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eingestion time: a timestamp recorded by Flink at the moment it ingests the event\u003c/li\u003e\n\u003cli\u003eprocessing time: the time when a specific operator in your pipeline is processing the event\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"event-time\"\u003eEvent Time\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#event-time\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIf you want to use event time, you will also need to supply a \u003cem\u003eTimestamp Extractor\u003c/em\u003e and \u003cem\u003eWatermark Generator\u003c/em\u003e that Flink will use to track the progress of event time.\u003c/p\u003e\n\u003ch4 id=\"watermark\"\u003eWatermark\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#watermark\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eImagine we are building a stream sorter, which simply takes in a stream of events and sorts them by their chronological occurrence (by timestamp). An inherent problem here is that since streams are continuous, we will never really be 100% sure if we have the right order, since events with lower timestamps may technically arrive later on in the stream. At the same time, we can't wait forever. Watermarks solve this problem\u003c/p\u003e\n\u003cp\u003eWatermarks allow Flink to implement the policy that defines when, for any given timestamped event, to stop waiting for the arrival of earlier events.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003ewatermark generators\u003c/em\u003e insert special timestamped elements into the stream (called watermarks). \n\u003cul\u003e\n\u003cli\u003eA watermark for time \u003ccode\u003et\u003c/code\u003e is an assertion that the stream is (probably) now complete up through time \u003ccode\u003et\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003etherefore, our stream sorter program will stop waiting for an event with timestamp prior to 2 once a watermark arrives with a timestamp of 2, or greater.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWatermarks give you control over the tradeoff between latency and completeness\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUnlike in batch processing, where one has the luxury of being able to have complete knowledge of the input before producing any results, with streaming you must eventually stop waiting to see more of the input, and produce some sort of result.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"watermarking-strategies\"\u003eWatermarking strategies\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#watermarking-strategies\"\u003e\u003c/a\u003e\u003c/h5\u003e\n\u003cp\u003e\"Bounded-out-of-orderness watermarking\" - The most simple strategy for watermarking is to assume that there is a maximum delay that we will wait for any \"chronologically prior events\" to show up\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efor most applications a fixed delay works well enough.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"example-stock-market-aggregator\"\u003eExample: Stock market aggregator\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#example-stock-market-aggregator\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eProblem statement: Imagine we have a program that consumes a stream of stock market trades and we want to get data on how many trades happen by industry each minute\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethis sort of problem has scalability problems built-in, since we can expect there to be a large amount of data, and we will have no forewarning of when trade volume unexpectedly increases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eImplementation: Configure Flink to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epartition the input stream based on the industry name of the stock\u003c/li\u003e\n\u003cli\u003eapply a moving average on a window of 1 minute.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eConsider that reading from the stream and applying an \u003ccode\u003eAVG\u003c/code\u003e calculation are 2 different tasks. Flink allows us to scale up each independently. Therefore, if we determine that trade volume has increased and there are more trades happening per minute, we simply scale up the number of readers. On the other hand, if the number of industries to group by has increased, we simply scale up the number of operators that calculate the \u003ccode\u003eAVG\u003c/code\u003e.\u003c/p\u003e\n\u003ch2 id=\"e-resources\"\u003eE Resources\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#e-resources\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://medium.com/archsaber/a-simple-introduction-to-apache-flink-2a603119041e\"\u003ehttps://medium.com/archsaber/a-simple-introduction-to-apache-flink-2a603119041e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2 types of clusters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esession cluster\u003c/li\u003e\n\u003cli\u003eapplication cluster\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cstrong\u003eChildren\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"/notes/tn4l8xw8kesm4rzzig6dpap\"\u003eDatastream\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/i2wJebVFZ4oLX54iz19Hg\"\u003eETL Pipeline\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"olZIVfSs2uLLr3BppFh4K","title":"Digital Garden","desc":"","updated":1674517603573,"created":1615482407722,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"effb007003ca6a91d7fd0c293e1d2436","links":[{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"testing.method.unit","alias":"unit testing","position":{"start":{"line":18,"column":121,"offset":1146},"end":{"line":18,"column":157,"offset":1182},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"testing.method.unit"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"general.arch.microservice","alias":"microservices","position":{"start":{"line":18,"column":188,"offset":1213},"end":{"line":18,"column":231,"offset":1256},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"general.arch.microservice"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"paradigm.oop","alias":"OOP","position":{"start":{"line":36,"column":227,"offset":2718},"end":{"line":36,"column":247,"offset":2738},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"paradigm.oop"}}],"anchors":{"tags":{"type":"header","text":"Tags","value":"tags","line":46,"column":0,"depth":3},"resources":{"type":"header","text":"Resources","value":"resources","line":55,"column":0,"depth":2},"ue-unexamined-resources":{"type":"header","text":"UE (Unexamined) Resources","value":"ue-unexamined-resources","line":56,"column":0,"depth":3},"e-examined-resources":{"type":"header","text":"E (Examined) Resources","value":"e-examined-resources","line":59,"column":0,"depth":3},"resources-1":{"type":"header","text":"Resources","value":"resources-1","line":62,"column":0,"depth":3}},"children":["40ubf88tephzbdjte8cdsa0","zFMjbn3xihVNHjUIdZCD1","IK6NOKemuDjhfstJBovKL","ZaTr28eWk1DrXOEsc1YVb","Vi0WYVLZunVM9iR8XZJn3","ngAbg7gluvbt1bY1UIRsi","RCPPXSakm5TvKka8kOmVp","LIcuGYV0DDt1VWbvH6Sed","MPx8ykTP57I40WTZvTP7f","ZU5WmdTG1bHoE8RcmZXZG","jqWiyYJff92RjXuUQt9PQ","KihijM8OQvZ4pASkkhqzL","F9vyqvls3OBCujtukqKhy","k6jxm2b3edgkhbordpuz6v1","facc2b01-755a-409f-99f6-57bef2d1501f","bZumdyapJ2H0wWWOmJ45i","elqpgoe2r951si4xrhujppj","m5ov42Vm6mR7RQWTvl1NW","cw8cerc0cos8lh871oz8rtf","PZxxZ4iESzSlbbHJKxuAZ","UDu62Pa6BaRqlG8uGoMhy","u3zcndycwqessho5h6x0nz9","ZgCUp366YrF2Tyky2NT73","DVpVUmavSoVDA7UIlPzLX","nMCtMXVvjBsJk3iw1zoIO","ANfx9Z4a6ZA0uJuSHtbIJ","BkG557LKUYbH1DnxprEVT","1TaH8zDTM98FZ9SYaeXLM","02v7ymc144e5c4pv3edkud7","dc79799f-a55d-48ab-a8be-c48adb1b19c0","f6feef57-b8f5-451f-a09a-7b63f6a07183","4sz47Y0LKs1Si73rWtyyh","g7ulqi8no93ezeocbesc3ll","5a9fb1df-478e-4687-9be0-5cb97e61ec57","1374e9e9-1cbc-4e1f-b1ca-66b8569533dd","D8Z3rjXkSj2EOymXQXF4Z","np3c1ykvnjqv5xoombpfwqz","f529cc34-aba0-45ca-ad7e-02ddda318941","0zcauha3il2NqtxZazIo7","9bbd6f68-03b2-41f4-92e4-2ca313e8b450","5a2ab598-fa7e-4471-8bda-9f5831b679ae","uV6w4mZPoohWyZV4Xaad0","QHXEIyeZGIGMVi5Q52UWI","RgE0mZLaUjPftFPZsiAoe","mytCOts26Pidush65tdRW","fwUzxfLSPMH1eL8oBoLWx","wazdsda6h25x66edvfmeuiv","TbW7PM9bg1y5TGkiWwQ8b","xiSIDeEtIc8X0lpUQlppI","0jxgntiLNHWFuCbzqtFGF","GkdMprLUe4QQULBxmGN6V","4SYc6v5hxY5g6Ip6kjpwO","czi7ilt2i1uoqm1f2otbntj","fqVQpS9FBiXgKsZX3R3sJ","L5JUZlGAGvTxrsEBB7DY8","ttyri4pwfyn5lcx0vp9804o","TeTedoeS2LdHPR632eCpM","ulicRRwo3lSFzh3tMfWH9","p7d24vyb8m00ombzn34t50c","e65188a1-177e-4ab2-99f9-75f85d537621","fajYnbVhCRDi74xn0H30R","bArGdGwqo3iFkyKMyE7qR","4Dcp7gbEVoLLgfu7bXFai","5d700782-fb81-416e-83f9-5dd88260e350","zubgzhNFE6KlTgXcjTz6O","8lpdfWa0cbSq1XJQbcYcY","2psluywdc416t7vrql0m058","17I8ZksXvqCH1mRtZDjHp","kF916Ow84qpJJeMRkWMIo","tMLkLcrIHHBz56xVmBLkP","CSePBQ6q7qhowKESqVwt7","laZ4OfLhZNK1Kuy6GaWUr","z5IJblOknQhMzZ5QZh4ye","N36FHxfxzwJfxDY3miWyX","3bb25f58-2b50-4fa3-af55-48ea9f88a081","7x07qjgbitozdfszqzuy7ix","u93Rz4fEWGu6VBR30Zraf","9hjMHnKvYT4jLKvuDSXaV","qC5GxCZBmNb4Ip6c0kU8x","LgW7mTIALODoXc54B3p6S","UYPxfHBFWX7fb6hHU5bB6","74lYtC8NKpCzcyFOZTfR1","sEoBNAEuaTxwSmTXDonZt","dj0jr9mpvs62e2pkg3zc3yy","z8ie0xjogb6ht7gzowav5xr","qxSOd6SPN8qf9ZUojVFDX","WQoMTf6VXBaxCgksXAVsj","2Mw4XgfyCHXHNOX5yoCIY","qcdt2f7jo51muquo5r95dpo","0XJqmcdtcMZu66glBI5O8","lK8r8BXS4ThiUTe4xKIZe","93de42ca-53ea-460a-baa7-b9ec5c47cb1e","Rxs2jaGpdFzqYtP7lAJFJ","aO8W81Z0PyIb6Hs7nOHPW","lhzisalkebu4w5n01np07i0","7b5l4b6fi65n7sv9org5q1l","I01hENHnh8Tqu3Ok8sLzG","03o3n0hz9v9jtb7j889zpd4","evqsPNutOaZ8hcBCqxFQu","zhhxcjZUHdU8uRLwGb9Zh","G1aFACZB2ooWGMGwyd3ZW","YWy1C4tgoaCcw1m8JJsr7","Szj3o5iaNxPpesiCqwrbu","613syb18hb3v0u1ydvor7ru","XhvCDW3fIw6h6MhY5ticq","Q70g7SusFZBQXzkuQifv4","AzfWDH3wp7jFpL2EYxBcW","y0fwpZ9qMqirsLiFyOciU","ZF8xj8wwDUqKlrwTrCFZ1","tAJvhqhdfyZZa87QHq1TU","4hRmipi8lxpBLyzWu5JVB","iTg0C7QjvnmqBZeEigJNs","3babc3d2-79ae-470a-9c06-ab8bba2e684e","bF3UsMFya3fMeXWDspVov","ULkfbL9WpktbVYnzhl6Jw","PpNOO8JYWe6dM8wruSa7x","di40pCxDn7IiqE8lFdD46","rmW0mkerqV35I8QPji6lM","3IFIK1ByzeIxZCByryGLN","iImkYAKfkw3beAl6pLbDn","ecDe8DNWrkeQTwpTEvHje","i2it5id8qwtg27n4usg8bo1","Yqhdd9mSJGN7OJOeyoSD2","Ws5tah8tpeyn9tK8VBTg8","gWAg15uBJgkS2B0wcpMAa","l7V3v2ep1YdDCt7DOr7Ci","yM2PJBdqJnHpD63cPA6sW","qn0bre7eLbi3QMbCfWkUi","fSu0KxFL41IRouotqmbHs","0gtg24Mj1a1bQFPRGQNlO","7iQPBMltLPLbFEz2qbjPu","yoh4pwoXcfELInGKRdYf6","dCGCWXgAmiOZXbdULT1m6","jMavlje07sNa6hSEIE8WA","Xxm0JE4dKHxrQAaZfzvxD","nRb6Im4Kcmc2ZWE7K1jZ1","PAEBZCyFBZJyR7OoMZ41E","PQ6km8RgRCuyICBPOYz8f","x5tm1nfjzyawwzedy3yitgd","2bhftt8rGuxYu4pFgNqru","hjYIZpHQWuXfeEoGeJEKW","K2M9bQqVq2eQfm29eslKL","X8obW1iKwYsvNgKWGyCzy","yJwSC7hqYIezTFHf5i0Ev","c4Z7ETcOHUILRMH32Sfjw","qiR6dIu857b9M9kTqjOyK","fc2coz74cnfy5czzofx4h5x","Ku1OgHMhELajzo61Gx7ye","LUrfhDWo8wuwZu7CN9TV8","osu6JGOnvXJ5gt3tpqWZY","1a6173cd-cf13-4b34-a522-8350bf9a364f","S2sBltrPfd8a7ICuD7CuH","GLQ2pmkJUNUa93THBDVsD","md6xitz4exia2joa06i490b","oWCuBXOg6JWfZzjmKxmNl","jOmhZ8ovLYTPbpM1vqSDx","p9bov84s0isgkl1ysaw93kk","FraC6xzLy1ei91l1ICyc9","6ceBas2RE9Q4787GDngH7","734cd78d-0bc9-426b-803d-1efc84dfffe5","k4Bb09px6r0FxIRs49SXV","oaG3H1S9IUBO644nGZigu","Ka7agQJkUMRSWN0uFdkWK","si3z090WsiLasMhJBa1Az","hs6rwzt4mogiicoc4gcykbi","ljKAVERmdEiKLK9hXGKBm","zxt3lhonfdhglvijd17ua8c","dd7dopve1dudqkoibkqvti4","923tgifqf59ovv5yldtyi0a","vrjwp01goqw47fqctm4f4lo","c99gdmmppju3r1tth8cb2jx","z2pvn5qxdz84zgygqzxage8"],"parent":null,"data":{},"body":"This Dendron vault of tech knowledge is organized according to domains and their sub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git would include things like `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner`, and so on.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the \"aha\" moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nAs a result, I have allowed myself to use potentially arcane concepts to explain other ones. For example, in my note on [[unit testing|testing.method.unit]], I have made reference to the [[microservices|general.arch.microservice]] note. If these notes were made with the public in mind, this would be a very bad strategy, given that you'd have to understand microservices to be able to draw that same parallel that I've already drawn. Since these notes are written for myself, I have been fine with taking these liberties.\n\nWhat I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course this is all\nvague sounding, and really depends on the domain along with the ends I am\ntrying to reach.\n\nTo achieve this, the system should be steadfast to:\n- be able to put information in relatively easily, without too much thought\n\trequired to its location. While location is important, Dendron makes it easy\n\tto relocate notes, if it becomes apparent that a different place makes more\n\tsense.\n- be able to extract the information that is needed, meaning there is a\n\thigh-degree in confidence in the location of the information. The idea is\n\tthat information loses a large amount of its value when it is unfindable.\n\tTherefore, a relatively strict ideology should be used when determining\n\twhere a piece of information belongs.\n\t- Some concepts might realistically belong to multiple domains. For instance, the concept of *access modifiers* can be found in both `C#` and `Typescript`. Therefore, this note should be abstracted to a common place, such as [[OOP|paradigm.oop]].\n\nThis Dendron vault is the sister component to the [General Second Brain](https://tech.kyletycholiz.com).\n\n### Tags\nThroughout the garden, I have made use of tags, which give semantic meaning to the pieces of information.\n\n- `ex.` - Denotes an *example* of the preceding piece of information\n- `spec:` - Specifies that the preceding information has some degree of *speculation* to it, and may not be 100% factual. Ideally this gets clarified over time as my understanding develops.\n- `anal:` - Denotes an *analogy* of the preceding information. Often I will attempt to link concepts to others that I have previously learned.\n- `mn:` - Denotes a *mnemonic*\n- `expl:` - Denotes an *explanation*\n\n## Resources\n### UE (Unexamined) Resources\nOften, I come across sources of information that I believe to be high-quality. They may be recommendations or found in some other way. No matter their origin, I may be in a position where I don't have the time to fully examine them (and properly extract notes), or I may not require the information at that moment in time. In cases like these, I will add reference to a section of the note called **UE Resources**. The idea is that in the future when I am ready to examine them, I have a list of resources that I can start with. This is an alternative strategy to compiling browser bookmarks, which I've found can quickly become untenable.\n\n### E (Examined) Resources\nOnce a resource has been thoroughly examined and has been mined for notes, it will be moved from *UE Resources* to *E Resources*. This is to indicate that (in my own estimation), there is nothing more to be gained from the resource that is not already in the note.\n\n### Resources\nThis heading is for inexhaustible resources. \n- A prime example would be a quality website that continually posts articles.  - Another example would be a tool, such as software that measures frequencies in a room to help acoustically treat it.\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.83.0","vaults":[{"fsPath":"../main/tech","name":"tech"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"The Tech Digital Garden of Kyle Tycholiz"},"github":{"cname":"tech.kyletycholiz.com","enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableMermaid":true,"siteUrl":"https://tech.kyletycholiz.com","duplicateNoteBehavior":{"action":"useVault","payload":["tech"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"ulicRRwo3lSFzh3tMfWH9"},"buildId":"fo6gubzKP_KV9RZPhaoiQ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>