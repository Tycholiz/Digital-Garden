<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>Tensors</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="The Tech Digital Garden of Kyle Tycholiz"/><meta property="og:title" content="Tensors"/><meta property="og:description" content="The Tech Digital Garden of Kyle Tycholiz"/><meta property="og:url" content="https://tech.kyletycholiz.com/notes/8qm42jxtuar5qn15w7nphwy/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="7/10/2023"/><meta property="article:modified_time" content="8/23/2023"/><link rel="canonical" href="https://tech.kyletycholiz.com/notes/8qm42jxtuar5qn15w7nphwy/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6b338472289fe290.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/fo6gubzKP_KV9RZPhaoiQ/_buildManifest.js" defer=""></script><script src="/_next/static/fo6gubzKP_KV9RZPhaoiQ/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="tensors">Tensors<a aria-hidden="true" class="anchor-heading icon-link" href="#tensors"></a></h1>
<p>Tensors are the primary data structure used for representing and manipulating data in deep learning models.</p>
<ul>
<li>they are the numerical representation of data as it is used as an input to the model, and it is also the numerical representation that is output by the model (ie. representation outputs)
<ul>
<li>therefore, the inputs and outputs of a neural network are tensors</li>
</ul>
</li>
<li>ex. Imagine we have a machine learning model that predicts if an image is of a shirt or not. In this case, the tensors would typically hold the image data in the form of numerical values. The image data is represented as pixel intensities, which can be transformed into tensors.
<ul>
<li>if we consider what an image is, it's simply an assortment of pixels, where each pixel is some combination of RGB. If we had a pixel art of Mario which was 16x12, then we could create a tensor of shape <code>(16, 12, 3)</code>, where the first dimension represents the height, the second represents the width, and the third represents the color channel (ie. each pixel's degree of RGB). With this tensor, we could recreate the image.</li>
</ul>
</li>
</ul>
<p>Tensors are often created internally by Pytorch (e.g., the weights of a neural network), but we as developers sometimes create tensors manually, especially when importing data or during preprocessing.</p>
<p>Many neural networks perform their learning by starting off with tensors full of random numbers, and then adjusting those numbers to better represent the data (by taking the source data, such as an image, and encoding its data and replacing the random numbers within the tensor).</p>
<ul>
<li>this is done to help break the symmetry and prevent the model from getting stuck in a suboptimal solution.</li>
<li>it also helps with reinforcement learning algorithms in order to introduce exploration during the learning process. By adding random noise to the actions taken by an agent, it can explore different states and actions, enabling it to discover new and potentially better strategies.</li>
</ul>
<p><img src="/assets/images/2023-07-09-10-19-06.png"></p>
<p>Tensors are similar to an <code>ndarray</code> (N-dimensional array), due to certain characteristics:</p>
<ul>
<li>it is multidimensional
<ul>
<li>Tensors can have different dimensions, including 0-dimensional (scalar), 1-dimensional (vector), 2-dimensional (matrix), and higher-dimensional arrays.</li>
</ul>
</li>
<li>each element is of the same type</li>
<li>each sub-array is of the same (fixed) length</li>
</ul>
<p>The main difference between a tensor and an ndarray is that a tensor can run on GPUs and TPUs, in addition to CPUs</p>
<p>The variable for tensors that are 0 or 1 dimension (ie. scalars or vectors) is lowercase, while the variable for tensors that are more than 2 (ie. matrices and tensors) is UPPERCASE.</p>
<h2 id="tensor-attributes">Tensor attributes<a aria-hidden="true" class="anchor-heading icon-link" href="#tensor-attributes"></a></h2>
<p>A tensor on PyTorch has attributes:</p>
<h3 id="shape">Shape<a aria-hidden="true" class="anchor-heading icon-link" href="#shape"></a></h3>
<p>the size of the tensor; ie. the size of each layer of nested <code>[]</code> </p>
<ul>
<li>ex. a tensor of shape <code>[2, 3, 4]</code> would have 2 elements in its outermost brackets, each of which would have 3 elements (ie. 3 arrays), and each element of that array would have 4 elements:</li>
</ul>
<pre class="language-py"><code class="language-py"><span class="token punctuation">[</span>
    <span class="token punctuation">[</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span>
        <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">]</span>
</code></pre>
<ul>
<li><code>TENSOR.shape</code> (or alias <code>TENSOR.size()</code>)</li>
</ul>
<h3 id="data-type">Data Type<a aria-hidden="true" class="anchor-heading icon-link" href="#data-type"></a></h3>
<p>the type of data stored in the tensor</p>
<ul>
<li><code>TENSOR.dtype</code></li>
<li>default is <code>float32</code></li>
<li>convert type: <code>TENSOR.type(torch.float32)</code> OR  <code>TENSOR.to(torch.float32)</code></li>
</ul>
<h3 id="device">Device<a aria-hidden="true" class="anchor-heading icon-link" href="#device"></a></h3>
<p>the device in which the tensor is stored</p>
<ul>
<li><code>TENSOR.device</code></li>
<li>default is <code>cpu</code></li>
<li>moving tensor from CPU to GPU: <code>TENSOR.to('cuda:0')</code></li>
</ul>
<h3 id="gradient-requirement">Gradient Requirement<a aria-hidden="true" class="anchor-heading icon-link" href="#gradient-requirement"></a></h3>
<p>the state of whether or not gradients need to be computed for the tensor</p>
<pre><code>- `TENSOR.requires_grad`
</code></pre>
<ul>
<li>spec: gradient is the same as slope if we are considering a straight line in a graph</li>
</ul>
<hr>
<p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.</p>
<pre class="language-py"><code class="language-py">data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
x_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre>
<h2 id="tensor-manipulation">Tensor Manipulation<a aria-hidden="true" class="anchor-heading icon-link" href="#tensor-manipulation"></a></h2>
<h3 id="tensor-operations">Tensor Operations<a aria-hidden="true" class="anchor-heading icon-link" href="#tensor-operations"></a></h3>
<p>The neural network will combine these tensor operations in some way in order to find patterns in numbers of a dataset</p>
<p>All operations (except matrix multiplication) are element-wise, meaning the operator (×, ÷ etc.) is applied to each element in place</p>
<p>Operations include:</p>
<ul>
<li>addition</li>
<li>subtraction</li>
<li>multiplication
<ul>
<li><code>torch.mul(TENSOR, 10)</code></li>
</ul>
</li>
<li>division</li>
<li>matrix multiplication
<ul>
<li><code>torch.matmul(TENSOR1, TENSOR2)</code> or <code>torch.mm()</code> or <code>TENSOR1 @ TENSOR2</code></li>
<li>matrix multiplication is the most common type of operation found in neural networks</li>
<li>see <a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">Matrix multiplication (Private)</a> Dendron node</li>
</ul>
</li>
</ul>
<h4 id="transpose">Transpose<a aria-hidden="true" class="anchor-heading icon-link" href="#transpose"></a></h4>
<p>Transposing a matrix is simply swapping the row contents with its column contents.</p>
<pre class="language-py"><code class="language-py">tensor_A
<span class="token comment"># tensor([[ 7,  8,  9],</span>
<span class="token comment">#        [10, 11, 12]])</span>
tensor_A<span class="token punctuation">.</span>T
<span class="token comment"># tensor([[ 7, 10],</span>
<span class="token comment">#        [ 8, 11],</span>
<span class="token comment">#        [ 9, 12]])</span>
</code></pre>
<p>We do this since we cannot multiply two matrices unless # rows of MatrixA = # columns of MatrixB, and # columns of MatrixA = # rows of MatrixB (see <a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">rules of matrix multiplication (Private)</a>), we need to transpose one of our matrices if the shapes don't line up properly.</p>
<h3 id="tensor-aggregation-tensor-methods">Tensor Aggregation (tensor methods)<a aria-hidden="true" class="anchor-heading icon-link" href="#tensor-aggregation-tensor-methods"></a></h3>
<p>Finding the <code>.min()</code>, <code>.max()</code>, <code>.mean()</code>, <code>.sum()</code> etc. of certain tensor values.</p>
<h3 id="reshaping-stacking-squeezing-and-unsqueezing-tensors">Reshaping, Stacking, Squeezing and Unsqueezing Tensors<a aria-hidden="true" class="anchor-heading icon-link" href="#reshaping-stacking-squeezing-and-unsqueezing-tensors"></a></h3>
<p>Each of these methods allows us to manipulate our tensors in order to change their shape/dimension.</p>
<ul>
<li>
<p>we often use these to solve issues that might arise due to tensors with mismatched shapes. </p>
</li>
<li>
<p><strong>Reshaping</strong> - reshapes an input tensor to a defined shape</p>
<ul>
<li>we do this because in order for multiple tensors to work together, we need their shape to correspond to one another   </li>
<li>the new shape of the reshaped tensor must be compatible with the original tensor
<ul>
<li>ex. if we have a tensor <code>[1, 2, 3, 4, 5, 6]</code>, we cannot reshape it into a 2x2 tensor. We could however put it into a 2x3.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>View</strong> - return a view of an input tensor of certain shape, but keep the same memory as the original tensor</p>
<ul>
<li>therefore if we assign a new variable <code>Z</code> to the view of an existing tensor and then change <code>Z</code>, it will also change the original tensor</li>
</ul>
</li>
<li>
<p><strong>Stacking</strong> - combine multiple tensors, either on top of each other (vstack) or side-by-side (hstack)</p>
</li>
<li>
<p><strong>Squeeze</strong> - removes all <code>1</code> dimensions from a tensor</p>
<ul>
<li><code>1</code> dimension is a dimension (ie. an array) has a single element in it</li>
<li>ex. if we have a tensor of shape <code>[2, 1, 2, 1, 2]</code> and squeeze it, it will become <code>[2, 2, 2]</code></li>
</ul>
</li>
<li>
<p><strong>Unsqueeze</strong> - add a <code>1</code> dimension to a target tensor</p>
<ul>
<li>we must specify the <code>dim</code> argument, which specified which index to add </li>
</ul>
</li>
<li>
<p><strong>Permute</strong> - returns a view that rearranges (ie. swaps) the dimensions of a tensor in a specified way</p>
<ul>
<li>ex. if we have a tensor (<code>x</code>) of shape <code>(2, 3, 5)</code> and permute it as <code>(2, 0, 1)</code>, the new tensor will have shape <code>(5, 2, 3)</code></li>
</ul>
</li>
</ul>
<h3 id="indexing">Indexing<a aria-hidden="true" class="anchor-heading icon-link" href="#indexing"></a></h3>
<p>Sometimes you'll want to select specific data from tensors </p>
<ul>
<li>ex. only the first column or second row.</li>
</ul>
<p>Aside from how indexing normally works (e.g. using <code>[0][0]</code> to access first element of the first element), we can also use <code>:</code> to specify "all values in this dimension"</p>
<pre class="language-py"><code class="language-py">x <span class="token operator">=</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># Get all values of 0th dimension and the 0 index of 1st dimension</span>
x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token comment"># tensor([[1, 2, 3]])</span>

<span class="token comment"># Get all values of 0th &#x26; 1st dimensions but only index 1 of 2nd dimension</span>
x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment"># tensor([[2, 5, 8]])</span>

<span class="token comment"># Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension</span>
x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment"># tensor([5])</span>

<span class="token comment"># Get index 0 of 0th and 1st dimension and all values of 2nd dimension (same as x[0][0])</span>
x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># tensor([1, 2, 3])</span>

<span class="token comment"># Get the first 10 values of the 0 dimension</span>
x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span>

<span class="token comment"># Get all values after the 10th index</span>
x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span>
</code></pre>
<h2 id="resources">Resources<a aria-hidden="true" class="anchor-heading icon-link" href="#resources"></a></h2>
<p><a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">see tensors Dendron node for mathematical definition (Private)</a></p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#tensor-attributes" title="Tensor attributes">Tensor attributes</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#shape" title="Shape">Shape</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#data-type" title="Data Type">Data Type</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#device" title="Device">Device</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#gradient-requirement" title="Gradient Requirement">Gradient Requirement</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#tensor-manipulation" title="Tensor Manipulation">Tensor Manipulation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#tensor-operations" title="Tensor Operations">Tensor Operations</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#transpose" title="Transpose">Transpose</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#tensor-aggregation-tensor-methods" title="Tensor Aggregation (tensor methods)">Tensor Aggregation (tensor methods)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#reshaping-stacking-squeezing-and-unsqueezing-tensors" title="Reshaping, Stacking, Squeezing and Unsqueezing Tensors">Reshaping, Stacking, Squeezing and Unsqueezing Tensors</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#indexing" title="Indexing">Indexing</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#resources" title="Resources">Resources</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"8qm42jxtuar5qn15w7nphwy","title":"Tensors","desc":"","updated":1692807161077,"created":1689029383059,"custom":{},"fname":"pytorch.tensors","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"bb37f2a5388222d121de54bf748b1f5e","links":[{"type":"wiki","from":{"fname":"pytorch.tensors","id":"8qm42jxtuar5qn15w7nphwy","vaultName":"tech"},"value":"math.algebra.linear","alias":"Matrix multiplication","position":{"start":{"line":90,"column":11,"offset":4522},"end":{"line":90,"column":78,"offset":4589},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"math.algebra.linear","anchorHeader":"matrix-multiplication"}},{"type":"wiki","from":{"fname":"pytorch.tensors","id":"8qm42jxtuar5qn15w7nphwy","vaultName":"tech"},"value":"math.algebra.linear","alias":"rules of matrix multiplication","position":{"start":{"line":105,"column":149,"offset":4989},"end":{"line":105,"column":209,"offset":5049},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"math.algebra.linear","anchorHeader":"rules"}},{"type":"wiki","from":{"fname":"pytorch.tensors","id":"8qm42jxtuar5qn15w7nphwy","vaultName":"tech"},"value":"math.algebra.linear.tensors","alias":"see tensors Dendron node for mathematical definition","position":{"start":{"line":160,"column":1,"offset":7808},"end":{"line":160,"column":85,"offset":7892},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"math.algebra.linear.tensors"}}],"anchors":{"tensor-attributes":{"type":"header","text":"Tensor attributes","value":"tensor-attributes","line":32,"column":0,"depth":2},"shape":{"type":"header","text":"Shape","value":"shape","line":35,"column":0,"depth":3},"data-type":{"type":"header","text":"Data Type","value":"data-type","line":54,"column":0,"depth":3},"device":{"type":"header","text":"Device","value":"device","line":60,"column":0,"depth":3},"gradient-requirement":{"type":"header","text":"Gradient Requirement","value":"gradient-requirement","line":66,"column":0,"depth":3},"tensor-manipulation":{"type":"header","text":"Tensor Manipulation","value":"tensor-manipulation","line":80,"column":0,"depth":2},"tensor-operations":{"type":"header","text":"Tensor Operations","value":"tensor-operations","line":82,"column":0,"depth":3},"transpose":{"type":"header","text":"Transpose","value":"transpose","line":98,"column":0,"depth":4},"tensor-aggregation-tensor-methods":{"type":"header","text":"Tensor Aggregation (tensor methods)","value":"tensor-aggregation-tensor-methods","line":113,"column":0,"depth":3},"reshaping-stacking-squeezing-and-unsqueezing-tensors":{"type":"header","text":"Reshaping, Stacking, Squeezing and Unsqueezing Tensors","value":"reshaping-stacking-squeezing-and-unsqueezing-tensors","line":116,"column":0,"depth":3},"indexing":{"type":"header","text":"Indexing","value":"indexing","line":135,"column":0,"depth":3},"resources":{"type":"header","text":"Resources","value":"resources","line":165,"column":0,"depth":2}},"children":[],"parent":"i2it5id8qwtg27n4usg8bo1","data":{}},"body":"\u003ch1 id=\"tensors\"\u003eTensors\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensors\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eTensors are the primary data structure used for representing and manipulating data in deep learning models.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethey are the numerical representation of data as it is used as an input to the model, and it is also the numerical representation that is output by the model (ie. representation outputs)\n\u003cul\u003e\n\u003cli\u003etherefore, the inputs and outputs of a neural network are tensors\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eex. Imagine we have a machine learning model that predicts if an image is of a shirt or not. In this case, the tensors would typically hold the image data in the form of numerical values. The image data is represented as pixel intensities, which can be transformed into tensors.\n\u003cul\u003e\n\u003cli\u003eif we consider what an image is, it's simply an assortment of pixels, where each pixel is some combination of RGB. If we had a pixel art of Mario which was 16x12, then we could create a tensor of shape \u003ccode\u003e(16, 12, 3)\u003c/code\u003e, where the first dimension represents the height, the second represents the width, and the third represents the color channel (ie. each pixel's degree of RGB). With this tensor, we could recreate the image.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTensors are often created internally by Pytorch (e.g., the weights of a neural network), but we as developers sometimes create tensors manually, especially when importing data or during preprocessing.\u003c/p\u003e\n\u003cp\u003eMany neural networks perform their learning by starting off with tensors full of random numbers, and then adjusting those numbers to better represent the data (by taking the source data, such as an image, and encoding its data and replacing the random numbers within the tensor).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethis is done to help break the symmetry and prevent the model from getting stuck in a suboptimal solution.\u003c/li\u003e\n\u003cli\u003eit also helps with reinforcement learning algorithms in order to introduce exploration during the learning process. By adding random noise to the actions taken by an agent, it can explore different states and actions, enabling it to discover new and potentially better strategies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2023-07-09-10-19-06.png\"\u003e\u003c/p\u003e\n\u003cp\u003eTensors are similar to an \u003ccode\u003endarray\u003c/code\u003e (N-dimensional array), due to certain characteristics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit is multidimensional\n\u003cul\u003e\n\u003cli\u003eTensors can have different dimensions, including 0-dimensional (scalar), 1-dimensional (vector), 2-dimensional (matrix), and higher-dimensional arrays.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eeach element is of the same type\u003c/li\u003e\n\u003cli\u003eeach sub-array is of the same (fixed) length\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe main difference between a tensor and an ndarray is that a tensor can run on GPUs and TPUs, in addition to CPUs\u003c/p\u003e\n\u003cp\u003eThe variable for tensors that are 0 or 1 dimension (ie. scalars or vectors) is lowercase, while the variable for tensors that are more than 2 (ie. matrices and tensors) is UPPERCASE.\u003c/p\u003e\n\u003ch2 id=\"tensor-attributes\"\u003eTensor attributes\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensor-attributes\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eA tensor on PyTorch has attributes:\u003c/p\u003e\n\u003ch3 id=\"shape\"\u003eShape\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#shape\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003ethe size of the tensor; ie. the size of each layer of nested \u003ccode\u003e[]\u003c/code\u003e \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. a tensor of shape \u003ccode\u003e[2, 3, 4]\u003c/code\u003e would have 2 elements in its outermost brackets, each of which would have 3 elements (ie. 3 arrays), and each element of that array would have 4 elements:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre class=\"language-py\"\u003e\u003ccode class=\"language-py\"\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\n        \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e4\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e5\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e6\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e7\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e8\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e9\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e8\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e7\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e6\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\n        \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e9\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e8\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e7\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e6\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e5\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e4\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e4\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eTENSOR.shape\u003c/code\u003e (or alias \u003ccode\u003eTENSOR.size()\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"data-type\"\u003eData Type\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#data-type\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003ethe type of data stored in the tensor\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eTENSOR.dtype\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003edefault is \u003ccode\u003efloat32\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003econvert type: \u003ccode\u003eTENSOR.type(torch.float32)\u003c/code\u003e OR  \u003ccode\u003eTENSOR.to(torch.float32)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"device\"\u003eDevice\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#device\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003ethe device in which the tensor is stored\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eTENSOR.device\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003edefault is \u003ccode\u003ecpu\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003emoving tensor from CPU to GPU: \u003ccode\u003eTENSOR.to('cuda:0')\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"gradient-requirement\"\u003eGradient Requirement\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#gradient-requirement\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003ethe state of whether or not gradients need to be computed for the tensor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e- `TENSOR.requires_grad`\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003espec: gradient is the same as slope if we are considering a straight line in a graph\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003eTensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.\u003c/p\u003e\n\u003cpre class=\"language-py\"\u003e\u003ccode class=\"language-py\"\u003edata \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e4\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\nx_data \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e torch\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003etensor\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003edata\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"tensor-manipulation\"\u003eTensor Manipulation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensor-manipulation\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"tensor-operations\"\u003eTensor Operations\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensor-operations\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe neural network will combine these tensor operations in some way in order to find patterns in numbers of a dataset\u003c/p\u003e\n\u003cp\u003eAll operations (except matrix multiplication) are element-wise, meaning the operator (×, ÷ etc.) is applied to each element in place\u003c/p\u003e\n\u003cp\u003eOperations include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eaddition\u003c/li\u003e\n\u003cli\u003esubtraction\u003c/li\u003e\n\u003cli\u003emultiplication\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003etorch.mul(TENSOR, 10)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003edivision\u003c/li\u003e\n\u003cli\u003ematrix multiplication\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003etorch.matmul(TENSOR1, TENSOR2)\u003c/code\u003e or \u003ccode\u003etorch.mm()\u003c/code\u003e or \u003ccode\u003eTENSOR1 @ TENSOR2\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ematrix multiplication is the most common type of operation found in neural networks\u003c/li\u003e\n\u003cli\u003esee \u003ca title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\"\u003eMatrix multiplication (Private)\u003c/a\u003e Dendron node\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"transpose\"\u003eTranspose\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#transpose\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eTransposing a matrix is simply swapping the row contents with its column contents.\u003c/p\u003e\n\u003cpre class=\"language-py\"\u003e\u003ccode class=\"language-py\"\u003etensor_A\n\u003cspan class=\"token comment\"\u003e# tensor([[ 7,  8,  9],\u003c/span\u003e\n\u003cspan class=\"token comment\"\u003e#        [10, 11, 12]])\u003c/span\u003e\ntensor_A\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eT\n\u003cspan class=\"token comment\"\u003e# tensor([[ 7, 10],\u003c/span\u003e\n\u003cspan class=\"token comment\"\u003e#        [ 8, 11],\u003c/span\u003e\n\u003cspan class=\"token comment\"\u003e#        [ 9, 12]])\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe do this since we cannot multiply two matrices unless # rows of MatrixA = # columns of MatrixB, and # columns of MatrixA = # rows of MatrixB (see \u003ca title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\"\u003erules of matrix multiplication (Private)\u003c/a\u003e), we need to transpose one of our matrices if the shapes don't line up properly.\u003c/p\u003e\n\u003ch3 id=\"tensor-aggregation-tensor-methods\"\u003eTensor Aggregation (tensor methods)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensor-aggregation-tensor-methods\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFinding the \u003ccode\u003e.min()\u003c/code\u003e, \u003ccode\u003e.max()\u003c/code\u003e, \u003ccode\u003e.mean()\u003c/code\u003e, \u003ccode\u003e.sum()\u003c/code\u003e etc. of certain tensor values.\u003c/p\u003e\n\u003ch3 id=\"reshaping-stacking-squeezing-and-unsqueezing-tensors\"\u003eReshaping, Stacking, Squeezing and Unsqueezing Tensors\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#reshaping-stacking-squeezing-and-unsqueezing-tensors\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eEach of these methods allows us to manipulate our tensors in order to change their shape/dimension.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ewe often use these to solve issues that might arise due to tensors with mismatched shapes. \u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eReshaping\u003c/strong\u003e - reshapes an input tensor to a defined shape\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ewe do this because in order for multiple tensors to work together, we need their shape to correspond to one another   \u003c/li\u003e\n\u003cli\u003ethe new shape of the reshaped tensor must be compatible with the original tensor\n\u003cul\u003e\n\u003cli\u003eex. if we have a tensor \u003ccode\u003e[1, 2, 3, 4, 5, 6]\u003c/code\u003e, we cannot reshape it into a 2x2 tensor. We could however put it into a 2x3.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eView\u003c/strong\u003e - return a view of an input tensor of certain shape, but keep the same memory as the original tensor\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etherefore if we assign a new variable \u003ccode\u003eZ\u003c/code\u003e to the view of an existing tensor and then change \u003ccode\u003eZ\u003c/code\u003e, it will also change the original tensor\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eStacking\u003c/strong\u003e - combine multiple tensors, either on top of each other (vstack) or side-by-side (hstack)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSqueeze\u003c/strong\u003e - removes all \u003ccode\u003e1\u003c/code\u003e dimensions from a tensor\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e1\u003c/code\u003e dimension is a dimension (ie. an array) has a single element in it\u003c/li\u003e\n\u003cli\u003eex. if we have a tensor of shape \u003ccode\u003e[2, 1, 2, 1, 2]\u003c/code\u003e and squeeze it, it will become \u003ccode\u003e[2, 2, 2]\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUnsqueeze\u003c/strong\u003e - add a \u003ccode\u003e1\u003c/code\u003e dimension to a target tensor\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ewe must specify the \u003ccode\u003edim\u003c/code\u003e argument, which specified which index to add \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePermute\u003c/strong\u003e - returns a view that rearranges (ie. swaps) the dimensions of a tensor in a specified way\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. if we have a tensor (\u003ccode\u003ex\u003c/code\u003e) of shape \u003ccode\u003e(2, 3, 5)\u003c/code\u003e and permute it as \u003ccode\u003e(2, 0, 1)\u003c/code\u003e, the new tensor will have shape \u003ccode\u003e(5, 2, 3)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"indexing\"\u003eIndexing\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#indexing\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eSometimes you'll want to select specific data from tensors \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex. only the first column or second row.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAside from how indexing normally works (e.g. using \u003ccode\u003e[0][0]\u003c/code\u003e to access first element of the first element), we can also use \u003ccode\u003e:\u003c/code\u003e to specify \"all values in this dimension\"\u003c/p\u003e\n\u003cpre class=\"language-py\"\u003e\u003ccode class=\"language-py\"\u003ex \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e tensor\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e4\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e5\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e6\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e7\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e8\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e9\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Get all values of 0th dimension and the 0 index of 1st dimension\u003c/span\u003e\nx\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e \u003cspan class=\"token comment\"\u003e# tensor([[1, 2, 3]])\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Get all values of 0th \u0026#x26; 1st dimensions but only index 1 of 2nd dimension\u003c/span\u003e\nx\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e \u003cspan class=\"token comment\"\u003e# tensor([[2, 5, 8]])\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\u003c/span\u003e\nx\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e \u003cspan class=\"token comment\"\u003e# tensor([5])\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Get index 0 of 0th and 1st dimension and all values of 2nd dimension (same as x[0][0])\u003c/span\u003e\nx\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e \u003cspan class=\"token comment\"\u003e# tensor([1, 2, 3])\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Get the first 10 values of the 0 dimension\u003c/span\u003e\nx\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token number\"\u003e10\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Get all values after the 10th index\u003c/span\u003e\nx\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token number\"\u003e10\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"resources\"\u003eResources\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#resources\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\"\u003esee tensors Dendron node for mathematical definition (Private)\u003c/a\u003e\u003c/p\u003e","noteIndex":{"id":"olZIVfSs2uLLr3BppFh4K","title":"Digital Garden","desc":"","updated":1674517603573,"created":1615482407722,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"../main/tech","name":"tech"},"contentHash":"effb007003ca6a91d7fd0c293e1d2436","links":[{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"testing.method.unit","alias":"unit testing","position":{"start":{"line":18,"column":121,"offset":1146},"end":{"line":18,"column":157,"offset":1182},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"testing.method.unit"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"general.arch.microservice","alias":"microservices","position":{"start":{"line":18,"column":188,"offset":1213},"end":{"line":18,"column":231,"offset":1256},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"general.arch.microservice"}},{"type":"wiki","from":{"fname":"root","id":"olZIVfSs2uLLr3BppFh4K","vaultName":"tech"},"value":"paradigm.oop","alias":"OOP","position":{"start":{"line":36,"column":227,"offset":2718},"end":{"line":36,"column":247,"offset":2738},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"paradigm.oop"}}],"anchors":{"tags":{"type":"header","text":"Tags","value":"tags","line":46,"column":0,"depth":3},"resources":{"type":"header","text":"Resources","value":"resources","line":55,"column":0,"depth":2},"ue-unexamined-resources":{"type":"header","text":"UE (Unexamined) Resources","value":"ue-unexamined-resources","line":56,"column":0,"depth":3},"e-examined-resources":{"type":"header","text":"E (Examined) Resources","value":"e-examined-resources","line":59,"column":0,"depth":3},"resources-1":{"type":"header","text":"Resources","value":"resources-1","line":62,"column":0,"depth":3}},"children":["40ubf88tephzbdjte8cdsa0","zFMjbn3xihVNHjUIdZCD1","IK6NOKemuDjhfstJBovKL","ZaTr28eWk1DrXOEsc1YVb","Vi0WYVLZunVM9iR8XZJn3","ngAbg7gluvbt1bY1UIRsi","RCPPXSakm5TvKka8kOmVp","LIcuGYV0DDt1VWbvH6Sed","MPx8ykTP57I40WTZvTP7f","ZU5WmdTG1bHoE8RcmZXZG","jqWiyYJff92RjXuUQt9PQ","KihijM8OQvZ4pASkkhqzL","F9vyqvls3OBCujtukqKhy","k6jxm2b3edgkhbordpuz6v1","facc2b01-755a-409f-99f6-57bef2d1501f","bZumdyapJ2H0wWWOmJ45i","elqpgoe2r951si4xrhujppj","m5ov42Vm6mR7RQWTvl1NW","cw8cerc0cos8lh871oz8rtf","PZxxZ4iESzSlbbHJKxuAZ","UDu62Pa6BaRqlG8uGoMhy","u3zcndycwqessho5h6x0nz9","ZgCUp366YrF2Tyky2NT73","DVpVUmavSoVDA7UIlPzLX","nMCtMXVvjBsJk3iw1zoIO","ANfx9Z4a6ZA0uJuSHtbIJ","BkG557LKUYbH1DnxprEVT","1TaH8zDTM98FZ9SYaeXLM","02v7ymc144e5c4pv3edkud7","dc79799f-a55d-48ab-a8be-c48adb1b19c0","f6feef57-b8f5-451f-a09a-7b63f6a07183","4sz47Y0LKs1Si73rWtyyh","g7ulqi8no93ezeocbesc3ll","5a9fb1df-478e-4687-9be0-5cb97e61ec57","1374e9e9-1cbc-4e1f-b1ca-66b8569533dd","D8Z3rjXkSj2EOymXQXF4Z","np3c1ykvnjqv5xoombpfwqz","f529cc34-aba0-45ca-ad7e-02ddda318941","0zcauha3il2NqtxZazIo7","9bbd6f68-03b2-41f4-92e4-2ca313e8b450","5a2ab598-fa7e-4471-8bda-9f5831b679ae","uV6w4mZPoohWyZV4Xaad0","QHXEIyeZGIGMVi5Q52UWI","RgE0mZLaUjPftFPZsiAoe","mytCOts26Pidush65tdRW","fwUzxfLSPMH1eL8oBoLWx","wazdsda6h25x66edvfmeuiv","TbW7PM9bg1y5TGkiWwQ8b","xiSIDeEtIc8X0lpUQlppI","0jxgntiLNHWFuCbzqtFGF","GkdMprLUe4QQULBxmGN6V","4SYc6v5hxY5g6Ip6kjpwO","czi7ilt2i1uoqm1f2otbntj","fqVQpS9FBiXgKsZX3R3sJ","L5JUZlGAGvTxrsEBB7DY8","ttyri4pwfyn5lcx0vp9804o","TeTedoeS2LdHPR632eCpM","ulicRRwo3lSFzh3tMfWH9","p7d24vyb8m00ombzn34t50c","e65188a1-177e-4ab2-99f9-75f85d537621","fajYnbVhCRDi74xn0H30R","bArGdGwqo3iFkyKMyE7qR","4Dcp7gbEVoLLgfu7bXFai","5d700782-fb81-416e-83f9-5dd88260e350","zubgzhNFE6KlTgXcjTz6O","8lpdfWa0cbSq1XJQbcYcY","2psluywdc416t7vrql0m058","17I8ZksXvqCH1mRtZDjHp","kF916Ow84qpJJeMRkWMIo","tMLkLcrIHHBz56xVmBLkP","CSePBQ6q7qhowKESqVwt7","laZ4OfLhZNK1Kuy6GaWUr","z5IJblOknQhMzZ5QZh4ye","N36FHxfxzwJfxDY3miWyX","3bb25f58-2b50-4fa3-af55-48ea9f88a081","7x07qjgbitozdfszqzuy7ix","u93Rz4fEWGu6VBR30Zraf","9hjMHnKvYT4jLKvuDSXaV","qC5GxCZBmNb4Ip6c0kU8x","LgW7mTIALODoXc54B3p6S","UYPxfHBFWX7fb6hHU5bB6","74lYtC8NKpCzcyFOZTfR1","sEoBNAEuaTxwSmTXDonZt","dj0jr9mpvs62e2pkg3zc3yy","z8ie0xjogb6ht7gzowav5xr","qxSOd6SPN8qf9ZUojVFDX","WQoMTf6VXBaxCgksXAVsj","2Mw4XgfyCHXHNOX5yoCIY","qcdt2f7jo51muquo5r95dpo","0XJqmcdtcMZu66glBI5O8","lK8r8BXS4ThiUTe4xKIZe","93de42ca-53ea-460a-baa7-b9ec5c47cb1e","Rxs2jaGpdFzqYtP7lAJFJ","aO8W81Z0PyIb6Hs7nOHPW","lhzisalkebu4w5n01np07i0","7b5l4b6fi65n7sv9org5q1l","I01hENHnh8Tqu3Ok8sLzG","03o3n0hz9v9jtb7j889zpd4","evqsPNutOaZ8hcBCqxFQu","zhhxcjZUHdU8uRLwGb9Zh","G1aFACZB2ooWGMGwyd3ZW","YWy1C4tgoaCcw1m8JJsr7","Szj3o5iaNxPpesiCqwrbu","613syb18hb3v0u1ydvor7ru","XhvCDW3fIw6h6MhY5ticq","Q70g7SusFZBQXzkuQifv4","AzfWDH3wp7jFpL2EYxBcW","y0fwpZ9qMqirsLiFyOciU","ZF8xj8wwDUqKlrwTrCFZ1","tAJvhqhdfyZZa87QHq1TU","4hRmipi8lxpBLyzWu5JVB","iTg0C7QjvnmqBZeEigJNs","3babc3d2-79ae-470a-9c06-ab8bba2e684e","bF3UsMFya3fMeXWDspVov","ULkfbL9WpktbVYnzhl6Jw","PpNOO8JYWe6dM8wruSa7x","di40pCxDn7IiqE8lFdD46","rmW0mkerqV35I8QPji6lM","3IFIK1ByzeIxZCByryGLN","iImkYAKfkw3beAl6pLbDn","ecDe8DNWrkeQTwpTEvHje","i2it5id8qwtg27n4usg8bo1","Yqhdd9mSJGN7OJOeyoSD2","Ws5tah8tpeyn9tK8VBTg8","gWAg15uBJgkS2B0wcpMAa","l7V3v2ep1YdDCt7DOr7Ci","yM2PJBdqJnHpD63cPA6sW","qn0bre7eLbi3QMbCfWkUi","fSu0KxFL41IRouotqmbHs","0gtg24Mj1a1bQFPRGQNlO","7iQPBMltLPLbFEz2qbjPu","yoh4pwoXcfELInGKRdYf6","dCGCWXgAmiOZXbdULT1m6","jMavlje07sNa6hSEIE8WA","Xxm0JE4dKHxrQAaZfzvxD","nRb6Im4Kcmc2ZWE7K1jZ1","PAEBZCyFBZJyR7OoMZ41E","PQ6km8RgRCuyICBPOYz8f","x5tm1nfjzyawwzedy3yitgd","2bhftt8rGuxYu4pFgNqru","hjYIZpHQWuXfeEoGeJEKW","K2M9bQqVq2eQfm29eslKL","X8obW1iKwYsvNgKWGyCzy","yJwSC7hqYIezTFHf5i0Ev","c4Z7ETcOHUILRMH32Sfjw","qiR6dIu857b9M9kTqjOyK","fc2coz74cnfy5czzofx4h5x","Ku1OgHMhELajzo61Gx7ye","LUrfhDWo8wuwZu7CN9TV8","osu6JGOnvXJ5gt3tpqWZY","1a6173cd-cf13-4b34-a522-8350bf9a364f","S2sBltrPfd8a7ICuD7CuH","GLQ2pmkJUNUa93THBDVsD","md6xitz4exia2joa06i490b","oWCuBXOg6JWfZzjmKxmNl","jOmhZ8ovLYTPbpM1vqSDx","p9bov84s0isgkl1ysaw93kk","FraC6xzLy1ei91l1ICyc9","6ceBas2RE9Q4787GDngH7","734cd78d-0bc9-426b-803d-1efc84dfffe5","k4Bb09px6r0FxIRs49SXV","oaG3H1S9IUBO644nGZigu","Ka7agQJkUMRSWN0uFdkWK","si3z090WsiLasMhJBa1Az","hs6rwzt4mogiicoc4gcykbi","ljKAVERmdEiKLK9hXGKBm","zxt3lhonfdhglvijd17ua8c","dd7dopve1dudqkoibkqvti4","923tgifqf59ovv5yldtyi0a","vrjwp01goqw47fqctm4f4lo","c99gdmmppju3r1tth8cb2jx","z2pvn5qxdz84zgygqzxage8"],"parent":null,"data":{},"body":"This Dendron vault of tech knowledge is organized according to domains and their sub-domains, along with specific implementation of those domains.\n\nFor instance, Git itself is a domain. Sub-domains of Git would include things like `commit`,\n`tags`, `reflog` etc. implementations of each of those could be `cli`, `strat`\n(strategies), `inner`, and so on.\n\nThe goal of the wiki is to present data in a manner that is from the perspective\nof a querying user. Here, a user is a programmer wanting to get key information\nfrom a specific domain. For instance, if a user wants to use postgres functions\nand hasn't done them in a while, they should be able to query\n`postgres.functions` to see what information is available to them.\n\nThis wiki has been written with myself in mind. While learning each of these\ndomains, I have been sensitive to the \"aha\" moments and have noted down my\ninsights as they arose. I have refrained from capturing information that I\nconsidered obvious or otherwise non-beneficial to my own understanding.\n\nAs a result, I have allowed myself to use potentially arcane concepts to explain other ones. For example, in my note on [[unit testing|testing.method.unit]], I have made reference to the [[microservices|general.arch.microservice]] note. If these notes were made with the public in mind, this would be a very bad strategy, given that you'd have to understand microservices to be able to draw that same parallel that I've already drawn. Since these notes are written for myself, I have been fine with taking these liberties.\n\nWhat I hope to gain from this wiki is the ability to step away from any\ngiven domain for a long period of time, and be able to be passably useful for\nwhatever my goals are within a short period of time. Of course this is all\nvague sounding, and really depends on the domain along with the ends I am\ntrying to reach.\n\nTo achieve this, the system should be steadfast to:\n- be able to put information in relatively easily, without too much thought\n\trequired to its location. While location is important, Dendron makes it easy\n\tto relocate notes, if it becomes apparent that a different place makes more\n\tsense.\n- be able to extract the information that is needed, meaning there is a\n\thigh-degree in confidence in the location of the information. The idea is\n\tthat information loses a large amount of its value when it is unfindable.\n\tTherefore, a relatively strict ideology should be used when determining\n\twhere a piece of information belongs.\n\t- Some concepts might realistically belong to multiple domains. For instance, the concept of *access modifiers* can be found in both `C#` and `Typescript`. Therefore, this note should be abstracted to a common place, such as [[OOP|paradigm.oop]].\n\nThis Dendron vault is the sister component to the [General Second Brain](https://tech.kyletycholiz.com).\n\n### Tags\nThroughout the garden, I have made use of tags, which give semantic meaning to the pieces of information.\n\n- `ex.` - Denotes an *example* of the preceding piece of information\n- `spec:` - Specifies that the preceding information has some degree of *speculation* to it, and may not be 100% factual. Ideally this gets clarified over time as my understanding develops.\n- `anal:` - Denotes an *analogy* of the preceding information. Often I will attempt to link concepts to others that I have previously learned.\n- `mn:` - Denotes a *mnemonic*\n- `expl:` - Denotes an *explanation*\n\n## Resources\n### UE (Unexamined) Resources\nOften, I come across sources of information that I believe to be high-quality. They may be recommendations or found in some other way. No matter their origin, I may be in a position where I don't have the time to fully examine them (and properly extract notes), or I may not require the information at that moment in time. In cases like these, I will add reference to a section of the note called **UE Resources**. The idea is that in the future when I am ready to examine them, I have a list of resources that I can start with. This is an alternative strategy to compiling browser bookmarks, which I've found can quickly become untenable.\n\n### E (Examined) Resources\nOnce a resource has been thoroughly examined and has been mined for notes, it will be moved from *UE Resources* to *E Resources*. This is to indicate that (in my own estimation), there is nothing more to be gained from the resource that is not already in the note.\n\n### Resources\nThis heading is for inexhaustible resources. \n- A prime example would be a quality website that continually posts articles.  - Another example would be a tool, such as software that measures frequencies in a room to help acoustically treat it.\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.83.0","vaults":[{"fsPath":"../main/tech","name":"tech"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"The Tech Digital Garden of Kyle Tycholiz"},"github":{"cname":"tech.kyletycholiz.com","enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableMermaid":true,"siteUrl":"https://tech.kyletycholiz.com","duplicateNoteBehavior":{"action":"useVault","payload":["tech"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"8qm42jxtuar5qn15w7nphwy"},"buildId":"fo6gubzKP_KV9RZPhaoiQ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>